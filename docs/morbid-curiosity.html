<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Choosing to view morbid information involves reward circuitry | Towards prediction</title>
  <meta name="description" content="5 Choosing to view morbid information involves reward circuitry | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Choosing to view morbid information involves reward circuitry | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Choosing to view morbid information involves reward circuitry | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="aomic.html"/>
<link rel="next" href="hypothesis-kernel-analysis.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>1.1</b> Inference done differently</a></li>
<li class="chapter" data-level="1.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>1.2</b> Towards prediction</a></li>
<li class="chapter" data-level="1.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.2.7</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.3</b> Results</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.3.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.3.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
<li class="chapter" data-level="4.4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.4.6</b> Code availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="5.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>5.2.2</b> Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>5.2.3</b> Materials</a></li>
<li class="chapter" data-level="5.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>5.2.4</b> Procedure</a></li>
<li class="chapter" data-level="5.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>5.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="5.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>5.2.6</b> Imaging details</a></li>
<li class="chapter" data-level="5.2.7" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>5.2.7</b> Data availability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>5.3</b> Results</a><ul>
<li class="chapter" data-level="5.3.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>5.3.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="5.3.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>5.3.3</b> ROI analyses</a></li>
<li class="chapter" data-level="5.3.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>5.3.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>6</b> Explainable models of facial movements predict emotion perception behavior</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#the-prediction-explanation-exploration-framework"><i class="fa fa-check"></i><b>6.1.1</b> The prediction-explanation-exploration framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>6.2.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>6.2.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>6.2.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="6.2.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>6.2.4</b> Evaluated mappings</a></li>
<li class="chapter" data-level="6.2.5" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-dataset"><i class="fa fa-check"></i><b>6.2.5</b> Dataset used to evaluate mappings</a></li>
<li class="chapter" data-level="6.2.6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-code"><i class="fa fa-check"></i><b>6.2.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#prediction"><i class="fa fa-check"></i><b>6.3.1</b> Prediction</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#explanation"><i class="fa fa-check"></i><b>6.3.2</b> Explanation</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#exploration"><i class="fa fa-check"></i><b>6.3.3</b> Exploration</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>7</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="7.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>7.2.1</b> Participants</a></li>
<li class="chapter" data-level="7.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>7.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="7.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>7.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>7.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="7.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>7.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>7.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="7.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>7.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>7.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="7.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>7.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>7.4</b> Discussion</a><ul>
<li class="chapter" data-level="7.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>7.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="7.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>7.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="7.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>7.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="7.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>7.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>8</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a><ul>
<li class="chapter" data-level="E.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supplementary-methods"><i class="fa fa-check"></i><b>E.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="E.1.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-in-detail"><i class="fa fa-check"></i><b>E.1.1</b> Hypothesis kernel analysis (in detail)</a></li>
<li class="chapter" data-level="E.1.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-noise-ceiling-detail"><i class="fa fa-check"></i><b>E.1.2</b> Noise ceiling estimation (in detail)</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supp-fig"><i class="fa fa-check"></i><b>E.2</b> Supplementary figures</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="morbid-curiosity" class="section level1">
<h1><span class="header-section-number">5</span> Choosing to view morbid information involves reward circuitry</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Oosterwijk, S., Snoek, L., Tekoppele, J., Engelbert, L. H., &amp; Scholte, H. S. (2020). Choosing to view morbid information involves reward circuitry. <em>Scientific reports, 10</em>(1), 1-13.</p>

<p><p><strong>Abstract</strong></p>

People often seek out stories, videos or images that detail death, violence or harm. Considering the ubiquity of this behavior, it is surprising that we know very little about the neural circuits involved in choosing negative information. Using fMRI, the present study shows that choosing intensely negative stimuli engages similar brain regions as those that support extrinsic incentives and “regular” curiosity. Participants made choices to view negative and positive images, based on negative (e.g., a soldier kicks a civilian against his head) and positive (e.g., children throw flower petals at a wedding) verbal cues. We hypothesized that the conflicting, but relatively informative act of choosing to view a negative image, resulted in stronger activation of reward circuitry as opposed to the relatively uncomplicated act of choosing to view a positive stimulus. Indeed, as preregistered, we found that choosing negative cues was associated with activation of the striatum, inferior frontal gyrus, anterior insula, and anterior cingulate cortex, both when contrasting against a passive viewing condition, and when contrasting against positive cues. These findings nuance models of decision-making, valuation and curiosity, and are an important starting point when considering the value of seeking out negative content.
</p>
<div id="morbid-curiosity-introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>Humans are active agents who often deliberately expose themselves to “morbid” information (e.g., information involving death, violence or harm). People choose to watch gruesome documentaries, click on links detailing terrifying attacks and visit locations of horrible events. Surprisingly, the fact that people experience curiosity for negative information, and often act on this feeling, is rarely addressed in theoretical models of curiosity and decision-making. Moreover, empirical work on this phenomenon is markedly limited and studies investigating the neural circuits involved in choosing negative information are virtually non-existent. Nevertheless, “morbid curiosity” is an important topic for investigation, because this ubiquitous behavior appears to be at odds with the idea that value and reward drive human information seeking. The present paper aims to expand the scientific inquiry of curiosity and choice, by investigating how the brain, and reward-related brain regions in particular, represent a deliberate choice to view intensely negative images that portray death, violence or harm.</p>
<p>In the last decades, much progress has been made in understanding the neuroscience of choice, valuation and curiosity. Yet, when studying choice, decision-making scientists typically focus on extrinsically rewarding stimuli, such as monetary rewards <span class="citation">(Braver et al., <a href="bibliography.html#ref-braver2014mechanisms" role="doc-biblioref">2014</a>)</span>. Decisions regarding intrinsically rewarding stimuli are targeted less frequently <span class="citation">(Murayama, <a href="bibliography.html#ref-murayama2018psychological" role="doc-biblioref">2018</a>)</span> and even less is known about the neural representation of seeking negative information that, at first glance, does not seem to have reward value at all <span class="citation">(Elliot, <a href="bibliography.html#ref-elliot2006hierarchical" role="doc-biblioref">2006</a>)</span>. Similarly, in the field of curiosity — defined as an intrinsically motivated drive state for information <span class="citation">(Golman &amp; Loewenstein, <a href="bibliography.html#ref-golman2015curiosity" role="doc-biblioref">2015</a>; Gottlieb et al., <a href="bibliography.html#ref-gottlieb2013information" role="doc-biblioref">2013</a>; Kidd &amp; Hayden, <a href="bibliography.html#ref-kidd2015psychology" role="doc-biblioref">2015</a>)</span> — research on curiosity for negative information is scarce <span class="citation">(Murayama, <a href="bibliography.html#ref-murayama2018psychological" role="doc-biblioref">2018</a>; see for exceptions Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>; Hsee &amp; Ruan, <a href="bibliography.html#ref-hsee2016pandora" role="doc-biblioref">2016</a>; Rimé et al., <a href="bibliography.html#ref-rime2005brief" role="doc-biblioref">2005</a>; Zuckerman &amp; Litle, <a href="bibliography.html#ref-zuckerman1986personality" role="doc-biblioref">1986</a>)</span>. A handful of neuroscience studies have demonstrated that curiosity engages similar neural circuits as extrinsic reward, but only when examining positive or neutral material, such as trivia questions <span class="citation">(e.g., Gruber et al., <a href="bibliography.html#ref-gruber2014states" role="doc-biblioref">2014</a>; Kang et al., <a href="bibliography.html#ref-kang2009wick" role="doc-biblioref">2009</a>)</span>. At present, it is thus unclear whether curiosity for “morbid” information is supported by similar neural mechanisms as those that support extrinsic incentives and “regular” curiosity.</p>
<p>Neuroscientific evidence suggests that curiosity, choice and reward are supported by a highly similar constellation of brain regions4,13. Reward-related decision making, predominantly studied by focusing on monetary gains or losses, engages the dorsal striatum (caudate, putamen), ventral striatum (NAcc), orbitofrontal cortex (OFC), bilateral anterior insula, anterior cingulate cortex (ACC), dorsomedial prefrontal cortex/supplementary motor area (dmPFC/SMA) and frontal and parietal regions often associated with cognitive control14,15,16,17,18. Several studies targeting curiosity demonstrated similar neural regions. For example, Kang and colleagues12 found increased activation in the inferior frontal gyrus (IFG), caudate and putamen when inducing curiosity by presenting trivia questions. When curiosity was relieved (i.e., when the answer to the question was given) they found engagement of the putamen and IFG. In another study targeting trivia questions, Gruber and colleagues11 found increased activation in the dorsal and ventral striatum and IFG for questions associated with high curiosity ratings. Other work has shown that the induction and relief of curiosity engages regions that are associated with salience detection and uncertainty19,20, including the anterior insula and ACC21,22. In short, the limited work on curiosity so far, demonstrates that curiosity for relatively positive and neutral material engages neural regions that are also recruited during the computation of value and the anticipation of reward. Whether these regions also engage when people act on their curiosity for negatively valenced information is currently unknown.</p>
<p>Neuroscientific evidence suggests that curiosity, choice and reward are supported by a highly similar constellation of brain regions <span class="citation">(Kidd &amp; Hayden, <a href="bibliography.html#ref-kidd2015psychology" role="doc-biblioref">2015</a>; Sakaki et al., <a href="bibliography.html#ref-sakaki2018curiosity" role="doc-biblioref">2018</a>)</span>. Reward-related decision making, predominantly studied by focusing on monetary gains or losses, engages the dorsal striatum (caudate, putamen), ventral striatum (NAcc), orbitofrontal cortex (OFC), bilateral anterior insula, anterior cingulate cortex (ACC), dorsomedial prefrontal cortex/supplementary motor area (dmPFC/SMA) and frontal and parietal regions often associated with cognitive control <span class="citation">(Bartra et al., <a href="bibliography.html#ref-bartra2013valuation" role="doc-biblioref">2013</a>; Diekhof et al., <a href="bibliography.html#ref-diekhof2012role" role="doc-biblioref">2012</a>; Levy &amp; Glimcher, <a href="bibliography.html#ref-levy2012root" role="doc-biblioref">2012</a>; Liu et al., <a href="bibliography.html#ref-liu2011common" role="doc-biblioref">2011</a>; Samanez-Larkin &amp; Knutson, <a href="bibliography.html#ref-samanez2015decision" role="doc-biblioref">2015</a>)</span>. Several studies targeting curiosity demonstrated similar neural regions. For example, <span class="citation">Kang et al. (<a href="bibliography.html#ref-kang2009wick" role="doc-biblioref">2009</a>)</span> found increased activation in the inferior frontal gyrus (IFG), caudate and putamen when inducing curiosity by presenting trivia questions. When curiosity was relieved (i.e., when the answer to the question was given) they found engagement of the putamen and IFG. In another study targeting trivia questions, <span class="citation">Gruber et al. (<a href="bibliography.html#ref-gruber2014states" role="doc-biblioref">2014</a>)</span> found increased activation in the dorsal and ventral striatum and IFG for questions associated with high curiosity ratings. Other work has shown that the induction and relief of curiosity engages regions that are associated with salience detection and uncertainty <span class="citation">(Menon &amp; Uddin, <a href="bibliography.html#ref-menon2010saliency" role="doc-biblioref">2010</a>; Singer et al., <a href="bibliography.html#ref-singer2009common" role="doc-biblioref">2009</a>)</span>, including the anterior insula and ACC <span class="citation">(Jepma et al., <a href="bibliography.html#ref-jepma2012neural" role="doc-biblioref">2012</a>; Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>)</span>. In short, the limited work on curiosity so far, demonstrates that curiosity for relatively positive and neutral material engages neural regions that are also recruited during the computation of value and the anticipation of reward. Whether these regions also engage when people act on their curiosity for negatively valenced information is currently unknown.</p>
<p>In the present study, we tested the preregistered hypothesis that the striatum and IFG (ROI-analyses) and the anterior insula and ACC (whole-brain analyses) will engage more when people deliberately choose to view negative images, as compared to a passive viewing condition. This hypothesis reflects our assumption that “morbid curiosity”, expressed by a choice to view a negative stimulus, engages similar neural regions as regular curiosity <span class="citation">(Gruber et al., <a href="bibliography.html#ref-gruber2014states" role="doc-biblioref">2014</a>; Jepma et al., <a href="bibliography.html#ref-jepma2012neural" role="doc-biblioref">2012</a>; Kang et al., <a href="bibliography.html#ref-kang2009wick" role="doc-biblioref">2009</a>; Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>; see for an overview Gruber &amp; Ranganath, <a href="bibliography.html#ref-gruber2019curiosity" role="doc-biblioref">2019</a>)</span>. In addition, we tested a second hypothesis that the regions described above will engage more strongly when people choose to view a negative stimulus, as compared to a positive stimulus. This hypothesis is based on our assumption that the informational value of negative images is relatively high. Compared to positive information, negative information may be more novel, rare, deviant, uncertain, challenging or complex <span class="citation">(Baumeister et al., <a href="bibliography.html#ref-baumeister2001bad" role="doc-biblioref">2001</a>; Unkelbach et al., <a href="bibliography.html#ref-unkelbach2008positive" role="doc-biblioref">2008</a>)</span> — these information characteristics engage reward circuitry and evoke curiosity <span class="citation">(Berlyne, <a href="bibliography.html#ref-berlyne1966curiosity" role="doc-biblioref">1966</a>; Kashdan &amp; Silvia, <a href="bibliography.html#ref-kashdan2009curiosity" role="doc-biblioref">2009</a>; Kidd &amp; Hayden, <a href="bibliography.html#ref-kidd2015psychology" role="doc-biblioref">2015</a>; Sakaki et al., <a href="bibliography.html#ref-sakaki2018curiosity" role="doc-biblioref">2018</a>)</span>. Furthermore, a choice for negativity may involve a tradeoff between benefits (e.g., understanding something complex) and costs (e.g., being emotionally perturbed by a stimulus). Choosing a positive stimulus (e.g., viewing a family picnicking in the park) may not involve such costs, and may also have less benefits in terms of accessing novel, deviant or complex information. In this sense, choosing negativity (or “morbid curiosity”) is a conflict state; people want information, without predicting that they will like the information <span class="citation">(see also Rimé et al., <a href="bibliography.html#ref-rime2005brief" role="doc-biblioref">2005</a>; Litman, <a href="bibliography.html#ref-litman2005curiosity" role="doc-biblioref">2005</a>)</span>. Previous work suggests that reward circuitry engagement is most pronounced when actions or decisions are ambiguous or unclear <span class="citation">(Floresco, <a href="bibliography.html#ref-floresco2015nucleus" role="doc-biblioref">2015</a>)</span>. In line with this, we predict that the conflicting, but relatively informative act of choosing to view a negative image will paradoxically result in stronger activation of reward circuitry as opposed to the relatively uncomplicated act of choosing to view a positive stimulus.</p>
<p>We build upon previous work that demonstrates that people are interested in and fascinated by negative images <span class="citation">(Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2016neural" role="doc-biblioref">2016</a>; Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span>. The present study specifically targets choice for social negative images (i.e., displaying death, violence or harm within a social context), because we found in previous research that people prefer to view these stimuli over neutral alternatives and choose to view these stimuli more often than images of attacking animals and graphic, decontextualized mutilation <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span>. We used an established choice paradigm <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span> that presented participants with choices to view social images that depicted negative and positive situations, taken from validated affective picture databases. Importantly, this paradigm solely targets intrinsic motivation; participants were not financially rewarded for their choices. Moreover, this paradigm targets a behavioral expression of “wanting”, and not the extent to which people “like” the images <span class="citation">(Berridge et al., <a href="bibliography.html#ref-berridge2009dissecting" role="doc-biblioref">2009</a>; Litman, <a href="bibliography.html#ref-litman2005curiosity" role="doc-biblioref">2005</a>)</span>. The active-choice condition was compared to a passive-viewing condition, using a yoked procedure that has been previously used to study responses to controllable and incontrollable stressors <span class="citation">(Amat et al., <a href="bibliography.html#ref-amat2005medial" role="doc-biblioref">2005</a>; Wood et al., <a href="bibliography.html#ref-wood2015controllability" role="doc-biblioref">2015</a>)</span>. In the present study, a yoked design allowed us to investigate the effect of choice, while controlling for general affective, semantic and visual processing.</p>
<p>In the choice condition, people were presented with verbal cues, describing negative images (e.g., a soldier kicks a civilian against his head) and positive images (e.g., children throw flower petals at a wedding). The presentation of the verbal cue was labeled the induction phase (see Figure <a href="morbid-curiosity.html#fig:fig-morbid-curiosity-1">5.1</a>). Following the cue, participants chose whether they wanted to see the image corresponding to the description, or not. In the relief phase (see Figure <a href="morbid-curiosity.html#fig:fig-morbid-curiosity-1">5.1</a>), participants viewed the corresponding image when responding yes and a blurred version when responding no. The passive-viewing condition was fully yoked to the active-choice condition. In other words, each participant in the passive-viewing condition did not make choices, but was confronted with the choice profile of a participant in the active-choice condition. Importantly, this yoked design isolates the psychological process that we aim to investigate (i.e., a deliberate choice to view a stimulus), while keeping all other factors constant (i.e., cues and images). In line with curiosity theories that argue that exploratory behavior is an important component of curiosity <span class="citation">(Kashdan &amp; Silvia, <a href="bibliography.html#ref-kashdan2009curiosity" role="doc-biblioref">2009</a>; Litman, <a href="bibliography.html#ref-litman2005curiosity" role="doc-biblioref">2005</a>; Loewenstein, <a href="bibliography.html#ref-loewenstein1994psychology" role="doc-biblioref">1994</a>)</span>, we propose that the potential to choose will make participants’ subjective experience of curiosity more salient. In other words, participants’ subjective state of curiosity may be more at the fore-front of consciousness in the active-choice condition (because it will inform participants’ decisions) than in the passive-viewing condition.</p>
<div class="figure"><span id="fig:fig-morbid-curiosity-1"></span>
<img src="_bookdown_files/morbid-curiosity-files/figures/figure_1.png" alt="Overview of paradigm. (A) The setup of the trials in the choice-condition and passive-viewing condition. Note that in the active-choice condition, participants chose whether they wanted to see the image corresponding to the description during the yes/no response event. In the passive-viewing condition participants did not choose, but confirmed the decision seemingly determined by the computer during the yes/no response event. (B) An example of a negative description and the consequence of a yes response (either given by the participant, or determined by the computer). (C) An example of a positive description and the consequence of a no response (either given by the participant, or determined by the computer)."  />
<p class="caption">
Figure 5.1: Overview of paradigm. (<strong>A</strong>) The setup of the trials in the choice-condition and passive-viewing condition. Note that in the active-choice condition, participants chose whether they wanted to see the image corresponding to the description during the yes/no response event. In the passive-viewing condition participants did not choose, but confirmed the decision seemingly determined by the computer during the yes/no response event. (<strong>B</strong>) An example of a negative description and the consequence of a yes response (either given by the participant, or determined by the computer). (<strong>C</strong>) An example of a positive description and the consequence of a no response (either given by the participant, or determined by the computer).
</p>
</div>

<p>We selected positive and negative images from the International Affective Picture System database <span class="citation">(Lang et al., <a href="bibliography.html#ref-lang1997international" role="doc-biblioref">1997</a>)</span> and the Nencki Affective Picture System database <span class="citation">(Marchewka et al., <a href="bibliography.html#ref-marchewka2014nencki" role="doc-biblioref">2014</a>)</span>. Importantly, we matched negative and positive images in terms of valence extremity to ensure that, on average, positive images were perceived as equally positive as negative stimuli were perceived negative. A similar procedure was performed for the positive and negative descriptions (i.e., cues; see <a href="morbid-curiosity.html#morbid-curiosity-methods">Methods</a> section for further details).</p>
<p>Our scanning protocol was performed on a 3 T scanner. For all details on image acquisition, preprocessing and first- and second-level analyses, please see the <a href="morbid-curiosity.html#morbid-curiosity-methods">Methods</a> section. Our analysis protocol held confirmatory and exploratory analyses. Hypotheses and corresponding contrasts for the confirmatory analyses, exclusion criteria, ROIs and corrections for multiple comparisons were preregistered on the Open Science Framework, prior to data analysis <span class="citation">(<a href="osf.io/gdtk9" role="doc-biblioref">osf.io/gdtk9</a>; Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017prereg" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017prereg" role="doc-biblioref">b</a>)</span>.</p>
<p>The first-level model included six predictors to capture the research design: 2 (phase: induction phase vs. relief phase) × 2 (valence: negative vs. positive) × 2 (choice: yes, full image vs. no, blurred image). Additionally, we added a single predictor for the motor response associated with the decision/confirmation response and six motion predictors based on estimated motion correction parameters. First level contrasts only involved trials associated with yes choices (i.e., full image trials); trials associated with no responses (i.e., blurred image trials) were not used for further group level analysis.</p>
<p>In the ROI-based analyses, we focused on voxels within two a-priori defined ROIs: bilateral striatum and bilateral inferior frontal gyrus (IFG). The ROIs were based on the Harvard-Oxford Subcortical Atlas (striatum; caudate, putamen and nucleus accumbens) and the Harvard-Oxford Cortical Atlas (IFG; pars opercularis and pars triangularis) with a threshold for probabilistic ROIs &gt; 0 <span class="citation">(Craddock et al., <a href="bibliography.html#ref-craddock2012whole" role="doc-biblioref">2012</a>)</span>. In the group-level analyses targeting these two ROIs, we calculated two contrasts that reflected our confirmatory hypotheses, separately for the induction and relief phase. We hypothesized stronger activation in the ROIs when participants processed a negative cue/image in the active-choice condition as compared to that same event in the passive-viewing condition (i.e., <span class="math inline">\((\beta_{\mathrm{neg|active}} -\beta_{\mathrm{neg|passive}}) &gt; 0\)</span>). In addition, we hypothesized stronger activation in the ROIs when participants processed a negative cue/image in the active-choice condition as compared to a positive cue/image in the active-choice condition, controlling for passive viewing (i.e., <span class="math inline">\((\beta_{\mathrm{neg|active}} - \beta_{\mathrm{neg|active}}) - (\beta_{\mathrm{pos|active}} - \beta_{\mathrm{pos|passive}}) &gt; 0\)</span>). For these confirmatory ROI analyses, we used nonparametric permutation-based inference in combination with Threshold-Free Cluster Enhancement <span class="citation">(TFCE; Smith &amp; Nichols, <a href="bibliography.html#ref-smith2009threshold" role="doc-biblioref">2009</a>)</span> as implemented in <em>FSL randomise</em> <span class="citation">(Winkler et al., <a href="bibliography.html#ref-winkler2014permutation" role="doc-biblioref">2014</a>)</span> and thresholded voxelwise results at <span class="math inline">\(p &lt; 0.025\)</span> (correction for two ROIs). Note that this analysis allows for voxel-wise inference (i.e., no cluster-based correction is used). In addition to the confirmatory ROI analysis, we conducted an exploratory whole-brain group-level analysis. In addition to the two confirmatory contrasts mentioned in the previous section, we tested three exploratory contrasts, separately for the induction and relief phase with a voxel-wise <em>p</em>-value threshold of 0.005 and a cluster-wise <em>p</em>-value of 0.05). Full details regarding the exploratory analyses can be found in the <a href="morbid-curiosity.html#morbid-curiosity-methods">Methods</a> section.</p>
</div>
<div id="morbid-curiosity-methods" class="section level2">
<h2><span class="header-section-number">5.2</span> Methods</h2>
<div id="morbid-curiosity-methods-participants" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Participants</h3>
<p>Participants consisted of a convenience sample of students at the University of Amsterdam. The study was approved by the Ethics Review Board of the department of Psychology at the University of Amsterdam (2017-SP-7871) and performed in accordance with relevant institutional guidelines. The budget allowed for scanning of a maximum of 60 participants. After applying the exclusion criteria, the total sample consisted of 54 participants, including 38 women (<em>M<sub>age</sub></em> = 22.4, <em>SD</em> = 2.9) and 16 men (<em>M<sub>age</sub></em> = 23.8, <em>SD</em> = 1.8).</p>
</div>
<div id="morbid-curiosity-methods-design" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Design</h3>
<p>This study used a 2 (choice: active-choice vs. passive-viewing) × 2 (phase: induction vs. relief) × 2 (valence: negative vs. positive) mixed design. The variable choice was varied between participants and consisted of an active-choice condition and a passive-viewing condition. The variable phase was varied within participants and reflected the presentation of the cue (i.e., induction) vs. the presentation of the image (i.e., relief). The variable valence was varied within participants and reflected the negative vs. positive content of the cues/images.</p>
</div>
<div id="morbid-curiosity-methods-materials" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Materials</h3>
<div id="morbid-curiosity-methods-materials-experimental-task" class="section level4">
<h4><span class="header-section-number">5.2.3.1</span> Experimental task</h4>
<p>The present study utilized a choice task <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span> that presented participants with verbal cues describing negative and positive images, and offered them a choice to see these images or blurred versions. We used a yoked design that isolated the effects of choice, controlling for general affective, semantic and visual processing <span class="citation">(see also Wood et al., <a href="bibliography.html#ref-wood2015controllability" role="doc-biblioref">2015</a>; Amat et al., <a href="bibliography.html#ref-amat2005medial" role="doc-biblioref">2005</a>)</span>. This yoked design resulted in two conditions: the active-choice condition and the passive-viewing condition. Participant in the passive-viewing condition did not make choices, but were confronted with the choice profile of a yoked participant in the choice condition. Tasks in both conditions were programmed in Neurobs Presentation (<a href="https://www.neurobs.com/presentation">https://www.neurobs.com/presentation</a>). Behavioral data preprocessing was done using Python 3.5 and analyzed using IBM SPSS Statistics 22.0.</p>
<p>In the active-choice condition, participants were presented with 35 negative cues (e.g., rescue workers treat a wounded man; a soldier kicks a civilian against his head) and 35 positive cues (e.g., children throw flower petals at a wedding; partying people carry a crowd surfer) that described images, in random order. In each trial participants could choose, based on the cue, whether they wanted to view the corresponding image or not. The choice task consisted of a total of 70 trials. Each trial started with a fixation cross, presented for 500 ms, followed by the cue, presented for 3,000 ms. The presentation of the cue was labelled as the induction phase <span class="citation">(see also Jepma et al., <a href="bibliography.html#ref-jepma2012neural" role="doc-biblioref">2012</a>; Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>)</span>. The cue was followed by a jittered interval varying between 500 and 2000 ms. Subsequently, participants saw the words ‘yes’ and ‘no’ on the screen, and chose whether they wanted to see the image that was described by the cue, or not, by pressing one of two pre-specified buttons. Immediately following their response, the word ‘yes’ or ‘no’ turned green, indicating that their response was registered. Participants had a 2000 ms. time window to make their choice. If they had not made a choice after 2000 ms, the choice was automatically set to ‘no’. The response phase was followed by a jittered interval varying between 500 and 2000 ms. The interval was followed by the relief phase, in which the participants were presented with the image (1,024 × 768 pixels) when they chose ‘yes’. When participants chose not to see the corresponding image, they were presented with a blurred version of the image that was unrecognizably distorted (filter). Images were blurred with the software IrfanView (version 4.44; <a href="https://www.irfanview.com/">https://www.irfanview.com/</a>) using the fast Gaussian blur (filter = 150 pixels). Both the image and the blurred image were presented for 3,000 ms. The relief phase was followed by a jittered inter-trial-interval varying between 2000 and 4,000 ms. For a visual representation of the paradigm, please see Figure <a href="morbid-curiosity.html#fig:fig-morbid-curiosity-1">5.1</a>.</p>
<p>In the passive-viewing condition, participants were presented with the choice profile of a participant in the active-choice condition (i.e., the exact pattern of ‘yes’ and ‘no’ responses to the positive and negative cues for each participant in the active-choice condition was saved, and then re-used once as the computer generated pre-determined choice pattern for a participant in the passive-viewing condition). Participants were told in the introduction to the study that the computer would determine which images would be shown. The trial setup was identical to the active-choice condition, except for the following aspect. After participants were presented with the cue, the word ‘yes’ or ‘no’ turned green, indicating the choice of the computer. Participants were asked to confirm the choice made by the computer by pressing one of two pre-specified buttons, to mirror the motor response made in the active-choice condition.</p>
<p>The active-choice condition came with a filling problem: because participants could choose whether they wanted to view an image or not, some participants would see many more images than others. This filling problem can pose problems for modelling the BOLD response, due to lower efficiency in estimating contrasts for one subject over the other. Based on individual differences in choosing to view social negative information <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span>, we formulated an a-priori defined and preregistered eligibility criterion that only participants in the active-choice condition who chose negative and/or positive images in 40% or more of the trials (14/35 stimuli) would be paired with a subject in the passive-viewing condition. Based on this criterion five out of 33 participants in the active-choice condition were excluded from the sample. One other participant was excluded, because the functional scan was stopped prematurely. This resulted in 27 participants in the active-choice condition. The choice profiles of these 27 participants were yoked with 27 participants in the passive-viewing condition.</p>
</div>
<div id="morbid-curiosity-methods-materials-cues" class="section level4">
<h4><span class="header-section-number">5.2.3.2</span> Cues</h4>
<p>Cues were written to describe positive and negative images in one sentence. In a pilot study the cues were rated on valence (0 = negative to 100 = positive), and arousal (0 = low arousal to 100 = high arousal). Negative cues were rated more negatively than positive cues (<em>M</em> = 20.69, <em>SD</em> = 8.42 vs. <em>M</em> = 77.99, <em>SD</em> = 4.49), <em>t</em>(68) = −35.51, <em>p</em> &lt; 0.001, and more arousing than positive cues (<em>M</em> = 68.45, <em>SD</em> = 6.38 vs. <em>M</em> = 28.93, <em>SD</em> = 5.99), <em>t</em>(68) = 26.71, <em>p</em> &lt; 0.001. Negative and positive cues were matched in terms of valence extremity. An analysis of mean-centered valence scores demonstrated that, on average, positive cues were perceived as equally positive (<em>M</em> = 29.31, <em>SD</em> = 8.43) as negative stimuli were perceived as negative (<em>M</em> = 27.99, <em>SD</em> = 4.49), <em>t</em>(68) = 0.82, <em>p</em> = 0.417.</p>
</div>
<div id="morbid-curiosity-methods-materials-images" class="section level4">
<h4><span class="header-section-number">5.2.3.3</span> Images</h4>
<p>Images were selected from the International Affective Picture System <span class="citation">(IAPS; Lang et al., <a href="bibliography.html#ref-lang1997international" role="doc-biblioref">1997</a>)</span> and the Nencki Affective Picture System <span class="citation">(NAPS; Marchewka et al., <a href="bibliography.html#ref-marchewka2014nencki" role="doc-biblioref">2014</a>)</span>; image codes are presented in the <a href="morbid-curiosity-supplement.html#morbid-curiosity-supplement">Supplementary Materials</a> (Table <a href="morbid-curiosity-supplement.html#tab:tab-morbid-curiosity-S4">D.4</a>). We selected negative images that portrayed situations of interpersonal violence, or social scenes involving a dead body or a harmed person. Negative images were selected when they had a valence rating below 4 (on a scale from 1 = negative to 9 = positive) and an arousal rating above 4.5 (on a scale from 1 = not arousing to 9 = extremely arousing). We selected positive images that portrayed joyful, loving or exciting interpersonal interactions. Positive images were selected when they had a valence rating above 6 (on a scale from 1 = negative to 9 = positive) and an arousal rating above 3 (on a scale from 1 = not arousing to 9 = extremely arousing). Negative and positive images differed significantly in terms of valence (<em>M</em> = 2.58, <em>SD</em> = 0.53 vs. <em>M</em> = 7.43, <em>SD</em> = 0.36), <em>t</em>(68) = −44.88, <em>p</em> &lt; 0.001, and arousal (<em>M</em> = 6.18, <em>SD</em> = 0.75 vs. <em>M</em> = 4.78, <em>SD</em> = 0.90), <em>t</em>(68) = 7.07, <em>p</em> &lt; 0.001. Negative and positive images were matched in terms of valence extremity. An analysis of mean-centered valence scores demonstrated that, on average, positive images were perceived as equally positive (<em>M</em> = 2.42, <em>SD</em> = 0.53) as negative stimuli were perceived as negative (<em>M</em> = 2.44, <em>SD</em> = 0.37), <em>t</em>(68) = − 0.22, <em>p</em> = 0.824.</p>
</div>
<div id="morbid-curiosity-methods-materials-questionnaires" class="section level4">
<h4><span class="header-section-number">5.2.3.4</span> Questionnaires</h4>
<p>After the scanning session was completed, participants filled in the ‘Morbid curiosity in daily-life’ questionnaire <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span> and the Dutch version of the Interpersonal Reactivity Index <span class="citation">(De Corte et al., <a href="bibliography.html#ref-de2007measuring" role="doc-biblioref">2007</a>)</span>. A short exit questionnaire asked participants two questions regarding the task they performed in the scanner. Participants in the active-choice condition were asked to rate to what extent they followed their curiosity when making choices for negative cues, and when making choices for positive cues, on a 1 (not at all) to 7 (very much) point scale. Participants in the passive-viewing condition were asked to rate to what extent they were curious about the negative cues, and the positive cues, on a 1 (not at all) to 7 (very much) point scale. The exit questionnaire concluded with demographic questions.</p>
</div>
</div>
<div id="morbid-curiosity-methods-procedure" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Procedure</h3>
<p>After signing the informed consent form, each participant received a thorough instruction. The active-choice condition was introduced as a study on how the brain represents choice. Participants were explained how they could make their choice, that they would always see the image of their choice, and that there were no right or wrong answers. Furthermore, participants were presented with an example of a negative and a positive cue, combined with the corresponding full image and blurred image, so that they knew what to expect when choosing the yes or no option. No mention was made of curiosity in the instruction. The passive-viewing condition was introduced as a study on the brain processes involved in reading image descriptions and viewing images. Participants were explained that the computer determined whether a description would be followed by a corresponding image. As in the active-choice condition, participants were presented with an example of a negative and a positive cue, combined with the corresponding full image and blurred image, so that they knew what to expect when the computer determined the yes or no option.</p>
<p>When comfortable and instructed, a structural T1-weighted anatomical scan was made. Then the participant performed the choice task or the passive task during fMRI acquisition in the scanner. After the scanning session, the participant filled in the questionnaires and received a thorough debriefing.</p>
</div>
<div id="morbid-curiosity-methods-behavioral-analysis" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Behavioral analysis</h3>
<p>In the active-choice condition, we compared the extent to which participants followed their curiosity when making choices for negative cues and positive cues. In the passive-viewing condition, we compared the extent to which participants were curious about negative cues and positive cues. For the active-choice condition, a Kolmogorov–Smirnov test indicated a normality violation (active-choice: skewness = 0.640, kurtosis = − 0.375, <em>D</em>(25) = 0.197, <em>p</em> = 0.013; passive-viewing: skewness = − 0.399, kurtosis = − 0.055, <em>D</em>(27) = 0.161, <em>p</em> = 0.071). We report two paired sample <em>t</em>-tests (two-tailed) to analyze the difference between cues, but results were fully corroborated with non-parametric Wilcoxon signed-rank tests. Effect sizes (Cohen’s <em>d<sub>z</sub></em>) were calculated using Lakens’ <span class="citation">(Lakens, <a href="bibliography.html#ref-lakens2013calculating" role="doc-biblioref">2013</a>)</span> spreadsheet.</p>
</div>
<div id="morbid-curiosity-methods-imaging-details" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Imaging details</h3>
<div id="morbid-curiosity-methods-imaging-details-image-acquisition" class="section level4">
<h4><span class="header-section-number">5.2.6.1</span> Image acquisition</h4>
<p>Participants were tested using a Philips Achieva 3T MRI scanner and a 32-channel SENSE headcoil. A survey scan was made for spatial planning of the subsequent scans. Following the survey scan, a 3-min structural T1-weighted scan was acquired using 3D fast field echo (TR: 82 ms, TE: 38 ms, flip angle: 8°, FOV: 240 × 188 mm, in-plane resolution 240 × 188, 220 slices acquired using single-shot ascending slice order and a voxel size of 1.0 × 1.0 × 1.0 mm). After the T1-weighted scan, functional T2*-weighted sequences were acquired using single shot gradient echo, echo planar imaging (TR = 2000 ms, TE = 27.63 ms, flip angle: 76.1°, FOV: 240 × 240 mm, in-plane resolution 64 × 64, 37 slices with ascending acquisition, slice thickness 3 mm, slice gap 0.3 mm, voxel size 3 × 3 × 3 mm), covering the entire brain. For the functional run, 495 volumes were acquired. After the functional run, a “B0” fieldmap scan (based on the phase difference between two consecutive echos) was acquired using 3D fast field echo (TR: 11 ms, TE: 3 ms and 8 ms, flip angle: 8°, FOV: 256 × 208, in-plane resolution 128 × 104, 128 slices).</p>
</div>
<div id="morbid-curiosity-methods-imaging-details-preprocessing" class="section level4">
<h4><span class="header-section-number">5.2.6.2</span> Preprocessing</h4>
<p>Results included in this manuscript come from preprocessing performed using <em>FMRIPREP</em> version 1.0.0 <span class="citation">(O. Esteban, Markiewicz, Blair, Moodie, Isik, Erramuzpe, Kent, Goncalves, DuPre, Snyder, &amp; others, <a href="bibliography.html#ref-esteban2019fmriprep" role="doc-biblioref">2019</a>; O. Esteban, Blair, et al., <a href="bibliography.html#ref-esteban_oscar_2017_1095198" role="doc-biblioref">2017</a>)</span>, a <em>Nipype</em> <span class="citation">(Gorgolewski et al., <a href="bibliography.html#ref-Gorgolewski2011-aa" role="doc-biblioref">2011</a>, <a href="bibliography.html#ref-gorgolewski_krzysztof_j_2017_581704" role="doc-biblioref">2017</a>)</span> based tool. Each T1 weighted volume was corrected for bias field using <em>N4BiasFieldCorrection</em> v2.1.0 <span class="citation">(Tustison et al., <a href="bibliography.html#ref-Tustison2010-tk" role="doc-biblioref">2010</a>)</span> and skullstripped using <em>antsBrainExtraction.sh</em> v2.1.0 (using OASIS template). Cortical surface was estimated using <em>FreeSurfer</em> v6.0.0 <span class="citation">(Dale et al., <a href="bibliography.html#ref-Dale1999-rk" role="doc-biblioref">1999</a>)</span>. The skullstripped T1w volume was segmented <span class="citation">(using <em>FSL FAST</em>; Zhang et al., <a href="bibliography.html#ref-Zhang2001-wa" role="doc-biblioref">2001</a>)</span> and coregistered to the skullstripped ICBM 152 Nonlinear Asymmetrical template version 2009c <span class="citation">(Fonov et al., <a href="bibliography.html#ref-Fonov2009-sr" role="doc-biblioref">2009</a>)</span> using nonlinear transformation implemented in <em>ANTs</em> v2.1.0 <span class="citation">(Avants et al., <a href="bibliography.html#ref-Avants2008-bv" role="doc-biblioref">2008</a>)</span>.</p>
<p>Functional data was motion corrected using <em>MCFLIRT</em> v5.0.9 <span class="citation">(Jenkinson et al., <a href="bibliography.html#ref-Jenkinson2002-wm" role="doc-biblioref">2002</a>)</span>. Distortion correction was performed using phase-difference fieldmaps processed with <em>FSL FUGUE</em> <span class="citation">(Jenkinson, <a href="bibliography.html#ref-jenkinson2003fast" role="doc-biblioref">2003</a>)</span>. This was followed by co-registration to the corresponding T1w using boundary-based registration <span class="citation">(Greve &amp; Fischl, <a href="bibliography.html#ref-Greve2009-da" role="doc-biblioref">2009</a>)</span> with 9 degrees of freedom, using <em>bbregister</em> (<em>FreeSurfer</em> v6.0.0). Motion correcting transformations, field distortion correcting warp, BOLD-to-T1w transformation and T1w-to-template (MNI) warp were concatenated and applied in a single step using <em>antsApplyTransforms</em> (<em>ANTs</em> v2.1.0) using Lanczos interpolation.</p>
<p>Many internal operations of <em>FMRIPREP</em> use <em>nilearn</em> <span class="citation">(Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>)</span>, principally within the BOLD-processing workflow. For more details of the pipeline see <a href="https://fmriprep.readthedocs.io/en/1.0.0/workflows.html">https://fmriprep.readthedocs.io/en/1.0.0/workflows.html</a>.</p>
</div>
<div id="morbid-curiosity-methods-imaging-first-level-analysis" class="section level4">
<h4><span class="header-section-number">5.2.6.3</span> First-level analysis</h4>
<p>We modeled the participants’ preprocessed time series in a “first-level” GLM using <em>FSL FEAT</em> <span class="citation">(Woolrich et al., <a href="bibliography.html#ref-woolrich2001temporal" role="doc-biblioref">2001</a>)</span>. The first-level modeling procedure was exactly the same for the participants in the active choice and passive viewing condition. As predictors, we included regressors for both the induction phase (i.e., the written description) and the relief phase (i.e., the full image). We separated trials with positive descriptions/images from trials with negative descriptions/images and separated trials in which participants saw the full version of the image from trials in which they saw a blurred version of the image. Note that in the active choice condition participants chose to see the full or blurred image, whereas in the passive viewing condition it was predetermined whether participants saw the full or blurred image. The final model held six predictors: 2 (phase: induction vs. relief) × 2 (valence: negative vs. positive) × 2 (seen: full image vs. blurred image). If participants did not have any blurred image trials, the associated predictors were left out. Additionally, we added a single predictor for the actual decision (i.e., modelled at the onset the button press) and six motion predictors based on the estimated motion correction parameters.</p>
<p>Before model estimation, we applied a high-pass filter (<span class="math inline">\(\sigma\)</span> = 50 s) and spatially smoothed the data (FWHM = 5 mm.). Standard prewhitening, as implemented in <em>FSL</em>, was applied. First-level contrasts only involved predictors associated with full image trials; that is, predictors associated with blurred image trials were not used for further analysis. For the remaining four predictors of interest — 2 (phase | full image) × 2 (valence | full image) — we defined contrasts against baseline, i.e.,<span class="math inline">\(\beta_{\mathrm{predictor}} \neq 0\)</span> and valence contrasts, i.e., <span class="math inline">\((\beta_{\mathrm{neg | induction}} - \beta_{\mathrm{pos | induction}}) \neq 0\)</span> and <span class="math inline">\((\beta_{\mathrm{neg | relief}} - \beta_{\mathrm{pos | relief}}) \neq 0\)</span>. The results (images with parameter and variance estimates) were subsequently registered to <em>FSL</em>’s default template (“MNI152NLin6Asym”) using a translation-only (3 parameter) affine transform using FSL Flirt (which is part of FSL FEAT) for group analysis.</p>
</div>
<div id="morbid-curiosity-methods-imaging-roi-analysis" class="section level4">
<h4><span class="header-section-number">5.2.6.4</span> ROI-based group analysis</h4>
<p>We tested two confirmatory hypotheses in this ROI-based group analysis, separately for the induction and relief phase:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\((\beta_{\mathrm{neg | active}} - \beta_{\mathrm{neg | passive}}) &gt; 0\)</span></li>
<li><span class="math inline">\((\beta_{\mathrm{neg | active}} - \beta_{\mathrm{neg | passive}}) - (\beta_{\mathrm{pos | active}} - \beta_{\mathrm{pos | passive}}) &gt; 0\)</span></li>
</ol>
<p>Note that the parameters (e.g., <span class="math inline">\(\beta_{\mathrm{neg | active}}\)</span>) reflect the average of the first-level parameters (e.g., <span class="math inline">\(\beta_{\mathrm{neg}}\)</span>) for a particular condition (e.g., active choice). As such, we tested four different group-level contrasts — 2 (phase) × 2 (hypothesis) — across two ROIs (striatum and IFG) in our group-level model.</p>
<p>For these confirmatory ROI-based group analyses, we used nonparametric permutation-based inference in combination with Threshold-Free Cluster Enhancement <span class="citation">(TFCE; Smith &amp; Nichols, <a href="bibliography.html#ref-smith2009threshold" role="doc-biblioref">2009</a>)</span> as implemented in <em>FSL randomise</em> <span class="citation">(Winkler et al., <a href="bibliography.html#ref-winkler2014permutation" role="doc-biblioref">2014</a>)</span>. We ran <em>randomise</em> with 5,000 permutations, corrected for multiple comparisons using the maximum statistic method (the method’s default multiple comparison correction procedure), and thresholded voxelwise results at <em>p</em> &lt; 0.025 (correction for two ROIs). Note that this analysis allows for voxel-wise inference (i.e., no cluster-based correction is used).</p>
<p>In these ROI-based analyses, we restricted the analysis to voxels within two a-priori specified ROIs: bilateral striatum and bilateral inferior frontal gyrus (IFG). The ROIs are based on the Harvard–Oxford Subcortical Atlas (striatum; caudate, putamen and nucleus accumbens) and the Harvard–Oxford Cortical Atlas (IFG; pars opercularis and pars triangularis) with a threshold for probabilistic ROIs &gt; 0 <span class="citation">(Craddock et al., <a href="bibliography.html#ref-craddock2012whole" role="doc-biblioref">2012</a>)</span>.</p>
<p>Whole-brain group analysis. In addition to the confirmatory ROI-based analysis, we conducted an exploratory whole-brain group-analysis. Besides the two hypotheses mentioned in the previous section, we tested the following hypotheses, again for both the induction and relief phase:</p>
<ol start="3" style="list-style-type: decimal">
<li><span class="math inline">\((\beta_{\mathrm{pos | active}} - \beta_{\mathrm{pos | passive}}) &gt; 0\)</span></li>
<li><span class="math inline">\((\beta_{\mathrm{pos | active}} - \beta_{\mathrm{pos | passive}}) - (\beta_{\mathrm{neg | active}} - \beta_{\mathrm{neg | passive}}) &gt; 0\)</span></li>
<li><span class="math inline">\((\beta_{\mathrm{neg | active}} - \beta_{\mathrm{neg | passive}}) \cap (\beta_{\mathrm{pos | active}} - \beta_{\mathrm{pos | passive}})\)</span></li>
</ol>
<p>The <span class="math inline">\(\cap\)</span> symbol in hypothesis 5 represents a conjunction analysis between two contrasts. For these exploratory whole-brain group analyses, we used <em>FSL FEAT</em> <span class="citation">(Woolrich et al., <a href="bibliography.html#ref-woolrich2004multilevel" role="doc-biblioref">2004</a>)</span> with a <em>FLAME1</em> mixed-effects model and automatic outlier detection <span class="citation">(Woolrich, <a href="bibliography.html#ref-woolrich2008robust" role="doc-biblioref">2008</a>)</span>. Resulting brain maps were thresholded with cluster-based correction <span class="citation">(Worsley, <a href="bibliography.html#ref-worsley2001statistical" role="doc-biblioref">2001</a>)</span> using an initial (one-tailed) voxel-wise <em>p</em>-value cutoff of 0.005 (corresponding to a <em>z</em>-value above 2.576) and a cluster-wise significance level of 0.05. For the conjunction analysis (hypothesis 5), we used the minimum statistic approach <span class="citation">(Nichols et al., <a href="bibliography.html#ref-nichols2005valid" role="doc-biblioref">2005</a>)</span> in combination with cluster-based correction using the same cutoff and significance value as for the other two (non-conjunction based) hypotheses.</p>
</div>
<div id="morbid-curiosity-methods-imaging-further-exploratory-analyses" class="section level4">
<h4><span class="header-section-number">5.2.6.5</span> Further exploratory analyses</h4>
<p>To aid interpretation of the results, we “decoded” the brain maps resulting from the whole-brain analysis using Neurosynth <span class="citation">(Yarkoni et al., <a href="bibliography.html#ref-yarkoni2011large" role="doc-biblioref">2011</a>)</span>. In Supplementary Table <a href="morbid-curiosity-supplement.html#tab:tab-morbid-curiosity-S1">D.1</a>, we list the ten Neurosynth terms (excluding anatomical terms) with the highest overall spatial correlation with our unthresholded brain maps (which are available on Neurovault, see <a href="morbid-curiosity.html#morbid-curiosity-data-availability">Data availability</a>).</p>
</div>
</div>
<div id="morbid-curiosity-data-availability" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Data availability</h3>
<p>All code used to preprocess, analyze, and plot the data is available from the project’s Github repository: <a href="https://github.com/lukassnoek/MorbidCuriosityFMRI">https://github.com/lukassnoek/MorbidCuriosityFMRI</a>. Much of this study’s code involves functionality from the <em>nilearn</em> Python package for neuroimaging analysis and visualization <span class="citation">(<a href="https://nilearn.github.io/" role="doc-biblioref">https://nilearn.github.io/</a> Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>)</span>. Unthresholded whole-brain group-level statistics maps are available from Neurovault <span class="citation">(Gorgolewski, Varoquaux, Rivera, Schwarz, Ghosh, Maumet, Sochat, Nichols, Poldrack, Poline, &amp; others, <a href="bibliography.html#ref-gorgolewski2015neurovault" role="doc-biblioref">2015</a>)</span>: <a href="https://identifiers.org/neurovault.collection:5591">https://identifiers.org/neurovault.collection:5591</a>. This repository contains whole-brain <em>z</em>-value maps for all possible contrasts across phases (induction vs. relief, referred to as “cue” and “stim”), valence (positive vs. negative, referred to as “pos” and “neg”), and group (active vs. passive, referred to as “act” vs. “pas”). For example, on Neurovault, the map associated with the <span class="math inline">\((\beta_{\mathrm{neg | active}} - \beta_{\mathrm{neg | passive}}) - (\beta_{\mathrm{pos | active}} - \beta_{\mathrm{pos | passive}}) &gt; 0\)</span> contrast in the induction phase is named “cue-posneg_contrast-act-pas”.</p>
</div>
</div>
<div id="morbid-curiosity-results" class="section level2">
<h2><span class="header-section-number">5.3</span> Results</h2>
<div id="morbid-curiosity-results-participants" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Participants</h3>
<p>In total, sixty participants signed informed consent and underwent our scanning protocol. We implemented a preregistered eligibility criterion that only participants who chose negative and/or positive images in 40% or more of the trials would be paired with a participant in the passive-viewing condition. Neuroimaging analyses were performed on a sample of 54 participants (38 women; <em>M<sub>age</sub></em> = 22.4, <em>SD<sub>age</sub></em> = 2.9); with 27 participants in the active-choice condition and 27 participants in the passive-viewing condition.</p>
</div>
<div id="behavior-and-subjective-report" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Behavior and subjective report</h3>
<p>In the active-choice condition, participants chose to view the negative image in 80.6% of the trials; participants chose to view the positive image in 94.8% of the trials. In the active-choice condition, participants reported that they followed their curiosity more when making choices for negative cues (<em>M</em> = 6.20, <em>SD</em> = 0.71) as compared to positive cues (<em>M</em> = 4.72, <em>SD</em> = 1.82), <em>t</em>(24) = 3.49, <em>p</em> = 0.002, 95% CI [0.60, 2.36], <em>d<sub>z</sub></em> = 0.70. Similarly, participants expressed more curiosity for negative cues (<em>M</em> = 5.41, <em>SD</em> = 1.28) than for positive cues (<em>M</em> = 4.44, <em>SD</em> = 1.60), <em>t</em>(26) = 2.42, <em>p</em> = 0.023, 95% CI [0.15, 1.78], <em>d<sub>z</sub></em> = 0.47, in the passive-viewing condition. This finding is consistent with previous results that people find negative social information generally more interesting than positive social information <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>)</span>.</p>
</div>
<div id="roi-analyses" class="section level3">
<h3><span class="header-section-number">5.3.3</span> ROI analyses</h3>
<p>Our first set of hypotheses focused on contrasting neural activity when participants processed a negative cue in the active-choice condition with that same event in the passive-viewing condition (i.e., negative<sub>active&gt;passive</sub>). As predicted, a confirmatory ROI analysis demonstrated more activation in the striatum when participants viewed a negative cue that was chosen (in the active-choice condition) as compared to watching that same negative cue in the passive-viewing condition. Figure <a href="morbid-curiosity.html#fig:fig-morbid-curiosity-2">5.2</a> shows that this contrast produced significant voxels across the striatum, in the caudate, putamen and NAcc. The ROI analysis targeting the IFG also demonstrated stronger activation in the active-choice condition as compared to the passive-viewing condition, further confirming our hypotheses.</p>
<div class="figure"><span id="fig:fig-morbid-curiosity-2"></span>
<img src="_bookdown_files/morbid-curiosity-files/figures/figure_2.png" alt="Results of confirmatory ROI analyses for the induction phase (A) the contrast negativeactive&gt;passive (B) the contrast negativeactive&gt;passive &gt; positiveactive&gt;passive. Voxels in red/yellow represent significant t-values (p &lt; 0.05, corrected for multiple comparisons using the maximum statistic approach). The colored outlines represent the different brain regions within the probabilistic ROIs for the striatum (left) and inferior frontal gyrus (IFG; right). The outlines represent the border of the ROIs thresholded at 0. When voxels within one ROI had a nonzero probability in more than one brain region (e.g., the caudate and nucleus accumbens), the voxel was assigned to the brain region with the largest probability."  />
<p class="caption">
Figure 5.2: Results of confirmatory ROI analyses for the induction phase (<strong>A</strong>) the contrast negative<sub>active&gt;passive</sub> (<strong>B</strong>) the contrast negative<sub>active&gt;passive</sub> &gt; positive<sub>active&gt;passive</sub>. Voxels in red/yellow represent significant <em>t</em>-values (<em>p</em> &lt; 0.05, corrected for multiple comparisons using the maximum statistic approach). The colored outlines represent the different brain regions within the probabilistic ROIs for the striatum (left) and inferior frontal gyrus (IFG; right). The outlines represent the border of the ROIs thresholded at 0. When voxels within one ROI had a nonzero probability in more than one brain region (e.g., the caudate and nucleus accumbens), the voxel was assigned to the brain region with the largest probability.
</p>
</div>

<p>Our second set of hypotheses focused on comparing a choice for negative information with a choice for positive information, controlling for general semantic, affective and visual processing (i.e., negative<sub>active&gt;passive</sub> &gt; positive<sub>active&gt;passive</sub>). As predicted, a confirmatory ROI analysis demonstrated more activation in the striatum when participants viewed a negative cue that was chosen as compared to a positive cue that was chosen, relative to watching that same negative or positive cue in the passive-viewing condition. Again, significant voxels were found across the striatum, in the caudate, putamen and NAcc. A similar effect was found in the IFG (see Figure <a href="morbid-curiosity.html#fig:fig-morbid-curiosity-2">5.2</a>). To explore the directionality of the effects, we extracted the parameter weights (<span class="math inline">\(\hat{\beta}\)</span>) for the individual regressors for both ROIs. A visual inspection of plotted weights suggest that the patterns of neural activation reported for the striatum, are driven both by activation in the striatum when viewing negative cues in the active-choice condition, and deactivation in the striatum when viewing negative cues in the passive-viewing condition. Further details can be found in Figure <a href="morbid-curiosity-supplement.html#fig:fig-morbid-curiosity-S1">D.1</a> of the <a href="morbid-curiosity-supplement.html#morbid-curiosity-supplement">Supplementary Materials</a>.</p>
</div>
<div id="whole-brain-analyses" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Whole-brain analyses</h3>
<p>In addition to the confirmatory analyses reported above, we performed a whole-brain analysis (cluster-corrected with a voxel-wise threshold of <em>p</em> &lt; 0.005 and a cluster-wise threshold of <em>p</em> &lt; 0.05) for the two confirmatory contrasts (see Figure <a href="morbid-curiosity.html#fig:fig-morbid-curiosity-3">5.3</a>). In addition to activation in the regions targeted in the confirmatory ROI analyses, the whole-brain analyses for the negative<sub>active&gt;passive</sub> contrast demonstrated robust activation in the ACC, paracingulate gyrus, superior frontal gyrus, middle frontal gyrus, OFC, insular cortex, frontal operculum, frontal pole, temporal pole, thalamus and brain stem, when participants viewed a negative cue that was chosen (in the active-choice condition) as compared to watching that same negative cue in the passive-viewing condition. A complete table of the significant clusters can be found in the <a href="morbid-curiosity-supplement.html#morbid-curiosity-supplement">Supplementary Materials</a> (Table <a href="morbid-curiosity-supplement.html#tab:tab-morbid-curiosity-S2">D.2</a>, as well as the significant clusters associated with the positive<sub>active&gt;passive</sub> contrast, Table <a href="morbid-curiosity-supplement.html#tab:tab-morbid-curiosity-S3">D.3</a>). The whole-brain results for the negative<sub>active&gt;passive</sub> &gt; positive<sub>active&gt;passive</sub> contrast are presented in Table <a href="morbid-curiosity.html#tab:tab-morbid-curiosity-1">5.1</a>. This contrast demonstrated stronger activation in the ACC, paracingulate gyrus, superior frontal gyrus, OFC, insular cortex and frontal operculum, when participants viewed a negative cue that was chosen as compared to a positive cue that was chosen (relative to watching that same negative or positive cue in the passive-viewing condition).</p>
<div class="figure"><span id="fig:fig-morbid-curiosity-3"></span>
<img src="_bookdown_files/morbid-curiosity-files/figures/figure_3.png" alt="Results of the exploratory whole-brain analyses for the induction phase: (A) the contrast negativeactive&gt;passive (red), positiveactive&gt;passive (blue), and their conjunction (yellow); (B) the contrast negativeactive&gt;passive &gt; positiveactive&gt;passive; (C) the contrast positiveactive&gt;passive &gt; negativeactive&gt;passive (empty)."  />
<p class="caption">
Figure 5.3: Results of the exploratory whole-brain analyses for the induction phase: (<strong>A</strong>) the contrast negative<sub>active&gt;passive</sub> (red), positive<sub>active&gt;passive</sub> (blue), and their conjunction (yellow); (<strong>B</strong>) the contrast negative<sub>active&gt;passive</sub> &gt; positive<sub>active&gt;passive</sub>; (<strong>C</strong>) the contrast positive<sub>active&gt;passive</sub> &gt; negative<sub>active&gt;passive</sub> (empty).
</p>
</div>

<table class="table" style="font-size: 9px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-morbid-curiosity-1">Table 5.1: </span>Cluster statistics and associated brain regions from the exploratory whole-brain analysis.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Cluster nr
</th>
<th style="text-align:right;">
Cluster size
</th>
<th style="text-align:right;">
Cluster max
</th>
<th style="text-align:left;">
X
</th>
<th style="text-align:right;">
Y
</th>
<th style="text-align:right;">
Z
</th>
<th style="text-align:left;">
Region
</th>
<th style="text-align:right;">
K
</th>
<th style="text-align:right;">
Max
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;width: 1em; ">
1
</td>
<td style="text-align:right;width: 1em; ">
3158
</td>
<td style="text-align:right;width: 1em; ">
4.82
</td>
<td style="text-align:left;width: 1em; ">
− 8
</td>
<td style="text-align:right;width: 1em; ">
30
</td>
<td style="text-align:right;width: 1em; ">
24
</td>
<td style="text-align:left;">
Left paracingulate gyrus
</td>
<td style="text-align:right;width: 1em; ">
831
</td>
<td style="text-align:right;width: 1em; ">
4.58
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right paracingulate gyrus
</td>
<td style="text-align:right;width: 1em; ">
741
</td>
<td style="text-align:right;width: 1em; ">
4.70
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left superior frontal gyrus
</td>
<td style="text-align:right;width: 1em; ">
657
</td>
<td style="text-align:right;width: 1em; ">
4.64
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right cingulate gyrus, anterior division
</td>
<td style="text-align:right;width: 1em; ">
317
</td>
<td style="text-align:right;width: 1em; ">
4.42
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left cingulate gyrus, anterior division
</td>
<td style="text-align:right;width: 1em; ">
300
</td>
<td style="text-align:right;width: 1em; ">
4.82
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right superior frontal gyrus
</td>
<td style="text-align:right;width: 1em; ">
201
</td>
<td style="text-align:right;width: 1em; ">
3.94
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left juxtapositional lobule cortex
</td>
<td style="text-align:right;width: 1em; ">
76
</td>
<td style="text-align:right;width: 1em; ">
3.27
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
2
</td>
<td style="text-align:right;width: 1em; ">
1928
</td>
<td style="text-align:right;width: 1em; ">
4.84
</td>
<td style="text-align:left;width: 1em; ">
− 42
</td>
<td style="text-align:right;width: 1em; ">
20
</td>
<td style="text-align:right;width: 1em; ">
4
</td>
<td style="text-align:left;">
Left frontal orbital cortex
</td>
<td style="text-align:right;width: 1em; ">
602
</td>
<td style="text-align:right;width: 1em; ">
4.64
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left insular cortex
</td>
<td style="text-align:right;width: 1em; ">
234
</td>
<td style="text-align:right;width: 1em; ">
4.17
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left inferior frontal gyrus, pars triangularis
</td>
<td style="text-align:right;width: 1em; ">
229
</td>
<td style="text-align:right;width: 1em; ">
4.17
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left frontal operculum cortex
</td>
<td style="text-align:right;width: 1em; ">
205
</td>
<td style="text-align:right;width: 1em; ">
4.84
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left inferior frontal gyrus, pars opercularis
</td>
<td style="text-align:right;width: 1em; ">
93
</td>
<td style="text-align:right;width: 1em; ">
3.65
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left temporal pole
</td>
<td style="text-align:right;width: 1em; ">
57
</td>
<td style="text-align:right;width: 1em; ">
4.31
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left subcallosal cortex
</td>
<td style="text-align:right;width: 1em; ">
29
</td>
<td style="text-align:right;width: 1em; ">
3.22
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left caudate
</td>
<td style="text-align:right;width: 1em; ">
121
</td>
<td style="text-align:right;width: 1em; ">
3.48
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left Putamen
</td>
<td style="text-align:right;width: 1em; ">
110
</td>
<td style="text-align:right;width: 1em; ">
3.86
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left thalamus
</td>
<td style="text-align:right;width: 1em; ">
48
</td>
<td style="text-align:right;width: 1em; ">
3.84
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left accumbens
</td>
<td style="text-align:right;width: 1em; ">
39
</td>
<td style="text-align:right;width: 1em; ">
3.52
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
3
</td>
<td style="text-align:right;width: 1em; ">
692
</td>
<td style="text-align:right;width: 1em; ">
4.64
</td>
<td style="text-align:left;width: 1em; ">
36
</td>
<td style="text-align:right;width: 1em; ">
24
</td>
<td style="text-align:right;width: 1em; ">
-16
</td>
<td style="text-align:left;">
Right frontal orbital cortex
</td>
<td style="text-align:right;width: 1em; ">
443
</td>
<td style="text-align:right;width: 1em; ">
4.64
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right insular cortex
</td>
<td style="text-align:right;width: 1em; ">
79
</td>
<td style="text-align:right;width: 1em; ">
3.80
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right frontal operculum cortex
</td>
<td style="text-align:right;width: 1em; ">
70
</td>
<td style="text-align:right;width: 1em; ">
3.39
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right temporal pole
</td>
<td style="text-align:right;width: 1em; ">
44
</td>
<td style="text-align:right;width: 1em; ">
3.40
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right inferior frontal gyrus, pars triangularis
</td>
<td style="text-align:right;width: 1em; ">
30
</td>
<td style="text-align:right;width: 1em; ">
3.27
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
4
</td>
<td style="text-align:right;width: 1em; ">
464
</td>
<td style="text-align:right;width: 1em; ">
4.74
</td>
<td style="text-align:left;width: 1em; ">
10
</td>
<td style="text-align:right;width: 1em; ">
14
</td>
<td style="text-align:right;width: 1em; ">
4
</td>
<td style="text-align:left;">
Right caudate
</td>
<td style="text-align:right;width: 1em; ">
265
</td>
<td style="text-align:right;width: 1em; ">
4.74
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Right putamen
</td>
<td style="text-align:right;width: 1em; ">
26
</td>
<td style="text-align:right;width: 1em; ">
3.49
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
5
</td>
<td style="text-align:right;width: 1em; ">
296
</td>
<td style="text-align:right;width: 1em; ">
3.81
</td>
<td style="text-align:left;width: 1em; ">
− 36
</td>
<td style="text-align:right;width: 1em; ">
12
</td>
<td style="text-align:right;width: 1em; ">
26
</td>
<td style="text-align:left;">
Left middle frontal gyrus
</td>
<td style="text-align:right;width: 1em; ">
111
</td>
<td style="text-align:right;width: 1em; ">
3.72
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left inferior frontal gyrus, pars opercularis
</td>
<td style="text-align:right;width: 1em; ">
111
</td>
<td style="text-align:right;width: 1em; ">
3.81
</td>
</tr>
<tr>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:right;width: 1em; ">
</td>
<td style="text-align:left;">
Left precentral gyrus
</td>
<td style="text-align:right;width: 1em; ">
64
</td>
<td style="text-align:right;width: 1em; ">
3.46
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> The X, Y, and Z coordinates refer to MNI152 (2 mm) space. The regions are taken from the Harvard–Oxford (sub)cortical atlas and voxels are assigned to regions based on their maximum probability across all ROIs within the atlas. <em>K</em> refers to the number of voxels within a particular region.
</td>
</tr>
</tfoot>
</table>
<p>To further interpret the whole brain results, we used the “decoder” function from Neurosynth <span class="citation">(Yarkoni et al., <a href="bibliography.html#ref-yarkoni2011large" role="doc-biblioref">2011</a>)</span> to find key terms associated with particular patterns of neural activation. The neural pattern produced by the negative<sub>active&gt;passive</sub> &gt; positive<sub>active&gt;passive</sub> contrast resulted in the following key terms (top-10): reward, task, monetary, semantic, anticipation, incentive, demands, fear, autobiographical, retrieval (see <a href="morbid-curiosity-supplement.html#morbid-curiosity-supplement">Supplementary Materials</a>, Table <a href="morbid-curiosity-supplement.html#tab:tab-morbid-curiosity-S1">D.1</a>). Although it is important to be careful with drawing reverse inference conclusions about the psychological meaning of neural activation <span class="citation">(Poldrack, <a href="bibliography.html#ref-poldrack2006can" role="doc-biblioref">2006</a>)</span>, these terms at minimum suggest that the neural pattern associated with choosing negative content (relative to choosing positive content and passive-viewing) is similar to neural patterns associated with reward and the processing of extrinsic incentives.</p>
<p>It is important to note that none of the confirmatory ROI analyses nor any of our exploratory analyses showed significant differences in the relief phase (i.e., when viewing the image). This is in line with previous work on curiosity <span class="citation">(Gruber et al., <a href="bibliography.html#ref-gruber2014states" role="doc-biblioref">2014</a>)</span> that found robust neural activation when inducing curiosity (e.g., presentation of trivia questions), but not when relieving curiosity (e.g., presentation of trivia answers). Other work on curiosity, that did find differences in the relief phase, contrasted a condition in which curiosity was relieved, with a condition that withheld information <span class="citation">(Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>)</span> or a condition that showed irrelevant information <span class="citation">(Jepma et al., <a href="bibliography.html#ref-jepma2012neural" role="doc-biblioref">2012</a>)</span>. In the present study, however, we contrasted the relief phase in the active-choice condition with viewing the exact same information in the passive-viewing condition. Since we found that participants in the passive-viewing condition also reported a reasonable amount of curiosity in response to the cue, curiosity relief may have occurred in both the active-choice and the passive-viewing condition.</p>
</div>
</div>
<div id="morbid-curiosity-discussion" class="section level2">
<h2><span class="header-section-number">5.4</span> Discussion</h2>
<p>In the last decade, neuroscientific research has made a major contribution to a better understanding of choice, value and curiosity. Most of this work, however, has focused on extrinsically rewarding information <span class="citation">(Bartra et al., <a href="bibliography.html#ref-bartra2013valuation" role="doc-biblioref">2013</a>; Braver et al., <a href="bibliography.html#ref-braver2014mechanisms" role="doc-biblioref">2014</a>)</span>, or on an intrinsically-motivated curiosity for neutral or positive information <span class="citation">(e.g., Gruber &amp; Ranganath, <a href="bibliography.html#ref-gruber2019curiosity" role="doc-biblioref">2019</a>; Kang et al., <a href="bibliography.html#ref-kang2009wick" role="doc-biblioref">2009</a>; Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>)</span>. The present study demonstrates that choosing intensely <em>negative</em> stimuli engages similar brain regions as those that support extrinsic incentives and regular curiosity <span class="citation">(Bartra et al., <a href="bibliography.html#ref-bartra2013valuation" role="doc-biblioref">2013</a>; Diekhof et al., <a href="bibliography.html#ref-diekhof2012role" role="doc-biblioref">2012</a>; Gruber &amp; Ranganath, <a href="bibliography.html#ref-gruber2019curiosity" role="doc-biblioref">2019</a>; Kidd &amp; Hayden, <a href="bibliography.html#ref-kidd2015psychology" role="doc-biblioref">2015</a>; Sakaki et al., <a href="bibliography.html#ref-sakaki2018curiosity" role="doc-biblioref">2018</a>)</span>. We found that a deliberate choice for death, violence or harm is associated with activation in the striatum (NAcc, caudate and putamen), inferior frontal gyrus, anterior insula, orbitofrontal cortex and anterior cingulate cortex. This pattern was present both when we contrasted negative cues (that were chosen) with passively viewing these cues, and when we contrasted negative cues (that were chosen) with positive cues (that were chosen), controlling for passive viewing. These findings reflect the induction phase in which participants anticipated on their choice; we found no differences in the relief phase in which participants viewed images.</p>
<p>Although we found activation in regions associated with reward and incentives, it is important to be careful with the conclusion that it is “rewarding” to choose negative information <span class="citation">(Poldrack, <a href="bibliography.html#ref-poldrack2006can" role="doc-biblioref">2006</a>)</span>. Reward regions are associated with a multitude of psychological processes <span class="citation">(Bartra et al., <a href="bibliography.html#ref-bartra2013valuation" role="doc-biblioref">2013</a>)</span>, and reward is a construct with different dissociable dimensions, including “liking” and “wanting” <span class="citation">(e.g., Berridge et al., <a href="bibliography.html#ref-berridge2009dissecting" role="doc-biblioref">2009</a>; Weber et al., <a href="bibliography.html#ref-weber2018frontostriatal" role="doc-biblioref">2018</a>)</span>. In addition, the distributed pattern of activation that we found most likely reflects psychological processes beyond reward as well. In the next section, we address several, non-mutually exclusive, interpretations of this distributed pattern, including an interpretation regarding the potential reward value of negativity, the possibility that this neural pattern reflects uncertainty, and an interpretation that considers the particular characteristics of the task.</p>
<p><span class="citation">Golman &amp; Loewenstein (<a href="bibliography.html#ref-golman2015curiosity" role="doc-biblioref">2015</a>)</span> propose that a desire to obtain information can be driven by a motive to make better decisions, a motive to experience pleasantness, or a motive to engage with information “for its own sake” (p. 3). The negative choices made by our participants are most likely consistent with the latter intrinsic motive. Participants chose images that were not hedonically pleasing and, in contrast to other recent (neuroscientific) studies targeting curiosity <span class="citation">(e.g., Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>; Kobayashi et al., <a href="bibliography.html#ref-kobayashi2019diverse" role="doc-biblioref">2019</a>)</span>, there was no monetary outcome associated with choosing (negative) images. The question that follows then is: what is the value of negativity?</p>
<p>One possibility is that knowledge acquisition is inherently valuable <span class="citation">(Marvin &amp; Shohamy, <a href="bibliography.html#ref-marvin2016curiosity" role="doc-biblioref">2016</a>; Murayama et al., <a href="bibliography.html#ref-murayama2019process" role="doc-biblioref">2019</a>)</span>, even when people acquire knowledge about negative social situations that involve death, violence or harm. Indeed, the knowledge update that follows from engaging with negative information might be valuable for building a realistic model of the world <span class="citation">(Baumeister et al., <a href="bibliography.html#ref-baumeister2001bad" role="doc-biblioref">2001</a>)</span> or for dealing with future aversive situations. Applying this interpretation to our findings, we propose that the brain predicts a larger information gain in the negative choice condition than in the positive choice condition. This is reflected in stronger activation of reward circuitry (e.g., NAcc, caudate, putamen) that might track the expected value of knowledge acquisition <span class="citation">(Murayama et al., <a href="bibliography.html#ref-murayama2019process" role="doc-biblioref">2019</a>)</span>, or the salience of the information <span class="citation">(Bartra et al., <a href="bibliography.html#ref-bartra2013valuation" role="doc-biblioref">2013</a>)</span>. Note that we focus here on the informational value of valenced stimuli in epistemic terms <span class="citation">(Berlyne, <a href="bibliography.html#ref-berlyne1966curiosity" role="doc-biblioref">1966</a>; Litman, <a href="bibliography.html#ref-litman2005curiosity" role="doc-biblioref">2005</a>; Loewenstein, <a href="bibliography.html#ref-loewenstein1994psychology" role="doc-biblioref">1994</a>; Murayama et al., <a href="bibliography.html#ref-murayama2019process" role="doc-biblioref">2019</a>)</span>, but the value of choosing to engage with negative or positive stimuli may also lie in the emotional experiences or sensations that are evoked by the stimulus <span class="citation">(Zuckerman, <a href="bibliography.html#ref-zuckerman1979" role="doc-biblioref">1979</a>; Zuckerman &amp; Litle, <a href="bibliography.html#ref-zuckerman1986personality" role="doc-biblioref">1986</a>; see for an overview Tamir, <a href="bibliography.html#ref-tamir2016people" role="doc-biblioref">2016</a>)</span>.</p>
<p>Another possible explanation for the present findings lies in the momentary uncertainty that people may experience when viewing negative cues (e.g., How extreme will the image be?), in combination with the predicted reduction of this uncertainty when choosing to view images. Curiosity is often seen as a desire to resolve uncertainty <span class="citation">(Berlyne, <a href="bibliography.html#ref-berlyne1966curiosity" role="doc-biblioref">1966</a>; Gottlieb &amp; Oudeyer, <a href="bibliography.html#ref-gottlieb2018towards" role="doc-biblioref">2018</a>; Loewenstein, <a href="bibliography.html#ref-loewenstein1994psychology" role="doc-biblioref">1994</a>)</span> and several recent studies have demonstrated that people experience higher levels of curiosity for uncertain stimuli <span class="citation">(Kobayashi et al., <a href="bibliography.html#ref-kobayashi2019diverse" role="doc-biblioref">2019</a>; Lieshout et al., <a href="bibliography.html#ref-van2018induction" role="doc-biblioref">2018</a>)</span>. People even engage with aversive stimuli to reduce uncertainty, preferring a reduction in uncertainty above a negative outcome <span class="citation">(Hsee &amp; Ruan, <a href="bibliography.html#ref-hsee2016pandora" role="doc-biblioref">2016</a>)</span>. Neuroscience studies have shown that uncertainty is associated with activation in the OFC, ACC and anterior insula <span class="citation">(Bach &amp; Dolan, <a href="bibliography.html#ref-bach2012knowing" role="doc-biblioref">2012</a>; Harris et al., <a href="bibliography.html#ref-harris2008functional" role="doc-biblioref">2008</a>; Singer et al., <a href="bibliography.html#ref-singer2009common" role="doc-biblioref">2009</a>)</span>, regions we also found to be active in the active-choice condition. Moreover, we found that the OFC, ACC and anterior insula were more strongly engaged when viewing chosen negative cues, as compared to chosen positive cues. The neural pattern found in the negative choice condition (as compared to the positive choice condition) may thus reflect a higher level of outcome uncertainty, and/or a stronger expected reduction in uncertainty, in interaction with, or irrespective of <span class="citation">(Harris et al., <a href="bibliography.html#ref-harris2008functional" role="doc-biblioref">2008</a>)</span>, the reward value of the information.</p>
<p>A final interpretation of the present findings revolves around the demands of the task that people performed in the choice-condition. Deciding how to respond to a negative cue may be an engaging and effortful task, that requires careful weighing of options. Several perspectives suggests that the IFG and ACC may support such processes. In a recent framework, <span class="citation">Gruber et al. (<a href="bibliography.html#ref-gruber2014states" role="doc-biblioref">2014</a>)</span> propose that curiosity involves an appraisal process, supported by the IFG, that determines one’s ability to deal with information <span class="citation">(see also Silvia, <a href="bibliography.html#ref-silvia2008interest" role="doc-biblioref">2008</a>)</span>. Notably, they predict enhanced involvement of the IFG in situations that involve a dilemma in which people have to choose between approaching or avoiding information. Other authors have suggested that the dACC may be particularly active in effortful or exploratory tasks that demand cognitive control, as compared to tasks that can be performed by engaging in automatic behavior <span class="citation">(Shenhav et al., <a href="bibliography.html#ref-shenhav2016dorsal" role="doc-biblioref">2016</a>)</span>. When applying these perspectives to the current findings, stronger ACC and/or IFG activation for negative choice as opposed to passive viewing may reflect the specific cognitive demands (e.g., appraisal, weighing options, effort) of the active-choice condition relative to the passive-viewing condition. Furthermore, stronger ACC and/or IFG activation for negative choice as opposed to positive choice may reflect the relatively complex cost–benefit analysis that precedes a choice to choose a negative stimulus, as compared to the relatively automatic or “default” decision to choose a positive stimulus. This interpretation is consistent with the characterization of “morbid curiosity” as a conflict state, in which people “want” information that they do not “like” <span class="citation">(Litman, <a href="bibliography.html#ref-litman2005curiosity" role="doc-biblioref">2005</a>; Rimé et al., <a href="bibliography.html#ref-rime2005brief" role="doc-biblioref">2005</a>)</span>.</p>
<p>The present study has a few limitations that we should address. First of all, all contrasts calculated in the present study are relative to the passive-viewing condition (as preregistered). We deliberately made this decision, because we wanted to isolate the neural activation associated with a deliberate choice to view a stimulus. More specifically, in a direct contrast between chosen negative cues and chosen positive cues, it would have been impossible to know whether the pattern of neural activation was driven by <em>choosing</em> negative versus positive information, or by simply <em>viewing</em> negative versus positive cues. The yoked procedure, and the resulting contrasts, control for the latter, since activation associated with viewing negative versus positive cues is subtracted out. Furthermore, confronting participants with emotional material that they cannot control, is common practice when scientists study affective/emotional experience <span class="citation">(Lang &amp; Bradley, <a href="bibliography.html#ref-lang2010emotion" role="doc-biblioref">2010</a>; Lindquist et al., <a href="bibliography.html#ref-lindquist2016brain" role="doc-biblioref">2016</a>)</span> and emotion regulation <span class="citation">(Buhle et al., <a href="bibliography.html#ref-buhle2014cognitive" role="doc-biblioref">2014</a>; Wager et al., <a href="bibliography.html#ref-wager2008prefrontal" role="doc-biblioref">2008</a>)</span>, and thus serves as a meaningful control condition. Nevertheless, it is important to note that our results in the striatum were partly driven by deactivation in the passive viewing condition (see Figure <a href="morbid-curiosity-supplement.html#fig:fig-morbid-curiosity-S1">D.1</a>, <a href="morbid-curiosity-supplement.html#morbid-curiosity-supplement">Supplementary Materials</a>). Since this pattern of deactivation was not predicted in our preregistered analysis protocol, and our study was not designed for optimal detection of directional effects, we will not discuss this finding further. A design that contrasts choosing versus passive viewing within-subjects may clarify whether explicitly anticipating a negative outcome that cannot be controlled indeed deactivates the striatum.</p>
<p>Second, the present design does not allow for a contrast between agreeing to view an image and refusing to view an image, because participants said “yes” to the cue in the vast majority of the trials (i.e., 81% for negative cues; 95% for positive cues). As a future direction, it may be insightful to develop a design that artificially balances yes and no responses within the same participant to investigate differences in reward circuitry between approaching and avoiding negative information. In the present study, however, we deliberately chose not to restrict or disregard participant’s choices, because we wanted to keep the decision process as natural as possible. Having said that, a fMRI scanner is a relatively boring environment, and previous research has shown that boredom prompts people to seek out novel experiences <span class="citation">(Bench &amp; Lench, <a href="bibliography.html#ref-bench2019boredom" role="doc-biblioref">2019</a>)</span>. Thus, the scanning environment may have stimulated participants to say yes more often (across both conditions) than they would have done in other settings.</p>
<p>A third limitation is that we cannot rule out the possibility that arousal contributes to our findings when contrasting negative and positive cues. Although we matched the social negative and positive cues (and images) in terms of valence extremity (i.e., positive cues were perceived as equally positive as negative cues were perceived as negative), the cues (and images) were not matched in terms of arousal. We accepted the difference in arousal as a consequence of the content criterion for negative images (i.e., displaying death, violence or harm within a social context), and our deliberate choice not to include erotic images, considering the practical considerations associated with erotic content <span class="citation">(Wierzba et al., <a href="bibliography.html#ref-wierzba2015erotic" role="doc-biblioref">2015</a>)</span>. It is important to note that in the negative choice vs. passive-viewing contrast the effect of viewing an arousing cue is subtracted out. The negative choice vs. positive choice contrast, however, does not control for the net difference between the arousing quality of the negative and positive cues. Therefore, it is possible that the pattern of neural activation when comparing negative vs. positive choice, in particular in the ACC and insula <span class="citation">(Citron et al., <a href="bibliography.html#ref-citron2014emotional" role="doc-biblioref">2014</a>; Satpute et al., <a href="bibliography.html#ref-satpute2019deconstructing" role="doc-biblioref">2019</a>)</span>, reflects, to some extent, the higher arousal value of the negative vs. positive verbal cues.</p>
<p>A fourth and final limitation is that the present study focused on behavior, without incorporating trial-by-trial ratings of curiosity. This restricts the extent to which the present findings speak to the subjective experience of curiosity. In regular curiosity, for example when processing trivia questions, there are little costs associated with acting on curiosity, and thus it may be sufficient to focus on subjective ratings of curiosity. With morbid curiosity, however, the stakes are higher. Although people can experience curiosity for a negative stimulus without choosing to engage with it, behavior is, in our opinion, the most straightforward indicator of how the conflict state of morbid curiosity is resolved. In other words, only when people choose to engage with negative information can we deduce that the predicted benefits (e.g., knowledge acquisition, uncertainty reduction) outweigh the predicted costs of engaging with the information (e.g., not being able to cope with the content). Furthermore, a focus on choice connects to the many behavioral expressions of this phenomenon in the real world (e.g., “rubbernecking” on the freeway; clicking on a social media link). Future research should investigate whether the subjective experience of curiosity for negative stimuli is associated with a similar neural pattern (e.g., reward circuitry, ACC, insula, OFC) as the pattern found with the present choice paradigm.</p>
<p>Despite the questions that the present study provokes, our findings represent an important step in nuancing models of decision-making, valuation and curiosity. In light of the ubiquity of exploring negativity in daily life, we believe that it is crucial to start thinking about the value of seeking out negative content.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="aomic.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-kernel-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
