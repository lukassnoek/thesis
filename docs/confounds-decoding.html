<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 How to control for confounds in decoding analyses of neuroimaging data | Towards prediction</title>
  <meta name="description" content="3 How to control for confounds in decoding analyses of neuroimaging data | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shared-states.html"/>
<link rel="next" href="aomic.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>1.1</b> Inference done differently</a></li>
<li class="chapter" data-level="1.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>1.2</b> Towards prediction</a></li>
<li class="chapter" data-level="1.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.2.7</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.3</b> Results</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.3.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.3.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
<li class="chapter" data-level="4.4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.4.6</b> Code availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="5.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>5.2.2</b> Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>5.2.3</b> Materials</a></li>
<li class="chapter" data-level="5.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>5.2.4</b> Procedure</a></li>
<li class="chapter" data-level="5.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>5.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="5.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>5.2.6</b> Imaging details</a></li>
<li class="chapter" data-level="5.2.7" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>5.2.7</b> Data availability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>5.3</b> Results</a><ul>
<li class="chapter" data-level="5.3.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>5.3.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="5.3.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>5.3.3</b> ROI analyses</a></li>
<li class="chapter" data-level="5.3.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>5.3.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>6</b> Explainable models of facial movements predict emotion perception behavior</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#the-prediction-explanation-exploration-framework"><i class="fa fa-check"></i><b>6.1.1</b> The prediction-explanation-exploration framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>6.2.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>6.2.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>6.2.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="6.2.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>6.2.4</b> Evaluated mappings</a></li>
<li class="chapter" data-level="6.2.5" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-dataset"><i class="fa fa-check"></i><b>6.2.5</b> Dataset used to evaluate mappings</a></li>
<li class="chapter" data-level="6.2.6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-code"><i class="fa fa-check"></i><b>6.2.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#prediction"><i class="fa fa-check"></i><b>6.3.1</b> Prediction</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#explanation"><i class="fa fa-check"></i><b>6.3.2</b> Explanation</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#exploration"><i class="fa fa-check"></i><b>6.3.3</b> Exploration</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>7</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="7.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>7.2.1</b> Participants</a></li>
<li class="chapter" data-level="7.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>7.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="7.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>7.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>7.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="7.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>7.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>7.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="7.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>7.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>7.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="7.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>7.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>7.4</b> Discussion</a><ul>
<li class="chapter" data-level="7.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>7.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="7.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>7.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="7.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>7.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="7.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>7.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>8</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a><ul>
<li class="chapter" data-level="E.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supplementary-methods"><i class="fa fa-check"></i><b>E.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="E.1.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-in-detail"><i class="fa fa-check"></i><b>E.1.1</b> Hypothesis kernel analysis (in detail)</a></li>
<li class="chapter" data-level="E.1.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-noise-ceiling-detail"><i class="fa fa-check"></i><b>E.1.2</b> Noise ceiling estimation (in detail)</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supp-fig"><i class="fa fa-check"></i><b>E.2</b> Supplementary figures</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confounds-decoding" class="section level1">
<h1><span class="header-section-number">3</span> How to control for confounds in decoding analyses of neuroimaging data</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Snoek, L.*, Miletić, S.*, &amp; Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. <em>NeuroImage</em>, 184, 741-760.</p>
<p>* Shared first authorship</p>

<p><p><strong>Abstract</strong></p>

Over the past decade, multivariate “decoding analyses” have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound “brain size”. We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one’s decoding analysis.
</p>
<div id="confounds-decoding-introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data <span class="citation">(Haxby, <a href="bibliography.html#ref-Haxby2012-sd" role="doc-biblioref">2012</a>; Norman et al., <a href="bibliography.html#ref-norman2006beyond" role="doc-biblioref">2006</a>)</span>. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is “decoding”, in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns <span class="citation">(Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span> and working memory representations from EEG data <span class="citation">(LaRocque et al., <a href="bibliography.html#ref-LaRocque2013-sh" role="doc-biblioref">2013</a>)</span>, as well between-subject factors such as Alzheimer’s disease (vs. healthy controls) from structural MRI data <span class="citation">(Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span> and major depressive disorder (vs. healthy controls) from resting-state functional connectivity <span class="citation">(Craddock et al., <a href="bibliography.html#ref-Craddock2009-kz" role="doc-biblioref">2009</a>)</span>. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do <span class="citation">(Jimura &amp; Poldrack, <a href="bibliography.html#ref-Jimura2012-lv" role="doc-biblioref">2012</a>)</span>. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>.</p>
<p>In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical <span class="citation">(Allefeld et al., <a href="bibliography.html#ref-Allefeld2016-xp" role="doc-biblioref">2016</a>; Davis et al., <a href="bibliography.html#ref-Davis2014-lw" role="doc-biblioref">2014</a>; Gilron et al., <a href="bibliography.html#ref-Gilron2017-tl" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>)</span> and more conceptual <span class="citation">(Naselaris &amp; Kay, <a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span> in nature. For the purposes of the current study, we focus on the specific criticism put forward by <span class="citation">Naselaris &amp; Kay (<a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>)</span> , who argue that decoding analyses are inherently ambiguous in terms of what information they use <span class="citation">(see Popov et al., <a href="bibliography.html#ref-popov2018practices" role="doc-biblioref">2018</a> for a similar argument in the context of encoding analyses)</span>. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information <span class="citation">(see also Carlson &amp; Wardle, <a href="bibliography.html#ref-Carlson2015-bz" role="doc-biblioref">2015</a>; Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span>. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of “source ambiguity”.</p>
<p>To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> (male/female) from structural MRI with the aim of contributing to the understanding of gender differences — an endeavor that generated considerable interest and controversy <span class="citation">(Chekroud et al., <a href="bibliography.html#ref-Chekroud2016-tc" role="doc-biblioref">2016</a>; Del Giudice et al., <a href="bibliography.html#ref-Del_Giudice2016-ns" role="doc-biblioref">2016</a>; Glezerman, <a href="bibliography.html#ref-Glezerman2016-xl" role="doc-biblioref">2016</a>; Joel &amp; Fausto-Sterling, <a href="bibliography.html#ref-Joel2016-uo" role="doc-biblioref">2016</a>; Rosenblatt, <a href="bibliography.html#ref-Rosenblatt2016-oy" role="doc-biblioref">2016</a>)</span>. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant’s gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways <span class="citation">(Good, Johnsrude, et al., <a href="bibliography.html#ref-Good2001-kv" role="doc-biblioref">2001</a><a href="bibliography.html#ref-Good2001-kv" role="doc-biblioref">b</a>; O’Brien et al., <a href="bibliography.html#ref-OBrien2011-lj" role="doc-biblioref">2011</a>)</span>. First, there is a <em>global</em> difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight <span class="citation">(18.7%; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Lüders et al., <a href="bibliography.html#ref-Luders2002-ms" role="doc-biblioref">2002</a>)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> Second, brains of men and women are known to differ <em>locally</em>: some specific brain areas are on average larger in women than in men <span class="citation">(e.g., in superior and middle temporal cortex; Good, Johnsrude, et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a><a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">a</a>)</span> and vice versa <span class="citation">(e.g., in frontomedial cortex; Goldstein et al., <a href="bibliography.html#ref-Goldstein2001-dy" role="doc-biblioref">2001</a>)</span>. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male <em>bodies</em> are on average larger than female bodies <span class="citation">(Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. As such, our hypothetical researcher is likely primarily interested in the <em>local</em> sources of variation in the neuroanatomy of male and female brains.</p>
<p>Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for “uninteresting” sources of information.</p>
<div id="confounds-decoding-introduction-true-vs-confounded" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</h3>
<p>Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate <em>prediction</em>, but not in <em>interpretability</em> of the model <span class="citation">(Bzdok, <a href="bibliography.html#ref-Bzdok2017-li" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is <em>only</em> using information in the neuroimaging data that is related to the variable that is being decoded <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. In this scenario, source ambiguity (i.e., the presence of <em>multiple</em> sources of information) <em>is</em> problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is <em>confounding</em> the decoding analysis of gender. Here, we define a confound as <em>a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.</em></p>
<p>To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable “schizophrenia-or-not” is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender <span class="citation">(i.e., men are more often diagnosed with schizophrenia than women; McGrath et al., <a href="bibliography.html#ref-McGrath2008-oj" role="doc-biblioref">2008</a>)</span> and substance abuse <span class="citation">(Dixon, <a href="bibliography.html#ref-Dixon1999-kl" role="doc-biblioref">1999</a>)</span>, which are also known to affect gray matter <span class="citation">(Bangalore et al., <a href="bibliography.html#ref-Bangalore2008-kc" role="doc-biblioref">2008</a>; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Van Haren et al., <a href="bibliography.html#ref-Van_Haren2013-iv" role="doc-biblioref">2013</a>)</span>. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate <em>prediction</em> of schizophrenia, then this ambiguity due to confounding sources of information is problematic.</p>
<p>Importantly, as our definition suggests, what <em>is</em> or <em>is not</em> regarded as a confound is relative — it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable “schizophrenia-or-no”” becomes the confounding variable. In other words, one researcher’s signal is another researcher’s confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the “true signal” (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the “confounded signal” (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-1">3.1</a>). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-1"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_1.png" alt="Visualization of how variance in brain data (\(X\)) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables."  />
<p class="caption">
Figure 3.1: Visualization of how variance in brain data (<span class="math inline">\(X\)</span>) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (<span class="math inline">\(X\)</span>), the confound (<span class="math inline">\(C\)</span>), and the target (<span class="math inline">\(y\)</span>). Overlapping circles indicate a non-zero (squared) correlation between the two variables.
</p>
</div>

<p>In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.</p>
</div>
<div id="confounds-decoding-introduction-methods" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Methods for confound control</h3>
<p>In decoding analyses, one aims to predict a certain target variable from patterns of neuroimaging data. Various methods discussed in this section are supplemented with a mathematical formalization; for consistency and readability, we define the notation we will use in Table <a href="confounds-decoding.html#tab:tab-confounds-decoding-1">3.1</a>.</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-confounds-decoding-1">Table 3.1: </span>Notation.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Symbol
</th>
<th style="text-align:left;">
Dims.
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of samples (usually subjects or trials)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of neuroimaging features (e.g., voxels or sensors)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(P\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of confound variables (e.g., age, reaction time, or brain size)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(X_{ij}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times K\)</span>
</td>
<td style="text-align:left;">
The neuroimaging patterns (often called the “data” in the current article), where the subescript <span class="math inline">\(i \in {1 \dots N}\)</span> refers to the individual samples (rows), and the subscript <span class="math inline">\(j \in {1 \dots K}\)</span> to individual features (columns)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(y\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times 1\)</span>
</td>
<td style="text-align:left;">
The target variable (i.e., what is to be decoded)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(C\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times P\)</span>
</td>
<td style="text-align:left;">
The confound variable(s)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(\hat{\beta}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K + 1\)</span>
</td>
<td style="text-align:left;">
The parameters estimated in a general linear model (GLM)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(w\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K + 1\)</span>
</td>
<td style="text-align:left;">
The parameters estimated in a decoding model
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(r_{Cy}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Sample Pearson correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(r_{y(X.C)}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Sample semipartial Pearson correlation coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>, controlled for <span class="math inline">\(C\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(p(r_{Cy})\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
<span class="math inline">\(p\)</span>-value of sample Pearson correlation between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span>
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Format based on Diedrichsen and Kriegeskorte (2017). For the correlations (<span class="math inline">\(r\)</span>), we assume that <span class="math inline">\(P = 1\)</span> and thus that the correlations in the table reduce to a scalar.
</td>
</tr>
</tfoot>
</table></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<div id="confounds-decoding-introduction-methods-apriori-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.1.2.1</span> A priori counterbalancing</h4>
<p>Ideally, one would prevent confounding variables from influencing the results as much as possible before the acquisition of the neuroimaging data.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> One common way do this (in both traditional “activation-based” and decoding analyses) is to make sure that potential confounding variables are <em>counterbalanced</em> in the experimental design <span class="citation">(Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. In experimental research, this would entail randomly assigning subjects to design cells (e.g., treatment groups) such that there is no structural correlation between characteristics of the subjects and design cells. In observational designs (e.g., in the gender/brain size example described earlier), it means that the sample is chosen such that there is no correlation between the confound (brain size) and <em>observed</em> target variable (gender). That is, given that men on average have larger brains than women, this would entail including only men with relatively small brains and women with relatively large brains.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> The distinction between experimental and observational studies is important because the former allow the researcher to randomly draw samples from the population, while the latter require the researcher to choose a sample that is not representative of the population, which limits the conclusions that can be drawn about the population (we will revisit this issue in the <a href="confounds-decoding.html#confounds-decoding-discussion">Discussion</a> section).</p>
<p>Formally, in decoding analyses, a design is counterbalanced when the confound <span class="math inline">\(C\)</span> and the target <span class="math inline">\(y\)</span> are statistically independent. In practice, this often means that the sample is chosen so that there is no significant correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> (although this does not necessarily imply that <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> are actually independent). To illustrate the process of counterbalancing, let’s consider another hypothetical experiment: suppose one wants to set up an fMRI experiment in which the goal is to decode abstract object category (e.g., faces vs. houses) from the corresponding fMRI patterns <span class="citation">(cf. Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span>, while controlling for the potential confounding influence of low-level or mid-level stimulus features, such as luminance, spatial frequency, or texture <span class="citation">(Long et al., <a href="bibliography.html#ref-Long2017-fb" role="doc-biblioref">2017</a>)</span>. Proper counterbalancing would entail making sure that the images used for this particular experiments have similar values for these low-level and mid-level features across object categories <span class="citation">(see for details Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. Thus, in this example, low-level and mid-level stimulus features should be counterbalanced with respect to object category, such that above chance decoding of object category cannot be attributed to differences in low-level or mid-level stimulus features (i.e., the confounds).</p>
<p>A priori counterbalancing of potential confounds is, however, not always feasible. For one, the exact measurement of a potentially confounding variable may be impossible until data acquisition. For example, the brain size of a participant is only known after data collection. Similarly, <span class="citation">Todd et al. (<a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> found that their decoding analysis of rule representations was confounded by response times of to the to-be-decoded trials. Another example of a “data-driven” confound is participant motion during data acquisition <span class="citation">(important in, for example, decoding analyses applied to data from clinical populations such as ADHD; Yu-Feng et al., <a href="bibliography.html#ref-Yu-Feng2007-sg" role="doc-biblioref">2007</a>)</span>. In addition, a priori counterbalancing of confounds may be challenging because of the limited size of populations of interest. Especially in clinical research settings, researchers may not have the luxury of selecting a counterbalanced sample due to the small number of patient subjects available for testing. Lastly, researchers may simply discover confounds after data acquisition.</p>
<p>Given that a priori counterbalancing is not possible or undesirable in many situations, it is paramount to explore the possibilities of controlling for confounding variables after data acquisition for the sake of model interpretability, which we discuss next.</p>
</div>
<div id="confounds-decoding-introduction-methods-include-in-data" class="section level4">
<h4><span class="header-section-number">3.1.2.2</span> Include confounds in the data</h4>
<p>One perhaps intuitive method to control for confounds in decoding analyses is to include the confound(s) in the data <span class="citation">(i.e., the neuroimaging data, <span class="math inline">\(X\)</span>; see, e.g., Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span> used by decoding model. That is, when applying a decoding analysis to neuroimaging data, the confound is added to the data as if it were another voxel (or sensor, in electrophysiology). This intuition may stem from the analogous situation in univariate (activation-based) analyses of neuroimaging data, in which confounding variables are controlled for by including them in the design matrix together with the stimulus/task regressors. For example, in univariate analyses of functional MRI, movement of the participant is often controlled for by including motion estimates in the design matrix of first-level analyses <span class="citation">(Johnstone et al., <a href="bibliography.html#ref-Johnstone2006-tn" role="doc-biblioref">2006</a>)</span>; in EEG, some control for activity due to eye-movements by including activity measured by concurrent electro-oculography as covariates in the design-matrix <span class="citation">(Parra et al., <a href="bibliography.html#ref-Parra2005-um" role="doc-biblioref">2005</a>)</span>. Usually, the general linear model is then used to estimate each predictor’s influence on the neuroimaging data. Importantly, the parameter estimates (<span class="math inline">\(\hat{\beta}\)</span>) are often interpreted as reflecting the unique contribution<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> of each predictor variable, independent from the influence of the confound.</p>
<p>Contrary to general linear models as employed in univariate (activation-based) analyses, including confound variables in the data as predictors for <em>decoding</em> models is arguably problematic. If a confound is included in the data in the context of decoding models, the parameter estimates of the features (often called “feature weights”, <span class="math inline">\(w\)</span>, in decoding models) may be corrected for the influence of the confound, but the <em>model performance</em> <span class="citation">(usually measured as explained variance, <span class="math inline">\(R^2\)</span>, or classification accuracy; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span> is not. That is, rather than providing an estimate of decoding performance “controlled for” a confound, one obtains a measure of performance when explicitly <em>including</em> the confound as an interesting source of variance that the decoder is allowed to use. This is problematic because research using decoding analyses generally does not focus on parameter estimates but on statistics of model performance. Model performance statistics (e.g., <span class="math inline">\(R^2\)</span>, classification accuracy) alone cannot disentangle the contribution of different sources of information as they only represent a single summary statistic of model fit <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. One might, then, argue that additionally inspecting feature weights of decoding models may help in disambiguating different sources of information <span class="citation">(Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. However, it has been shown that feature weights cannot be reliably mapped to specific sources of information, i.e., as being task-related or confound-related <span class="citation">(e.g., features with large weights may be completely uncorrelated with the target variable; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. As such, it does not make sense to include confounds in the set of predictors when the goal is to disambiguate the different sources of information in decoding analyses.</p>
<p>Recently, another approach similar to including confounds in the data has been proposed, which is based on the idea of a dose-response curve <span class="citation">(Alizadeh et al., <a href="bibliography.html#ref-alizadeh2017decoding" role="doc-biblioref">2017</a>)</span>. In this method, instead of adding the confound(s) to the model directly, the relative contribution of true and confounded signal is systematically controlled. The authors show that this approach is able to directly quantify the unique contribution of each source of information, thus effectively controlling for confounded signal. However, while sophisticated in its approach, this method only seems to work for categorical confounds, as it is difficult (if not impossible) to systematically vary the proportion of confound-related information when dealing with continuous confounds or when dealing with more than one confound.</p>
</div>
<div id="confounds-decoding-introduction-methods-pattern-estimation" class="section level4">
<h4><span class="header-section-number">3.1.2.3</span> Control for confounds during pattern estimation</h4>
<p>Another method that was used in some decoding studies on functional MRI data aims to control for confounds in the initial procedure of estimating activity patterns of the to-be-decoded events, by leveraging the ability of the GLM to yield parameter estimates reflecting unique variance <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>. In this method, an initial “first-level” (univariate) analysis models the fMRI time series (<span class="math inline">\(s\)</span>) as a function of both predictors-of-interest (<span class="math inline">\(X\)</span>) and the confounds (<span class="math inline">\(C\)</span>), often using the GLM<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>:</p>
<p><span class="math display">\[\begin{equation}
s = X\beta_{x} + C\beta_{c} + \epsilon
\end{equation}\]</span></p>
<p>Then, only the estimated parameters (<span class="math inline">\(\hat{\beta}\)</span>, or normalized parameters, such as <em>t</em>-values or <em>z</em>-values) corresponding to the predictors-of-interest (<span class="math inline">\(\hat{\beta}_{x}\)</span>) are used as activity estimates (i.e., the used for predicting the target <span class="math inline">\(y\)</span>) in the subsequent decoding analyses. This method thus takes advantage of the shared variance partitioning in the pattern estimation step to control for potential confounding variables. However, while elegant in principle, this method is not applicable in between-subject decoding studies <span class="citation">(e.g., clinical decoding studies; Waarde et al., <a href="bibliography.html#ref-Van_Waarde2014-sh" role="doc-biblioref">2014</a>; Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span>, in which confounding variables are defined across subjects, or in electrophysiology studies, in which activity patterns do not have to be<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> estimated in a first-level model, thus limiting the applicability of this method.</p>
</div>
<div id="confounds-decoding-introduction-methods-posthoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.1.2.4</span> Post hoc counterbalancing of confounds</h4>
<p>When a priori counterbalancing is not possible, some have argued that post hoc counterbalancing might control for the influence of confounds <span class="citation">(Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>, pp. 24, 38)</span>. In this method, given that there is some sample correlation between the target and confound (<span class="math inline">\(r_{Cy} \neq 0\)</span>) in the entire dataset, one takes a subset of samples in which there is no empirical relation between the confound and the target (e.g., when <span class="math inline">\(r_{Cy} \approx 0\)</span>). In other words, post hoc counterbalancing is a way to <em>decorrelate</em> the confound and the target by subsampling the data. Then, subsequent decoding analysis on the subsampled data can only capitalize on true signal, as there is no confounded signal anymore (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-2">3.2</a>). While intuitive in principle, we are not aware of whether this method has been evaluated before and whether it yields unbiased performance estimates.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-2"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_2.png" alt="A schematic visualization how the main two confound control methods evaluated in this article deal with the “confounded signal”, making sure decoding models only capitalize on the “true signal”."  />
<p class="caption">
Figure 3.2: A schematic visualization how the main two confound control methods evaluated in this article deal with the “confounded signal”, making sure decoding models only capitalize on the “true signal”.
</p>
</div>

</div>
<div id="confounds-decoding-introduction-methods-confound-regression" class="section level4">
<h4><span class="header-section-number">3.1.2.5</span> Confound regression</h4>
<p>The last and perhaps most common method to control for confounds is removing the variance that can be explained by the confound (i.e., the confounded signal) from the neuroimaging data directly <span class="citation">(Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>; Dukart et al., <a href="bibliography.html#ref-Dukart2011-aq" role="doc-biblioref">2011</a>; Kostro et al., <a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>; Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>; Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> — a process we refer to as <em>confound regression</em> <span class="citation">(also known as “image correction”; Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>)</span>. In this method, a (usually linear) regression model is fitted on each feature in the neuroimaging data (i.e., a single voxel or sensor) with the confound(s) as predictor(s). Thus, each feature in the neuroimaging data <span class="math inline">\(X\)</span> is modelled as a linear function of the confounding variable(s), <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[\begin{equation}
X_{j} = C\beta + \epsilon
\end{equation}\]</span></p>
<p>We can estimate the parameter(s) for feature using, for example, ordinary least squares as follows <span class="citation">(for an example using a different model, see Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{j} = (C^{T}C)^{-1}C^{T}X_{j}
\end{equation}\]</span></p>
<p>Then, to remove the variance of (or “regress out”) the confound from the neuroimaging data, we can subtract the variance in the data associated with confound (<span class="math inline">\(C\hat{\beta}_{j}\)</span>) from the original data:</p>
<p><span class="math display">\[\begin{equation}
X_{j,\mathrm{corr}} = X_{j} - C\hat{\beta}_{j}
\end{equation}\]</span></p>
<p>In which <span class="math inline">\(X_{j,\mathrm{corr}}\)</span> represents the neuroimaging feature <span class="math inline">\(X_{j}\)</span> from which all variance of the confound is removed (including the variance shared with <span class="math inline">\(y\)</span>, i.e., the confounded signal; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-2">3.2</a>). When subsequently applying a decoding analysis on this corrected data, one can be sure that the decoder is not capitalizing on signal that is correlated with the confound, which thus improves interpretability of the decoding analysis.</p>
<p>Confound regression has been applied in several decoding studies. <span class="citation">Todd et al. (<a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> were, as far as the current authors are aware, the first to use this method to control for a confound (in their case, reaction time) that was shown to correlate with their target variable (rule A vs. rule B). Notably, they both regressed out reaction time from the first-level time series data (similar to the “Control for confounds during pattern estimation” method) <em>and</em> regressed out reaction time from the trial-by-trial activity estimates (i.e., confound regression as described in this section). They showed that controlling for reaction time in this way completely eliminated the above chance decoding performance. Similarly, <span class="citation">Kostro et al. (<a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>)</span> observe a substantial drop in classification accuracy when controlling for scanner site in the decoding analysis of Huntington’s disease, but only when scanner site and disease status were actually correlated. Lastly, <span class="citation">Rao et al. (<a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>)</span> found that, in contrast to Kostro et al. and Todd et al., confound regression yielded similar (or slightly lower, but still significant) performance compared to the model without confound control, but it should be noted that this study used a regression model (instead of a classification model) and evaluated confound control in the specific situation when the training set is confounded, but the test set is not.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> In sum, while confound regression has been used before, it has yielded variable results, possibly due to slightly different approaches and differences in the correlation between the confounding variable and the target.</p>
</div>
</div>
<div id="confounds-decoding-introduction-current-study" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Current study</h3>
<p>In summary, multiple methods have been proposed to deal with confounds in decoding analyses. Often, these methods have specific assumptions about the nature or format of the data (such as “A priori counterbalancing” and “Confound control during pattern estimation”), differ in their objective (e.g., <em>prediction</em> vs. <em>interpretation</em>, such as in “Include confounds in the data”), or have yielded variable results (such as “Confound regression”). Therefore, given that we are specifically interested in interpreting decoding analyses, the current study evaluates the two methods that are applicable in most contexts: post hoc counterbalancing and confound regression (but see <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a> for a tentative evaluation of this method based on simulated functional MRI data). In addition to these two methods, we propose a third method — a modified version of confound regression —– which we show yields plausible, seemingly unbiased, and interpretable results.</p>
<p>To test whether these methods are able to effectively control for confounds and whether they yield plausible results, we apply them to empirical data, as well as to simulated data in which the ground truth with respect to the signal in the data (i.e., the proportion of true signal and confounded signal) is known. For our empirical data, we enact the previously mentioned hypothetical study in which participant gender is decoded from structural MRI data. We use a large dataset (<span class="math inline">\(N = 217\)</span>) of structural MRI data and try to predict subjects’ gender (male/female) from gray and white matter patterns while controlling for the confound of “brain size” using the aforementioned methods, which we compare to a baseline model in which confounds are not controlled for. Given the previously reported high correlations between brain size and gender <span class="citation">(Barnes et al., <a href="bibliography.html#ref-Barnes2010-pu" role="doc-biblioref">2010</a>; Smith &amp; Nichols, <a href="bibliography.html#ref-Smith2018-th" role="doc-biblioref">2018</a>)</span>, we expect that successfully controlling for brain size yields lower decoding performance than using uncorrected data, but not below chance level. Note that higher decoding performance after controlling for confounds is theoretically possible when the correlation between the confound and variance in the data <em>unrelated</em> to the target (e.g., noise) is sufficiently high to cause suppressor effects <span class="citation">(see Figure 1 in Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. However, because our confound, brain size, is known to correlate strongly with our target gender <span class="citation">(approx. <span class="math inline">\(r = 0.63\)</span>; Smith &amp; Nichols, <a href="bibliography.html#ref-Smith2018-th" role="doc-biblioref">2018</a>)</span>, it is improbable that it also correlates highly with variance in brain data that is unrelated to gender. It follows then that classical suppression effects are unlikely and we thus expect lower model performance after controlling for brain size.</p>
<p>However, shown in detail below, both post hoc counterbalancing and confound regression lead to unexpected results in our empirical analyses: counterbalancing fails to reduce model performance while confound regression consistently yields low model performance up to the point of significant below chance accuracy. In subsequent analyses of simulated data, we show that both methods lead to <em>biased</em> results: post hoc counterbalancing yields inflated model performance (i.e., positive bias) because subsampling selectively selects a subset of samples in which features correlate more strongly with the target variable, suggesting (indirect) circularity in the analysis <span class="citation">(Kriegeskorte et al., <a href="bibliography.html#ref-kriegeskorte2009circular" role="doc-biblioref">2009</a>)</span>. Furthermore, our simulations show that negative bias (including significant below chance classification) after confound regression on the entire dataset is due to reducing the signal below what is expected by chance <span class="citation">(Jamalabadi et al., <a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span>, which we show is related to and can be predicted by the standard deviation of the empirical distribution of correlations between the features in the data and the target. We propose a minor but crucial addition to the confound regression procedure, in which we cross-validate the confound regression models (which we call “cross-validated confound regression”, CVCR), which solves the below chance accuracy issue and yields plausible model performance in both our empirical and simulated data.</p>
</div>
</div>
<div id="confounds-decoding-methods" class="section level2">
<h2><span class="header-section-number">3.2</span> Methods</h2>
<div id="confounds-decoding-methods-data" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Data</h3>
<p>For the empirical analyses, we used voxel-based morphometry (VBM) data based on T1-weighted scans and tract-based spatial statistics (TBSS) data based on diffusion tensor images from 217 participants (122 women, 95 men), acquired with a Philips Achieva 3T MRI-scanner and a 32-channel head coil at the Spinoza Centre for Neuroimaging (Amsterdam, The Netherlands).</p>
<div id="confounds-decoding-methods-data-vbm" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> VBM acquisition &amp; analysis</h4>
<p>The T1-weighted scans with a voxel size of 1.0 × 1.0 × 1.0 mm were acquired using 3D fast field echo (TR: 8.1 ms, TE: 3.7 ms, flip angle: 8°, FOV: 240 × 188 mm, 220 slices). We used “FSL-VBM” protocol <span class="citation">(Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span> from the FSL software package <span class="citation">(version 5.0.9; Smith et al., <a href="bibliography.html#ref-Smith2004-sc" role="doc-biblioref">2004</a>)</span>; using default and recommended parameters (including non-linear registration to standard space). The resulting VBM-maps were spatially smoothed using a Gaussian kernel (3 mm FWHM). Subsequently, we organized the data in the standard pattern-analysis format of a 2D (<span class="math inline">\(N \times K\)</span>) array of shape 217 (subjects) × 412473 (non-zero voxels).</p>
</div>
<div id="confounds-decoding-methods-data-tbss" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> TBSS acquisition &amp; analysis</h4>
<p>Diffusion tensor images with a voxel size of 2.0 × 2.0 × 2.0 mm were acquired using a spin-echo echo-planar imaging (SE-EPI) protocol (TR: 7476 ms, TE: 86 ms, flip angle: 90°, FOV: 224 × 224 mm, 60 slices), which acquired a single b = 0 (non-diffusion-weighted) image and 32 (diffusion-weighted) b = 1000 images. All volumes were corrected for eddy-currents and motion (using the FSL command “eddy_correct”) and the non-diffusion-weighted image was skullstripped (using FSL-BET with the fractional intensity threshold set to 0.3) to create a mask that was subsequently used in the fractional anisotropy (FA) estimation. The FA-images resulting from the diffusion tensor fitting procedure were subsequently processed by FSL’s tract-based spatial statistics (TBSS) pipeline <span class="citation">(Smith et al., <a href="bibliography.html#ref-Smith2006-sf" role="doc-biblioref">2006</a>)</span>, using the recommended parameters (i.e., non-linear registration to FSL’s 1 mm FA image, construction of mean FA-image and skeletonized mean FA-image based on the data from all subjects, and a threshold of 0.2 for the skeletonized FA-mask). Subsequently, we organized the resulting skeletonized FA-maps into a 2D (<span class="math inline">\(N \times K\)</span>) array of shape 217 (subjects) × 128340 (non-zero voxels).</p>
</div>
<div id="confounds-decoding-methods-data-brainsize" class="section level4">
<h4><span class="header-section-number">3.2.1.3</span> Brain size estimation</h4>
<p>To estimate the values for our confound, global brain size, we calculated for each subject the total number of non-zero voxels in the gray matter and white matter map resulting from the segmentation step in the FSL-VBM pipeline <span class="citation">(using FSL’s segmentation algorithm “FAST”; Zhang et al., <a href="bibliography.html#ref-Zhang2001-wa" role="doc-biblioref">2001</a>)</span>. The number of non-zero voxels from the gray matter map was used as the confound for the VBM-based analyses and the number of non-zero voxels from the white matter map was used as the confound for the TBSS-based analyses. Note that brain size estimates from total white matter volume and total gray matter volume correlated strongly, <span class="math inline">\(r (216) = 0.93\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>.</p>
</div>
<div id="confounds-decoding-methods-data-data-and-code" class="section level4">
<h4><span class="header-section-number">3.2.1.4</span> Data and code availability</h4>
<p>In the Github repository corresponding to this article (<a href="https://github.com/lukassnoek/MVCA">https://github.com/lukassnoek/MVCA</a>), we included a script (<code>download_data.py</code>) to download the data (the 4D VBM and TBSS nifti-images as well as the non-zero 2D samples × features arrays). The repository also contains detailed Jupyter notebooks with the annotated empirical analyses and simulations reported in this article.</p>
</div>
</div>
<div id="confounds-decoding-methods-pipeline" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Decoding pipeline</h3>
<p>All empirical analyses and simulations used a common decoding pipeline, implemented using functionality from the <em>scikit-learn</em> Python package for machine learning <span class="citation">(Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>; Pedregosa et al., <a href="bibliography.html#ref-pedregosa2011scikit" role="doc-biblioref">2011</a>)</span>. This pipeline included univariate feature selection (based on a prespecified amount of voxels with highest univariate difference in terms of the ANOVA <em>F</em>-statistic), feature-scaling (ensuring zero mean and unit standard deviation for each feature), and a support vector classifier (SVC) with a linear kernel, fixed regularization parameter (<em>C</em> = 1), and sample weights set to be inversely proportional to class frequency (to account for class imbalance). In our empirical analyses, we evaluated model performance for different numbers of voxels (as selected by the univariate feature selection). For our empirical analyses, we report model performance as the <span class="math inline">\(F_{1}\)</span> score, which is insensitive to class imbalance (which, in addition to adjusted sample weights, prevents the classifier from learning the relative probabilities of target classes instead of representative information in the features; see also Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S14">B.14</a> for a replication of part of the results using AUROC, another metric that is insensitive to class imbalance). At chance level classification, the <span class="math inline">\(F_{1}\)</span> score is expected to be 0.5. For our simulations, in which there is no class imbalance, we report model performance using accuracy scores. In figures showing error bars around the average model performance scores, the error bars represent 95% confidence intervals estimated using the “bias-corrected and accelerated” (BCA) bootstrap method using 10,000 bootstrap replications <span class="citation">(Efron, <a href="bibliography.html#ref-efron1987better" role="doc-biblioref">1987</a>)</span>. For calculating BCA bootstrap confidence intervals, we used the implementation from the open source “scikits.bootstrap” Python package (<a href="https://github.com/cgevans/scikits-bootstrap">https://github.com/cgevans/scikits-bootstrap</a>). Statistical significance was calculated using non-parametric permutation tests, as implemented in scikit-learn, with 1000 permutations <span class="citation">(Ojala &amp; Garriga, <a href="bibliography.html#ref-Ojala2010-rc" role="doc-biblioref">2010</a>)</span>.</p>
</div>
<div id="confounds-decoding-methods-evaluated-methods" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Evaluated methods for confound control</h3>
<div id="confounds-decoding-methods-evaluated-methods-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Post hoc counterbalancing</h4>
<p>We implemented post hoc counterbalancing in two steps. First, to quantify the strength of relation between the confound and the target in our dataset, we estimated the point-biserial correlation coefficient between the confound, <span class="math inline">\(C\)</span> (brain size), and the target, <span class="math inline">\(y\)</span> (gender) across the entire dataset (including all samples <span class="math inline">\(i = 1 \dots N\)</span>). Because of both sampling noise and measurement noise, sample correlation coefficients vary around the population correlation coefficient and are thus improbable to be 0 <em>exactly</em>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> Therefore, in the next step, we subsampled the data until the correlation coefficient between and becomes non-significant at some significance threshold <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(p(r_{Cy}) &gt; \alpha\)</span>.</p>
<p>In our analyses, we used an <span class="math inline">\(\alpha\)</span> of 0.1. Note that this is more “strict”<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> than the conventionally used threshold (<span class="math inline">\(\alpha = 0.05\)</span>), but given that decoding analyses are often more sensitive to signal in the data (whether it is confounded or true signal), we chose to err on the safe side and counterbalance the data using a relatively strict threshold of <span class="math inline">\(\alpha = 0.1\)</span>.</p>
<p>Subsampling was done by iteratively removing samples that contribute most to the correlation between the confound and the target until the correlation becomes non-significant. In our empirical data in which brain size is positively correlated with gender (coded as male = 1, female = 0) this amounted to iteratively removing the male subject with the largest brain size and the female subject with the smallest brain size. This procedure optimally balances (1) minimizing the correlation between target and confound and (2) maximizing sample size. As an alternative to this “targeted subsampling”, we additionally implemented a procedure which draws random subsamples of a given sample size until it finds a subsample with a non-significant correlation coefficient. If such a subsample cannot be found after 10,000 random draws, sample size is decreased by 1, which is repeated until a subsample is found. This procedure resulted in much smaller subsamples than the targeted subsampling procedure (i.e., a larger power loss) since the optimal subsample is hard to find randomly.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> In the analyses below, therefore, we used the targeted subsampling procedure. Importantly, even with extreme power loss, random subsampling can cause the same biases as will be described for the targeted subsampling method below (cf. Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a> and Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-10">3.10</a> and Supplementary Figures <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S13">B.13</a> and <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S14">B.14</a>).</p>
<p>Then, given that the subsampled dataset is counterbalanced with respect to the confound, a random stratified K-fold cross-validation scheme is repeatedly initialized until a scheme is found in which <em>all</em> splits are counterbalanced as well <span class="citation">(cf. Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. This particular counterbalanced cross-validation scheme is subsequently used to cross-validate the MVPA pipeline. We implemented this post hoc counterbalancing method as a scikit-learn-style cross-validator class, available from the aforementioned Github repository (in the <code>counterbalance.py</code> module).</p>
</div>
<div id="confound-regression" class="section level4">
<h4><span class="header-section-number">3.2.3.2</span> Confound regression</h4>
<p>In our empirical analyses and simulations, we tested two different versions of confound regression, which we call “whole-dataset confound regression” (WDCR) and “cross-validated confound regression” (CVCR). In WDCR, we regressed out the confounds from the predictors <em>from the entire dataset at once</em>, i.e., before entering the iterative cross-validated MVPA pipeline <span class="citation">(the approach taken by Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>; Dubois et al., <a href="bibliography.html#ref-dubois2018resting" role="doc-biblioref">2018</a>; Kostro et al., <a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>; Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span>. Note that we can do this for all <span class="math inline">\(K\)</span> voxels at once using the closed-form OLS solution, in which we first estimated the parameters <span class="math inline">\(\hat{\beta}_{C}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{C} = (C^{T}C)^{-1}C^{T}X
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C\)</span> is an array in which the first column contained an intercept and the second column contained the confound brain size. Accordingly, <span class="math inline">\(\hat{\beta}_{C}\)</span> is an <span class="math inline">\(2 \times K\)</span> array. We then removed the variance associated with the confound from our neuroimaging data as follows:</p>
<p><span class="math display">\[\begin{equation}
X_{\mathrm{corr}} = X - C\hat{\beta}_{C}
\end{equation}\]</span></p>
<p>Now, <span class="math inline">\(X_{\mathrm{corr}}\)</span> is an array with the same shape as the original <span class="math inline">\(X\)</span> array, but without any variance that can be explained by confound, <span class="math inline">\(C\)</span> (i.e., <span class="math inline">\(X\)</span> is residualized with regard to <span class="math inline">\(C\)</span>).</p>
<p>In our proposed cross-validated version of confound regression <span class="citation">(which was mentioned but not evaluated by Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>, p. 25)</span>, “CVCR”, we similarly regressed out the confounds from the neuroimaging data, but instead of estimating <span class="math inline">\(\hat{\beta}_{C}\)</span> on the entire dataset, we estimated this within each fold of training data (<span class="math inline">\(X_{\mathrm{train}}\)</span>):</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{C,\mathrm{train}} = (C^{T}_{\mathrm{train}}C_{\mathrm{train}})^{-1}C^{T}_{\mathrm{train}}X_{\mathrm{train}}
\end{equation}\]</span></p>
<p>And we subsequently used these parameters (<span class="math inline">\(\hat{\beta}_{C,\mathrm{train}}\)</span>) to remove the variance related to the confound from both the train set (<span class="math inline">\(X_{\mathrm{train}}\)</span> and <span class="math inline">\(C_{\mathrm{train}}\)</span>):</p>
<p><span class="math display" id="eq:cvcr-train">\[\begin{equation}
X_{\mathrm{train, corr}} = X_{\mathrm{train}} - C_{\mathrm{train}}\hat{\beta}_{C,\mathrm{train}}
\tag{3.1}
\end{equation}\]</span></p>
<p>and the test test (<span class="math inline">\(X_{\mathrm{test}}\)</span> and <span class="math inline">\(C_{\mathrm{test}}\)</span>):</p>
<p><span class="math display" id="eq:cvcr-test">\[\begin{equation}
X_{\mathrm{test, corr}} = X_{\mathrm{test}} - C_{\mathrm{test}}\hat{\beta}_{C,\mathrm{test}}
\tag{3.2}
\end{equation}\]</span></p>
<p>Thus, essentially, CVCR is the cross-validated version of WDCR. One might argue that regressing the confound from the train set only, i.e., implementing only equation <a href="confounds-decoding.html#eq:cvcr-train">(3.1)</a>, not equation <a href="confounds-decoding.html#eq:cvcr-test">(3.2)</a>, is sufficient to control for confounds as it prevents the decoding model from relying on signal related to the confound. We evaluated this method and report the corresponding results in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S10">B.10</a>.</p>
<p>We implemented these confound regression techniques as a <em>scikit-learn</em> compatible transformer class, available in the open-source <em>skbold</em> Python package (<a href="https://github.com/lukassnoek/skbold">https://github.com/lukassnoek/skbold</a>) and in the aforementioned Github repository.</p>
</div>
<div id="control-for-confounds-during-pattern-estimation" class="section level4">
<h4><span class="header-section-number">3.2.3.3</span> Control for confounds during pattern estimation</h4>
<p>In addition to post hoc counterbalancing and confound regression, we also evaluated how well the “control for confounds during pattern estimation” method controls for the influence of confounds in decoding analyses of (simulated) fMRI data. The simulation methods and results can be found in the <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a>.</p>
</div>
</div>
<div id="analyses-of-simulated-data" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Analyses of simulated data</h3>
<p>In addition to the empirical evaluation of counterbalancing and confound regression in the gender decoding example, we ran three additional analyses on simulated data. First, we investigated the efficacy of the three confound control methods on synthetic data with known quantities of “true signal” and “confounded signal”, in order to detect potential biases. Second, we ran additional analyses on simulated data to investigate the positive bias in model performance observed after post hoc counterbalancing. Third, we ran additional analyses on simulated data to investigate the negative bias in model performance observed after WDCR. In the <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a>, we investigate whether the confound regression results generalize to (simulated) functional MRI data (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S1">B.1</a> and <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S2">B.2</a>).</p>
<div id="efficacy-analyses" class="section level4">
<h4><span class="header-section-number">3.2.4.1</span> Efficacy analyses</h4>
<p>In this simulation, we evaluated the efficacy of the three methods for confound control on synthetic data with a prespecified correlation between the confound and the target, <span class="math inline">\(r_{Cy}\)</span>, and varying amounts of “confounded signal” (i.e., the explained variance in <span class="math inline">\(y\)</span> driven by shared variance between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>). These simulations allowed us to have full control over (and knowledge of) the influence of both signal and confound in the data, and thereby help us diagnose biases associated with post hoc counterbalancing and confound regression.</p>
<p>Specifically, in this efficacy analysis, we generated hypothetical data sets holding the correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> constant, while varying the amount of true signal and confounded signal. We operationalized true signal as the squared semipartial Pearson correlation between <span class="math inline">\(y\)</span> and each feature in <span class="math inline">\(X\)</span>, controlled for <span class="math inline">\(C\)</span>. As such, we will refer to this term as <span class="math inline">\(\mathrm{signal}\ R^{2}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{signal}\ R^{2} = r_{y(X.C)}^{2}
\end{equation}\]</span></p>
<p>In the simulations reported and shown in the main article, we used <span class="math inline">\(r_{Cy} = 0.65\)</span>, which corresponds to the observed correlation between brain size and gender in our dataset. To generate synthetic data with this prespecified structure, we generated (1) a data matrix <span class="math inline">\(X\)</span> of shape <span class="math inline">\(N\times K\)</span>, (2) a target variable <span class="math inline">\(y\)</span> of shape <span class="math inline">\(N \times 1\)</span>, and (3) a confound variable <span class="math inline">\(C\)</span> of shape <span class="math inline">\(N \times P\)</span>. For all simulations, we used the following parameters: <span class="math inline">\(N = 200\)</span>, <span class="math inline">\(K = 5\)</span>, and <span class="math inline">\(P = 1\)</span> (i.e., a single confound variable). We generated <span class="math inline">\(y\)</span> as a categorical variable with binary values, <span class="math inline">\(y \in \{0, 1\}\)</span>, with equal class probabilities (i.e., 50%), given that most decoding studies focus on binary classification. We generated <span class="math inline">\(C\)</span> as a continuous random variable drawn from a standard normal distribution. We generated each feature <span class="math inline">\(X_{j}\)</span> as a linear combination of <span class="math inline">\(y\)</span> and <span class="math inline">\(C\)</span> plus Gaussian noise. Thus, for each predictor <span class="math inline">\(j = 1 \dots K\)</span> in <span class="math inline">\(X_{j}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
X_{j} = \beta_{y}y + \beta_{C}C + \epsilon, \epsilon \sim \mathcal{N}(0, \gamma)
\end{equation}\]</span></p>
<p>in which <span class="math inline">\(\beta_{y}\)</span> represented the weight given to <span class="math inline">\(y\)</span>, and <span class="math inline">\(\beta_{C}\)</span> represented the weight given to <span class="math inline">\(C\)</span> in the generation of the feature <span class="math inline">\(X_{j}\)</span>, and <span class="math inline">\(\mathcal{N}(0, \gamma)\)</span> is the normal distribution with zero mean and standard deviation <span class="math inline">\(\gamma\)</span>. The parameters <span class="math inline">\(\beta_{y}\)</span> and <span class="math inline">\(\beta_{C}\)</span> were both initialized with a value of 1. First, if the difference between the total variance explained and the sum of the desired signal <span class="math inline">\(R^2\)</span> and confound <span class="math inline">\(R^2\)</span> values was larger than 0.01, the standard deviation of the normal distribution from which the errors were drawn (i.e., <span class="math inline">\(\gamma\)</span>) was adjusted (decreased with 0.01 when the total <span class="math inline">\(R^2\)</span> is too low, increased with 0.01 when the total <span class="math inline">\(R^2\)</span> is too high), after which was generated again. This process was iterated until the target total <span class="math inline">\(R^2\)</span> value is found. Then, the total variance explained was partitioned into confound <span class="math inline">\(R^2\)</span> and signal <span class="math inline">\(R^2\)</span>. If one or both of these values differed from the targeted values by more than 0.01, the generative parameters <span class="math inline">\(\beta_{y}\)</span> and <span class="math inline">\(\beta_{C}\)</span> were adjusted: if signal <span class="math inline">\(R^2\)</span> is too low, was increased with 0.01, and decreased with 0.01 otherwise. If confound <span class="math inline">\(R^2\)</span> is too low, <span class="math inline">\(\beta_{C}\)</span> was increased with 0.01, and decreased with 0.01 otherwise. After adjusting these parameters, <span class="math inline">\(X_{j}\)</span> was generated again. This process was iterated until the data contain the desired “true signal” and “confounded signal”.</p>
<p>We evaluated the different methods for confound control for two values of signal <span class="math inline">\(R^2\)</span> (0.004, representing plausible null data,<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> and 0.1, representing a plausible true effect) and a range of confound <span class="math inline">\(R^2\)</span> values (in steps of 0.05: <span class="math inline">\(0.00, 0.05, 0.10, \dots , 0.35\)</span>). This simulation was iterated 10 times (with different partitions of the folds) to ensure the results were not influenced by random noise. Importantly, the specific scenario in which confound <span class="math inline">\(R^2\)</span> equals 0, which represents data without any confounded signal (<span class="math inline">\(r_{yX}^2\)</span>), served as “reference model performance” to which we can compare the efficacy the confound control methods. This comparison allowed us to detect potential biases.</p>
<p>After the data were generated, a baseline model (no confound control) and the three methods outlined above (post hoc counterbalancing, WDCR, and CVCR) were applied to the simulated data using the standard pipeline described in the <a href="confounds-decoding.html#confounds-decoding-methods-pipeline">Decoding pipeline</a> section (but without univariate feature selection) and compared to the reference performance.</p>
</div>
<div id="confounds-decoding-methods-counterbalancing-bias" class="section level4">
<h4><span class="header-section-number">3.2.4.2</span> Analysis of positive bias after post hoc counterbalancing</h4>
<p>As detailed below, post hoc counterbalancing did not lead to the expected decrease in model performance; instead, there appeared to be a trend towards an <em>increase</em> in model performance. To further investigate the cause of this unexpected result, we simulated a multivariate normal dataset with three variables, reflecting our data (<span class="math inline">\(X\)</span>), target (<span class="math inline">\(y\)</span>), and confound (<span class="math inline">\(C\)</span>), with 1000 samples (<span class="math inline">\(N\)</span>) and a single feature (<span class="math inline">\(K = 1\)</span>). We iterated this data generation process 1000 times and subsequently selected the dataset which yielded the largest (positive) difference between model performance after post hoc counterbalancing versus no confound control. In other words, we used the dataset in which the counterbalancing issue was most apparent. While not necessarily representative of typical (neuroimaging) datasets, this process allowed us to explain and visualize how it is possible that model performance increases after counterbalancing the data.</p>
<p>To generate data from a multivariate normal distribution, we first generated variance-covariance matrices with unit variance for all variables, so that covariances can be interpreted as correlations. The covariances in the matrix were generated as pairwise correlations (<span class="math inline">\(r_{yX}\)</span>, <span class="math inline">\(r_{Cy}\)</span>, <span class="math inline">\({r_CX}\)</span>), each sampled from a uniform distribution with range <span class="math inline">\([-0.65, 0.65]\)</span>. We generated data using such prespecified correlation structure because the relative increase in model performance after counterbalancing did not appear to occur when generating completely random (normally distributed) data. Moreover, we restricted the range of the uniform distribution from which the pairwise correlations are drawn to <span class="math inline">\([-0.65, 0.65]\)</span> because a larger range can result in covariance matrices that are not positive-semidefinite. After generating the three variables, we binarized the target variable (<span class="math inline">\(y\)</span>) using a mean-split (<span class="math inline">\(y = 0\)</span> if <span class="math inline">\(y &lt; \bar{y}\)</span>, <span class="math inline">\(y = 1\)</span> otherwise) to frame the analysis as a classification problem rather than a regression problem.</p>
<p>We then subsampled the selected dataset using our post hoc counterbalancing algorithm and subsequently ran the decoding pipeline (without univariate feature selection) on the subsampled (“retained”) data in a 10-fold stratified cross-validation scheme. Notably, we cross-validated our fitted pipeline not only to the left-out <em>retained</em> data, but also to the data that did not survive the subsampling procedure (the <em>rejected</em> data; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-3">3.3</a>). Across the 10 folds, we kept track of two statistics from the retained and rejected samples: (1) the classification performance, and (2) the signed distance to the decision boundary. Negative distances in binary classification (in simple binary classification with <span class="math inline">\(y \in \{0, 1\}\)</span>) reflect a prediction of the sample as <span class="math inline">\(y = 0\)</span>, while positive distances reflect a prediction of the sample as <span class="math inline">\(y = 1\)</span>. As such, a correctly classified sample of class 0 has a negative distance from the decision boundary, while a correctly classified sample of class 1 has a positive distance from the decision boundary. Here, however, we wanted to count the distance of samples that are on the “incorrect” side of the decision boundary as <em>negative</em> distances, while counting the distance of samples that are on the “correct” side of the decision boundary as positive distances. To this end, we used a “re-coded” version of the target variable (<span class="math inline">\(y^{*} = -1\)</span> if <span class="math inline">\(y = 0\)</span>, <span class="math inline">\(y^{*} = 1\)</span> otherwise) and multiplied it with the distance. Consequently, negative distances of <em>correct</em> samples of condition 0 become positive and positive distances of <em>incorrect</em> samples of condition 0 become negative (by multiplying them by <span class="math inline">\(-1\)</span>). As such, we calculated the signed distance from the decision boundary (<span class="math inline">\(\delta_{i}\)</span>) for any sample <span class="math inline">\(i\)</span> as:</p>
<p><span class="math display">\[\begin{equation}
\delta_{i} = y^{*}(w^{T}X_{i} + b)
\end{equation}\]</span></p>
<p>in which <span class="math inline">\(w\)</span> refers to the feature weights (coefficients) and <span class="math inline">\(b\)</span> refers to the intercept term. Any differences in these two statistics (proportion correctly classified and signed distance to the classification boundary) between the retained and rejected samples may signify biases in model performance estimates (i.e., better cross-validated model performance on the retained data than on the rejected data would confirm positive bias, as it indicates that subsampling tends to reject hard-to-classify samples). We applied this analysis also to the empirical data (separately for the different values of <span class="math inline">\(K\)</span>) to show that the effect of counterbalancing, as demonstrated using simulated data, also occurs in the empirical data.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-3"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_3.png" alt="Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates."  />
<p class="caption">
Figure 3.3: Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.
</p>
</div>

</div>
<div id="analysis-of-negative-bias-after-wdcr" class="section level4">
<h4><span class="header-section-number">3.2.4.3</span> Analysis of negative bias after WDCR</h4>
<p>As also detailed below, WDCR can lead to significantly below chance accuracy. To investigate the cause of this below chance performance (and to demonstrate that CVCR does not lead to such results), we performed two follow-up simulations. The first follow-up simulation shows that the occurrence of below chance accuracy depends on the distribution of feature-target correlations <span class="citation">(<span class="math inline">\(r_{yX}\)</span>; see for a similar argument Jamalabadi et al., <a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span>, and the second follow-up simulation shows that WDCR artificially narrows this distribution. This artificial narrowing of the distribution is exacerbated both by an increasing number of features (<span class="math inline">\(K\)</span>), as well as higher correlations between the target and confound (<span class="math inline">\(r_{Cy}\)</span>).</p>
<p>In the first simulation, we simulated random null data (drawn from a standard normal distribution) with 100 samples (<span class="math inline">\(N\)</span>) and 200 features (<span class="math inline">\(K\)</span>), as well as a binary target feature (<span class="math inline">\(y \in \{0, 1\}\)</span>). We then calculated the cross-validated prediction accuracy using the standard pipeline (without univariate feature selection) described in the <a href="confounds-decoding.html#confounds-decoding-methods-pipeline">Decoding pipeline</a> section; we iterate this process 500 times. Then, we show that the variance of the cross-validated accuracy is accurately predicted by the standard deviation (i.e., “width”) of the distribution of correlations between the features and the target (<span class="math inline">\(r_{yX_{j}}\)</span> with <span class="math inline">\(j = 1 \dots K\)</span>), which we will denote by <span class="math inline">\(sd(r_{yX})\)</span>. Importantly, we show that below chance accuracy likely occurs when the standard deviation of the feature-target correlation distribution is lower than the standard deviation of the sampling distribution of the Pearson correlation coefficient parameterized with the same number of samples (<span class="math inline">\(N = 200\)</span>) and the same effect size (i.e., <span class="math inline">\(\rho = 0\)</span>, because we simulated random null data). The sampling distribution of the Pearson correlation coefficient is described by <span class="citation">Kendall &amp; Stuart (<a href="bibliography.html#ref-kendall1973functional" role="doc-biblioref">1973</a>)</span>. When <span class="math inline">\(\rho = 0\)</span> (as in our simulations), the equation is as follows:</p>
<p><span class="math display">\[\begin{equation}
f(r; N) = (1 -r^2)^{\frac{N-4}{2}}[\mathcal{B}](\frac{1}{2}, \frac{N-2}{2})^{-1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}(a, b)\)</span> represents the Beta-function.</p>
<p>Then, in a second simulation, we similarly simulated null data as in the previous simulation, but now we also generate a continuous confound (<span class="math inline">\(C\)</span>) with a varying correlation with the target (<span class="math inline">\(r_{Cy} \in \{0.0, 0.1, 0.2, \dots, 1.0\}\)</span>). Before subjecting the data to the decoding pipeline, we regressed out the confound from the data (i.e., WDCR). We did this for different numbers of features (<span class="math inline">\(K \in \{1, 5, 10, 50, 100, 500, 1000\}\)</span>). Then, we applied CVCR on the simulated data as well for comparison.</p>
</div>
</div>
</div>
<div id="results" class="section level2">
<h2><span class="header-section-number">3.3</span> Results</h2>
<div id="influence-of-brain-size" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Influence of brain size</h3>
<p>Before evaluating the different methods for confound control, we determined whether brain size is truly a confound given our proposed definition (“a variable that is not of primary interest, correlates with the target, and is encoded in the neuroimaging data”). We evaluated the relationship between the target and the confound in two ways. First, we calculated the (point-biserial) correlation between gender and brain size, which was significant for both the estimation based on white matter, <span class="math inline">\(r(216) = .645, p &lt; 0.001\)</span>, and the estimation based on grey matter, <span class="math inline">\(r(216) = .588, p &lt; 0.001\)</span>, corroborating the findings by <span class="citation">Smith &amp; Nichols (<a href="bibliography.html#ref-Smith2018-th" role="doc-biblioref">2018</a>)</span>. Second, as recommended by <span class="citation">Görgen et al. (<a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>, who argue that the potential influence of confounds can be discovered by running a classification analysis using the confound as the (single) feature predicting the target, we ran our decoding pipeline (without univariate feature selection) using brain size as a single feature to predict gender. This analysis yielded a mean classification performance (<span class="math inline">\(F_{1}\)</span> score) of 0.78 (<em>SD</em> = .10) when using brain size estimated from white matter and 0.81 (<em>SD</em> = .09) when using brain size estimated from gray matter, which are both significant with <span class="math inline">\(p &lt; 0.001\)</span> (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-4">3.4</a>A).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-4"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_4.png" alt="A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \(\rho = 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection."  />
<p class="caption">
Figure 3.4: A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual <span class="math inline">\(F_{1}\)</span> scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (<span class="math inline">\(r_{XC}\)</span>), overlayed with the analytic sampling distribution of correlation coefficients when <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 217\)</span>, for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule <span class="citation">(Scott, <a href="bibliography.html#ref-scott1979optimal" role="doc-biblioref">1979</a>)</span> for bandwidth selection.
</p>
</div>

<p>To estimate whether brain size is encoded in the neuroimaging data, we compared the distribution of bivariate correlation coefficients (of each voxel with brain size) with the sampling distribution of correlation coefficients when <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 217\)</span> (see section <a href="confounds-decoding.html#confounds-decoding-results-wdcr-bias">Analysis of negative bias after WDCR</a> for details). Under the null hypothesis that there are no correlations between brain size and voxel intensities, each individual correlation coefficient between a voxel and the confound can be regarded as an independent sample with <span class="math inline">\(N = 217\)</span> (ignoring correlations between voxels for simplicity). Because <span class="math inline">\(K\)</span> is very large for both the VBM and TBSS data, the empirical distribution of correlation coefficients should, under the null hypothesis, approach the analytic distribution of correlation coefficients parametrized by <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 217\)</span>. Contrarily, the density plots in Fig. <a href="confounds-decoding.html#fig:fig-confounds-decoding-4">3.4</a>B clearly show that the observed correlation coefficients distribution does not follow the sampling distribution (with both an increase in variance and a shift of the mode). This indicates that at least some of the correlation coefficients between voxel intensities and brain size are extremely unlikely under the null hypothesis. Note that this interpretation is contingent on the assumption that the relation between brain size and VBM/TBSS data is linear. In the Supplementary Materials and Results (Supplementary Figures <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S7">B.7</a>-<a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S9">B.9</a>), we provide some evidence for the validity of this assumption.</p>
</div>
<div id="baseline-model-no-confound-control" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Baseline model: no confound control</h3>
<p>In our baseline model on the empirical data, for different numbers of voxels, we predicted gender from structural MRI data (VBM and TBSS) without controlling for brain size (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-5">3.5</a>). The results show significant above chance performance of the MVPA pipeline based on both the VBM data and the TBSS data. All performance scores averaged across folds were significant (<span class="math inline">\(p &lt; 0.001\)</span>).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-5"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_5.png" alt="Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p &lt; 0.001\), ** = \(p &lt; 0.01\), * = \(p &lt; 0.05\)."  />
<p class="caption">
Figure 3.5: Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average <span class="math inline">\(F_{1}\)</span> score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = <span class="math inline">\(p &lt; 0.001\)</span>, ** = <span class="math inline">\(p &lt; 0.01\)</span>, * = <span class="math inline">\(p &lt; 0.05\)</span>.
</p>
</div>

<p>These above chance performance estimates replicate previous studies on gender decoding using structural MRI data <span class="citation">(Del Giudice et al., <a href="bibliography.html#ref-Del_Giudice2016-ns" role="doc-biblioref">2016</a>; Rosenblatt, <a href="bibliography.html#ref-Rosenblatt2016-oy" role="doc-biblioref">2016</a>; Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span> and will serve as a baseline estimate of model performance to which the confound control methods will be compared.</p>
<p>In the next three subsections, we will report the results from the three discussed methods to control for confounds: post hoc counterbalancing, whole-dataset confound regression (WDCR), and cross-validated confound regression (CVCR).</p>
</div>
<div id="post-hoc-counterbalancing" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Post hoc counterbalancing</h3>
<div id="empirical-results" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Empirical results</h4>
<p>In order to decorrelate brain size and gender (i.e., <span class="math inline">\(r_{Cy} &gt; 0.1\)</span>), our subsampling algorithm selected 117 samples in the VBM data (i.e., a sample size reduction of 46.1%) and 131 samples in the TBSS data (i.e., a reduction of 39.6%). The model performance for different values of (number of voxels) are shown in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-6">3.6</a>. Contrary to our expectations, the predictive accuracy of our decoding pipeline after counterbalancing was similar to baseline performance. This is particularly surprising in light of the large reductions in sample size, which results in a substantial loss in power, which in turn is expected to lead to lower model performance.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-6"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_6.png" alt="Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p &lt; 0.001\), ** = \(p &lt; 0.01\), * = \(p &lt; 0.05\)."  />
<p class="caption">
Figure 3.6: Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) <span class="math inline">\(F_{1}\)</span> score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = <span class="math inline">\(p &lt; 0.001\)</span>, ** = <span class="math inline">\(p &lt; 0.01\)</span>, * = <span class="math inline">\(p &lt; 0.05\)</span>.
</p>
</div>

<p>One could argue that the lack of expected decrease in model performance after counterbalancing can be explained by the possibility that the subsampling and counterbalancing procedure just leads to the selection of different features during univariate feature selection compared to the baseline model. In other words, the increase in model performance may be caused by the feature selection function, which selects “better” voxels (i.e., containing more “robust” signal), resulting in similar model performance in spite of the reduction in sample size. However, this does not explain the similar scores for counterbalancing and the baseline model when using all voxels (the data points at <span class="math inline">\(K \mathrm{voxels} = \dots \mathrm{(all)}\)</span> in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-6">3.6</a>). Another possibility for the relative increase in model performance based on the counterbalanced data versus the baseline model is that counterbalancing increased the amount of signal in the data. Indeed, counterbalancing appeared to increase the (absolute) correlations between the data and the target (<span class="math inline">\(r_{yX}\)</span>), which is visualized in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-7">3.7</a>, suggesting an increase in signal.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-7"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_7.png" alt="Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection."  />
<p class="caption">
Figure 3.7: Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule <span class="citation">(Scott, <a href="bibliography.html#ref-scott1979optimal" role="doc-biblioref">1979</a>)</span> for bandwidth selection.
</p>
</div>

<p>This apparent increase in the correlations between the target and neuroimaging data goes against the intuition that removing the influence of a confound that is highly correlated with the target will reduce decoding performance. To further investigate this, we replicated this effect of post hoc counterbalancing on simulated data, as described in the next section (<a href="confounds-decoding.html#confounds-decoding-results-cb-efficacy">Efficacy analyses</a>), and additionally investigated the cause of the negative bias observed after WDCR using a separate set of simulations.</p>
</div>
<div id="confounds-decoding-results-cb-efficacy" class="section level4">
<h4><span class="header-section-number">3.3.3.2</span> Efficacy analysis</h4>
<p>To evaluate the efficacy of the three confound control methods, we simulated data in which we varied the strength of confound <span class="math inline">\(R^2\)</span> and signal <span class="math inline">\(R^2\)</span>, after which we applied the three confound control methods to the data. The results of this analysis show that counterbalancing maintains chance-level model performance when there is almost no signal in the data (i.e., signal <span class="math inline">\(R^2 = 0.004\)</span>; Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a>, left graph, green line). However, when there is some signal (i.e., signal <span class="math inline">\(R^2 = 0.1\)</span>; Fig. 8, right graph), we observed that counterbalancing yields similar or even higher scores than the baseline model, replicating the effects observed in the empirical analyses.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-8"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_8.png" alt="Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a “reference performance” for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later."  />
<p class="caption">
Figure 3.8: Results from the different confound control methods on simulated data without any experimental effect (signal <span class="math inline">\(R^2 = 0.004\)</span>; left graph) and with some experimental effect (signal <span class="math inline">\(R^2 = 0.1\)</span>; right graph) for different values of confound <span class="math inline">\(R^2\)</span>. The orange line represents the average performance (±1 SD) when confound <span class="math inline">\(R^2 = 0\)</span>, which serves as a “reference performance” for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, <span class="math inline">\(r_{yC}\)</span>, is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.
</p>
</div>

<p>As is apparent from Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a> (right panel), when there is some signal, the counterbalanced data seem to yield better performance than the baseline model only for relatively low confound <span class="math inline">\(R^2\)</span> values (confound <span class="math inline">\(R^2 &lt; 0.15\)</span>). As suggested by our findings in the empirical data (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-7">3.7</a>), we hypothesized that the observed improvement in model performance after counterbalancing was caused by the increase in correlations between the target and features in the neuroimaging data. In support of this hypothesis, Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-9">3.9</a> illustrates the relations between the strength of the confound (confound <span class="math inline">\(R^2\)</span>, color coded), the increase in correlations after post hoc counterbalancing (<span class="math inline">\(\delta r_{yX} = r_{yX}^{\mathrm{after}} - r_{yX}^{\mathrm{after}}\)</span>; x-axis) for each confound <span class="math inline">\(R^2\)</span>, and the resulting difference in model performance (<span class="math inline">\(\mathrm{ACC}_{\mathrm{CB}} - \mathrm{ACC}_{\mathrm{baseline}}\)</span>; y-axis). The figure shows that the increase or decrease in accuracy after counterbalancing (compared to baseline) depends on <span class="math inline">\(\delta r_{yX}\)</span> (<span class="math inline">\(r(79) = .922\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>), which in turn depends on confound <span class="math inline">\(R^2\)</span> (<span class="math inline">\(r(79) = -0.987\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>). To reiterate, these differences in model performance are only due to the post hoc counterbalancing procedure and not due to varying signal in the simulated data. The effect of post hoc counterbalancing on model performance thus seems to depend on the strength of the confound.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-9"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_9.png" alt="The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm{ACC}_{\mathrm{CB}} - \mathrm{ACC}_{\mathrm{baseline}}\))."  />
<p class="caption">
Figure 3.9: The relationship between the increase in correlations between target and data (<span class="math inline">\(r_{yX}\)</span>) after subsampling, confound <span class="math inline">\(R^2\)</span>, difference in model performance (here: accuracy) between the counterbalance model and baseline model (<span class="math inline">\(\mathrm{ACC}_{\mathrm{CB}} - \mathrm{ACC}_{\mathrm{baseline}}\)</span>).
</p>
</div>

<p>While this relationship in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-9">3.9</a> might be statistically interesting, it does not explain why post hoc counterbalancing tends to increase the correlations between neuroimaging data and target, and even outperforms the baseline model when confound <span class="math inline">\(R^2\)</span> is low and some signal is present. More importantly, it does not tell us whether the post hoc counterbalancing procedure uncovers signal that is truly related to the target — in which case the procedure suppresses noise — or inflates performance estimates and thereby introduces positive bias. Therefore, in the next section, we report and discuss results from a follow-up simulation that intuitively shows why post hoc counterbalancing leads to an increase in performance, and furthermore shows that this increase is in fact a positive bias.</p>
</div>
<div id="analysis-of-positive-bias-after-post-hoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.3.3.3</span> Analysis of positive bias after post hoc counterbalancing</h4>
<p>With this follow-up analysis, we aimed to visualize the scenario in which post hoc counterbalancing leads to a clearly better performance than model performance without confound control. As such, we generated 1000 data sets using a covariance matrix that we knew leads to a large difference between the baseline model and model performance after counterbalancing (i.e., data with a low confound <span class="math inline">\(R^2\)</span>). From these 1000 datasets, we selected the dataset that yielded the largest difference for our visualization (see the <a href="confounds-decoding.html#confounds-decoding-methods-counterbalancing-bias">Analysis of positive bias after post hoc counterbalancing</a> section in the Methods for details).</p>
<p>The data that yielded the largest difference (i.e., a performance increase from 0.613 to 0.804, a 31% increase) are visualized in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-10">3.10</a>. Each sample’s confound value (<span class="math inline">\(C\)</span>) is plotted against its feature value (<span class="math inline">\(X\)</span>), both before subsampling (upper scatter plot) and after subsampling (lower scatter plot). From visual inspection, it appears that the samples rejected by the subsampling procedure (i.e., the samples with the white border) have relatively large absolute values of the confound variable, which tend to lie close to or on the “wrong” side of the classification boundary (i.e., the dashed black line) in this specific configuration of the data. In other words, subsampling seems to reject samples that are harder to classify or would be incorrectly classified based on the data (here, the single feature of <span class="math inline">\(X\)</span>). The density plots in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-10">3.10</a> show the same effect in a different way: while the difference in the modes of the distributions of the confound (<span class="math inline">\(C\)</span>) between classes is reduced after subsampling (i.e., the density plots parallel to the y-axis), the difference in the modes of the distributions of the data (<span class="math inline">\(X\)</span>) between classes is actually increased after subsampling (i.e., the density plots parallel to the x-axis).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-10"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_10.png" alt="Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection."  />
<p class="caption">
Figure 3.10: Both scatterplots visualize the relationship between the data (<span class="math inline">\(X\)</span> with <span class="math inline">\(K=1\)</span>, on the x-axis), the confound (<span class="math inline">\(C\)</span>, on the y-axis) and the target (<span class="math inline">\(y\)</span>). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of <span class="math inline">\(y = 0\)</span> and a green background means a prediction of <span class="math inline">\(y = 1\)</span>). The density plots parallel to the y-axis depict the distribution of the confound (<span class="math inline">\(C\)</span>) for the samples in which <span class="math inline">\(y = 0\)</span> (blue) and in which <span class="math inline">\(y = 1\)</span> (green). The density plots parallel to x-axis depict the distribution of the data (<span class="math inline">\(X\)</span>) for the samples in which <span class="math inline">\(y = 0\)</span> (blue) and in which <span class="math inline">\(y = 1\)</span> (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule <span class="citation">(Scott, <a href="bibliography.html#ref-scott1979optimal" role="doc-biblioref">1979</a>)</span> for bandwidth selection.
</p>
</div>

<p>We quantified this effect of subsampling by comparing the signed distance from the decision boundary (i.e., the dashed line in the upper scatter plot) between the retained samples and the rejected (subsampled) samples, in which a larger distance from the decision boundary reflects a higher confidence of the classifier’s prediction (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-3">3.3</a> for a visualization of this method). Indeed, we found that samples that are removed by subsampling lie significantly closer to (or on the “wrong” side of) the decision boundary (<em>M</em> = -.358, <em>SD</em> = .619) than samples that are retained after subsampling (<em>M</em> = .506, <em>SD</em> = .580), as indicated by a independent samples <em>t</em>-test, <span class="math inline">\(t(998) = 22.32\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>. Also (which follows from the previous observation), samples that would have been removed by subsampling are more often classified incorrectly (75% incorrect) than the samples that would have been retained by subsampling (20% incorrect), as indicated by a chi-squared test, <span class="math inline">\(\chi^{2} = 270.29\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>.</p>
<p>To show that the same effect (i.e., removing samples that tend to be hard to classify or would be wrongly classified) occurred in the empirical data after counterbalancing as well, we applied the same analysis of comparing model performance and distance to boundary between the retained and rejected samples to the empirical data. Indeed, across all different numbers of voxels (<span class="math inline">\(K\)</span>), the retained samples were significantly more often classified correctly (Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-11">3.11</a>A) and had a significantly larger distance to the classification boundary (Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-11">3.11</a>B) than the rejected samples. This demonstrates that the same effect of post hoc counterbalancing, as shown in the simulated data, likely underlies the increase in model performance of the counterbalanced data relative to the baseline model in the empirical data.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-11"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_11.png" alt="A) The proportion of samples classified correctly, separately for the “retained” samples (blue line) and “rejected” samples (green line); the dashed line represents chance level (0.5). B) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the “wrong” side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p &lt; 0.001\), ** = \(p &lt; 0.01\), * = \(p &lt; 0.05\)."  />
<p class="caption">
Figure 3.11: <strong>A</strong>) The proportion of samples classified correctly, separately for the “retained” samples (blue line) and “rejected” samples (green line); the dashed line represents chance level (0.5). <strong>B</strong>) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the “wrong” side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = <span class="math inline">\(p &lt; 0.001\)</span>, ** = <span class="math inline">\(p &lt; 0.01\)</span>, * = <span class="math inline">\(p &lt; 0.05\)</span>.
</p>
</div>

<p>One can wonder how much the occurrence of these observed biases in post hoc counterbalancing depends on the specific method of subsampling used. Random subsampling led to qualitatively similar results as targeted subsampling (cf. Supplementary Figures <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S13">B.13</a> and <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S14">B.14</a> with random subsampling). Instead, the bias is introduced through features that weakly correlate with the target in the whole sample, but strongly in subsamples where there is no correlation between target and the confound (features which, as our results show, exist in the neuroimaging data). That is, the bias is an indirect result of decorrelating target and confound in the sample, which is an essential step in post hoc counterbalancing (in fact, it is the <em>goal</em> of counterbalancing). For this reason, we consider it unlikely (but not impossible) that there exists a way to subsample data without introducing biases.</p>
<p>In summary, removing a subset of observations to correct for the influence of a confound can induce substantial bias by removing samples that are harder to classify using the available data. The bias itself can be subtle (e.g., in our empirical results, the predictive performance falls in a realistic range of predictive performances), and could remain undetected when present. Therefore, we believe that post hoc counterbalancing by subsampling the data is an inappropriate method to control for confounds.</p>
</div>
</div>
<div id="whole-dataset-confound-regression-wdcr" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Whole-dataset confound regression (WDCR)</h3>
<div id="empirical-results-1" class="section level4">
<h4><span class="header-section-number">3.3.4.1</span> Empirical results</h4>
<p>In addition to post hoc counterbalancing, we evaluated the efficacy of “whole-dataset confound regression” (WDCR), i.e. regressing out the confound from each feature separately using all samples from the dataset to control for confounds. Compared to the baseline model, WDCR yielded a strong decrease in performance, even dropping (significantly) below chance for all TBSS analyses and a subset of the VBM analyses (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-12">3.12</a>).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-12"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_12.png" alt="Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p &lt; 0.001\), ** = \(p &lt; 0.01\), * = \(p &lt; 0.05\)."  />
<p class="caption">
Figure 3.12: Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average <span class="math inline">\(F_{1}\)</span> score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = <span class="math inline">\(p &lt; 0.001\)</span>, ** = <span class="math inline">\(p &lt; 0.01\)</span>, * = <span class="math inline">\(p &lt; 0.05\)</span>.
</p>
</div>

<p>This strong (and implausible) reduction in model performance after WDCR is investigated in more detail in the next two sections on the results from the simulations.</p>
</div>
<div id="efficacy-analysis" class="section level4">
<h4><span class="header-section-number">3.3.4.2</span> Efficacy analysis</h4>
<p>The results from the analyses investigating the efficacy of the confound control methods (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a>) show that WDCR accurately corrects for the confound in both in data without signal (i.e., when signal <span class="math inline">\(R^2 = 0.004\)</span>) and in data with some signal (i.e., when signal <span class="math inline">\(R^2 = 0.1\)</span>), as evident from the fact that the performance after WDCR is similar to the reference performance. This result (i.e., plausible performance after confound control) stands in contrast to the results from the empirical analyses, which is why we ran a follow-up analysis on simulated data to investigate this specific issue.</p>
</div>
<div id="confounds-decoding-results-wdcr-bias" class="section level4">
<h4><span class="header-section-number">3.3.4.3</span> Analysis of negative bias after WDCR</h4>
<p>Inspired by the work of <span class="citation">Jamalabadi et al. (<a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span> on below chance accuracy in decoding analyses, we ran several follow-up analyses to get insight into why WDCR leads to below chance model performance. As Jamalabadi et al. show, below chance model performance occurs when the data contain little signal. In our first follow-up simulation, we sought to refine the explanation of the cause of below chance model performance by linking it to the observed standard deviation of the empirical distribution of correlations between the data (<span class="math inline">\(X\)</span>) and the target (<span class="math inline">\(y\)</span>). To do so, we simulated random data (<span class="math inline">\(X\)</span>) and a binary target (<span class="math inline">\(y \in \{0, 1\}\)</span>) and estimated (per fold) the cross-validated classification accuracy using the standard pipeline described in the methods section. We repeated this process 500 times, yielding 500 data sets. The expected <em>average</em> predictive accuracy for each dataset is 0.5, but this varies randomly across folds and iterations. We hypothesized that this variance can be explained by the standard deviation (“width”) of the initial feature-target correlation distribution, <em>sd</em>(<span class="math inline">\(r_{Xy}\)</span>): narrower distributions may yield relatively lower cross-validated classification accuracy than relatively wider feature-target correlation distributions. Indeed, we find that the initial standard deviation of this distribution is significantly correlated with the cross-validated accuracy, <span class="math inline">\(r(499) = 0.73\)</span>, <span class="math inline">\(p &lt; 0.001\)</span> (Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-13">3.13</a>A). Importantly, we find that this relationship holds for different values of <span class="math inline">\(N\)</span> (see Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S15">B.15</a>, for different sizes of the test set (see Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S16">B.16</a>), and for different sizes of <span class="math inline">\(K\)</span> (see Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S17">B.17</a>).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-13"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_13.png" alt="A) The relationship between the standard deviation of the distribution of feature-target correlations, sd(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \(\rho = 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure B.15 for the same plot with different values for \(N\)). B) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) “flips” between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. C) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation (3.3)) and accuracy."  />
<p class="caption">
Figure 3.13: <strong>A</strong>) The relationship between the standard deviation of the distribution of feature-target correlations, <em>sd</em>(<span class="math inline">\(r_{yX}\)</span>), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 100\)</span> (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S15">B.15</a> for the same plot with different values for <span class="math inline">\(N\)</span>). <strong>B</strong>) The relationship between the proportion of features of which the sign of their correlation with the target (<span class="math inline">\(r_{Xy}\)</span>) “flips” between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. <strong>C</strong>) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation <a href="confounds-decoding.html#eq:dataset-shift">(3.3)</a>) and accuracy.
</p>
</div>

<p>This observation, then, begs the question: <em>why</em> do narrower-than-chance correlation distributions lead to below chance accuracy? One potential explanation of below chance accuracy is that the classifier may learn a particular (linear) relationship between features and the target in the train set (e.g., <span class="math inline">\(r_{Xy} = 0.05\)</span>), while the sign of this relationship is “flipped” in the test set <span class="citation">(e.g., <span class="math inline">\(r_{Xy} = -0.03\)</span>; see Jamalabadi et al., <a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span>, which is known in the machine learning literature as “dataset shift” <span class="citation">(Quionero-Candela et al., <a href="bibliography.html#ref-Quionero-Candela2009-ge" role="doc-biblioref">2009</a>)</span>. This situation would lead classifiers to predict the exact opposite classes for samples in the test set, leading to below chance accuracy. In the results of our simulated data, the standard deviation of the feature-target distribution was indeed significantly negatively correlated with the proportion of features that flipped the sign of their correlation between the train set and test set, <span class="math inline">\(r(499) = -.687\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>. This means that a higher density of feature-target correlations around 0 (i.e., a narrower width of the corresponding distribution) leads to more “sign flips”. This phenomenon of “sign flipping” has been reported before in the context of (a priori) counterbalancing of categorical variables (<span class="math inline">\(X\)</span>) with respect to the target (<span class="math inline">\(y\)</span>), where it was observed that complete counterbalancing led to consistent “sign flipping” and consequently 0% accuracy <span class="citation">(Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. Similarly, we found that the proportion of features that flip sign was significantly negatively correlated with accuracy, <span class="math inline">\(r = -.565\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>, indicating that larger proportions of features that flip sign leads to lower accuracy (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-13">3.13</a>B). Interestingly, at a proportion of 0.5, accuracy is approximately at chance level (0.5; dashed lines in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-13">3.13</a>B).</p>
<p>This relationship between “sign flipping” and accuracy, however, leaves room for improvement in terms of explaining the variance of accuracy scores. Therefore, we sought to further refine our “model” of accuracy by defining dataset shift not by the proportion of sign flips, but by the average <em>difference</em> between the feature-target correlations between the train set and test set. Moreover, because not all features contribute equally strongly to a classifier’s prediction (i.e., they are weighted), we furthermore weighed each feature’s “shift” by the associated classifier weight (<span class="math inline">\(w_{j}\)</span>). Formally, we estimated dataset shift (<span class="math inline">\(\hat{ds}\)</span>) thus as follows:</p>
<p><span class="math display" id="eq:dataset-shift">\[\begin{equation}
\hat{ds} = \frac{1}{K}\sum_{j=1}^{K}(r_{X_{j,\mathrm{train}},y_\mathrm{train}} - r_{X_{j,\mathrm{test}},y_\mathrm{test}})w_{j}
\tag{3.3}
\end{equation}\]</span></p>
<p>Indeed, the correlation between this particular operationalization of “dataset shift” and accuracy across simulations was much higher than just the proportion of sign flips, <span class="math inline">\(r(499) = −0.934\)</span> (Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-13">3.13</a>B).</p>
<p>Having established the relation between the standard deviation of the initial feature-target correlation distribution and accuracy, we followed up our simulation by investigating specifically the effect of WDCR on the standard deviation of the correlation distribution. We investigated this by simulating data with different strengths of the correlation between the confound and the target (<span class="math inline">\(r_{Cy}\)</span>) and the number of features (<span class="math inline">\(K\)</span>). From Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-14">3.14</a>A, it is clear that, while the expected chance level is 0.5 in all cases, model performance quickly drops below chance for increasing correlations between the target and the confound, as well as for increasing numbers of features; even leading to a model performance of 0% when the confound is perfectly correlated with the target and when using 1000 features. Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-14">3.14</a>C shows that, indeed, higher values lead to narrower correlation distributions, which is shown in Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-14">3.14</a>D to yield relatively lower accuracy scores.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-14"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_14.png" alt="A) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). B) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. C) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, sd(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. D) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure B.18 for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data."  />
<p class="caption">
Figure 3.14: <strong>A</strong>) The effect of WDCR on data varying in the correlation of the confound with the target (<span class="math inline">\(r_{Cy}\)</span>; x-axis) and the number of features (<span class="math inline">\(K\)</span>; different lines). <strong>B</strong>) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. <strong>C</strong>) The relation between the correlation of the confound with the target (<span class="math inline">\(r_{Cy}\)</span>) and the standard deviation of the feature-target correlation distribution, <em>sd</em>(<span class="math inline">\(r_{yX}\)</span>) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. <strong>D</strong>) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when <span class="math inline">\(K = 100\)</span>; see Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S18">B.18</a> for visualizations of this effect for different values of <span class="math inline">\(K\)</span>). The data depicted in all panels are null data.
</p>
</div>

<p>In summary, our simulations show that below chance accuracy is accurately predicted by the standard deviation (i.e., “width”) of the distribution of empirical feature-target correlations and that WDCR reduces this standard deviation, which explains why the empirical analyses yielded below chance model performance (especially for larger numbers of voxels).</p>
</div>
</div>
<div id="cross-validated-confound-regression-cvcr" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Cross-validated confound regression (CVCR)</h3>
<div id="empirical-results-2" class="section level4">
<h4><span class="header-section-number">3.3.5.1</span> Empirical results</h4>
<p>As the results from the empirical analyses and simulations suggest, the use of WDCR is problematic because of the partitioning of the dataset into a separate train set and test set after confound regression. As such, our proposed cross-validated confound regression (CVCR) methods suggests to move the confound regression procedure inside the cross-validation loop, thereby also cross-validating this step. As expected, compared to the baseline model (i.e., no confound control), the results from the empirical analyses using CVCR show reduced (but not below chance) model performance for both VBM and TBSS data, and all different numbers of voxels (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-15">3.15</a>). Notably, for some numbers of voxels, model performance was not significantly above chance level.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-15"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_15.png" alt="Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p &lt; 0.001\), ** = \(p &lt; 0.01\), * = \(p &lt; 0.05\)."  />
<p class="caption">
Figure 3.15: Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average <span class="math inline">\(F_{1}\)</span> score across 10 folds; error bars reflect 95% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = <span class="math inline">\(p &lt; 0.001\)</span>, ** = <span class="math inline">\(p &lt; 0.01\)</span>, * = <span class="math inline">\(p &lt; 0.05\)</span>.
</p>
</div>

<p>We also evaluated whether regressing the confound from the train set only was sufficient to control for confounds, but found that it does not effectively control for confounds when there is no true signal (i.e., there is positive bias), which is visualized in more detail in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S10">B.10</a> (cf. Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a>).</p>
</div>
<div id="efficacy-analysis-1" class="section level4">
<h4><span class="header-section-number">3.3.5.2</span> Efficacy analysis</h4>
<p>Similar to WDCR, CVCR yielded plausible and unbiased model performance (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a>, pink line). Moreover, when applied to the simulated null data, CVCR yielded model performance scores at chance level across all levels of the confound-target correlation and numbers of features (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-14">3.14</a>B).</p>
</div>
</div>
<div id="summary-methods-for-confound-control" class="section level3">
<h3><span class="header-section-number">3.3.6</span> Summary methods for confound control</h3>
<p>In this section, we investigated the effects of different method to control confounds (post hoc counterbalancing, WDCR, and CVCR) on empirical MRI data and simulated data (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-16">3.16</a> for a summary of the empirical results). Post hoc counterbalancing was, at least using the subsampling method described, clearly unable to effectively control for confounding influences, which is putatively caused by indirect circularity in the analysis process due to subsampling. Confound regression showed an expected drop in model performance (but not below chance level), but only when the confound regression step is properly cross-validated (i.e., the CVCR version).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-16"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_16.png" alt="An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR."  />
<p class="caption">
Figure 3.16: An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.
</p>
</div>

</div>
</div>
<div id="confounds-decoding-discussion" class="section level2">
<h2><span class="header-section-number">3.4</span> Discussion</h2>
<p>Decoding analyses have become a popular alternative to univariate analyses of neuroimaging data. This analysis approach, however, inherently suffers from ambiguity in terms of which source of information is picked up by the decoder <span class="citation">(Naselaris &amp; Kay, <a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>)</span>. Given that one is often interested in model interpretability rather than merely accurate prediction <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>, one should strive to control for alternative sources of information (i.e., other than the target of interest) that might drive decoding. Effectively controlling for these alternative sources of information, or <em>confounds</em>, helps in disambiguating decoding models. In this article, we reviewed and tested two generic, broadly applicable methods that aim to control for confounds in decoding analyses: post hoc counterbalancing and confound regression. Additionally, we proposed a third method that, unlike the other two methods, has shown to effectively control for confounds.</p>
<p>Both when applied to empirical and simulated data, we found that neither post hoc counterbalancing nor (whole-dataset) confound regression yielded plausible and unbiased model performance estimates. First, we found that post hoc counterbalancing leads to <em>optimistic</em> (i.e., positively biased) model performance estimates, which is a result of removing samples that are hard to classify or would be wrongly classified, during the subsampling process. Because this subsampling process is applied to the entire dataset at once (i.e., it is not cross-validated), it can be seen as a form of indirect circular analysis <span class="citation">(Kriegeskorte et al., <a href="bibliography.html#ref-kriegeskorte2009circular" role="doc-biblioref">2009</a>)</span>, in which the data themselves are used to inform analysis decisions, which can lead to biased generalization estimates. Second, our initial evaluation of confound regression, which was applied on the entire dataset (“WDCR”), yielded pessimistic (i.e., negatively biased) and even significantly below chance model performance estimates. Extending previous research <span class="citation">(Jamalabadi et al., <a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span>, we show that this negative bias occurs when the “signal” in the data (operationalized as the width of the feature-target correlation distribution) is lower than would be expected by chance, which we link to the sampling distribution of the Pearson correlation coefficient. Importantly, we show that WDCR systematically narrows the width of the correlation distribution — and thus leads to lower model performance — which is exacerbated by both higher correlations between target and confound, as well as by a larger number of features.</p>
<p>The negative bias observed in WDCR is caused by the fact that it is performed on the whole dataset at once, leading to statistical dependencies between subsequent train and test partitions. To overcome this negative bias, we propose to cross-validate the confound regression procedure (which we call “Cross-Validated Confound Regression”, CVCR). We show that this method yields plausible model performance in the empirical analyses (i.e., significantly above chance model performance) and nearly unbiased model performance in the simulations, for different datasets varying in the amount of features (<span class="math inline">\(K\)</span>) and the strength of the confound (<span class="math inline">\(r_{Cy}\)</span>). Moreover, initial supplementary simulations suggest that these results generalize to (simulated) fMRI data (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S1">B.1</a>), seemingly demonstrating effective control of confounds across different degrees of autocorrelation (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S2">B.2</a>). The method may show some negative bias in some scenarios due to the fact that, in the train set, CVCR will remove all variance associated with the confound (even variance <em>spuriously</em> correlated with the confound). However, this bias seems, at least in the simulated scenarios, very small. Overall, we believe that our results demonstrate that CVCR is a flexible and effective method to control for confounds in decoding analyses of neuroimaging data.</p>
<div id="relevance-and-consequences-for-previous-and-future-research" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Relevance and consequences for previous and future research</h3>
<div id="a-priori-and-post-hoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> A priori and post hoc counterbalancing</h4>
<p>We believe our results have implications not only for post hoc counterbalancing, but a priori counterbalancing in observational designs in general. In both behavioral research <span class="citation">(Wacholder et al., <a href="bibliography.html#ref-Wacholder1992-wb" role="doc-biblioref">1992</a>)</span> and neuroimaging research <span class="citation">(Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>, a priori counterbalancing (or case-control “matching”) is a common strategy to avoid effects of confounds. However, as we show in the current study, this may unintentionally remove samples that are harder to predict, especially when there is little shared variance between the confound and the other predictors (i.e., when there is low confound <span class="math inline">\(R^2\)</span>). Because, conceptually, this represents a form of circular analysis, counterbalancing — regardless of whether it is applied a priori or post hoc — can yield biased model performance estimates. To some extent, the bias in the post hoc counterbalancing results should not come as a surprise: as noted in the <a href="confounds-decoding.html#confounds-decoding-methods">Methods</a> section, counterbalancing in observational research requires the researcher to choose a sample that is not representative of the population <span class="citation">(see also Sedgwick, <a href="bibliography.html#ref-Sedgwick2013-op" role="doc-biblioref">2013</a>)</span>. As a result, out-of-sample predictive performance drops significantly, in our case even to chance level.</p>
<p>Since post hoc counterbalancing does not show any positive bias in model performance when there is no signal at all (i.e., signal <span class="math inline">\(R^2\)</span>), one could argue that any observed significant above chance effect, while positively biased in terms of effect magnitude, can be interpreted as evidence that there must be signal in the data in the first place. However, we argue against this interpretation for two reasons. First, any above chance predictive performance of models fitted after subsampling is not only positively biased, but also does not cross-validate to the rejected samples (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-11">3.11</a>). That is, the model picks up relations between features and target that are only present in the subsample, and not in the samples left out of the analysis. As a result, it is questionable whether (and if so, how) the model should be interpreted — after all, we assume that the rejected samples were drawn from the population of interest in a valid way. Second, any possible absence of above chance model performance after subsampling can neither be interpreted as evidence for an <em>absence</em> of a true effect, since the subsampling procedure necessarily leads to a (often substantial) power loss. It could still well be that in the original sample there was a true relation between features and target. Thus, interpretation of modelling efforts after subsampling is problematic in case of both <em>presence</em> and <em>absence</em> of above chance model performances.</p>
</div>
<div id="confound-regression-1" class="section level4">
<h4><span class="header-section-number">3.4.1.2</span> Confound regression</h4>
<p>In contrast to post hoc counterbalancing, confound regression in its uncross-validated form (i.e., WDCR) has been applied widely in the context of decoding analyses <span class="citation">(Dubois et al., <a href="bibliography.html#ref-dubois2018resting" role="doc-biblioref">2018</a>; Kostro et al., <a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>; Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>; Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span>. Indeed, the first study that systematically investigated the effect of confounds in decoding analyses <span class="citation">(Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> used WDCR to account for the confounding effect of reaction times (RT) on decoding of rule representations and found that WDCR completely eliminated the predictive performance that was found without controlling for RT. This observation, however, can potentially be explained by the negative bias induced by WDCR. This possible explanation is corroborated by a follow-up study that similarly looked into RT confounding the decoding of rule representations <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>, who did not use WDCR but accounted for RT confounding by including it as a covariate during the pattern estimation procedure (see <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a> for a tentative evaluation of this method), which in contrast to the study by Todd et al. yielded significant decoding performance. Moreover, while not specifically investigated here, we expect a similar negative bias to occur when a confound is removed from a continuous target variable using WDCR — which may offer an explanation for the null finding of <span class="citation">Dubois et al. (<a href="bibliography.html#ref-dubois2018resting" role="doc-biblioref">2018</a>)</span>, who fail to decode personality characteristics from resting-state fMRI.</p>
</div>
<div id="relevance-to-other-analysis-methods" class="section level4">
<h4><span class="header-section-number">3.4.1.3</span> Relevance to other analysis methods</h4>
<p>While this article focuses on controlling for confounds in decoding analyses specifically, we believe that our findings may be relevant for analysis methods beyond decoding analyses as well. In fact, methods for controlling for confounds (or alternative sources of information) have previously been investigated and applied in another type of MVPA named “representational similarity analysis” <span class="citation">(RSA; Kriegeskorte et al., <a href="bibliography.html#ref-kriegeskorte2008representational" role="doc-biblioref">2008</a>)</span>. In the context of RSA, the explained variance in the neural data is often partitioned into different (model-based) feature sets (i.e., sources of information), which allows one to draw conclusions about the unique influence of each source of information <span class="citation">(see, e.g., Groen et al., <a href="bibliography.html#ref-Groen2018-qo" role="doc-biblioref">2018</a>; Hebart et al., <a href="bibliography.html#ref-Hebart2018-dz" role="doc-biblioref">2018</a>; Ramakrishnan et al., <a href="bibliography.html#ref-Ramakrishnan2014-ki" role="doc-biblioref">2014</a>)</span>. Specifically, variance partitioning in RSA is done by removing the variance from the representational dissimilarity matrix (RDM) based on the feature set that needs to be controlled for. Notably, the variance of the RDMs that are not of interest can be removed from only the neural RDM <span class="citation">(Hebart et al., <a href="bibliography.html#ref-Hebart2018-dz" role="doc-biblioref">2018</a>; Ramakrishnan et al., <a href="bibliography.html#ref-Ramakrishnan2014-ki" role="doc-biblioref">2014</a>)</span> or both from the neural RDM and the RDM of interest <span class="citation">(Groen et al., <a href="bibliography.html#ref-Groen2018-qo" role="doc-biblioref">2018</a>)</span>. While the analysis context is different, the underlying technique is identical to confound regression as described and evaluated in this article. Importantly, the studies employing this variance partitioning technique <span class="citation">(Groen et al., <a href="bibliography.html#ref-Groen2018-qo" role="doc-biblioref">2018</a>; Hebart et al., <a href="bibliography.html#ref-Hebart2018-dz" role="doc-biblioref">2018</a>; Ramakrishnan et al., <a href="bibliography.html#ref-Ramakrishnan2014-ki" role="doc-biblioref">2014</a>)</span> similarly report plausible model performances after confound regression (i.e., relatively lower but not below chance performance), corroborating our results with (cross-validated) confound regression. Note that the distinction between WDCR and CVCR in the context of most RSA studies (including the aforementioned studies) is largely irrelevant, as representational similarity analyses are not commonly cross-validated. However, recently, some have proposed to use cross-validated distance measures <span class="citation">(such as the cross-validated Mahalanobis distance; Guggenmos et al., <a href="bibliography.html#ref-Guggenmos2018-rr" role="doc-biblioref">2018</a>; Walther et al., <a href="bibliography.html#ref-Walther2016-je" role="doc-biblioref">2016</a>)</span> in RSA, which could suffer from negative bias when combined with (not cross-validated) variance partitioning similar to what we observed with WDCR in the context of decoding analyses.</p>
<p>We believe that especially our findings with regard to WDCR and CVCR may be relevant for any cross-validated analysis, regardless of the “direction” of analysis (encoding vs. decoding) and the dimensionality of the neural data (univariate vs. multivariate approaches). In general, our findings with respect to negative bias after WDCR were to be expected, as it introduces dependence between the train set and the test set which violates the crucial assumption of independence of any cross-validated analysis. While a violation of the independence assumption often leads to positive bias such as in “double dipping” <span class="citation">(Kriegeskorte et al., <a href="bibliography.html#ref-kriegeskorte2009circular" role="doc-biblioref">2009</a>)</span>, we show here that it may also lead to negative bias. Either way, our findings reinforce the idea that data analysis operations should <em>never</em> be applied to the entire dataset before subjecting the data to a cross-validated analysis. Therefore, we believe that our findings with respect to WDCR and CVCR will generalize to any cross-validated analysis <span class="citation">(such as cross-validated MANOVA, Allefeld &amp; Haynes, <a href="bibliography.html#ref-allefeld2014searchlight" role="doc-biblioref">2014</a>; or cross-validated encoding models, Naselaris et al., <a href="bibliography.html#ref-Naselaris2011-oh" role="doc-biblioref">2011</a>)</span>, but future research is necessary to substantiate this claim.</p>
</div>
<div id="importance-for-gender-decoding-studies" class="section level4">
<h4><span class="header-section-number">3.4.1.4</span> Importance for gender decoding studies</h4>
<p>The importance of proper confound control is moreover highlighted by the empirical question we address. Without any optimization of the prediction pipeline, we were able to predict gender with a model performance up to approximately 0.85 without confound control. This is in line with reports from various other studies <span class="citation">(Del Giudice et al., <a href="bibliography.html#ref-Del_Giudice2016-ns" role="doc-biblioref">2016</a>; Rosenblatt, <a href="bibliography.html#ref-Rosenblatt2016-oy" role="doc-biblioref">2016</a>; Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. However, this predictive performance is driven by a mixture two sources of information: global and local differences in brain structure. With confound control, however, we show that predictive performance using only local differences lies around 0.6 for VBM data and 0.7 for TBSS data — a substantial drop in performance. Especially because the remaining predictive performance is lower than predictive performance using only brain size, we argue that the use of proper confound control may lead one to draw substantially different conclusions about the differences in brain structure between men and women. For the debate on gender dimorphism, it is thus extremely important to take global brain size into account in the context of decoding analyses <span class="citation">(as has been similarly recommended for mass-univariate analyses; Barnes et al., <a href="bibliography.html#ref-Barnes2010-pu" role="doc-biblioref">2010</a>)</span>.</p>
</div>
</div>
<div id="choosing-a-confound-model-linear-vs.-nonlinear-models" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Choosing a confound model: linear vs. nonlinear models</h3>
<p>In the present paper, we focused on the use of linear models for confound control. It is crucial to note that the efficacy of confound control depends on the suitability of the confound regression model employed. Removing variance associated with a confound using a linear model removes only the variance of data (features) that is linearly related to the confound. When a confound is nonlinearly related to the data, some variance associated with the confound can remain in the data after a linear confound model is used to regress out variance. It is possible that the decoding model subsequently applied still picks up this residual “confounded” variance. In other words, an unsuitable confound model may control for confounds imperfectly.</p>
<p>The exact relation between confound and (brain) data is hardly ever known a priori. However, it is possible to explore the nature of this relation using the data at hand. For example, a researcher can apply a cross-validated prediction pipeline to predict a feature (e.g., VBM voxel intensity) from the confound. The researcher can then test what type of model (linear or nonlinear) describes the relation between confound and data best. In the <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a> (section “Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size”), we provide an example of this approach. We used linear, quadratic, and cubic regression models to predict VBM and TBSS voxel intensity using brain size as feature. In the Supplementary Results, we show that linear models perform equally well as or better than polynomial models for the majority of voxels (Supplementary Figures <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S7">B.7</a> and <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S9">B.9</a>). Further, for voxels where polynomials outperform linear models, the difference between model performances is minimal (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S8">B.8</a>). Thus, in the empirical research question explored in this paper, a linear confound model seems to suit the data very well.</p>
</div>
<div id="practical-recommendations" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Practical recommendations</h3>
<p>As indicated by the title of this article, we will now outline some practical recommendations for dealing with confounds in decoding analyses of neuroimaging data. First, one needs to obtain an accurate measurement of potential confounds <span class="citation">(Westfall &amp; Yarkoni, <a href="bibliography.html#ref-westfall2016statistically" role="doc-biblioref">2016</a>)</span>. While we assumed the availability of such a measure in this article, this is not always trivial. In experimental settings, for example, reaction times can potentially be identified as a confound <span class="citation">(Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>; Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>, but arguably, it is not reaction time but rather an unobserved variable related to reaction time (e.g., difficulty or attention) that confounds the analysis. In such scenarios, the best one can do is measure reaction time as a proxy, and be aware that any subsequent confound control method is limited by how well this proxy corresponds to the actual confound. Second, one needs to identify which variables actually confound a decoding analysis. To detect confounds, we recommend using the “same analysis approach” outlined by <span class="citation">Görgen et al. (<a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. In short, this method involves trying to predict the target variable using your confound(s) as predictive features (for example, when using only brain size to predict gender). In case of significant above chance decoding performance, and assuming the confounds are actually encoded in the neuroimaging data, the hypothesized confounds will most likely influence the actual decoding analysis. While in the current article we focused on simple univariate confounding effects (i.e., confounding by a single variable), the same analysis approach is not limited to detecting univariate confounds — it facilitates detecting multivariate (i.e., confounding by multiple variables) or interaction effects (i.e., confounding by interaction effects between variables) as well. For example, if one hypothesizes that the target variable is related to the interaction between confound <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span> (i.e., <span class="math inline">\(C_{1} \times C_{2}\)</span>), one can simply use the interaction term as the potential confound in the same analysis approach to evaluate the potential confounding influence.</p>
<p>Once the specific confound terms have been identified, we recommend regressing out the confound from the data in a cross-validated manner (i.e., using CVCR). Specifically, we recommend including confound regression as the first step in your decoding pipeline to avoid the effect of confounds on other operations in the pipeline <span class="citation">(such as univariate feature selection; Chu et al., <a href="bibliography.html#ref-chu2012does" role="doc-biblioref">2012</a>)</span>. In this article, we used ordinary least squares (OLS) regression to remove the influence of confounds from the data, because a linear model describes the relation between brain size and VBM/TBSS voxel intensities well (see Supplementary Figures <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S7">B.7</a>-<a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S9">B.9</a>). However, not only linear models can be used to remove variance associated with a confound from the data — it is possible to use nonlinear models (potentially with multiple confounds and interactions between them) if it is clear that the relation between confounds and neuroimaging features is nonlinear (see previous section for details on choosing a confound model). However, as a limitation to the presented results, we did not test whether CVCR also leads to (nearly) unbiased results when used with nonlinear models. We advise, therefore, in such cases, to first test in a simulation study whether CVCR provides an unbiased confound control method with nonlinear models before use with actual data.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">3.5</span> Conclusion</h2>
<p>In general, we believe that the contributions of the current study are twofold. First and foremost, it provides a systematic evaluation of widely applicable methods to control for confounds and shows that, of the methods investigated, only one (“cross-validated confound regression”) appears to yield plausible and almost unbiased results. The results from this evaluation hopefully prevents researchers from using post hoc counterbalancing and whole-dataset confound regression, which we show may introduce (unintended) biases. Moreover, we made all analyses and preprocessed data openly available (<a href="https://github.com/lukassnoek/MVCA">https://github.com/lukassnoek/MVCA</a>) and provide a simple implementation for cross-validated confound regression that interfaces with the popular scikit-learn package in the Python programming language. Second, we believe that this study improves understanding of the elusive phenomenon of below chance accuracy <span class="citation">(building on previous work by Jamalabadi et al., <a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span>. In general, we hope that this study helps researchers in gaining more insight into their decoding analyses by providing a method that disentangles the contributions of different sources of information that may be encoded in their data.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>The terms “gender” and “sex” are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.<a href="confounds-decoding.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration “squeezes” relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa <span class="citation">(Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span>. This effect of global brain size similarly affects functional MRI analyses <span class="citation">(Brodtmann et al., <a href="bibliography.html#ref-brodtmann2009regional" role="doc-biblioref">2009</a>)</span>.<a href="confounds-decoding.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant’s gender.<a href="confounds-decoding.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>In the context of behavioral data, a priori counterbalancing is often called “matching” or a employing a “case-control design” (Cook, 2002)<a href="confounds-decoding.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Note that the counterbalancing process is the same for both traditional univariate (activation-based) studies and decoding studies, but the direction of analysis is reversed in univariate (e.g., gender → brain) and decoding studies (e.g., brain → gender). As such, in univariate studies the confound (e.g., brain size) is counterbalanced with respect to the predictor(s) (e.g., gender) while in decoding studies the confound (e.g., brain size) is counterbalanced with respect to the target (e.g., gender).<a href="confounds-decoding.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>However, parameter estimates only reflect unique variance when ordinary, weighted, or generalized least squares is used to find the model parameters. Other (regularized) linear models, such as ridge regression or LASSO, are not guaranteed to yield parameters that explain unique proportions of variance.<a href="confounds-decoding.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span>, here, refer to (usually HRF-convolved) predictors of the time series signal (<span class="math inline">\(s\)</span>) for a single voxel. In the rest of the article, <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> refer to features that are defined across samples (not time).<a href="confounds-decoding.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Note that, technically, one could use the “Control for confounds during pattern estimation” method in electrophysiology as well, by first fitting a univariate model explaining the neuroimaging data (<span class="math inline">\(X_{j}\)</span> for <span class="math inline">\(j = 1 \dots K\)</span>) as a function of both the target (<span class="math inline">\(y\)</span>) and the confound (<span class="math inline">\(C\)</span>) and subsequently only using the parameter estimates of the target-predictor (<span class="math inline">\(\hat{\beta}_{x}\)</span>) as patterns in the subsequent decoding analysis.<a href="confounds-decoding.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>Note that we did not discuss studies that implement a different confound regression procedure <span class="citation">(e.g., Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>; Dukart et al., <a href="bibliography.html#ref-Dukart2011-aq" role="doc-biblioref">2011</a>)</span>, in which confound regression is only estimated on the samples from a single class of the target variable (e.g., in our gender decoding example, this would mean that confound regression models are only estimated on the data from male, or female, subjects). As this form of confound regression does not disambiguate the sources of information driving the decoder, it is not discussed further in this article.<a href="confounds-decoding.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>For continuous confounds, it is practically impossible to achieve a correlation with the target of <em>exactly</em> zero, which is the reason we subsample until it is smaller than a prespecified threshold. For categorical confounds, however, a correlation between the confound and the target of exactly zero is possible <span class="citation">(this amounts to equal proportions of levels of <span class="math inline">\(c\)</span> within each class of <span class="math inline">\(y\)</span>; Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>, even <em>necessary</em>, because it is impossible to find a (<em>K</em>-fold) cross-validation partitioning in which each split is counterbalanced w.r.t. the confound if the correlation <em>in the entire dataset</em> between the target and the confound is not zero.<a href="confounds-decoding.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>We refer to a relatively high α as “strict”, here, because we use it here for the purpose of demonstrating no effect.<a href="confounds-decoding.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>One could run the “random subsampling” procedure with more than 10,000 draws in order to reduce the aforementioned power loss; but in the extreme, this would result in the same optimal subsample that can be found much faster by targeted subsampling.<a href="confounds-decoding.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>Note that plausible null data do not reflect a signal <span class="math inline">\(R^2\)</span> of 0, because this statistic is biased towards values larger than 0 (because it represents a squared number) when dealing with noisy data, hence our choice of signal <span class="math inline">\(R^2 = 0.004\)</span>.<a href="confounds-decoding.html#fnref16" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shared-states.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="aomic.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
