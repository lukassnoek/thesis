<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>E Supplement to Chapter 6 | Towards prediction</title>
  <meta name="description" content="E Supplement to Chapter 6 | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="E Supplement to Chapter 6 | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="E Supplement to Chapter 6 | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="morbid-curiosity-supplement.html"/>
<link rel="next" href="static-vs-dynamic-supplement.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>1.1</b> Inference done differently</a></li>
<li class="chapter" data-level="1.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>1.2</b> Towards prediction</a></li>
<li class="chapter" data-level="1.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.2.7</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.3</b> Results</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.3.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.3.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
<li class="chapter" data-level="4.4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.4.6</b> Code availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="5.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>5.2.2</b> Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>5.2.3</b> Materials</a></li>
<li class="chapter" data-level="5.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>5.2.4</b> Procedure</a></li>
<li class="chapter" data-level="5.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>5.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="5.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>5.2.6</b> Imaging details</a></li>
<li class="chapter" data-level="5.2.7" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>5.2.7</b> Data availability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>5.3</b> Results</a><ul>
<li class="chapter" data-level="5.3.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>5.3.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="5.3.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>5.3.3</b> ROI analyses</a></li>
<li class="chapter" data-level="5.3.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>5.3.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>6</b> Explainable models of facial movements predict emotion perception behavior</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#the-prediction-explanation-exploration-framework"><i class="fa fa-check"></i><b>6.1.1</b> The prediction-explanation-exploration framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>6.2.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>6.2.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>6.2.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="6.2.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>6.2.4</b> Evaluated mappings</a></li>
<li class="chapter" data-level="6.2.5" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-dataset"><i class="fa fa-check"></i><b>6.2.5</b> Dataset used to evaluate mappings</a></li>
<li class="chapter" data-level="6.2.6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-code"><i class="fa fa-check"></i><b>6.2.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#prediction"><i class="fa fa-check"></i><b>6.3.1</b> Prediction</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#explanation"><i class="fa fa-check"></i><b>6.3.2</b> Explanation</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#exploration"><i class="fa fa-check"></i><b>6.3.3</b> Exploration</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>7</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="7.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>7.2.1</b> Participants</a></li>
<li class="chapter" data-level="7.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>7.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="7.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>7.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>7.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="7.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>7.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>7.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="7.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>7.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>7.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="7.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>7.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>7.4</b> Discussion</a><ul>
<li class="chapter" data-level="7.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>7.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="7.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>7.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="7.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>7.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="7.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>7.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>8</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a><ul>
<li class="chapter" data-level="E.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supplementary-methods"><i class="fa fa-check"></i><b>E.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="E.1.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-in-detail"><i class="fa fa-check"></i><b>E.1.1</b> Hypothesis kernel analysis (in detail)</a></li>
<li class="chapter" data-level="E.1.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-noise-ceiling-detail"><i class="fa fa-check"></i><b>E.1.2</b> Noise ceiling estimation (in detail)</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supp-fig"><i class="fa fa-check"></i><b>E.2</b> Supplementary figures</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-kernel-analysis-supplement" class="section level1">
<h1><span class="header-section-number">E</span> Supplement to Chapter 6</h1>
<div id="hka-supplementary-methods" class="section level2">
<h2><span class="header-section-number">E.1</span> Supplementary methods</h2>
<p>Below, we describe the methodology behind hypothesis kernel analysis and noise ceiling estimation in more detail.</p>
<div id="hypothesis-kernel-analysis-in-detail" class="section level3">
<h3><span class="header-section-number">E.1.1</span> Hypothesis kernel analysis (in detail)</h3>
<div id="step-1-encoding-mappings" class="section level4">
<h4><span class="header-section-number">E.1.1.1</span> Step 1: encoding mappings</h4>
<p>The first step in our method is the embedding of hypotheses in a common space. In the context of AU-emotion mappings, this amounts to formalizing these mappings as points in “AU space”. Here, AU space is a multidimensional space in which each of the AUs under consideration represents one dimension and each AU-emotion mapping (e.g., “disgust = AU9 + AU10”) can be represented as a single point in this space. For example, suppose that we only consider a limited set of five AUs (AU4, AU9, AU10, AU12, and AU23). We then can represent the hypothetical mapping “disgust = AU9 + AU10”, <span class="math inline">\(\mathbf{M}_{\mathrm{disgust}}\)</span>, as a point with five coordinates (i.e., a vector), which value indicates whether a given AU is part of the hypothesized configuration (1) or not (0):</p>
<p><span class="math display">\[\begin{equation}
\mathbf{M}_{\mathrm{disgust}} = \begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}
\end{equation}\]</span></p>
<p>Note that, in the above example, values at the positions of hypothesized AUs are all encoded as 1, which implies that each AU within the configuration is expressed equally intensely. This does not have to be the case; if, for example, the aforementioned mapping hypothesized that disgust is expressed with a combination of AU9 at 100% intensity but AU10 at 50% intensity, then its embedding can be expressed as follows:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{M}_{\mathrm{disgust}} = \begin{bmatrix} 0 &amp; 1 &amp; .5 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}
\end{equation}\]</span></p>
<p>For simplicity, we assume in this example that each hypothesized AU is expressed at equal intensity (such that vectors are binary). Importantly, many studies outline mappings with regard to multiple emotions, which we will refer to here as classes. For this example, we assume that our hypothetical mapping <span class="math inline">\(\mathbf{M}\)</span> limits its mappings to the six basic emotions. Specifically, suppose that mapping <span class="math inline">\(\mathbf{M}\)</span> outlines, in addition the the previously defined happiness mapping, specific hypothetical AU-emotion mappings for the following categorical emotions:</p>
<ul>
<li>anger = AU4 + AU5 + AU7</li>
<li>disgust = AU9 + AU15</li>
<li>fear = AU1 + AU2 + AU4 + AU7 + AU26</li>
<li>sadness = AU1 + AU4 + AU15</li>
<li>surprise = AU1 + AU2 + AU5 + AU26</li>
</ul>
<p>Accordingly, we can encode the entire set of AU-emotion mappings for a given mapping, <span class="math inline">\(\mathbf{M}\)</span>, with <span class="math inline">\(C\)</span> classes and <span class="math inline">\(D\)</span> dimensions into a <span class="math inline">\(C \times D\)</span> matrix, by vertically stacking the <span class="math inline">\(C\)</span> different row mapping vectors. For our hypothetical mapping, <span class="math inline">\(\mathbf{M}\)</span>, its associated “mapping matrix” would look like the following:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{M} = \begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\end{equation}\]</span></p>
<p>where its rows represent the different classes (categorical emotions) and the columns the involvement of a specific AU. Note that although the above represents a hypothetical mapping, its sparsity is something we would expect, as facial expressions are unlikely to be generated by a full (dense) set of action units <span class="citation">(Yu et al., <a href="bibliography.html#ref-Yu2012-ag" role="doc-biblioref">2012</a>)</span>.</p>
</div>
<div id="step-2-encoding-stimuli" class="section level4">
<h4><span class="header-section-number">E.1.1.2</span> Step 2: encoding stimuli</h4>
<p>In the previous section we outlined how to formalize AU-emotion mappings as points (or, equivalently, vectors) in AU space. One way to <em>evaluate</em> these formalized mappings is to subject them to actual categorical emotion ratings from human participants in response to stimuli with known AU configurations. Ideally, the stimuli from such an experiment sample the AU space as densely and uniformly as possible in order not to bias the results towards hypothesized mappings. Many experiments on facial emotion expressions, however, use posed and stereotyped stimuli (e.g., facial expressions of intense joy or anger), which cover only a small part of the entire AU space and thus do not allow for unbiased evaluation of AU-based theories. In contrast, reverse correlation-based experiments, which are characterized by randomly and parametrically varying the input space (defined by AU configurations) and collection of resulting percepts (here: perception of categorical emotion) do not impose such constraints <span class="citation">(Jack et al., <a href="bibliography.html#ref-Jack2017-qp" role="doc-biblioref">2017</a>)</span> and thus present an ideal type of dataset to subject to our formalized AU-emotion mappings.</p>
<p>In reference to our previously defined hypothetical 10-dimensional AU space, assume that we have categorical emotion ratings <span class="math inline">\(e\)</span> from a set of emotions <span class="math inline">\(E\)</span> (<span class="math inline">\(e \in E\)</span>) in response to a collection of <span class="math inline">\(N\)</span> facial expression stimuli parameterized with random AU configurations, drawn from the same 10-dimensional AU space discussed before. With such data, we can encode the stimuli in AU space in the same way we did in the previous section for AU-emotion mappings, i.e., we can quantify each stimulus, <span class="math inline">\(\mathbf{S}_{i}\)</span>, as a 10-dimensional “stimulus vector” containing nonzero values at positions associated with active AUs for that stimulus and zeros elsewhere. Note that, as is the case with mapping vectors, the stimulus vector’s nonzero values at positions associated with active AUs can be all ones (if assumed to be equally “active”) or be values proportional to the amplitude (or “activity”) of the active AUs.</p>
<p>For example, suppose that stimulus <span class="math inline">\(\mathbf{S}_{i}\)</span> contains AU1, AU5, and AU26 with amplitudes 0.1, 0.5, and 0.8 respectively (where an amplitude of 1 would represent an AU at maximum intensity). Then, formally, we can represent this particular stimulus, <span class="math inline">\(\mathbf{S}_{i}\)</span>, as the following stimulus vector:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{S}_{i} = \begin{bmatrix} .1 &amp; 0 &amp; 0 &amp; .5 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; .8 \end{bmatrix}
\end{equation}\]</span></p>
<p>In case of multiple stimuli (<span class="math inline">\(\mathbf{S}_{i}\)</span> for <span class="math inline">\(i = \{1, \dots, N\}\)</span>), their mapping vectors can be vertically stacked in a single <span class="math inline">\(N \times D\)</span> “stimulus matrix”, <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>Given that both a mapping (<span class="math inline">\(\mathbf{M}\)</span>) and set of stimuli (<span class="math inline">\(\mathbf{S}\)</span>) are encoded as matrices in the same <span class="math inline">\(D\)</span>-dimensional AU space, we can discuss using kernels to generate quantitative predictors for stimuli given a particular theory.</p>
</div>
<div id="step-3-kernel-functions" class="section level4">
<h4><span class="header-section-number">E.1.1.3</span> Step 3: kernel functions</h4>
<p>Kernel functions (or simply kernels) are functions that are, broadly speaking, measures of similarity between two vectors. Applied to our use case, we use kernel functions (<span class="math inline">\(\kappa\)</span>) to quantify the similarity, i.e. “closeness” in AU space, (<span class="math inline">\(\phi\)</span>) between a stimulus with a known AU configuration (<span class="math inline">\(\mathbf{S}_{i}\)</span>) and a mapping vector for a specific emotion, indexed by <span class="math inline">\(j\)</span> (<span class="math inline">\(\mathbf{M}_{j}\)</span>, e.g., happiness)<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>:</p>
<p><span class="math display">\[\begin{equation}
\phi_{ij} = \kappa(\mathbf{S}_{i}, \mathbf{M}_{j})
\end{equation}\]</span></p>
<p>Most (linear) kernel functions are based on the dot (inner) product between the two vectors. In the current study, we primarily use the cosine kernel, which normalizes the dot product between two vectors with the product of their L2 (Euclidean) norm:</p>
<p><span class="math display">\[\begin{equation}
\kappa(\mathbf{S}_{i}, \mathbf{M}_{j}) = \frac{\mathbf{S}_{i}\mathbf{M}_{j}^{T}}{\left\Vert \mathbf{S}_{i} \right\Vert \left\Vert \mathbf{M}_{j} \right\Vert}
\end{equation}\]</span></p>
<p>Without such normalization, similarity values monotonically increase with increasing magnitudes of the stimulus vector, even if the stimulus vector increasingly deviates from the mapping.</p>
</div>
<div id="step-4-computing-predictions" class="section level4">
<h4><span class="header-section-number">E.1.1.4</span> Step 4: computing predictions</h4>
<p>Although the similarity to a particular mapping vector can, generally speaking, be interpreted as being proportional to the evidence for that particular class, it is not strictly speaking a prediction. To generate a quantitative prediction for stimulus <span class="math inline">\(\mathbf{S}_{i}\)</span> (i.e., <span class="math inline">\(\hat{e}_{i}\)</span>), one needs to formulate a decision function that maps the data to a prediction. One possibility is to determine the prediction to be the emotion (across <span class="math inline">\(C\)</span> classes) that maximizes its similarity, for some kernel function (<span class="math inline">\(\kappa\)</span>), to the stimulus:</p>
<p><span class="math display">\[\begin{equation}
\hat{e}_{i} = \underset{j}{\operatorname{\mathrm{argmax}}}\ \kappa(\mathbf{S}_{i}, \mathbf{M}_{j})
\end{equation}\]</span></p>
<p>In a slightly more sophisticated version of this decision function, we can generate probabilistic predictions instead of discrete predictions. To do so, we normalize the similarity vector using the <em>softmax</em> function, which returns a vector that sums to 1 such that their elements can be interpreted as probabilities (i.e., the probability of an emotion given a stimulus and mapping matrix):</p>
<p><span class="math display">\[\begin{equation}
P(E_{j} | \mathbf{M}, \mathbf{S}_{i}) = \frac{\exp(\beta\phi_{ij})}{\sum_{j=1}^{C}\exp(\beta\phi_{ij})}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is the “inverse temperature” parameter — a scaling parameter — which distributes relatively more mass onto the largest values within the sequence of similarities. In our framework, we can treat this parameter as a model hyperparameter (i.e., a parameter that is not fit, but could be manually tuned using cross-validation). In our analyses, we use an inverse temperature parameter of 1 (see Supplementary Figure <a href="hypothesis-kernel-analysis-supplement.html#fig:fig-hka-S2">E.2</a> for a comparison of the effect of different parameter values on model performance).</p>
</div>
<div id="step-5-quantifying-model-performance" class="section level4">
<h4><span class="header-section-number">E.1.1.5</span> Step 5: quantifying model performance</h4>
<p>To evaluate the performance of each mapping, we can compare their (discrete or probabilistic) emotion predictions for a set of stimuli with emotion labels from participants who rated the same stimuli. In other words, we can quantitatively assess how well the theoretical predictions match with actual behavior. Model performance, or “score”, can be quantified using any function (<span class="math inline">\(q\)</span>), or “metric”, that takes as inputs a set of predicted labels (<span class="math inline">\(\hat{e}\)</span>) and a set of “true” labels (<span class="math inline">\(e\)</span>) and returns a single number that summarizes the model performance, or “score”:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{score} = q(e, \hat{e})
\end{equation}\]</span></p>
<p>Instead of returning a single, class-average model performance estimate, some metrics are also able to return class-specific model performance scores. In our analyses, we used the “area under the curve of the receiver operating characteristic” (AUROC) which summarizes the quality of probabilistic predictions in a range from 0 to 1, where 0.5 is chance level performance and 1 is a perfect prediction. Note that our proposed method does not require a specific performance metric. We prefer to use AUROC as it can be used for both discrete and probabilistic predictions, is insensitive to class imbalance (i.e., unequal frequencies of target classes), and allows for class-specific performance estimates.</p>
</div>
</div>
<div id="hka-noise-ceiling-detail" class="section level3">
<h3><span class="header-section-number">E.1.2</span> Noise ceiling estimation (in detail)</h3>
<p>Suppose that, for a given dataset, we find that a particular mapping yields a (class-average) AUROC score of 0.8 — what can and should be concluded from this score? It is certainly above chance level (a score of 0.5) but also substantially below perfect performance (i.e., a score of 1). Here, we argue that one should not interpret performance relative to a theoretical maximum score, but relative to a noise ceiling &amp; a concept borrowed from systems neuroscience <span class="citation">(Lage-Castellanos et al., <a href="bibliography.html#ref-lage2019methods" role="doc-biblioref">2019</a>)</span> — which represents an upper bound that incorporates the within- and between-subject “variance” in ratings. In other words, a noise ceiling is a way to estimate an upper bound for predictive models that is adjusted for the “consistency” of the target variable.</p>
<p>One important reason to quantify a model’s noise ceiling is that is partitions the unexplained variance (i.e., part of the data that was not predicted correctly) into unexplained but in principle explainable variance (i.e., the noise ceiling minus the model performance) and “irreducible” noise (i.e., the theoretical maximum performance minus the noise ceiling; see Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-3">6.3</a>). The amount of explainable variance in turn quantifies how much there is to gain in terms of model improvement: if this component is large, one might consider different or more complex models; if this component is small (i.e., the model performance is at or near the noise ceiling), one can conclude that the model cannot be improved any further (which does not mean that it is the <em>correct</em> model, however). When applied in the context of AU-emotion mappings, the noise ceiling illustrates what portion of the variance in emotion inferences can be, in principle, explained by AUs.</p>
<p>While noise ceilings are routinely used in systems and cognitive neuroscience <span class="citation">(Hsu et al., <a href="bibliography.html#ref-Hsu2004-hs" role="doc-biblioref">2004</a>; Huth et al., <a href="bibliography.html#ref-Huth2012-yc" role="doc-biblioref">2012</a>; Kay et al., <a href="bibliography.html#ref-Kay2013-ch" role="doc-biblioref">2013</a>; Nili et al., <a href="bibliography.html#ref-Nili2014-ar" role="doc-biblioref">2014</a>)</span>, existing methods for estimating noise ceilings are limited to regression models (assuming a continuous target variable, usually some type of brain measurement). In the current study, however, we are dealing with classification models, as we are trying to predict a categorical target variable (i.e., categorical emotion ratings). Here, we propose a novel approach to estimate a noise ceiling for predictive performance of classification models.</p>
<p>A crucial and necessary element for most noise ceiling estimation methods, including the one proposed here, is the availability of repeated trials. Using repeated trials, the variance (or, inversely, the “consistency”) in the target variable can be estimated and used to estimate an upper bound on predictive performance. In other words, a noise ceiling formalizes the idea that a model can only perform as well as the consistency of subjects. Importantly, trials are considered to be “repeats” if their representation in the model is the same. Thus, if a model only considers AUs, then stimuli with the same AU configuration are considered repeats, even if they differ in other features (such as face identity). Moreover, repeated trials may occur “within subjects” (e.g., a trial with a particular AU configuration presented multiple times) or “between subjects” (e.g., a trial with a particular AU configuration presented to different subjects). Within-subject repeats can be used to estimate a within-subject noise ceiling when working with subject-specific models <span class="citation">(which is common in cognitive neuroscience; Lage-Castellanos et al., <a href="bibliography.html#ref-lage2019methods" role="doc-biblioref">2019</a>)</span> between-subject repeats can similarly be used to estimate a between-subject noise ceiling when working with a single, between-subject model. While both within- and between-subject variance are expected to affect the noise ceiling, in this study we only consider between-subject variance (as our dataset only contains between-subject repeats).</p>
<p>To illustrate the computation of the noise ceiling (for a between-subject model), let us consider the following minimal example. Suppose three subjects rated the same two facial expression stimuli (<span class="math inline">\(\mathbf{S}_{1}\)</span> and <span class="math inline">\(\mathbf{S}_{2}\)</span>). As summarized in Table <a href="hypothesis-kernel-analysis-supplement.html#tab:tab-hka-S1">E.1</a>, the ratings are inconsistent across subjects, i.e., not each stimulus is consistently rated as displaying the same emotion.</p>
<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:tab-hka-S1">Table E.1: </span>Hypothetical emotion ratings from three subjects in response to two stimuli
</caption>
<thead>
<tr>
<th style="text-align:left;">
Stimulus
</th>
<th style="text-align:left;">
Ratings subject 1
</th>
<th style="text-align:left;">
Ratings subject 2
</th>
<th style="text-align:left;">
Ratings subject 3
</th>
<th style="text-align:left;">
Optimal pred.
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(\mathbf{S}_{1}\)</span>
</td>
<td style="text-align:left;">
Anger
</td>
<td style="text-align:left;">
Disgust
</td>
<td style="text-align:left;">
Anger
</td>
<td style="text-align:left;">
Anger
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(\mathbf{S}_{2}\)</span>
</td>
<td style="text-align:left;">
Disgust
</td>
<td style="text-align:left;">
Disgust
</td>
<td style="text-align:left;">
Anger
</td>
<td style="text-align:left;">
Disgust
</td>
</tr>
</tbody>
</table>
<p>As mentioned, the noise ceiling represents an upper bound of predictive performance for a given set of observations. In other words, the noise ceiling (<span class="math inline">\(\mathrm{nc}\)</span>) represents the performance that an <em>optimal model</em> would obtain:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{nc} = q(e, e_{\mathrm{optimal}})
\end{equation}\]</span></p>
<p>Here, the optimal model can be any conceivable type of model, but is constrained in one crucial aspect: it should make the same predictions for repeated observations. The reason for this constraint is that, to a model, repeated observations represent identical input (i.e., stimuli parameterized by their stimulus vector) and should logically receive the same prediction.</p>
<p>When working with discrete predictions (i.e., a single predicted label per trial), the optimal model predicts the mode across repeated observations. In our example scenario, the optimal model would thus predict <span class="math inline">\(\mathbf{S}_{1}\)</span> as “Anger” and <span class="math inline">\(\mathbf{S}_{2}\)</span> as “Disgust”. The noise ceiling is subsequently computed as the performance, for a particular performance metric (such as AUROC or simple accuracy), of this optimal model given the true labels. In our example above, the optimal model predicts two out of three ratings per stimulus correctly, resulting in a class-average AUROC noise ceiling of 0.6667.</p>
<p>The disadvantage of using discrete predictions when computing the noise ceiling is that it may result in multiple modes (e.g., a given stimulus might be rated “anger” in 50% of subjects and “disgust” in the other 50% of subjects). One could pick a random mode as the optimal prediction, but this may arbitrarily impact the class-specific noise ceiling for the classes represented by the tied modes. As an alternative, we suggest using probabilistic instead of discrete predictions. When working with probabilistic predictions, the optimal model does not predict the mode, but a probability distribution across labels equal to the proportion of each label. Formally, for a given stimulus, <span class="math inline">\(\mathbf{S}_{i}\)</span>, with <span class="math inline">\(R\)</span> repeats, the probability of each class, <span class="math inline">\(P(E_{j})\)</span>, is computed as the proportion of labels, <span class="math inline">\(e_{i}\)</span>, equal to that class:</p>
<p><span class="math display">\[\begin{align}
P(E_{j} | \mathbf{S}_{i}) = \frac{1}{R}\sum_{k=1}^{R} \boldsymbol{1}(e_{ik} = E_{j})
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{1}\)</span> represents an indicator function returning 1 when the emotion label <span class="math inline">\(e_{ik}\)</span> is equal to emotion label <span class="math inline">\(E_{j}\)</span> and 0 otherwise. Therefore, in our example data, the optimal prediction for each repetition of <span class="math inline">\(\mathbf{S}_{1}\)</span> is [0.667, 0.333] and the optimal prediction for each repetition of <span class="math inline">\(\mathbf{S}_{2}\)</span> is <span class="math inline">\([0.667, 0.333]\)</span>, where the numbers represent the probability of “Anger” and “Disgust” respectively. Similar to the noise ceiling based on discrete predictions, we can compute the noise ceiling as the performance, for a particular metric, of the optimal model given the true labels. In the above example, the class-average AUROC noise ceiling would coincidentally, just like in the scenario with discrete predictions, be 0.6667.</p>
</div>
</div>
<div id="hka-supp-fig" class="section level2">
<h2><span class="header-section-number">E.2</span> Supplementary figures</h2>
<div class="figure"><span id="fig:fig-hka-S1"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_S1.png" alt="Difference in class-average model performance (AUROC) between discrete and probabilistic predictions."  />
<p class="caption">
Figure E.1: Difference in class-average model performance (AUROC) between discrete and probabilistic predictions.
</p>
</div>

<div class="figure"><span id="fig:fig-hka-S2"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_S2.png" alt="Performance of different models for different values of the “inverse temperature” (\(\beta\)) parameter. A cosine kernel was used."  />
<p class="caption">
Figure E.2: Performance of different models for different values of the “inverse temperature” (<span class="math inline">\(\beta\)</span>) parameter. A cosine kernel was used.
</p>
</div>

<div class="figure"><span id="fig:fig-hka-S3"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_S3.png" alt="Performance of different models for different kernel functions. The cosine, sigmoid, and linear kernels are measures of similarity, but the Euclidean, L1, and L2 kernels measure distance. For these distance functions, the distances were converted to similarities by inverting them. A fixed “inverse temperature” (\(\beta\)) parameter of 1 was used."  />
<p class="caption">
Figure E.3: Performance of different models for different kernel functions. The cosine, sigmoid, and linear kernels are measures of similarity, but the Euclidean, L1, and L2 kernels measure distance. For these distance functions, the distances were converted to similarities by inverting them. A fixed “inverse temperature” (<span class="math inline">\(\beta\)</span>) parameter of 1 was used.
</p>
</div>

<div class="figure"><span id="fig:fig-hka-S4"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_S4.png" alt="Results from the simulation analysis with random mapping matrices in which the number of AUs per configuration (A) and the number of configurations per output class (B) were systematically varied. Bars represents the average AUROC score across 1000 simulations (error bars represent ±1 SD)."  />
<p class="caption">
Figure E.4: Results from the simulation analysis with random mapping matrices in which the number of AUs per configuration (A) and the number of configurations per output class (B) were systematically varied. Bars represents the average AUROC score across 1000 simulations (error bars represent ±1 <em>SD</em>).
</p>
</div>


<div class="figure"><span id="fig:fig-hka-S5"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_S5.png" alt="Changes in model performance (AUROC) for each emotion and mapping after ablating an AU. Error bars indicate a 95% confidence interval obtained with 1000 bootstraps of the data."  />
<p class="caption">
Figure E.5: Changes in model performance (AUROC) for each emotion and mapping after ablating an AU. Error bars indicate a 95% confidence interval obtained with 1000 bootstraps of the data.
</p>
</div>


<div class="figure"><span id="fig:fig-hka-S6"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_S6.png" alt=". The proportion of explained AUROC (from 0.5 to top of bar), unexplained AUROC (from top of bar to noise ceiling), and irreducible noise/variance due to individual differences (from noise ceiling to 1.0) expressed as a percentage of the total AUROC."  />
<p class="caption">
Figure E.6: . The proportion of explained AUROC (from 0.5 to top of bar), unexplained AUROC (from top of bar to noise ceiling), and irreducible noise/variance due to individual differences (from noise ceiling to 1.0) expressed as a percentage of the total AUROC.
</p>
</div>

<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-hka-S2">Table E.2: </span>AUs with the largest ablation affects per emotion across mappings
</caption>
<thead>
<tr>
<th style="text-align:left;">
Emotion
</th>
<th style="text-align:left;">
AU
</th>
<th style="text-align:right;">
<span class="math inline">\(\Delta\)</span> AUROC
</th>
<th style="text-align:left;">
Affected mappings
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 3em; ">
Anger
</td>
<td style="text-align:left;width: 3em; ">
AU09
</td>
<td style="text-align:right;width: 3em; ">
-0.024
</td>
<td style="text-align:left;">
Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU10R
</td>
<td style="text-align:right;width: 3em; ">
-0.017
</td>
<td style="text-align:left;">
EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU10L
</td>
<td style="text-align:right;width: 3em; ">
-0.017
</td>
<td style="text-align:left;">
EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU22
</td>
<td style="text-align:right;width: 3em; ">
-0.012
</td>
<td style="text-align:left;">
Matsumoto, EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:right;width: 3em; ">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
Disgust
</td>
<td style="text-align:left;width: 3em; ">
AU09
</td>
<td style="text-align:right;width: 3em; ">
-0.057
</td>
<td style="text-align:left;">
Matsumoto, Keltner, Cordaro (ref.), Cordaro (ICP), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU10L
</td>
<td style="text-align:right;width: 3em; ">
-0.035
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Cordaro (ICP), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU10R
</td>
<td style="text-align:right;width: 3em; ">
-0.026
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Cordaro (ICP), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU25
</td>
<td style="text-align:right;width: 3em; ">
0.020
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Keltner, Cordaro (ICP), EMFACS
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:right;width: 3em; ">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
Fear
</td>
<td style="text-align:left;width: 3em; ">
AU20R
</td>
<td style="text-align:right;width: 3em; ">
-0.036
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Keltner, Cordaro (ref.), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU20L
</td>
<td style="text-align:right;width: 3em; ">
-0.034
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Keltner, Cordaro (ref.), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU04
</td>
<td style="text-align:right;width: 3em; ">
-0.024
</td>
<td style="text-align:left;">
Matsumoto, Keltner, Cordaro (ref.), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU05
</td>
<td style="text-align:right;width: 3em; ">
-0.022
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU02L
</td>
<td style="text-align:right;width: 3em; ">
0.022
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Keltner, Cordaro (ref.), Cordaro (ICP), EMFACS
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU02R
</td>
<td style="text-align:right;width: 3em; ">
0.013
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Keltner, Cordaro (ref.), Cordaro (ICP), EMFACS
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:right;width: 3em; ">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
Happy
</td>
<td style="text-align:left;width: 3em; ">
AU12L
</td>
<td style="text-align:right;width: 3em; ">
-0.046
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU12R
</td>
<td style="text-align:right;width: 3em; ">
-0.038
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU14L
</td>
<td style="text-align:right;width: 3em; ">
-0.012
</td>
<td style="text-align:left;">
Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU25
</td>
<td style="text-align:right;width: 3em; ">
-0.012
</td>
<td style="text-align:left;">
Keltner, Cordaro (ICP), Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU14R
</td>
<td style="text-align:right;width: 3em; ">
-0.011
</td>
<td style="text-align:left;">
Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU13
</td>
<td style="text-align:right;width: 3em; ">
-0.010
</td>
<td style="text-align:left;">
Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU06R
</td>
<td style="text-align:right;width: 3em; ">
0.017
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU06L
</td>
<td style="text-align:right;width: 3em; ">
0.017
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:right;width: 3em; ">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
Sadness
</td>
<td style="text-align:left;width: 3em; ">
AU04
</td>
<td style="text-align:right;width: 3em; ">
-0.048
</td>
<td style="text-align:left;">
Matsumoto, Keltner, Cordaro (ref.), Cordaro (ICP), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU43
</td>
<td style="text-align:right;width: 3em; ">
-0.040
</td>
<td style="text-align:left;">
Cordaro (ICP), Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU17
</td>
<td style="text-align:right;width: 3em; ">
-0.024
</td>
<td style="text-align:left;">
Matsumoto, Keltner, EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU15
</td>
<td style="text-align:right;width: 3em; ">
-0.010
</td>
<td style="text-align:left;">
Darwin, Matsumoto, Keltner, EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU05
</td>
<td style="text-align:right;width: 3em; ">
0.030
</td>
<td style="text-align:left;">
Cordaro (ref.)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU06R
</td>
<td style="text-align:right;width: 3em; ">
0.021
</td>
<td style="text-align:left;">
Keltner, EMFACS
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU06L
</td>
<td style="text-align:right;width: 3em; ">
0.017
</td>
<td style="text-align:left;">
Keltner, EMFACS
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU11L
</td>
<td style="text-align:right;width: 3em; ">
0.010
</td>
<td style="text-align:left;">
EMFACS
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:right;width: 3em; ">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
Surprise
</td>
<td style="text-align:left;width: 3em; ">
AU01
</td>
<td style="text-align:right;width: 3em; ">
-0.041
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU05
</td>
<td style="text-align:right;width: 3em; ">
-0.041
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU26
</td>
<td style="text-align:right;width: 3em; ">
-0.036
</td>
<td style="text-align:left;">
All
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU27
</td>
<td style="text-align:right;width: 3em; ">
-0.033
</td>
<td style="text-align:left;">
Cordaro (ICP), EMFACS, Jack/Schyns
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
</td>
<td style="text-align:left;width: 3em; ">
AU02L
</td>
<td style="text-align:right;width: 3em; ">
-0.021
</td>
<td style="text-align:left;">
All
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Only AUs with an absolute change in AUROC larger than 0.01 are included. The affected mappings column indicates which mappings contained this AU.
</td>
</tr>
</tfoot>
</table>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>Instead of using measures of similarity between two vectors (i.e., “kernels”), one could use measures of <em>distances</em> (<span class="math inline">\(\delta_{ij}\)</span>) between two vectors instead and subsequently invert it to get a similarity score again, i.e., <span class="math inline">\(\phi_{ij} = \delta_{ij}^{-1}\)</span>. In practice, we find that it does not make much of a difference in terms of predictive performance (see Supplementary Figure <a href="hypothesis-kernel-analysis-supplement.html#fig:fig-hka-S3">E.3</a>).<a href="hypothesis-kernel-analysis-supplement.html#fnref21" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="morbid-curiosity-supplement.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="static-vs-dynamic-supplement.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"download": "docs/thesis.pdf"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
