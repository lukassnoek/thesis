<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses | Towards prediction</title>
  <meta name="description" content="4 The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4 The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="confounds-decoding.html"/>
<link rel="next" href="morbid-curiosity.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>1.1</b> Inference done differently</a></li>
<li class="chapter" data-level="1.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>1.2</b> Towards prediction</a></li>
<li class="chapter" data-level="1.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.2.7</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.3</b> Results</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.3.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.3.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
<li class="chapter" data-level="4.4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.4.6</b> Code availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="5.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>5.2.2</b> Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>5.2.3</b> Materials</a></li>
<li class="chapter" data-level="5.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>5.2.4</b> Procedure</a></li>
<li class="chapter" data-level="5.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>5.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="5.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>5.2.6</b> Imaging details</a></li>
<li class="chapter" data-level="5.2.7" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>5.2.7</b> Data availability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>5.3</b> Results</a><ul>
<li class="chapter" data-level="5.3.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>5.3.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="5.3.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>5.3.3</b> ROI analyses</a></li>
<li class="chapter" data-level="5.3.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>5.3.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>6</b> Explainable models of facial movements predict emotion perception behavior</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#the-prediction-explanation-exploration-framework"><i class="fa fa-check"></i><b>6.1.1</b> The prediction-explanation-exploration framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>6.2.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>6.2.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>6.2.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="6.2.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>6.2.4</b> Evaluated mappings</a></li>
<li class="chapter" data-level="6.2.5" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-dataset"><i class="fa fa-check"></i><b>6.2.5</b> Dataset used to evaluate mappings</a></li>
<li class="chapter" data-level="6.2.6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-code"><i class="fa fa-check"></i><b>6.2.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#prediction"><i class="fa fa-check"></i><b>6.3.1</b> Prediction</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#explanation"><i class="fa fa-check"></i><b>6.3.2</b> Explanation</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#exploration"><i class="fa fa-check"></i><b>6.3.3</b> Exploration</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>7</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="7.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>7.2.1</b> Participants</a></li>
<li class="chapter" data-level="7.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>7.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="7.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>7.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>7.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="7.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>7.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>7.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="7.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>7.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>7.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="7.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>7.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>7.4</b> Discussion</a><ul>
<li class="chapter" data-level="7.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>7.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="7.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>7.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="7.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>7.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="7.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>7.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>8</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a><ul>
<li class="chapter" data-level="E.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supplementary-methods"><i class="fa fa-check"></i><b>E.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="E.1.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-in-detail"><i class="fa fa-check"></i><b>E.1.1</b> Hypothesis kernel analysis (in detail)</a></li>
<li class="chapter" data-level="E.1.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-noise-ceiling-detail"><i class="fa fa-check"></i><b>E.1.2</b> Noise ceiling estimation (in detail)</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supp-fig"><i class="fa fa-check"></i><b>E.2</b> Supplementary figures</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 7</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aomic" class="section level1">
<h1><span class="header-section-number">4</span> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Snoek, L., van der Miesen, M.M., Beemsterboer, T., van der Leij, A., Eigenhuis, A., &amp; Scholte, H.S. (2021). The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses. <em>Nature Scientific Data, 8</em>, 85.</p>

<p><p><strong>Abstract</strong></p>

We present the Amsterdam Open MRI Collection (AOMIC): three datasets with multimodal (3T) MRI data including structural (T1-weighted), diffusion-weighted, and (resting-state and task-based) functional BOLD MRI data, as well as detailed demographics and psychometric variables from a large set of healthy participants (<em>N</em> = 928, <em>N</em> = 226, and <em>N</em> = 216). Notably, task-based fMRI was collected during various robust paradigms (targeting naturalistic vision, emotion perception, working memory, face perception, cognitive conflict and control, and response inhibition) for which extensively annotated event-files are available. For each dataset and data modality, we provide the data in both raw and preprocessed form (both compliant with the Brain Imaging Data Structure), which were subjected to extensive (automated and manual) quality control. All data is publicly available from the OpenNeuro data sharing platform.
</p>
<div id="background-summary" class="section level2">
<h2><span class="header-section-number">4.1</span> Background &amp; summary</h2>
<p>It is becoming increasingly clear that robust effects in neuroimaging studies require very large sample sizes <span class="citation">(Button et al., <a href="bibliography.html#ref-Button2013-zu" role="doc-biblioref">2013</a>; Yarkoni, <a href="bibliography.html#ref-Yarkoni2009-pz" role="doc-biblioref">2009</a>)</span>, especially when investigating between-subject effects <span class="citation">(Dubois &amp; Adolphs, <a href="bibliography.html#ref-Dubois2016-zz" role="doc-biblioref">2016</a>)</span>. With this in mind, we have run several large-scale “population imaging” MRI projects over the past decade at the University of Amsterdam, with the aim to reliably estimate the (absence) of structural and functional correlates of human behavior and mental processes. After publishing several articles using these datasets <span class="citation">(Elk &amp; Snoek, <a href="bibliography.html#ref-Van_Elk2020-xo" role="doc-biblioref">2020</a>; Hoogeveen et al., <a href="bibliography.html#ref-Hoogeveen2020-qp" role="doc-biblioref">2020</a>; Koolschijn et al., <a href="bibliography.html#ref-Koolschijn2015-hd" role="doc-biblioref">2015</a>; Ramakrishnan et al., <a href="bibliography.html#ref-Ramakrishnan2014-ki" role="doc-biblioref">2014</a>; Snoek et al., <a href="bibliography.html#ref-Snoek2019-my" role="doc-biblioref">2019</a>)</span>, we believe that making the data from these projects publicly available will benefit the neuroimaging community most. To this end, we present the Amsterdam Open MRI Collection (AOMIC) — three large-scale datasets with high-quality, multimodal 3T MRI data and detailed demographic and psychometric data, which are publicly available from the OpenNeuro data sharing platform. In this article, we describe the characteristics and contents of these three datasets in a manner that complies with the guidelines of the COBIDAS MRI reporting framework <span class="citation">(Nichols et al., <a href="bibliography.html#ref-Nichols2017-ze" role="doc-biblioref">2017</a>)</span>.</p>
<p>We believe that AOMIC represents a useful contribution to the growing collection of publicly available population imaging MRI datasets <span class="citation">(Babayan et al., <a href="bibliography.html#ref-Babayan2019-mo" role="doc-biblioref">2019</a>; Mendes et al., <a href="bibliography.html#ref-Mendes2019-yh" role="doc-biblioref">2019</a>; Miller et al., <a href="bibliography.html#ref-Miller2016-oi" role="doc-biblioref">2016</a>; Van Essen et al., <a href="bibliography.html#ref-Van_Essen2013-df" role="doc-biblioref">2013</a>)</span>. AOMIC contains a large representative dataset of the general population, “ID1000” (<em>N</em> = 928), and two large datasets with data from university students, “PIOP1” (<em>N</em> = 216) and “PIOP2” (<em>N</em> = 226; <em>P</em>opulation <em>I</em>maging <em>o</em>f <em>P</em>sychology). Each dataset contains MRI data from multiple modalities (structural, diffusion, and functional MRI), concurrently measured physiological (respiratory and cardiac) data, and a variety of well-annotated demographics (age, sex, handedness, educational level, etc.), psychometric measures (intelligence, personality), and behavioral information related to the task-based fMRI runs (see Figure <a href="aomic.html#fig:fig-aomic-1">4.1</a> and Table <a href="aomic.html#tab:tab-aomic-1">4.1</a> for an overview). Furthermore, AOMIC offers, in addition to the raw data, also preprocessed data from well-established preprocessing and quality control pipelines, all consistently formatted according to the Brain Imaging Data Structure <span class="citation">(Gorgolewski et al., <a href="bibliography.html#ref-Gorgolewski2016-in" role="doc-biblioref">2016</a>)</span>. As such, researchers can quickly and easily prototype and implement novel secondary analyses without having to worry about quality control and preprocessing themselves.</p>
<p>Due to the size and variety of the data in AOMIC, there are many ways in which it can be used for secondary analysis. One promising direction is to use the data for the development of generative and discriminative machine learning-based algorithms, which often need large datasets to train models on. Another, but related, use of AOMIC’s data is to use it as a validation dataset (rather than train-set) for already developed (machine learning) algorithms to assess the algorithm’s ability to generalize to different acquisition sites or protocols. Lastly, due to the rich set of confound variables shipped with each dataset (including physiology-derived noise regressors), AOMIC can be used to develop, test, or validate (novel) denoising methods.</p>
<div class="figure"><span id="fig:fig-aomic-1"></span>
<img src="_bookdown_files/aomic-files/figures/figure_1.png" alt="General overview of AOMIC’s contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of “derivatives”, i.e., data derived from the original “raw” data through state-of-the-art preprocessing pipelines."  />
<p class="caption">
Figure 4.1: General overview of AOMIC’s contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of “derivatives”, i.e., data derived from the original “raw” data through state-of-the-art preprocessing pipelines.
</p>
</div>


<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-1">Table 4.1: </span>Overview of the number of subjects per dataset and tasks.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
ID1000
</th>
<th style="text-align:left;">
PIOP1
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
PIOP2
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 5em; ">
N subj.
</td>
<td style="text-align:left;">
928
</td>
<td style="text-align:left;">
216
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
226
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
T1w
</td>
<td style="text-align:left;">
928
</td>
<td style="text-align:left;">
216
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
226
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
DWI
</td>
<td style="text-align:left;">
925
</td>
<td style="text-align:left;">
211
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
226
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
Fieldmap
</td>
<td style="text-align:left;">
n/a
</td>
<td style="text-align:left;">
n/a
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
226
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Mov
</td>
<td style="text-align:left;">
RS
</td>
<td style="text-align:left;">
Emo
</td>
<td style="text-align:left;">
G-str
</td>
<td style="text-align:left;">
FP
</td>
<td style="text-align:left;">
WM
</td>
<td style="text-align:left;">
Antic
</td>
<td style="text-align:left;">
RS
</td>
<td style="text-align:left;">
Emo
</td>
<td style="text-align:left;">
WM
</td>
<td style="text-align:left;">
Stop
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
fMRI
</td>
<td style="text-align:left;">
881
</td>
<td style="text-align:left;">
210
</td>
<td style="text-align:left;">
208
</td>
<td style="text-align:left;">
208
</td>
<td style="text-align:left;">
203
</td>
<td style="text-align:left;">
207
</td>
<td style="text-align:left;">
203
</td>
<td style="text-align:left;">
214
</td>
<td style="text-align:left;">
222
</td>
<td style="text-align:left;">
224
</td>
<td style="text-align:left;">
226
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
Physiology
</td>
<td style="text-align:left;">
790
</td>
<td style="text-align:left;">
198
</td>
<td style="text-align:left;">
194
</td>
<td style="text-align:left;">
194
</td>
<td style="text-align:left;">
189
</td>
<td style="text-align:left;">
194
</td>
<td style="text-align:left;">
188
</td>
<td style="text-align:left;">
216
</td>
<td style="text-align:left;">
216
</td>
<td style="text-align:left;">
211
</td>
<td style="text-align:left;">
217
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Mov: movie watching, RS: resting-state, Emo: emotion matching, G-str: gender-stroop, FP: face perception, WM: working memory, Antic: anticipation, Stop: stop-signal. Tasks without this subscript were acquired without multiband acceleration (i.e., “sequential” acquisition).
</td>
</tr>
</tfoot>
</table>

</div>
<div id="methods" class="section level2">
<h2><span class="header-section-number">4.2</span> Methods</h2>
<p>In this section, we describe the details of the data acquisition for each dataset in AOMIC. We start with a common description of the MRI scanner used to collect the data. The next two sections describe the participant characteristics, data collection protocols, experimental paradigms (for functional MRI), and previous analyses separately for the ID1000 study and the PIOP studies. Then, two sections describe the recorded subject-specific variables (such as educational level, background socio-economic status, age, etc.) and psychometric measures from questionnaires and tasks (such as intelligence and personality). Finally, we describe how we standardized and preprocessed the data, yielding an extensive set of “derivatives” (i.e., data derived from the original raw data).</p>
<div id="scanner-details-and-general-scanning-protocol-all-datasets" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Scanner details and general scanning protocol (all datasets)</h3>
<p>Data from all three datasets were acquired on the same Philips 3T scanner (Philips, Best, the Netherlands), but underwent several upgrades in between the three studies. The ID1000 dataset was scanned on the “Intera” version, after which the scanner was upgraded to the “Achieva” version (converting a part of the signal acquisition pathway from analog to digital) on which the PIOP1 dataset was scanned. After finishing the PIOP1 study, the scanner was upgraded to the “Achieva dStream” version (with even earlier digitalization of the MR signal resulting in less noise interference), on which the PIOP2 study was scanned. All studies were scanned with a 32-channel head coil (though the head coil was upgraded at the same time as the dStream upgrade). Although not part of AOMIC, a separate dataset (dStreamUpgrade) with raw and preprocessed data (DWI, T1-weighted, and both resting-state and task-based functional MRI scans) acquired before and after the dStream upgrade using the same sequences and participants in both sessions is available on OpenNeuro <span class="citation">(K Gorgolewski et al., <a href="bibliography.html#ref-Gorgolewski2017-uu" role="doc-biblioref">2017</a>)</span>. In addition, unthresholded group-level temporal signal-to-noise ratio (tSNR) maps and group-level GLM maps (for both the pre-upgrade and post-upgrade data as well as the post-pre difference) are available on NeuroVault <span class="citation">(Gorgolewski, Varoquaux, Rivera, Schwarz, Ghosh, Maumet, Sochat, Nichols, Poldrack, Poline, Yarkoni, et al., <a href="bibliography.html#ref-Gorgolewski2015-hj" role="doc-biblioref">2015</a>)</span>.</p>
<p>At the start of each scan session, a low resolution survey scan was made, which was used to determine the location of the field-of-view. For all structural (T1-weighted), fieldmap (phase-difference based B0 map), and diffusion (DWI) scans, the slice stack was not angled. This was also the case for the functional MRI scans of ID1000 and PIOP1, but for PIOP2 the slice stack for functional MRI scans was angled such that the eyes were excluded as much as possible in order to reduce signal dropout in orbitofrontal cortex. While the set of scans acquired for each study is relatively consistent (i.e., at least one T1-weighted anatomical scan, at least one diffusion-weighted scan, and at least one functional BOLD MRI scan), the parameters for a given scan vary slightly between the three studies. Notably, the functional MRI data from the resting-state and faces tasks from PIOP1 were acquired with multi-slice acceleration (factor 3; referred to in the current article as “multiband” acquisition), while all other functional MRI scans, including those from ID1000 and PIOP2, were acquired without multi-slice acceleration (referred to in the current article as “sequential” acquisition). The reason for reverting to sequential (instead of multiband) acquisition of the resting state scan in PIOP2 is that we did not think the reduction in tSNR observed in multiband scans outweighs the increase in number of volumes. In Supplementary Tables <a href="aomic-supplement.html#tab:tab-aomic-S1">C.1</a>-<a href="aomic-supplement.html#tab:tab-aomic-S4">C.4</a>, the parameters for the different types of scans across all three studies are listed.</p>
<p>During functional MRI scans, additional physiological data was recorded. Respiratory traces were recorded using a respiratory belt (air filled cushion) bound on top of the subject’s diaphragm using a velcro band. Cardiac traces were recorded using a plethysmograph attached to the subject’s left ring finger. Data was transferred to the scanner PC as plain-text files (Philips “SCANPHYSLOG” files) using a wireless recorder with a sampling frequency of 496 Hz.</p>
<p>Experimental paradigms for the functional MRI runs were shown on a 61 <span class="math inline">\(\times\)</span> 36 cm screen using a DLP projector with a 60 Hz refresh rate (ID1000 and PIOP1) or on a Cambridge Electronics BOLDscreen 32 IPS LCD screen with a 120 Hz refresh rate (PIOP2), both placed at 113 cm distance from the mirror mounted on top of the head coil. Sound was presented via a MRConfon sound system. Experimental tasks were programmed using Neurobs Presentation (Neurobehavioral Systems Inc, Berkeley, U.S.A.) and run on a Windows computer with a dedicated graphics card. To allow subject responses in experimental tasks (PIOP1 and PIOP2 only), participants used MRI-compatible fibre optic response pads with four buttons for each hand (Cambridge Research Systems, Rochester, United Kingdom).</p>
</div>
<div id="id1000-specifics" class="section level3">
<h3><span class="header-section-number">4.2.2</span> ID1000 specifics</h3>
<p>In this section, we describe the subject recruitment, subject characteristics, data collection protocol, functional MRI paradigm, and previous analyses of the ID1000 study.</p>
<div id="subjects" class="section level4">
<h4><span class="header-section-number">4.2.2.1</span> Subjects</h4>
<p>The data from the ID1000 sample was collected between 2010 and 2012. The faculty’s ethical committee approved this study before data collection started (EC number: 2010-BC-1345). Potential subjects were a priori excluded from participation if they did not pass a safety checklist with MRI contraindications. We recorded data from 992 subjects of which 928 are included in the dataset (see <a href="#aomic-technical-validation">Technical validation</a> for details on the post-hoc exclusion procedure). Subjects were recruited through a recruitment agency (Motivaction International B.V.) in an effort to get a sample that was representative of the general Dutch population in terms of educational level <span class="citation">(as defined by the Dutch government, Onderwijsindeling, <a href="bibliography.html#ref-Onderwijsindeling2016-tb" role="doc-biblioref">2016</a>)</span>, but drawn from only a limited age range (19 to 26). We chose this limited age range to minimize the effect of aging on any brain-related covariates. A more detailed description of educational level and other demographic variables can be found in the section “Subject variables”.</p>
</div>
<div id="data-collection-protocol" class="section level4">
<h4><span class="header-section-number">4.2.2.2</span> Data collection protocol</h4>
<p>Prior to the experiment, subjects were informed about the goal and scope of the research, the MRI procedure, safety measures, general experimental procedures, privacy and data sharing concerns, and voluntary nature of the project (i.e., subjects were told that they could stop with the experiment at any time, without giving a reason for it). Before coming to the scan center, subjects also completed a questionnaire on background information (to determine educational level, which was used to draw a representative sample). If participants were invited they provided informed consent and completed an MRI screening checklist. Subjects then completed an extensive set of questionnaires and tests, including a general demographics questionnaire, the Intelligence Structure Test <span class="citation">(Amthauer et al., <a href="bibliography.html#ref-Amthauer2001-yg" role="doc-biblioref">2001</a>; Vorst, <a href="bibliography.html#ref-Vorst2010-ex" role="doc-biblioref">2010</a>)</span>, the “trait” part of the State-Trait Anxiety Inventory <span class="citation">(STAI; Spielberger et al., <a href="bibliography.html#ref-Spielberger1970-td" role="doc-biblioref">1970</a>)</span>, a behavioral avoidance/inhibition questionnaire <span class="citation">(BIS/BAS; Carver &amp; White, <a href="bibliography.html#ref-Carver1994-wp" role="doc-biblioref">1994</a>; Franken et al., <a href="bibliography.html#ref-Franken2005-jg" role="doc-biblioref">2005</a>)</span>, multiple personality questionnaires — amongst them the MPQ <span class="citation">(Eigenhuis et al., <a href="bibliography.html#ref-Eigenhuis2013-xo" role="doc-biblioref">2013</a>)</span> and the NEO-FFI <span class="citation">(Hoekstra et al., <a href="bibliography.html#ref-Hoekstra1996-kv" role="doc-biblioref">1996</a>; Van der Ploeg, <a href="bibliography.html#ref-Van_der_Ploeg1980-tq" role="doc-biblioref">1980</a>)</span> and several behavioral tasks. The psychometric variables of the tests included in the current dataset are described in the section “Psychometric variables”.</p>
<p>Testing took place from 9 AM until 4 PM and on each day two subjects were tested. One subject began with the IST intelligence test, while the other subject started with the imaging part of the experiment. For the MRI part, we recorded three T1-weighted scans, three diffusion-weighted scans, and one functional (BOLD) MRI scan (in that order). The scanning session lasted approximately 60 minutes. Afterwards, the subjects switched and completed the other part. After these initial tasks, the subjects participated in additional experimental tasks, some of which have been reported in other publications <span class="citation">(Gazendam et al., <a href="bibliography.html#ref-Gazendam2015-fr" role="doc-biblioref">2015</a>; Pinto et al., <a href="bibliography.html#ref-Pinto2013-kh" role="doc-biblioref">2013</a>)</span> and are not included in this dataset.</p>
</div>
<div id="functional-mri-paradigm" class="section level4">
<h4><span class="header-section-number">4.2.2.3</span> Functional MRI paradigm</h4>
<p>During functional MRI acquisition, subjects viewed a movie clip consisting of a (continuous) compilation of 22 natural scenes taken from the movie Koyaanisqatsi <span class="citation">(Reggio, <a href="bibliography.html#ref-Reggio1982-ex" role="doc-biblioref">1982</a>)</span> with music composed by Philip Glass. The scenes were selected because they broadly sample a set of visual parameters (textures and objects with different sizes and different rates of movement). Importantly, the focus on variation of visual parameters means, in this case, that the movie lacks a narrative and thus may be inappropriate to investigate semantic or other high-level processes.</p>
<p>The scenes varied in length from approximately 5 to 40 seconds with “cross dissolve” transitions between scenes. The movie clip extended 16 degrees visual angle (resolution 720 <span class="math inline">\(\times\)</span> 576, movie frame rate of 25 Hz). The onset of the movie clip was triggered by the first volume of the fMRI acquisition and had a duration of 11 minutes (which is slightly longer than the fMRI scan, i.e., 10 minutes and 38 seconds). The movie clip is available in the “stimuli” subdirectory of the ID1000 dataset (with the filename <code>task-moviewatching_desc-koyaanisqatsi_movie.mp4</code>).</p>
</div>
<div id="previous-analyses" class="section level4">
<h4><span class="header-section-number">4.2.2.4</span> Previous analyses</h4>
<p>The MRI data of ID1000 has been analyzed previously by two studies. One study <span class="citation">(Ramakrishnan et al., <a href="bibliography.html#ref-Ramakrishnan2014-ki" role="doc-biblioref">2014</a>)</span> analyzed the functional MRI data of a subset of 20 subjects by relating features from computational models applied to the movie data to the voxelwise time series using representational similarity analysis. Another study <span class="citation">(Koolschijn et al., <a href="bibliography.html#ref-Koolschijn2015-hd" role="doc-biblioref">2015</a>)</span> analyzed the relationship between autistic traits and voxel-based morphometry (VBM, derived from the T1-weighted scans) as well as fractional anisotropy (FA, derived from the DWI scans) in a subset of 508 subjects.</p>
</div>
</div>
<div id="piop1-and-piop2-specifics" class="section level3">
<h3><span class="header-section-number">4.2.3</span> PIOP1 and PIOP2 specifics</h3>
<p>In this section, we describe the subject recruitment, subject characteristics, data collection protocol, functional MRI paradigm, and previous analyses of the PIOP1 and PIOP2 studies. These two studies are described in a common section because their data collection protocols were very similar. Information provided in this section (including sample characteristics, sample procedure, data acquisition procedure and scan sequences) can be assumed to apply to both PIOP1 and PIOP2, unless explicitly stated otherwise. As such, these datasets can be used as train/test (or validation) partitions. The last subsection (“Differences between PIOP1 and PIOP2”) describes the main differences between the two PIOP datasets.</p>
<div id="subjects-1" class="section level4">
<h4><span class="header-section-number">4.2.3.1</span> Subjects</h4>
<p>Data from the PIOP1 dataset were collected between May 2015 and April 2016 and data from the PIOP2 dataset between March 2017 and July 2017. The faculty’s ethical committee approved these studies before data collection started (PIOP1 EC number: 2015-EXT-4366, PIOP2 EC number: 2017-EXT-7568). Potential subjects were a priori excluded from participation if they did not pass a safety checklist with MRI contraindications. Data was recorded from 248 subjects (PIOP1) and 242 subjects (PIOP2), of which 216 (PIOP1) and 226 (PIOP2) are included in AOMIC (see <a href="#aomic-technical-validation">Technical validation</a> for details on the post-hoc exclusion procedure). Subjects from both PIOP1 and PIOP2 were all university students (from the Amsterdam University of Applied Sciences or the University of Amsterdam) recruited through the University websites, posters placed around the university grounds, and Facebook. A description of demographic and other subject-specific variables can be found in the section “Subject variables”.</p>
</div>
<div id="data-collection-protocol-1" class="section level4">
<h4><span class="header-section-number">4.2.3.2</span> Data collection protocol</h4>
<p>Prior to the research, subjects were informed about the goal of the study, the MRI procedure and safety, general experimental procedure, privacy and data sharing issues, and the voluntary nature of participation through an information letter. Each testing day (which took place from 8.30 AM until 1 PM), four subjects were tested. First, all subjects filled in an informed consent form and completed an MRI screening checklist. Then, two subjects started with the MRI part of the experiment, while the other two completed the demographic and psychometric questionnaires (described below) as well as several tasks that are not included in AOMIC.</p>
<p>The MRI session included a survey scan, followed by a T1-weighted anatomical scan. Then, several functional MRI runs (described below) and a single diffusion-weighted scan were recorded. Details about the scan parameters can be found in Supplementary Tables <a href="aomic-supplement.html#tab:tab-aomic-S1">C.1</a>-<a href="aomic-supplement.html#tab:tab-aomic-S4">C.4</a>. The scan session lasted approximately 60 minutes. The scans were always recorded in the same order. For PIOP1, this was the following: <em>faces</em> (fMRI), <em>gender-stroop</em> (fMRI), T1-weighted scan, <em>emotion matching</em> (fMRI), <em>resting-state</em> (fMRI), <em>phase-difference</em> fieldmap (B0) scan, DWI scan, <em>working memory</em> (fMRI), <em>emotion anticipation</em> (fMRI). For PIOP2, this was the following: T1-weighted scan, <em>working memory</em> (fMRI), <em>resting-state</em> (fMRI), DWI scan, <em>stop signal</em> (fMRI), <em>emotion matching</em> (fMRI).</p>
</div>
<div id="functional-mri-paradigms" class="section level4">
<h4><span class="header-section-number">4.2.3.3</span> Functional MRI paradigms</h4>
<p>In this section, we will describe the experimental paradigms used during fMRI acquisition. See Figure <a href="aomic.html#fig:fig-aomic-2">4.2</a> for a visual representation of each paradigm. None of the designs were optimized for efficiency, except for the emotion anticipation task (which was optimized for efficiency to estimate the effect of each condition). All paradigms during functional MRI were started and synced to the functional MRI acquisition using a transistor-transistor logic (TTL) pulse sent to the stimulus presentation computer.</p>
<div class="figure"><span id="fig:fig-aomic-2"></span>
<img src="_bookdown_files/aomic-files/figures/figure_2.png" alt="A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval."  />
<p class="caption">
Figure 4.2: A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.
</p>
</div>

<p><strong>Emotion matching (PIOP1+2)</strong>. The goal of the “emotion matching” task is to measure processes related to (facial) emotion processing. The paradigm we used was based on <span class="citation">Hariri et al. (<a href="bibliography.html#ref-Hariri2000-sc" role="doc-biblioref">2000</a>)</span>. Trials were presented using a blocked design. In each trial, subjects were presented with either color images of an emotional target face (top) and two emotional probe faces (bottom left and bottom right; “emotion” condition) or a target oval (top) and two probe ovals (bottom left and bottom right; “control” condition) on top of a gray background (RGB: 248, 248, 248) and were instructed to either match the emotional expression of the target face (“emotion” condition) or the orientation or the target oval (“control” condition) as quickly as possible by pushing a button with the index finger of their left or right hand. The target and probes disappeared when the subject responded (or after 4.8 seconds). A new trial always appeared 5 seconds after the onset of each trial. In between the subject’s response and the new trial, a blank screen was shown. Trials were presented in alternating “control” and “emotion” blocks consisting of six stimuli of 5 seconds each (four blocks each, i.e., 48 stimuli in total). Stimuli always belonged to the same block, but the order of stimuli within blocks was randomized across participants. The task took 270 seconds in total (i.e., 135 volumes with a 2 second TR).</p>
<p>The faces always displayed either stereotypical anger or fear. Within trials, always exactly two faces portrayed the same expression. Both male and female faces and white, black, and Asian faces were used, but within a single trial, faces were always of the same sex and ethnicity category (white or Asian/black). Face pictures were derived from the NimStim Face Stimulus set <span class="citation">(Tottenham et al., <a href="bibliography.html#ref-Tottenham2009-vn" role="doc-biblioref">2009</a>)</span>. The oval stimuli were created by pixelating the face stimuli and were approximately the same area as the face stimuli (making them color and size matched to the face stimuli) and were either presented horizontally (i.e., the long side was horizontally aligned) or vertically (i.e., the long side was vertically aligned). Within trials, always exactly two ovals were aligned in the same way.</p>
<p>The fMRI “event files” (with the identifier *_events*) associated with this task contain information of the trial onset (the moment the faces/ovals appeared on screen, in seconds), duration (how long the faces/ovals were presented, in seconds), trial type (either “control” or “emotion”), response time (how long it took the subject to respond, logged “n/a” in case of no response), response hand (either “left”, “right”, or “n/a” in case of no response), response accuracy (either “correct”, “incorrect”, or “miss”), orientation to match (either “horizontal”, “vertical”, or “n/a” in case of emotion trials), emotion match (either “fear”, “anger”, or “n/a” in case of control trials), gender of the faces (either “male”, “female”, of “n/a” in case of control trials), and ethnicity of the target and probe faces (either “caucasian”, “asian”, “black”, or “n/a” in case of control trials).</p>
<p><strong>Working memory task (PIOP1+2)</strong>. The goal of the working memory task was to measure processes related to visual working memory. The paradigm we used was based on <span class="citation">Pessoa et al. (<a href="bibliography.html#ref-Pessoa2002-tb" role="doc-biblioref">2002</a>)</span>. Trials were presented using a fixed event-related design, in which trial order the same for each subject. Trials belonged to one of three conditions: “active (change)”, “active (no change)”, or “passive”. Each trial consisted of six phases: an alert phase (1 second), an encoding phase (1 second), a retention phase (2 seconds), a test phase (1 second), a response phase (1 second), and an inter-stimulus interval (0-4 seconds). Subjects were instructed to keep focusing on the fixation target, which was shown throughout the entire trial, and completed a set of practice trials before the start of the actual task. The task took 324 seconds in total (i.e., 162 volumes with a 2 second TR).</p>
<p>In all trial types, trials started with an alert phase: a change of color of the fixation sign (a white plus sign changing to green, RGB [0, 255, 0]), lasting for 1 second. In the encoding phase, for “active” trials, an array of six white bars with a size of 2 degrees visual angle with a random orientation (either 0, 45, 90, or 135 degrees) arranged in a circle was presented for 1 second. This phase of the trial coincided with a change in background luminance from black (RGB: [0, 0, 0]) to gray (RGB: [120, 120, 120]). For “passive” trials, only the background luminance changed (but no bars appeared) in the encoding phase. In the subsequent retention phase, for all trial types, a fixation cross was shown and the background changed back to black, lasting 2 seconds. In the test phase, one single randomly chosen bar appeared (at one of the six locations from the encoding phase) which either matched the original orientation (for “active (no change)” trials) or did not match the original orientation (for “active (change)” trials), lasting for 1 second on a gray background. For “passive” trials, the background luminance changed and, instead of a bar, the cue “respond left” or “respond right” was shown in the test phase. In the response phase, lasting 1 second, the background changed back to black and, for “active” trials, subjects had to respond whether the array changed (button press with right index finger) or did not change (button press with left index finger). For “passive” trials, subjects had to respond with the hand that was cued in the test phase. In the inter-stimulus interval (which varied from 0 to 4 seconds), only a black background with a fixation sign was shown.</p>
<p>In total, there were 8 “passive” trials, 16 “active (change)” and “active (no change)” trials, in addition to 20 “null” trials of 6 seconds (which are equivalent to an additional inter-stimulus interval of 6 seconds). The sequence trials was, in terms of conditions (active, passive, null) exactly the same for all participations in both PIOP1 and PIOP2, but which bar or cue was shown in the test phase was chosen randomly.</p>
<p>The fMRI event files associated with this task contain information of the trial onset (the moment when the alert phase started, in seconds), duration (from the alert phase up to and including the response phase, i.e., always 6 seconds), trial type (either “active (change)”, “active (no change)”, or “passive”; “null” trials were not logged), response time (how long it took the subject to respond, “n/a” in case of no response), response hand (either “left”, “right”, or “n/a” in case of no response), and response accuracy (either “correct”, “incorrect”, or “miss”). Note that, in order to model the response to one or more phases of the trial, the onsets and durations should be adjusted accordingly (e.g., to model the response to the retention phase, add 2 seconds to all onsets and change the duration to 2 seconds).</p>
<p><strong>Resting state (PIOP1+2)</strong>. During the resting state scans, participants were instructed to keep their gaze fixated on a fixation cross in the middle of the screen with a gray background (RGB: [150, 150, 150]) and to let their thoughts run freely. Eyetracking data was recorded during this scan but is not included in this dataset. The resting state scans lasted 6 minutes (PIOP1; i.e., 480 volumes with a 0.75 second TR) and 8 minutes (PIOP2; i.e., 240 volumes with a 2 second TR).</p>
<p><strong>Face perception (PIOP1)</strong>. The face perception task was included to measure processes related to (emotional) facial expression perception. Trials were presented using an event-related design, in which trial order was randomized per subject. In each trial, subjects passively viewed dynamic facial expressions (i.e., short video clips) taken from the Amsterdam Facial Expression Set <span class="citation">(ADFES; Schalk et al., <a href="bibliography.html#ref-Van_der_Schalk2011-bq" role="doc-biblioref">2011</a>)</span>, which displayed either anger, contempt, joy, or pride, or no expression (“neutral”). Each clip depicted a facial movement from rest to a full expression corresponding to one of the four emotions, except for “neutral” faces, which depicted no facial movement. All clips lasted 2 seconds and contained either North-European or Mediterranean models, all of whom were female. After each video, a fixed inter-stimulus interval of 5 seconds followed. Each emotional facial expression (including “neutral”) was shown 6 times (with different people showing the expression each time), except for one, which was shown 9 times. Which emotional expression (or “neutral”) was shown an extra three times was determined randomly for each subject. Importantly, the three extra presentations always contained the same actor and were always presented as the first three trials. This was done in order to make it possible to evaluate the possible effects of stimulus repetition. The task took 247.5 seconds in total (i.e., 330 volumes with a 0.75 second TR).</p>
<p>The fMRI event files associated with this task contain information of the trial onset (the moment when the clip appeared on screen, in seconds), duration (of the clip, i.e., always 2 seconds), trial type (either “anger”, “joy”, “contempt”, “pride”, or “neutral”), sex of the model (all “female”), ethnicity of the model (either “North-European” or “Mediterranean”), and the ADFES ID of the model.</p>
<p><strong>Gender-stroop task (PIOP1 only)</strong>. The goal of the gender-stroop task was to measure processes related to cognitive conflict and control <span class="citation">(Milham et al., <a href="bibliography.html#ref-Milham2003-zc" role="doc-biblioref">2003</a>; see also Hoogeveen et al., <a href="bibliography.html#ref-Hoogeveen2020-qp" role="doc-biblioref">2020</a> for an investigation of these processes using the PIOP1 gender-stroop data)</span>. We used the face-gender variant of the Stroop task <span class="citation">(which was adapted from Egner et al., <a href="bibliography.html#ref-Egner2010-ot" role="doc-biblioref">2010</a>)</span>, often referred to as the “gender-stroop” task. In this task, pictures of twelve different male and twelve different female faces are paired with the corresponding (i.e., congruent) or opposite (i.e., incongruent) label. For the labels, we used the Dutch words for “man”, “sir”, “woman”, and “lady” using either lower or upper case letters. The labels were located just above the head of the face. Trials were presented using an event-related design, in which trial order was randomized per subject. The task took 490 seconds in total (i.e., 245 volumes with a 2 second TR).</p>
<p>On each trial, subjects were shown a face-label composite on top of a gray background (RGB: [105, 105, 105]) for 0.5 seconds, which was either “congruent” (same face and label gender) or “incongruent” (different face and label gender). Stimulus presentation was followed by an inter-stimulus interval ranging between 4 and 6 seconds (in steps of 0.5 seconds). Subjects were always instructed to respond to the gender of the pictured face, ignoring the distractor word, as fast as possible using their left index finger (for male faces) or right index finger (for female faces). There were 48 stimuli for each condition (“congruent” and “incongruent”).</p>
<p>The fMRI event files associated with this task contain information of the trial onset (the moment when the face-label composite appeared on screen, in seconds), duration (of the face-label composite, i.e., always 0.5 seconds), trial type (either “incongruent” or “congruent”), gender of the face (either “male or “female”), gender of the word (either “male” or “female”), response time (in seconds), response hand (either “left”, “right”, or “n/a” in case of no response), response accuracy (either “correct”, “incorrect”, or “miss”).</p>
<p><strong>Emotion anticipation task (PIOP1 only)</strong>. We included the emotion anticipation task to measure processes related to (emotional) anticipation and curiosity. The paradigm was based on paradigms previously used to investigate (morbid) curiosity <span class="citation">(Oosterwijk, <a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">2017</a><a href="bibliography.html#ref-oosterwijk2017choosing" role="doc-biblioref">a</a>; Oosterwijk et al., <a href="bibliography.html#ref-Oosterwijk2020-uf" role="doc-biblioref">2020</a>)</span>. Trials were presented using an event-related design, in which trial order was randomized per subject. In this task, subjects viewed a series of trials containing a cue and an image. The cue could either signal an 80% chance of being followed by a negatively valenced image (and a 20% chance of a neutral image) or an 80% chance of being followed by a neutral image (and a 20% change of a negatively valenced image). The cue was shown on top of a black background for 2 seconds, followed by a fixed interval of 3 seconds. After this interval, either a negative or neutral image was shown for 3 seconds, with a frequency that corresponds to the previously shown cue. In other words, for all trials with, for example, a cue signalling an 80% chance of being followed by a neutral image, it was in fact followed by a neutral image in 80% of the times. After the image, a fixed inter-stimulus interval of five seconds followed. In total, 15 unique negative images and 15 unique neutral images were shown, of which 80% (i.e., 12 trials) was preceded by a “valid” cue. Which stimuli were paired with valid or invalid cues was determined randomly for each subject. The order of the trials, given the four possible combinations (valid cue + negative image, invalid cue + negative image, valid cue + neutral image, invalid cue + neutral image), was drawn randomly from one of four possible sequences, which were generated using OptSeq (<a href="https://surfer.nmr.mgh.harvard.edu/optseq/" class="uri">https://surfer.nmr.mgh.harvard.edu/optseq/</a>) to optimize the chance of finding a significant interaction between cue type and image valence. The task took 400 seconds in total (i.e., 200 volumes with a 2 second TR).</p>
<p>The cue was implemented as a pie chart with the probability of a neutral image in blue and the probability of the negative image in red with the corresponding labels and probabilities (e.g., “negative 20%”) superimposed for clarity. The images were selected from the IAPS database37 and contained images of mutilation, violence, and death (negative condition) and of people in neutral situations (neutral condition).</p>
<p>The fMRI event files associated with this task contain information on the trial onset (the moment when either the cue or image appeared on the screen, in seconds), duration (of either the cue or image), and trial type. Trial type was logged separately for the cues (“negative”, indicating 80% probability of a negative image, and “neutral”, indicating 80% probability of a neutral image) and images (“negative” or “neutral”).</p>
<p><strong>Stop-signal task (PIOP2 only)</strong>. The stop-signal task was included to measure processes related to response inhibition. This specific implementation of the stop-signal paradigm was based on <span class="citation">Jahfari et al. (<a href="bibliography.html#ref-Jahfari2015-ix" role="doc-biblioref">2015</a>)</span>. Trials were presented using an event-related design, in which trial order was randomized per subject. Subjects were presented with trials (<em>n</em> = 100) in which an image of either a female or male face (chosen from 9 exemplars) was shown for 500 ms on a black background. Subjects had to respond whether the face was female (right index finger) or male (left index finger) as quickly and accurately as possible, except when an auditory stop signal (a tone at 450 Hz for 0.5 seconds) was presented (on average 33% of the trials). The delay in presentation of the stop signal (i.e., the “stop signal delay”) was at start of the experiment 250 milliseconds, but was shortened with 50 ms if stop performance, up to that point, was better than 50% accuracy and shortened with 50 ms if it was worse. Each trial had a duration of 4000 ms and was preceded by a jitter interval (0, 500, 1000 or 1500 ms). If subjects responded too slow, or failed to respond an additional feedback trial of 2000 ms was presented. Additionally 10% (on average) null trials with a duration of 4000 ms were presented randomly. Note that due to this additional feedback trial and the fact that subjects differed in how many feedback trials they received, the fMRI runs associated with this task differ in length across subjects (i.e., the scan was manually stopped after 100 trials; minimum: 210 volumes, maximum: 250 volumes, median: 224 volumes, corresponding to a duration of 448 seconds with a 2 second TR).</p>
<p>The fMRI event files associated with this task contain information on the trial onset (the moment when a face was presented, in seconds), duration (always 0.5083 seconds), trial type (go, succesful_stop, unsuccesful_stop), if and when a stop signal was given (in seconds after stimulus onset), if and when a response time was given (in seconds after stimulus onset), response of the subject (left, right), and the sex of the image (male, female).</p>
</div>
<div id="previous-analyses-1" class="section level4">
<h4><span class="header-section-number">4.2.3.4</span> Previous analyses</h4>
<p>The PIOP1 data has been previously analyzed by three studies. One methodological study <span class="citation">(Snoek et al., <a href="bibliography.html#ref-Snoek2019-my" role="doc-biblioref">2019</a>)</span> used the T1-weighted data (VBM, specifically) and DWI data (tract-based spatial statistics, TBSS, specifically) and the self-reported biological sex of subjects to empirically test an approach to correct for confounds in decoding analyses of neuroimaging data. Two other studies analyzed the participants’ religiosity scores. One study <span class="citation">(Elk &amp; Snoek, <a href="bibliography.html#ref-Van_Elk2020-xo" role="doc-biblioref">2020</a>)</span> performed a voxelwise VBM analysis on the religiosity scores while the other study <span class="citation">(Hoogeveen et al., <a href="bibliography.html#ref-Hoogeveen2020-qp" role="doc-biblioref">2020</a>)</span> related the religiosity data to the <em>incongruent - congruent</em> activity differences from the gender-stroop functional MRI data. Notably, both studies were pre-registered. There have been no (published) analyses on the PIOP2 data.</p>
</div>
<div id="differences-between-piop1-and-piop2" class="section level4">
<h4><span class="header-section-number">4.2.3.5</span> Differences between PIOP1 and PIOP2</h4>
<p>One important difference between PIOP1 and PIOP2 is that they are not the same in terms of task-based functional MRI they acquired. Specifically, the “emotion anticipation”, “faces”, and “gender stroop” tasks were only acquired in PIOP1 and the “stop signal” task was only acquired in PIOP2 (see Table <a href="aomic.html#tab:tab-aomic-1">4.1</a>). In terms of scan sequences, the only notable difference between PIOP1 and PIOP2 is that the resting-state functional MRI scan in PIOP1 was acquired with multiband factor 3 (resulting in a TR of 750 ms) while the resting-state functional MRI scan in PIOP2 was acquired without multiband (i.e., with a sequential acquisition, resulting in a TR of 2000 ms; see Supplementary Tables <a href="aomic-supplement.html#tab:tab-aomic-S1">C.1</a>-<a href="aomic-supplement.html#tab:tab-aomic-S4">C.4</a>). Additionally, because PIOP1 was acquired before the dStream upgrade and PIOP2 after the upgrade, the PIOP1 data generally has a lower tSNR but this does not seem to lead to substantial differences in effects (see <a href="#aomic-technical-validation">Technical Validation</a>).</p>
</div>
</div>
<div id="subject-variables-all-datasets" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Subject variables (all datasets)</h3>
<p>In AOMIC, several demographic and other subject-specific variables are included per dataset. Below, we describe all variables in turn. Importantly, all subject variables and psychometric variables are stored in the participants.tsv file in each study’s data repository. Note that missing data in this file is coded as “n/a”. In Supplementary Table <a href="aomic-supplement.html#tab:tab-aomic-S5">C.5</a>, all variables and associated descriptions are listed for convenience.</p>
<div id="age" class="section level4">
<h4><span class="header-section-number">4.2.4.1</span> Age</h4>
<p>We asked subjects for their date of birth at the time they participated. From this, we computed their age rounded to the nearest quartile (for privacy reasons). See Table <a href="aomic.html#tab:tab-aomic-2">4.2</a> for descriptive statistics of this variable.</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-2">Table 4.2: </span>Descriptive statistics for biological sex, age, and education level for all three datasets.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
% by biological sex
</th>
<th style="text-align:left;">
Mean age (sd; range)
</th>
<th style="text-align:left;">
% per education level / category
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 3em; ">
ID1000
</td>
<td style="text-align:left;">
M: 47%, F: 52%
</td>
<td style="text-align:left;">
22.85 (1.71; 19-26)
</td>
<td style="text-align:left;">
Low: 10%, Medium: 43%, High: 43%
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
PIOP1
</td>
<td style="text-align:left;">
M: 41.2%, F: 55.6%
</td>
<td style="text-align:left;">
22.18 (1.80; 18.25-26.25)
</td>
<td style="text-align:left;">
Applied: 56.5%, Academic: 43.5%
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
PIOP2
</td>
<td style="text-align:left;">
M: 42.5%, F: 57.0%
</td>
<td style="text-align:left;">
21.96 (1.79; 18.25-25.75)
</td>
<td style="text-align:left;">
Applied: 53%, Academic: 46%
</td>
</tr>
</tbody>
</table>
</div>
<div id="biological-sex-and-gender-identity" class="section level4">
<h4><span class="header-section-number">4.2.4.2</span> Biological sex and gender identity</h4>
<p>In all three studies, we asked subjects for their biological sex (of which the options were either male or female). For the ID1000 dataset, after the first 400 subjects, we additionally asked to what degree subjects felt male and to what degree they felt female (i.e., gender identity; separate questions, 7 point likert scale, 1 = not at all, 7 = strong). The exact question in Dutch was: “ik voel mij een man”, “ik voel mij een vrouw”. This resulted in 0.3% of subjects scoring opposite on gender identity compared to their biological sex and 92% of females and 90% of males scoring conformable with their sex.</p>
</div>
<div id="sexual-orientation" class="section level4">
<h4><span class="header-section-number">4.2.4.3</span> Sexual orientation</h4>
<p>For the ID1000 dataset, after the first 400 subjects, we additionally asked to what degree subjects were attracted to men and women (both on a 7 point likert scale, 1 = not at all, 7 = strong). The exact question in Dutch was: “ik val op mannen” and “ik val op vrouwen”. Of the 278 subjects with a male sex 7.6% indicated to be attracted to men (score of 4 or higher), of the 276 subjects with a female sex 7.2% indicated to be attracted to women (score of 4 or higher). Of the 554 subjects who completed these questions 0.4% indicated not be attracted to either men or women and 0.9% indicated to be strongly attracted to both men and women.</p>
</div>
<div id="bmi" class="section level4">
<h4><span class="header-section-number">4.2.4.4</span> BMI</h4>
<p>Subjects were asked for (PIOP1 and PIOP2) or we measured (ID1000) subjects’ height and weight on the day of testing, from which we calculated their body-mass-index (BMI), which we rounded to the nearest integer. Note that height and weight are not included in AOMIC (for privacy reasons), but BMI is.</p>
</div>
<div id="handedness" class="section level4">
<h4><span class="header-section-number">4.2.4.5</span> Handedness</h4>
<p>Subjects were asked for their dominant hand. In ID1000, the options were “left” and “right” (left: 102, right: 826). For PIOP1 and PIOP2 we also included the option “both” (PIOP1, left: 24, right: 180, ambidextrous: 5, n/a: 7; PIOP2, left: 22, right: 201, ambidextrous: 1, n/a: 2).</p>
</div>
<div id="educational-level-category" class="section level4">
<h4><span class="header-section-number">4.2.4.6</span> Educational level / category</h4>
<p>Information about subjects’ educational background is recorded differently for ID1000 and the PIOP datasets, so they are discussed separately. Importantly, while we included data on educational <em>level</em> for the ID1000 dataset, we only include data on educational <em>category</em> for the PIOP datasets because they contain little variance in terms of educational level.</p>
<p><strong>Educational level (ID1000)</strong>. As mentioned, for the ID1000 dataset we selected subjects based on their educational level in order to achieve a representative sample of the Dutch population (on that variable). We did this by asking for their highest completed educational level. In AOMIC, however, we report the educational level (three point scale: low, medium, high) on the basis of the completed <em>or</em> current level of education (which included the scenario in which the subject was still a student), which we believe reflects educational level for our relatively young (19-26 year old) better. Note that this difference in criterion causes a substantial skew towards a higher educational level in our sample relative to the distribution of educational level in the Dutch population (see Table <a href="aomic.html#tab:tab-aomic-3">4.3</a>).</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-3">Table 4.3: </span>Distribution of educational level in the Dutch population (in 2010) and in ID1000.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Educational level
</th>
<th style="text-align:left;">
Population
</th>
<th style="text-align:left;">
Subjects
</th>
<th style="text-align:left;">
Father
</th>
<th style="text-align:left;">
Mother
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Low
</td>
<td style="text-align:left;">
35%
</td>
<td style="text-align:left;">
10%
</td>
<td style="text-align:left;">
36%
</td>
<td style="text-align:left;">
32%
</td>
</tr>
<tr>
<td style="text-align:left;">
Medium
</td>
<td style="text-align:left;">
39%
</td>
<td style="text-align:left;">
43%
</td>
<td style="text-align:left;">
27%
</td>
<td style="text-align:left;">
24%
</td>
</tr>
<tr>
<td style="text-align:left;">
High
</td>
<td style="text-align:left;">
26%
</td>
<td style="text-align:left;">
47%
</td>
<td style="text-align:left;">
37%
</td>
<td style="text-align:left;">
44%
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> The data from the education level of subjects’ parents was used to compute background socio-economic status.
</td>
</tr>
</tfoot>
</table>
<p><strong>Educational category (PIOP1+2)</strong>. Relative to ID1000, there is much less variance in educational level within the PIOP datasets as these datasets only contain data from university students. As such, we only report whether subjects were, at the time of testing, studying at the Amsterdam University of Applied Sciences (category: “applied”) or at the University of Amsterdam (category: “academic”).</p>
<p><strong>Background socio-economic status (SES)</strong>. In addition to reporting their own educational level, subjects also reported the educational level (see Table <a href="aomic.html#tab:tab-aomic-3">4.3</a>) of their parents and the family income in their primary household. Based on this information, we determined subjects’ background social economical status (SES) by adding the household income — defined on a three point scale (below modal income, 25%: 1, between modal and 2 × modal income, 57%: 2, above 2 × modal income, 18%: 3) — with the average educational level of the parents — defined on a three point scale (low: 1, medium: 2, high: 3). This revealed that, while the educational level of the subjects is somewhat skewed towards “high”, SES is well distributed across the entire spectrum (see Table <a href="aomic.html#tab:tab-aomic-4">4.4</a>).</p>
<table class="table" style="font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-4">Table 4.4: </span>Distribution of background SES.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Background SES
</th>
<th style="text-align:left;">
% of subjects
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
3-Feb
</td>
<td style="text-align:left;">
16%
</td>
</tr>
<tr>
<td style="text-align:left;">
4-Mar
</td>
<td style="text-align:left;">
26%
</td>
</tr>
<tr>
<td style="text-align:left;">
5-Apr
</td>
<td style="text-align:left;">
28%
</td>
</tr>
<tr>
<td style="text-align:left;">
6-May
</td>
<td style="text-align:left;">
19%
</td>
</tr>
<tr>
<td style="text-align:left;">
&gt;6
</td>
<td style="text-align:left;">
11%
</td>
</tr>
</tbody>
</table>
</div>
<div id="religion-piop1-and-id1000-only" class="section level4">
<h4><span class="header-section-number">4.2.4.7</span> Religion (PIOP1 and ID1000 only)</h4>
<p>For both the PIOP1 and ID1000 datasets, we asked subjects whether they considered themselves religious, which we include as a variable for these datasets (recoded into the levels “yes” and “no”). Of the subjects that participated in the PIOP1 study 18.0% indicated to be religious, for the subjects in the ID1000 projects this was 21.2%. For the ID1000 dataset we also asked subjects if they were raised religiously (N = 928, 34.1%) and to what degree religion played a daily role in their lives (in Dutch, “Ik ben dagelijks met mijn geloof bezig”, 5 point likert scale, 1 = not at all applicable, 5 = very applicable).</p>
</div>
</div>
<div id="psychometric-variables-all-datasets" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Psychometric variables (all datasets)</h3>
<p><strong>BIS/BAS (ID1000 only)</strong>. The BIS/BAS scales are based on the idea that there are two general motivational systems underlying behavior and affect: a behavioral activation system (BAS) and a behavioral inhibition system (BIS). The scales of the BIS/BAS attempt to measure these systems <span class="citation">(Carver &amp; White, <a href="bibliography.html#ref-Carver1994-wp" role="doc-biblioref">1994</a>)</span>. The BAS is believed to measure a system that generates positive feedback while the BIS is activated by conditioned stimuli associated with punishment.</p>
<p>The BIS/BAS questionnaire consists of 20 items (4 point scale). The BIS scale consists of 7 items. The BAS scale consists of 13 items and contains three subscales, related to impulsivity (BAS-Fun, 4 items), reward responsiveness (BAS-Reward, 5 items) and the pursuit of rewarding goals (BAS-Drive, 4 items).</p>
<p><strong>STAI-T (ID1000)</strong>. We used the STAI <span class="citation">(Spielberger et al., <a href="bibliography.html#ref-Spielberger1970-td" role="doc-biblioref">1970</a>; Van der Ploeg, <a href="bibliography.html#ref-Van_der_Ploeg1980-tq" role="doc-biblioref">1980</a>)</span> to measure trait anxiety (STAI-<em>T</em>). The questionnaire consists of two scales (20 questions each) that aim to measure the degree to which anxiety and fear are a trait and part of the current state of the subject; subjects only completed the trait part of the questionnaire, which we include in the ID1000 dataset.</p>
<p><strong>NEO-FFI (all datasets)</strong>. The NEO-FFI is a Big 5 personality questionnaire that consists of 60 items <span class="citation">(12 per scale; Hoekstra et al., <a href="bibliography.html#ref-Hoekstra1996-kv" role="doc-biblioref">1996</a>; McCrae &amp; Costa, <a href="bibliography.html#ref-McCrae1987-ww" role="doc-biblioref">1987</a>)</span>. It measures neuroticism (“NEO-N”), extraversion (“NEO-E”), openness to experience (“NEO-O”), agreeableness (“NEO-A”), and conscientiousness (“NEO-C”). Neuroticism is the opposite of emotional stability, central to this construct is nervousness and negative emotionality. Extraversion is the opposite of introversion, central to this construct is sociability — the enjoyment of others’ company. Openness to experience is defined by having original, broad interests, and being open to ideas and values. Agreeableness is the opposite of antagonism. Central to this construct are trust, cooperation and dominance. Conscientiousness is the opposite of un-directedness. Adjectives associated with this construct are thorough, hard-working and energetic.</p>
<p><strong>IST (ID1000 only)</strong>. The Intelligence Structure Test <span class="citation">(IST; Vorst, <a href="bibliography.html#ref-Vorst2010-ex" role="doc-biblioref">2010</a>; Amthauer et al., <a href="bibliography.html#ref-Amthauer2001-yg" role="doc-biblioref">2001</a>)</span> is an intelligence test measuring crystallized intelligence, fluid intelligence, and memory, through tests using verbal, numerical, and figural information. The test consists of 590 items. The three measures (crystallized intelligence, fluid intelligence, and memory) are strongly positively correlated (between <span class="math inline">\(r = .58\)</span> and <span class="math inline">\(r = .68\)</span>) and the sum score of these values form the variable “total intelligence”.</p>
<p><strong>Raven’s matrices (PIOP only)</strong>. As a proxy for intelligence, subjects performed the 36 item version (set II) of the Raven’s Advanced Progressive Matrices Test <span class="citation">(Raven et al., <a href="bibliography.html#ref-Raven1998-om" role="doc-biblioref">1998</a>; Raven, <a href="bibliography.html#ref-Raven2000-hs" role="doc-biblioref">2000</a>)</span>. We included the sum-score (with a maximum score of 36) in the PIOP datasets.</p>
</div>
<div id="aomic-derivatives" class="section level3">
<h3><span class="header-section-number">4.2.6</span> Data standardization, preprocessing, and derivatives</h3>
<p>In this section, we describe the data curation and standardization process as well as the preprocessing applied to the standardized data and the resulting “derivatives” (see Figure <a href="aomic.html#fig:fig-aomic-3">4.3</a> for a schematic overview). This section does not describe this process separately for each dataset, because they are largely identically standardized and (pre)processed. Exceptions to this will be explicitly mentioned. In this standardization process, we adhered to the guidelines outlined in the Brain Imaging Data Structure <span class="citation">(BIDS, v1.2.2; Gorgolewski et al., <a href="bibliography.html#ref-Gorgolewski2016-in" role="doc-biblioref">2016</a>)</span>, both for the “raw” data as well as the derivatives (whenever BIDS guidelines exist for that data or derivative modality).</p>
<div class="figure"><span id="fig:fig-aomic-3"></span>
<img src="_bookdown_files/aomic-files/figures/figure_3.png" alt="Overview of the types of data and “derivatives” included in AOMIC and the software packages used to preprocess and analyze them."  />
<p class="caption">
Figure 4.3: Overview of the types of data and “derivatives” included in AOMIC and the software packages used to preprocess and analyze them.
</p>
</div>

<div id="raw-data-standardization" class="section level4">
<h4><span class="header-section-number">4.2.6.1</span> Raw data standardization</h4>
<p>Before subjecting the data to any preprocessing pipeline, we converted the data to BIDS using the in-house developed package bidsify (see <a href="aomic.html#aomic-code-availability">Code availability</a> section for more details about the software used in the standardization process). The “BIDSification” process includes renaming of files according to BIDS convention, conversion from Philips PAR/REC format to compressed nifti, removal of facial characteristics from anatomical scans (“defacing”), and extraction of relevant metadata into JSON files.</p>
<p>The results from the standardization process were checked using the <em>bids-validator</em> software package, which revealed no validation errors. However, the validation process raised several warnings about possible issues with the datasets (as shown in the BIDS validation section in their OpenNeuro repositories), which we explain in turn.</p>
<p>First, all three datasets contain subjects with incomplete data (i.e., missing MRI or physiology data), which causes the “Not all subjects contain the same files” warning. Second, the <em>bids-validator</em> issues a warning that “[n]ot all subjects/sessions/runs have the same scanning parameters”. One cause for this warning (in PIOP1 and PIOP2) is that the time to repetition (TR) parameter of some DWI scans varies slightly (PIOP1: <em>min</em> = 7382 ms, <em>max</em> = 7700, <em>median</em> = 7390; PIOP2: <em>min</em> = 7382, <em>max</em> = 7519, <em>median</em> = 7387), which is caused by acquiring the DWI data with the shortest possible TR (i.e., TR = “shortest” setting on Philips scanners; see Supplementary Tables <a href="aomic-supplement.html#tab:tab-aomic-S1">C.1</a>-<a href="aomic-supplement.html#tab:tab-aomic-S4">C.4</a>). Because the shortest possible TR depends on the exact angle of the slice box, the actual TR varies slightly from scan to scan. Notably, this warning is absent for the ID1000 dataset, because the TR value in the nifti file header is incorrect (i.e., it is set to 1 for each scan). Like the DWI scans in PIOP1 and PIOP2, the DWI scans from ID1000 were acquired with the shortest possible TR, resulting in slightly different TRs from scan to scan (<em>min</em> = 6307, <em>max</em> = 6838, <em>median</em> = 6312). The correct TR values in seconds for each DWI scan in ID1000 was added to the dataset’s <em>participants.tsv</em> file (with the column names “DWI_TR_run1”, “DWI_TR_run2”, and “DWI_TR_run3”).</p>
<p>In addition, the functional MRI scans from ID1000 were cropped in the axial and coronal direction by conservatively removing axial slices with low signal intensity in order to save disk space, causing slightly different dimensions across participants. Finally, in PIOP2, the functional MRI scans from the stop-signal task differ in the exact number of volumes (<em>min</em> = 210, <em>max</em> = 250, <em>median</em> = 224) because the scan was stopped manually after the participant completed the task (which depended on their response times).</p>
</div>
<div id="anatomical-and-functional-mri-preprocessing" class="section level4">
<h4><span class="header-section-number">4.2.6.2</span> Anatomical and functional MRI preprocessing</h4>
<p>Results included in this manuscript come from preprocessing performed using <em>Fmriprep</em> version 1.4.1 <span class="citation">(RRID:SCR_016216; O. Esteban, Markiewicz, Blair, Moodie, Isik, Erramuzpe, Kent, Goncalves, DuPre, Snyder, Oya, et al., <a href="bibliography.html#ref-Esteban2019-ri" role="doc-biblioref">2019</a>; Esteban et al., <a href="bibliography.html#ref-Esteban2020-qw" role="doc-biblioref">2020</a>)</span>, a Nipype based tool <span class="citation">(RRID:SCR_002502; Gorgolewski et al., <a href="bibliography.html#ref-Gorgolewski2011-aa" role="doc-biblioref">2011</a>; Gorgolewski, Esteban, et al., <a href="bibliography.html#ref-Gorgolewski2017-gb" role="doc-biblioref">2017</a>)</span>. Each T1w (T1-weighted) volume was corrected for INU (intensity non-uniformity) using <em>N4BiasFieldCorrection</em> v2.1.0 <span class="citation">(Tustison et al., <a href="bibliography.html#ref-Tustison2010-tk" role="doc-biblioref">2010</a>)</span> and skull-stripped using <em>antsBrainExtraction.sh</em> v2.1.0 (using the OASIS template). Brain surfaces were reconstructed using recon-all from <em>FreeSurfer</em> v6.0.1 <span class="citation">(RRID:SCR_001847; Dale et al., <a href="bibliography.html#ref-Dale1999-rk" role="doc-biblioref">1999</a>)</span>, and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle <span class="citation">(RRID:SCR_002438; Klein et al., <a href="bibliography.html#ref-Klein2017-su" role="doc-biblioref">2017</a>)</span>. Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template version 2009c <span class="citation">(RRID:SCR_008796; Fonov et al., <a href="bibliography.html#ref-Fonov2009-sr" role="doc-biblioref">2009</a>)</span> was performed through nonlinear registration with the <em>antsRegistration</em> tool of ANTs v2.1.0 <span class="citation">(RRID:SCR_004757; Avants et al., <a href="bibliography.html#ref-Avants2008-bv" role="doc-biblioref">2008</a>)</span>, using brain-extracted versions of both T1w volume and template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using FSL <em>fast</em> <span class="citation">(FSL v5.0.9, RRID:SCR_002823; Zhang et al., <a href="bibliography.html#ref-Zhang2001-wa" role="doc-biblioref">2001</a>)</span>.</p>
<p>Functional data was not slice-time corrected. Functional data was motion corrected using <em>mcflirt</em> <span class="citation">(FSL v5.0.9; Jenkinson et al., <a href="bibliography.html#ref-Jenkinson2002-wm" role="doc-biblioref">2002</a>)</span> using the average volume after a first-pass motion correction procedure as the reference volume and normalized correlation as the image similarity cost function. “Fieldmap-less” distortion correction was performed by co-registering the functional image to the same-subject T1w image with intensity inverted <span class="citation">(Huntenburg, <a href="bibliography.html#ref-Huntenburg2014-ps" role="doc-biblioref">2014</a>; Wang et al., <a href="bibliography.html#ref-Wang2017-nk" role="doc-biblioref">2017</a>)</span>, constrained with an average fieldmap template <span class="citation">(Treiber et al., <a href="bibliography.html#ref-Treiber2016-mc" role="doc-biblioref">2016</a>)</span>, implemented with <em>antsRegistration</em> (ANTs). Note that this fieldmap-less method was used even for PIOP2, which contained a phase-difference (B0) fieldmap, because we observed that Fmriprep’s fieldmap-based method led to notably less accurate unwarping than its fieldmap-less method.</p>
<p>Distortion-correction was followed by co-registration to the corresponding T1w using boundary-based registration <span class="citation">(Greve &amp; Fischl, <a href="bibliography.html#ref-Greve2009-da" role="doc-biblioref">2009</a>)</span> with 6 degrees of freedom, using <em>bbregister</em> (FreeSurfer v6.0.1). Motion correcting transformations, field distortion correcting warp, BOLD-to-T1w transformation and T1w-to-template (MNI) warp were concatenated and applied in a single step using <em>antsApplyTransforms</em> (ANTs v2.1.0) using Lanczos interpolation.</p>
<p>Physiological noise regressors were extracted by applying <em>CompCor</em> <span class="citation">(Behzadi et al., <a href="bibliography.html#ref-Behzadi2007-eb" role="doc-biblioref">2007</a>)</span>. Principal components were estimated for the two <em>CompCor</em> variants: temporal (<em>tCompCor</em>) and anatomical (<em>aCompCor</em>). A mask to exclude signal with cortical origin was obtained by eroding the brain mask, ensuring it only contained subcortical structures. Six <em>tCompCor</em> components were then calculated including only the top 5% variable voxels within that subcortical mask. For <em>aCompCor</em>, six components were calculated within the intersection of the subcortical mask and the union of CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run. Framewise displacement <span class="citation">(Power et al., <a href="bibliography.html#ref-Power2014-gh" role="doc-biblioref">2014</a>)</span> was calculated for each functional run using the implementation of <em>Nipype</em>.</p>
<p>Many internal operations of Fmriprep use <em>Nilearn</em> <span class="citation">(RRID:SCR_001362; Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>)</span>, principally within the BOLD-processing workflow. For more details of the pipeline see
<a href="https://fmriprep.readthedocs.io/en/1.4.1/workflows.html" class="uri">https://fmriprep.readthedocs.io/en/1.4.1/workflows.html</a>.</p>
</div>
<div id="diffusion-mri-preprocessing" class="section level4">
<h4><span class="header-section-number">4.2.6.3</span> Diffusion MRI (pre)processing</h4>
<p>DWI scans were preprocessed using a custom pipeline combining tools from MRtrix3 <span class="citation">(Tournier et al., <a href="bibliography.html#ref-Tournier2019-hh" role="doc-biblioref">2019</a>)</span> and FSL. Because we acquired multiple DWI scans per participant in the ID1000 study (but not in PIOP1 and PIOP2), we concatenated these files as well as the diffusion gradient table (<em>bvecs</em>) and b-value information (<em>bvals</em>) prior to preprocessing. Using MRtrix3, we denoised the diffusion-weighted data using <em>dwidenoise</em> <span class="citation">(Veraart, Fieremans, et al., <a href="bibliography.html#ref-Veraart2016-zi" role="doc-biblioref">2016</a>; Veraart, Novikov, et al., <a href="bibliography.html#ref-Veraart2016-rv" role="doc-biblioref">2016</a>)</span>, removed Gibbs ringing artifacts using <em>mrdegibbs</em> <span class="citation">(Kellner et al., <a href="bibliography.html#ref-Kellner2016-xb" role="doc-biblioref">2016</a>)</span>, and performed eddy current and motion correction using <em>dwipreproc</em>. Notably, <em>dwipreproc</em> is a wrapper around the GPU-accelerated (CUDA v9.1) FSL tool <em>eddy</em> <span class="citation">(Andersson &amp; Sotiropoulos, <a href="bibliography.html#ref-Andersson2016-pg" role="doc-biblioref">2016</a>)</span>. Within <em>eddy</em>, we used a quadratic first-level (<em>–flm=quadratic</em>) and linear second-level model (<em>–slm=linear</em>) and outlier replacement <span class="citation">(Andersson et al., <a href="bibliography.html#ref-Andersson2016-nm" role="doc-biblioref">2016</a>)</span> with default parameters (<em>–repol</em>). Then, we performed bias correction using <em>dwibiascorrect</em> (which is based on ANTs; v2.3.1), extracted a brain mask using <em>dwi2mask</em> <span class="citation">(Dhollander et al., <a href="bibliography.html#ref-Dhollander2016-dx" role="doc-biblioref">2016</a>)</span>, and corrected possible issues with the diffusion gradient table using <em>dwigradcheck</em> <span class="citation">(Jeurissen et al., <a href="bibliography.html#ref-Jeurissen2014-cd" role="doc-biblioref">2014</a>)</span>.</p>
<p>After preprocessing, using MRtrix3 tools, we fit a diffusion tensor model on the preprocessed diffusion-weighted data using weighted linear least squares (with 2 iterations) as implemented in <em>dwi2tensor</em> <span class="citation">(Veraart et al., <a href="bibliography.html#ref-Veraart2013-ya" role="doc-biblioref">2013</a>)</span>. From the estimated tensor image, a fractional anisotropy (FA) image was computed and a map with the first eigenvectors was extracted using <em>tensor2metric</em>. Finally, a population FA template was computed using <em>population_template</em> (using an affine and an additional non-linear registration).</p>
<p>The following files are included in the DWI derivatives: a binary brain mask, the preprocessed DWI data as well as preprocessed gradient table (<em>bvec</em>) and b-value (<em>bval</em>) files, outputs from the eddy correction procedure (for quality control purposes; see <a href="#aomic-technical-validation">Technical validation</a> section), the estimated parameters from the diffusion tensor model, the eigenvectors from the diffusion tensor model, and a fractional anisotropy scalar map computed from the eigenvectors. All files are named according to BIDS Extension Proposal 16 (BEP016: diffusion weighted imaging derivatives).</p>
</div>
<div id="freesurfer-morphological-statistics" class="section level4">
<h4><span class="header-section-number">4.2.6.4</span> Freesurfer morphological statistics</h4>
<p>In addition to the complete Freesurfer directories containing the full surface reconstruction per participant, we provide a set of tab-separated values (TSV) files per participant with several morphological statistics per brain region for four different anatomical parcellations/segmentations. For cortical brain regions, we used two atlases shipped with Freesurfer: the Desikan-Killiany <span class="citation">(<em>aparc</em> in Freesurfer terms; Desikan et al., <a href="bibliography.html#ref-Desikan2006-gh" role="doc-biblioref">2006</a>)</span> and Destrieux <span class="citation">(<em>aparc.a2009</em> in Freesurfer terms; Destrieux et al., <a href="bibliography.html#ref-Destrieux2010-rd" role="doc-biblioref">2010</a>)</span> atlases. For these parcellations, the included morphological statistics are volume in mm<span class="math inline">\(^3\)</span>, area in mm<span class="math inline">\(^2\)</span>, thickness in mm, and integrated rectified mean curvature in mm<span class="math inline">\(^{-1}\)</span>. For subcortical and white matter brain regions, we used the results from the subcortical segmentation (<em>aseg</em> in Freesurfer terms) and white matter segmentation (<em>wmparc</em> in Freesurfer terms) done by Freesurfer. For these parcellations, the included morphological statistics are volume in mm<span class="math inline">\(^3\)</span> and average signal intensity (arbitrary units). The statistics were extracted from the Freesurfer output directories using the Freesufer functions <em>asegstats2table</em> and <em>aparcstats2table</em> and further formatted using custom Python code. The TSV files (and accompanying JSON metadata files) are formatted according to BIDS Extension Proposal 11 (BEP011: structural preprocessing derivatives).</p>
</div>
<div id="voxel-based-morphology" class="section level4">
<h4><span class="header-section-number">4.2.6.5</span> Voxel-based morphology</h4>
<p>In addition to the Fmriprep-preprocessed anatomical T1-weighted scans, we also provide voxelwise gray matter volume maps estimated using voxel-based morphometry (VBM). We used a modified version of the FSL VBM pipeline <span class="citation">(<a href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLVBM" class="uri" role="doc-biblioref">http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLVBM</a>; Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span>, an optimised VBM protocol <span class="citation">(Good, Johnsrude, et al., <a href="bibliography.html#ref-Good2001-kv" role="doc-biblioref">2001</a><a href="bibliography.html#ref-Good2001-kv" role="doc-biblioref">b</a>)</span> carried out with FSL tools <span class="citation">(Smith et al., <a href="bibliography.html#ref-Smith2004-sc" role="doc-biblioref">2004</a>)</span>. We skipped the initial brain-extraction stage (<em>fslvbm_1_bet</em>) and segmentation stage (first part of <em>fslvbm_3_proc</em>) and instead used the probabilistic gray matter segmentation file (in native space) from <em>Fmriprep</em> (i.e., <em>label-GM_probseg.nii.gz</em> files) directly. These files were registered to the MNI152 standard space using non-linear registration <span class="citation">(Andersson et al., <a href="bibliography.html#ref-Andersson2007-st" role="doc-biblioref">2007</a>)</span>. The resulting images were averaged and flipped along the x-axis to create a left-right symmetric, study-specific grey matter template. Second, all native grey matter images were non-linearly registered to this study-specific template and “modulated” to correct for local expansion (or contraction) due to the non-linear component of the spatial transformation.</p>
</div>
<div id="physiological-noise-processing" class="section level4">
<h4><span class="header-section-number">4.2.6.6</span> Physiological noise processing</h4>
<p>Physiology files were converted to BIDS-compatible compressed TSV files using the <em>scanphyslog2bids</em> package (see <a href="aomic.html#aomic-code-availability">Code availability</a>). Each TSV file contains three columns: the first contains the cardiac trace, the second contains the respiratory trace, and the third contains the volume onset triggers (binary, where 1 represents a volume onset). Each TSV file is accompanied by a JSON metadata file with the same name, which contains information about the start time of the physiology recording relative to the onset of the first volume. Because the physiology recording always starts before the fMRI scan starts, the start time is always negative (e.g., a start time of -42.01 means that the physiology recording started 42.01 seconds before the onset of the first volume). After conversion to BIDS, the estimated volume triggers and physiology traces were plotted, visually inspected for quality, and excluded if either of the physiology traces had missing data for more than ten seconds or if the volume triggers could not be estimated.</p>
<p>The physiology data was subsequently used to estimate fMRI-appropriate nuisance regressors using the <em>TAPAS PhysIO</em> package <span class="citation">(Kasper et al., <a href="bibliography.html#ref-Kasper2017-lp" role="doc-biblioref">2017</a>)</span>. Using this package, we specifically estimated 18 “RETROICOR” regressors <span class="citation">(Glover et al., <a href="bibliography.html#ref-Glover2000-or" role="doc-biblioref">2000</a>)</span> based on a Fourier expansion of cardiac (order: 2) and respiratory (order: 3) phase and their first-order multiplicative terms <span class="citation">(as defined in Harvey et al., <a href="bibliography.html#ref-Harvey2008-nt" role="doc-biblioref">2008</a>)</span>. In addition, we estimated a heart-rate variability (HRV) regressor by convolving the cardiac trace with a cardiac response function <span class="citation">(Chang et al., <a href="bibliography.html#ref-Chang2009-vu" role="doc-biblioref">2009</a>)</span> and a respiratory volume by time (RVT) regressor by convolving the respiratory trace with a respiration response function <span class="citation">(Birn et al., <a href="bibliography.html#ref-Birn2008-ti" role="doc-biblioref">2008</a>)</span>.</p>
</div>
</div>
</div>
<div id="data-records" class="section level2">
<h2><span class="header-section-number">4.3</span> Data records</h2>
<div id="data-formats-and-types" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Data formats and types</h3>
<p>In AOMIC, the majority of the data is stored in one of four formats. First, all volumetric (i.e., 3D or 4D) MRI data is stored in compressed “NIfTI” files (NIfTI-1 version; extension: <em>.nii.gz</em>). NIfTI files contain both the data and metadata (stored in the header) and can be loaded into all major neuroimaging analysis packages and programming languages using, e.g., the <em>nibabel</em> package for Python (<a href="https://nipy.org/nibabel" class="uri">https://nipy.org/nibabel</a>), the <em>oro.nifti</em> package in R (<a href="https://cran.r-project.org/web/packages/oro.nifti" class="uri">https://cran.r-project.org/web/packages/oro.nifti</a>), and natively in Matlab (version R2017b and higher). Second, surface (i.e., vertex-wise) MRI data is stored in “Gifti” files (<a href="https://www.nitrc.org/projects/gifti" class="uri">https://www.nitrc.org/projects/gifti</a>; extension: <em>.gii</em>). Like NIfTI files, Gifti files contain both data and metadata and can be loaded in several major neuroimaging software packages (including Freesurfer, FSL, AFNI, SPM, and Brain Voyager) and programming languages using, e.g., the <em>nibabel</em> package for Python and the <em>gifti</em> package for R (<a href="https://cran.rstudio.com/web/packages/gifti" class="uri">https://cran.rstudio.com/web/packages/gifti</a>).</p>
<p>Third, data organized as tables (i.e., observations in rows and properties in columns), such as physiological data and task-fMRI event log files, are stored in tab-separated values (TSV) files, which contain column names as the first row. TSV files can be opened using spreadsheet software (such as Microsoft Excel or Libreoffice Calc) and read using most major programming languages. Fourth, (additional) metadata is stored as key-value pairs in plain-text JSON files. A small minority of data in AOMIC is stored using different file formats (such as <em>hdf5</em> for composite transforms of MRI data and some Freesurfer files), but these are unlikely to be relevant for most users.</p>
<p>Apart from data <em>formats</em>, we can distinguish different data <em>types</em> within AOMIC. Following BIDS convention, data types are distinguished based on an “identifier” at the end of the file name (before the extension). For example, T1-weighted files (e.g., <em>sub-0001_T1w.nii.gz</em>) are distinguished by the *_T1w* identifier and event log files for task-based functional MRI data (e.g., <em>sub-001_task-workingmemory_acq-seq_events.tsv</em>) are distinguished by the *_events* identifier. All data types and associated identifiers within AOMIC are listed in Supplementary Table <a href="aomic-supplement.html#tab:tab-aomic-S6">C.6</a>.</p>
</div>
<div id="data-repositories-used" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Data repositories used</h3>
<p>Data from AOMIC can be subdivided into two broad categories. The first category encompasses all subject-level data, both raw data and derivatives. The second category encompasses group-level aggregates of data, such as an average (across subjects) tSNR map or group-level task fMRI activation maps. Data from these two categories are stored in separate, dedicated repositories: subject-level data is stored on OpenNeuro <span class="citation">(<a href="https://openneuro.org" class="uri" role="doc-biblioref">https://openneuro.org</a>; K Gorgolewski et al., <a href="bibliography.html#ref-Gorgolewski2017-uu" role="doc-biblioref">2017</a>)</span>
and the subject-aggregated data is stored on NeuroVault <span class="citation">(<a href="https://neurovault.org" class="uri" role="doc-biblioref">https://neurovault.org</a>; Gorgolewski, Varoquaux, Rivera, Schwarz, Ghosh, Maumet, Sochat, Nichols, Poldrack, Poline, Yarkoni, et al., <a href="bibliography.html#ref-Gorgolewski2015-hj" role="doc-biblioref">2015</a>)</span>. Data from each dataset — PIOP1, PIOP2, and ID1000 — are stored in separate repositories on OpenNeuro <span class="citation">(L. Snoek et al., <a href="bibliography.html#ref-Snoek2020-id1000" role="doc-biblioref">2020</a><a href="bibliography.html#ref-Snoek2020-id1000" role="doc-biblioref">b</a>, <a href="bibliography.html#ref-Snoek2020-piop1" role="doc-biblioref">2020</a><a href="bibliography.html#ref-Snoek2020-piop1" role="doc-biblioref">d</a>, <a href="bibliography.html#ref-Snoek2020-piop2" role="doc-biblioref">2020</a><a href="bibliography.html#ref-Snoek2020-piop2" role="doc-biblioref">f</a>)</span> and NeuroVault <span class="citation">(L. Snoek et al., <a href="bibliography.html#ref-Snoek2020n-id1000" role="doc-biblioref">2020</a><a href="bibliography.html#ref-Snoek2020n-id1000" role="doc-biblioref">a</a>, <a href="bibliography.html#ref-Snoek2020n-piop1" role="doc-biblioref">2020</a><a href="bibliography.html#ref-Snoek2020n-piop1" role="doc-biblioref">c</a>, <a href="bibliography.html#ref-Snoek2020n-piop2" role="doc-biblioref">2020</a><a href="bibliography.html#ref-Snoek2020n-piop2" role="doc-biblioref">e</a>)</span>. URLs to these repositories for all datasets can be found in Table <a href="aomic.html#tab:tab-aomic-5">4.5</a>. Apart from the option to download data using a web browser, we provide instructions to download the data programmatically on <a href="https://nilab-uva.github.io/AOMIC.github.io" class="uri">https://nilab-uva.github.io/AOMIC.github.io</a>.</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-5">Table 4.5: </span>Data repository identifiers for subject data (OpenNeuro) and group-level data (NeuroVault).
</caption>
<thead>
<tr>
<th style="text-align:left;">
Repository
</th>
<th style="text-align:left;">
ID1000
</th>
<th style="text-align:left;">
PIOP1
</th>
<th style="text-align:left;">
PIOP2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
OpenNeuro ID
</td>
<td style="text-align:left;">
ds003097
</td>
<td style="text-align:left;">
ds002785
</td>
<td style="text-align:left;">
ds002790
</td>
</tr>
<tr>
<td style="text-align:left;">
Neurovault ID
</td>
<td style="text-align:left;">
7105
</td>
<td style="text-align:left;">
7103
</td>
<td style="text-align:left;">
7104
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> To go to the OpenNeuro web repositories, prefix the OpenNeuro ID with “<a href="https://openneuro.org/datasets/" class="uri">https://openneuro.org/datasets/</a>”. To go to the NeuroVault web repositories, prefix the NeuroVault ID with “<a href="https://neurovault.org/collections/" class="uri">https://neurovault.org/collections/</a>”.
</td>
</tr>
</tfoot>
</table>
<div id="data-anonymization" class="section level4">
<h4><span class="header-section-number">4.3.2.1</span> Data anonymization</h4>
<p>In curating this collection, we took several steps in ensuring the anonymity of participants. All measures were discussed with the data protection officer of the University of Amsterdam and the data steward of the department of psychology, who deemed the anonymized data to be in accordance with the European General Data Protection Regulation (GDPR).</p>
<p>First, all personally identifiable information (such as subjects’ name, date of birth, and contact information) in all datasets were irreversibly destroyed. Second, using the <em>pydeface</em> software package <span class="citation">(Gulban et al., <a href="bibliography.html#ref-Gulban2019-sv" role="doc-biblioref">2019</a>)</span>, we removed facial characteristics (mouth and nose) from all anatomical scans, i.e., the T1-weighted anatomical scans and (in PIOP2) magnitude and phase-difference images from the B0 fieldmap. The resulting defaced images were checked visually to confirm that the defacing succeeded. Third, the data files were checked for timestamps and removed when present. Lastly, we randomized the subject identifiers (<em>sub-xxxx</em>) for all files. In case participants might have remembered their subject number, they will not be able to look up their own data within our collection.</p>
</div>
</div>
</div>
<div id="technical-validation" class="section level2">
<h2><span class="header-section-number">4.4</span> Technical validation</h2>
<p>In this section, we describe the measures taken for quality control of the data. This is described per data type (e.g., anatomical T1-weighted images, DWI images, physiology, etc.), rather than per dataset, as the procedure for quality control per data type was largely identical across the datasets. Importantly, we take a conservative approach towards exclusion of data, i.e., we generally did not exclude data unless (1) it was corrupted by scanner-related incorrigible artifacts, such as reconstruction errors, (2) when preprocessing fails due to insufficient data quality (e.g., in case of strong spatial inhomogeneity of structural T1-weighted scans, preventing accurate segmentation), (3) an absence of a usable T1-weighted scan (which is necessary for most preprocessing pipelines), or (4) incidental findings. This way, the data from AOMIC can also be used to evaluate artifact-correction methods and other preprocessing techniques aimed to post-hoc improve data quality and, importantly, this places the responsibility for inclusion and exclusion of data in the hands of the users of the datasets.</p>
<p>Researchers not interested in using AOMIC data for artifact-correction or preprocessing techniques may still want to exclude data that do not meet their quality standards. As such, we include, for each modality (T1-weighted, BOLD, and DWI) separately, a file with several quality control metrics across subjects. The quality control metrics for the T1-weighted and functional (BOLD) MRI scans were computed by the Mriqc package <span class="citation">(O. Esteban, Birman, et al., <a href="bibliography.html#ref-Esteban2017-mv" role="doc-biblioref">2017</a>)</span> and are stored in the <em>group_T1w.tsv</em> and <em>group_T1w.tsv</em> files in the <em>mriqc</em> derivatives folder. The quality control metrics for the DWI scans were derived from the output of FSL’s <em>eddy</em> algorithm and are stored in the <em>group_dwi.tsv</em> file in the dwipreproc derivatives folder. Using these precomputed quality control metrics, researchers can decide which data to include based on their own quality criteria.</p>
<div id="t1-weighted-scans" class="section level3">
<h3><span class="header-section-number">4.4.1</span> T1-weighted scans</h3>
<p>All T1-weighted scans were run through the <em>Mriqc</em> pipeline, which outputs several quality control metrics as well as a report with visualizations of different aspects of the data. All individual subject reports were visually checked for artifacts including reconstruction errors, failure of defacing, normalization issues, and segmentation issues (and the corresponding data excluded when appropriate). In Figure <a href="aomic.html#fig:fig-aomic-4">4.4</a>, we visualize several quality control metrics related to the T1-weighted scans across all three datasets. In general, data quality appears to increase over time (with ID1000 being the oldest dataset, followed by PIOP1 and PIOP2), presumably due to improvements in hardware (see Scanner details and general scanning protocol). All quality control metrics related to the T1-weighted scans, including those visualized in Figure <a href="aomic.html#fig:fig-aomic-4">4.4</a>, are stored in the <em>group_T1w.tsv</em> file in the <em>mriqc</em> derivatives folder.</p>
<div class="figure"><span id="fig:fig-aomic-4"></span>
<img src="_bookdown_files/aomic-files/figures/figure_4.png" alt="Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., 2006); CJV: coefficient of joint variation (Ganzetti et al., 2016), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., 1997), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95% percentile of all signal intensities; low values may lead to problems with tissue segmentation."  />
<p class="caption">
Figure 4.4: Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio <span class="citation">(Magnotta et al., <a href="bibliography.html#ref-Magnotta2006-zs" role="doc-biblioref">2006</a>)</span>; CJV: coefficient of joint variation <span class="citation">(Ganzetti et al., <a href="bibliography.html#ref-Ganzetti2016-yy" role="doc-biblioref">2016</a>)</span>, an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion <span class="citation">(Atkinson et al., <a href="bibliography.html#ref-Atkinson1997-eu" role="doc-biblioref">1997</a>)</span>, an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95% percentile of all signal intensities; low values may lead to problems with tissue segmentation.
</p>
</div>

</div>
<div id="functional-bold-scans" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Functional (BOLD) scans</h3>
<p>Like the T1-weighted images, the functional (BOLD) scans were run through the Mriqc pipeline. The resulting reports were visually checked for artifacts including reconstruction errors, registration issues, and incorrect brain masks.</p>
<p>In Figure <a href="aomic.html#fig:fig-aomic-5">4.5</a>, we visualize several quality control metrics related to the functional scans across all three datasets. Similar to the T1-weighted quality control metrics, the functional quality control metrics indicate an improvement of quality over time. Also note the clear decrease in temporal signal-to-noise ratio (tSNR) for multiband-accelerated scans (consistent with <span class="citation">Demetriou et al. (<a href="bibliography.html#ref-Demetriou2018-xp" role="doc-biblioref">2018</a>)</span>). All quality control metrics related to the functional MRI scans, including those visualized in Figure <a href="aomic.html#fig:fig-aomic-5">4.5</a>, are stored in the <em>group_bold.tsv</em> file in the <em>mriqc</em> derivatives folder.</p>
<div class="figure"><span id="fig:fig-aomic-5"></span>
<img src="_bookdown_files/aomic-files/figures/figure_5.png" alt="Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., 2012), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., 2013); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis."  />
<p class="caption">
Figure 4.5: Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement <span class="citation">(Power et al., <a href="bibliography.html#ref-Power2012-kt" role="doc-biblioref">2012</a>)</span>, an index of overall movement; GCOR: global correlation, an index of the presence of global signals <span class="citation">(Saad et al., <a href="bibliography.html#ref-Saad2013-zd" role="doc-biblioref">2013</a>)</span>; GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.
</p>
</div>

<p>In Figure <a href="aomic.html#fig:fig-aomic-6">4.6</a>, we visualize these tSNR maps for each dataset (and separately for the sequential and multiband scans of PIOP1). Again, there appears to be an increase in tSNR across time. Corresponding whole-brain tSNR maps can be viewed and downloaded from NeuroVault (i.e., files with the *_tsnr* identifier).</p>
<div class="figure"><span id="fig:fig-aomic-6"></span>
<img src="_bookdown_files/aomic-files/figures/figure_6.png" alt="Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault."  />
<p class="caption">
Figure 4.6: Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.
</p>
</div>

<p>For the fMRI data with an explicit task (i.e., all fMRI data except for the PIOP resting-state fMRI scans and the ID1000 movie watching fMRI scan), we additionally computed group-level whole-brain statistics maps. To do so, using the <em>nistats</em> Python package, we ran mass-univariate first-level GLS models (using an AR1 noise model) based on a design matrix including task-regressors based on the events convolved with a canonical HRF function as well as a discrete cosine basis set functioning as a high-pass filter of 128 seconds and six motion regressors and computed first-level contrast maps for each subject which were subsequently analyzed in a random effects group-level (intercept) model, resulting in whole-brain <em>z</em>-value maps. The data was never spatially smoothed. In Figure <a href="aomic.html#fig:fig-aomic-7">4.7</a>, we show the (uncorrected) whole-brain group-level results for each task. Note that we chose these specific contrasts to demonstrate that the tasks elicit to-be expected effects (e.g., amygdala activity in the emotion matching task and cingulate cortex activity in the gender-stroop task). Different, and more sophisticated analyses, including analysis of between-subject factors, are possible with this data and the associated event files.</p>
<div class="figure"><span id="fig:fig-aomic-7"></span>
<img src="_bookdown_files/aomic-files/figures/figure_7.png" alt="Results from task-specific group-level analyses. Brain maps show uncorrected effects (p &lt; 0.00001, two-sided) and were linearly interpolated for visualization in FSLeyes. Unthresholded whole-brain z-value maps are available on NeuroVault. Unthresholded whole-brain z-value maps are available on NeuroVault."  />
<p class="caption">
Figure 4.7: Results from task-specific group-level analyses. Brain maps show uncorrected effects (<em>p</em> &lt; 0.00001, two-sided) and were linearly interpolated for visualization in <em>FSLeyes</em>. Unthresholded whole-brain <em>z</em>-value maps are available on NeuroVault. Unthresholded whole-brain <em>z</em>-value maps are available on NeuroVault.
</p>
</div>

<p>To validate the quality of the resting-state functional MRI scans in PIOP1 and PIOP2, we ran dual regression analyses <span class="citation">(Beckmann et al., <a href="bibliography.html#ref-Beckmann2009-rs" role="doc-biblioref">2009</a>)</span> using the spatial ICA maps from Smith and colleagues <span class="citation">(10-component version; Smith et al., <a href="bibliography.html#ref-Smith2009-kj" role="doc-biblioref">2009</a>)</span>. Prior to the dual regression analyses, the data was cleaned and high-pass filtered (using a 128 second cutoff) by regressing out a discrete cosine basis set and six motion parameter estimates and spatially smoothed with a Gaussian kernel with a 5 mm FWHM. Figure <a href="aomic.html#fig:fig-aomic-8">4.8</a> shows the group-level dual regression results from both PIOP1 and PIOP2 for the first four components next to the original ICA map from <span class="citation">Smith et al. (<a href="bibliography.html#ref-Smith2009-kj" role="doc-biblioref">2009</a>)</span>.</p>
<div class="figure"><span id="fig:fig-aomic-8"></span>
<img src="_bookdown_files/aomic-files/figures/figure_8.png" alt="Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded z-value maps are available on NeuroVault."  />
<p class="caption">
Figure 4.8: Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded <em>z</em>-value maps are available on NeuroVault.
</p>
</div>

<p>Finally, to assess the quality of the ID1000 functional MRI data, we performed a voxelwise whole-brain “inter-subject correlation” (ISC) analysis <span class="citation">(Hasson et al., <a href="bibliography.html#ref-Hasson2004-xb" role="doc-biblioref">2004</a>)</span>, using the BrainIAK software package <span class="citation">(Kumar et al., <a href="bibliography.html#ref-Kumar2020-eo" role="doc-biblioref">2020</a>)</span> on data from a subset of 100 participants (randomly drawn from the ID1000 dataset). Before computing the inter-subject correlations, the data were masked by an intersection of a functional brain mask and a grey matter mask (probability &gt; 0.1). Low-frequency drift (with a cutoff of 128 seconds), the mean signal within the cerebrospinal fluid, global (whole-brain average) signal, and six motion parameters were regressed out before computing the ISCs. The average (across subjects) voxelwise ISCs are visualized in Figure <a href="aomic.html#fig:fig-aomic-9">4.9</a>, which shows the expected inter-subject synchrony in the ventral and dorsal visual stream. The emphasis on variance in visual parameters rather than narrative when composing the movie stimulus likely caused the high ISC values to be largely restricted to visual brain areas.</p>
<div class="figure"><span id="fig:fig-aomic-9"></span>
<img src="_bookdown_files/aomic-files/figures/figure_9.png" alt="Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault."  />
<p class="caption">
Figure 4.9: Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.
</p>
</div>

</div>
<div id="diffusion-weighted-scans" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Diffusion-weighted scans</h3>
<p>Before preprocessing, the b=0 volume from each DWI scan was extracted and visually checked for severe artifacts and reconstruction errors (in which case the data was excluded). After preprocessing and DTI model fitting, we furthermore visualized each estimated fractional anisotropy (FA) map and the color-coded FA-modulated (absolute) eigenvectors for issues with the gradient directions. These images are included in the DWI derivatives.</p>
<p>Furthermore, we extracted quality control metrics based on outputs from the eddy correction/motion correction procedure in the DWI preprocessing pipeline as implemented in FSL’s <em>eddy</em> algorithm (based on the procedure outlined in <span class="citation">Bastiani et al. (<a href="bibliography.html#ref-Bastiani2019-sm" role="doc-biblioref">2019</a>)</span>). Specifically, we computed the mean framewise displacement across volumes based on the realignment parameters from motion correction, the percentage of “outlier slices” (as determined by FSL <em>eddy</em>) in total and per volume, and the standard deviation of the estimated linear eddy current distortions across volumes. These metrics are visualized in Figure <a href="aomic.html#fig:fig-aomic-10">4.10</a>. Note that the y-axis for the standard deviation of the eddy currents for ID1000 has a larger range than for PIOP1 and PIOP2 to show the scans with particularly strong eddy current fluctuations.</p>
<div class="figure"><span id="fig:fig-aomic-10"></span>
<img src="_bookdown_files/aomic-files/figures/figure_10.png" alt="Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm."  />
<p class="caption">
Figure 4.10: Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.
</p>
</div>

<p>Finally, for each dataset, we transformed all preprocessed DTI eigenvectors to a population template estimated on all FA images using MRTrix’ <em>population_template</em> (using a linear followed by a non-linear registration procedure) and computed the voxelwise median across subjects. The median eigenvector images are visualized in Figure <a href="aomic.html#fig:fig-aomic-11">4.11</a> as “diffusion-encoded color” (DEC) images, in which values are modulated by the associated FA values.</p>
<div class="figure"><span id="fig:fig-aomic-11"></span>
<img src="_bookdown_files/aomic-files/figures/figure_11.png" alt="Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion."  />
<p class="caption">
Figure 4.11: Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.
</p>
</div>

</div>
<div id="physiological-data" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Physiological data</h3>
<p>After conversion to BIDS, physiological data was visually checked for quality by plotting the scanner triggers (i.e., volume onsets) and the cardiac and respiratory traces. Files missing a substantial window of data (&gt;10 seconds) were excluded as well as files for which the scanner triggers could not be estimated reliably. Figures of the physiology traces and scanner triggers for each file are included in the physiology derivatives. Additionally, using the same approach as described for the task-based technical validation analyses, we fit first-level (subject-specific) and subsequently group-level (subject-average) models using the physiology regressors (all 18 RETROICOR regressors, one HRV, and one RVT regressor) for each dataset. In Figure <a href="aomic.html#fig:fig-aomic-12">4.12</a>, we visualize the effects of the different RETROICOR components (respiratory, cardiac, and interaction regressors; an <em>F</em>-test, but converted to and visualized as <em>z</em>-scores) and the HRV and RVT regressors (a <em>t</em>-test, but converted to and visualized as <em>z</em>-scores). Unthresholded whole-brain maps are available from NeuroVault.</p>
<div class="figure"><span id="fig:fig-aomic-12"></span>
<img src="_bookdown_files/aomic-files/figures/figure_12.png" alt="Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at z &gt; 6) and were linearly interpolated for visualization in FSLeyes. Unthresholded whole-brain z-value maps are available on NeuroVault."  />
<p class="caption">
Figure 4.12: Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at <em>z</em> &gt; 6) and were linearly interpolated for visualization in <em>FSLeyes</em>. Unthresholded whole-brain <em>z</em>-value maps are available on NeuroVault.
</p>
</div>

</div>
<div id="psychometric-data" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Psychometric data</h3>
<p>The patterns of correlations within the scales of the questionnaires are consistent with those reported in literature, indicating that this data is overall reliable. The pattern of correlations between scales of different questionnaires and external variables is also consistent with those reported in literature and what would be expected on theoretical grounds.</p>
<div id="intelligence-structure-test-ist" class="section level4">
<h4><span class="header-section-number">4.4.5.1</span> Intelligence Structure Test (IST)</h4>
<p>The subscales of the IST (fluid and crystallized intelligence and memory) are strongly correlated with each other. The validity of the measure data is supported by the correlation with relevant external variables like educational level, <span class="math inline">\(r(926) = 0.46\)</span>) and background SES, <span class="math inline">\(r(926) = 0.35\)</span> (see Table <a href="aomic.html#tab:tab-aomic-6">4.6</a>).</p>
<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-6">Table 4.6: </span>Correlations between total score, and subscales of the IST and relevant external variables.
</caption>
<thead>
<tr>
<th style="text-align:left;">
IST (N = 926)
</th>
<th style="text-align:right;">
IST Int
</th>
<th style="text-align:left;">
IST Crystal
</th>
<th style="text-align:left;">
IST Memory
</th>
<th style="text-align:left;">
IST Fluid
</th>
<th style="text-align:left;">
Background SES
</th>
<th style="text-align:left;">
Education
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
IST Int
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
0.82**
</td>
<td style="text-align:left;">
0.79**
</td>
<td style="text-align:left;">
0.96**
</td>
<td style="text-align:left;">
0.35**
</td>
<td style="text-align:left;">
0.46**
</td>
</tr>
<tr>
<td style="text-align:left;">
IST Crystallized
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.58**
</td>
<td style="text-align:left;">
0.68**
</td>
<td style="text-align:left;">
0.37**
</td>
<td style="text-align:left;">
0.44**
</td>
</tr>
<tr>
<td style="text-align:left;">
IST Memory
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.65**
</td>
<td style="text-align:left;">
0.25**
</td>
<td style="text-align:left;">
0.39**
</td>
</tr>
<tr>
<td style="text-align:left;">
IST Fluid
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.31**
</td>
<td style="text-align:left;">
0.41**
</td>
</tr>
<tr>
<td style="text-align:left;">
Background SES
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.38**
</td>
</tr>
<tr>
<td style="text-align:left;">
Education
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> IST: Intelligence Structure Test, Int: Total Intelligence, SES: background social-economic status. ** indicates p &lt; 0.01.
</td>
</tr>
</tfoot>
</table>
</div>
<div id="personality-neo-ffi" class="section level4">
<h4><span class="header-section-number">4.4.5.2</span> Personality: NEO-FFI</h4>
<p>The cross-correlation patterns of the five NEO-FFI scales are depicted in Table <a href="aomic.html#tab:tab-aomic-7">4.7</a>. Significant correlations exist between the scales, and the correlation pattern is overall consistent with the reported norm data for this test <span class="citation">(Hoekstra et al., <a href="bibliography.html#ref-Hoekstra1996-kv" role="doc-biblioref">1996</a>)</span>. The correlation between cross-correlation patterns of the three datasets is very consistent (<span class="math inline">\(r = 0.88\)</span> between PIOP1 and PIOP2, and on average <span class="math inline">\(r = 0.74\)</span> between ID1000 and PIOP), with as a notable outlier a negative correlation, <span class="math inline">\(r(928) = -0.13\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>, between extraversion and agreeableness in the ID1000 dataset and a positive correlation for these scales in the PIOP1, <span class="math inline">\(r(216) = 0.20\)</span>, <span class="math inline">\(p &lt; 0.005\)</span>, and PIOP2, <span class="math inline">\(r(226) = 0.26\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>. A source for this discrepancy could be the difference in population sample between the PIOP1 and PIOP2 studies and the ID1000 study.</p>
<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-7">Table 4.7: </span>Cross-correlations for the subscales of the NEO-FFI for the ID1000, PIOP1 and PIOP2 samples.
</caption>
<thead>
<tr>
<th style="text-align:left;">
ID1000 (N = 927)
</th>
<th style="text-align:right;">
Neuroticism
</th>
<th style="text-align:left;">
Extraversion
</th>
<th style="text-align:left;">
Openness
</th>
<th style="text-align:left;">
Agreeableness
</th>
<th style="text-align:left;">
Conscientiousness
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Neuroticism
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
-0.30**
</td>
<td style="text-align:left;">
0.15**
</td>
<td style="text-align:left;">
-0.08*
</td>
<td style="text-align:left;">
-0.43**
</td>
</tr>
<tr>
<td style="text-align:left;">
Extraversion
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.20**
</td>
<td style="text-align:left;">
-0.13**
</td>
<td style="text-align:left;">
0.12**
</td>
</tr>
<tr>
<td style="text-align:left;">
Openness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
-0.06
</td>
<td style="text-align:left;">
-0.19**
</td>
</tr>
<tr>
<td style="text-align:left;">
Agreeableness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.16**
</td>
</tr>
<tr>
<td style="text-align:left;">
Conscientiousness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
PIOP1 (N = 216)
</td>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Neuroticism
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
-0.29**
</td>
<td style="text-align:left;">
0.18**
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
-0.25**
</td>
</tr>
<tr>
<td style="text-align:left;">
Extraversion
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
-0.05
</td>
<td style="text-align:left;">
0.20**
</td>
<td style="text-align:left;">
0.14*
</td>
</tr>
<tr>
<td style="text-align:left;">
Openness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.08
</td>
<td style="text-align:left;">
-0.18**
</td>
</tr>
<tr>
<td style="text-align:left;">
Agreeableness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.20**
</td>
</tr>
<tr>
<td style="text-align:left;">
Conscientiousness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
PIOP2 (N = 226)
</td>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
<td style="text-align:left;font-weight: bold;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Neuroticism
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
-0.38**
</td>
<td style="text-align:left;">
0.12
</td>
<td style="text-align:left;">
-0.07
</td>
<td style="text-align:left;">
-0.24**
</td>
</tr>
<tr>
<td style="text-align:left;">
Extraversion
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.11
</td>
<td style="text-align:left;">
0.26**
</td>
<td style="text-align:left;">
0.25**
</td>
</tr>
<tr>
<td style="text-align:left;">
Openness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
-0.04
</td>
</tr>
<tr>
<td style="text-align:left;">
Agreeableness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.15*
</td>
</tr>
<tr>
<td style="text-align:left;">
Conscientiousness
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> With the exception of the correlation between agreeableness and extraversion the cross-correlation patterns are very similar across samples. * indicates p&lt;0.05. ** indicates p&lt;0.01.
</td>
</tr>
</tfoot>
</table>
<p>In terms of external validity we note that openness to experience has a positive correlation with intelligence in all three samples (ID1000: <span class="math inline">\(r(925) = 0.22\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>, PIOP1: <span class="math inline">\(r(216) = 0.25\)</span>, <span class="math inline">\(p = 0.000196\)</span>), PIOP2: <span class="math inline">\(r(225) = 0.24\)</span>, <span class="math inline">\(p = 0.000276\)</span>).</p>
</div>
<div id="bisbas" class="section level4">
<h4><span class="header-section-number">4.4.5.3</span> BIS/BAS</h4>
<p>The cross-correlation patterns of the BIS/BAS scales are depicted in Table <a href="aomic.html#tab:tab-aomic-8">4.8</a>. The cross-correlation between the scales are similar to the one reported by <span class="citation">Franken et al. (<a href="bibliography.html#ref-Franken2005-jg" role="doc-biblioref">2005</a>)</span> and contrary to what <span class="citation">Carver &amp; White (<a href="bibliography.html#ref-Carver1994-wp" role="doc-biblioref">1994</a>)</span> predicted, with a positive correlation between the three different BAS-scales, but also between BIS and BAS-Reward, <span class="math inline">\(r(927) = 0.194\)</span>.</p>
<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-aomic-8">Table 4.8: </span>Cross-correlations for the subscales of the BIS/BAS for the ID1000 sample.
</caption>
<thead>
<tr>
<th style="text-align:left;">
BIS/BAS (N = 928)
</th>
<th style="text-align:right;">
BAS drive
</th>
<th style="text-align:left;">
BAS fun
</th>
<th style="text-align:left;">
BAS reward
</th>
<th style="text-align:left;">
BIS
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
BAS drive
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
0.45**
</td>
<td style="text-align:left;">
0.34**
</td>
<td style="text-align:left;">
-0.19**
</td>
</tr>
<tr>
<td style="text-align:left;">
BAS fun
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.39**
</td>
<td style="text-align:left;">
-0.13**
</td>
</tr>
<tr>
<td style="text-align:left;">
BAS reward
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.19**
</td>
</tr>
<tr>
<td style="text-align:left;">
BIS
</td>
<td style="text-align:right;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> ** indicates p&lt;0.01.
</td>
</tr>
</tfoot>
</table>
</div>
<div id="stai-t" class="section level4">
<h4><span class="header-section-number">4.4.5.4</span> STAI-T</h4>
<p>The STAI-T scale measures trait anxiety. Because this instrument only consists of one scale we evaluate its reliability on the degree in which it shows correlations with other questionnaire scales that also have a pretension of measuring negative emotionality. Because we observe positive correlations with both Neuroticisms, <span class="math inline">\(r(927) = 0.77\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>) and BIS, <span class="math inline">\(r(927) = 0.514\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>) we conclude that the reported scales are reliable and consistent.</p>
</div>
</div>
<div id="aomic-code-availability" class="section level3">
<h3><span class="header-section-number">4.4.6</span> Code availability</h3>
<p>All code used for curating, annotating, and (pre)processing AOMIC are version-controlled using git and can be found in project-specific Github repositories within the NILAB-UvA Github organization: <a href="https://github.com/orgs/NILAB-UvA" class="uri">https://github.com/orgs/NILAB-UvA</a>. Many pre and postprocessing steps were identical across datasets, so the code for these procedures is stored in a single repository: <a href="https://github.com/NILAB-UvA/AOMIC-common-scripts" class="uri">https://github.com/NILAB-UvA/AOMIC-common-scripts</a>. Possible parameters are all hard-coded within the scripts, except for a single positional parameter pointing to the directory to be processed. For custom Python-based scripts, we used Python version 3.7. All code was developed on a Linux system with 56 CPUs (Intel Xeon E5-2680 v4, 2.40GHz) and 126GB RAM running Ubuntu 16.04. All curation, preprocessing, and analyses were run on said Linux system, apart from the <em>Fmriprep</em>, <em>Mriqc</em>, and <em>Freesurfer</em> analyses, which were run in a Docker container provided by those software packages. Custom code was parallelized to run on multiple CPUs concurrently using the Python package <em>joblib</em> (<a href="https://joblib.readthedocs.io" class="uri">https://joblib.readthedocs.io</a>).</p>
<p>For curation, preprocessing, and analysis of the datasets, we used a combination of existing packages and custom scripts (written in Python or bash). To convert the data to the Brain Imaging Data Structure (<em>BIDS</em>), we used the in-house developed, publicly available software package <em>bidsify</em> (v0.3; <a href="https://github.com/NILAB-UvA/bidsify" class="uri">https://github.com/NILAB-UvA/bidsify</a>), which in turn uses the <em>dcm2niix</em> <span class="citation">(v1.0.20181125; Li et al., <a href="bibliography.html#ref-Li2016-ss" role="doc-biblioref">2016</a>)</span> to convert the Philips PAR/REC files to compressed nifti files. In contrast to the data from PIOP1 and PIOP2 (which were converted to nifti using <em>dcm2niix</em>), <em>r2aGUI</em> (v2.7.0; <a href="http://r2agui.sourceforge.net" class="uri">http://r2agui.sourceforge.net</a>) was used to convert the data from ID1000. Because <em>r2aGUI</em> does not correct the gradient table of DWI scans for slice angulation, we used the <em>angulation_correction_Achieva</em> Matlab script (version December 29, 2007) from Jonathan Farrell to do so (available for posterity at <a href="https://github.com/NILAB-UvA/ID1000/blob/master/code/bidsify/DTI_gradient_table_ID1000.m" class="uri">https://github.com/NILAB-UvA/ID1000/blob/master/code/bidsify/DTI_gradient_table_ID1000.m</a>). To remove facial characteristics from anatomical scans, we used the <em>pydeface</em> package <span class="citation">(v.1.1.0; Gulban et al., <a href="bibliography.html#ref-Gulban2019-sv" role="doc-biblioref">2019</a>)</span>. Finally, to convert the raw physiology files (i.e., Philips “SCANPHYSLOG” files) to BIDS, we used the in-house developed, publicly available Python package <em>scanphyslog2bids</em> (v0.1; <a href="https://github.com/lukassnoek/scanphyslog2bids" class="uri">https://github.com/lukassnoek/scanphyslog2bids</a>). The outputs from the BIDS-conversion pipeline were checked using the <em>bids-validator</em> software package (v1.4.3).</p>
<p>Anatomical and functional MRI preprocessing were done using <em>Fmriprep</em> (v1.4.1; see the <a href="aomic.html#aomic-derivatives">Derivatives</a> section for extensive information about <em>Fmriprep</em>’s preprocessing pipeline]. For our DWI preprocessing pipeline, we used tools from the MRtrix3 package <span class="citation">(www.mrtrix.org; v3.0_RC3; Tournier et al., <a href="bibliography.html#ref-Tournier2019-hh" role="doc-biblioref">2019</a>)</span> and FSL <span class="citation">(v6.0.1; Jenkinson et al., <a href="bibliography.html#ref-Jenkinson2012-ui" role="doc-biblioref">2012</a>)</span>. For the VBM and dual regression pipelines, we used <em>FSL</em> <span class="citation">(v6.0.1; Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>; Good, Johnsrude, et al., <a href="bibliography.html#ref-Good2001-kv" role="doc-biblioref">2001</a><a href="bibliography.html#ref-Good2001-kv" role="doc-biblioref">b</a>; Smith et al., <a href="bibliography.html#ref-Smith2004-sc" role="doc-biblioref">2004</a>)</span>. To create the files with <em>Freesurfer</em>-based metrics across all participants, we used <em>Freesurfer</em> version 6.0.095. Physiological nuisance regressors (RETROICOR and HRV/RVT regressors) were estimated using the <em>TAPAS PhysIO</em> Matlab package <span class="citation">(v3.2.0; Kasper et al., <a href="bibliography.html#ref-Kasper2017-lp" role="doc-biblioref">2017</a>)</span>.</p>
<p>First-level functional MRI analyses for technical validation were implemented using the Python package <em>nistats</em> <span class="citation">(v0.0.1b2; Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>)</span> and <em>nilearn</em> <span class="citation">(v0.6.2; Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>; Pedregosa et al., <a href="bibliography.html#ref-pedregosa2011scikit" role="doc-biblioref">2011</a>)</span>. For the inter-subject correlation analysis the <em>Brain Imaging Analysis Kit</em> was used <span class="citation">(<em>BrainIAK</em>, <a href="http://brainiak.org" class="uri" role="doc-biblioref">http://brainiak.org</a>, v0.10; RRID:SCR_014824; Kumar et al., <a href="bibliography.html#ref-Kumar2020-eo" role="doc-biblioref">2020</a>)</span>. Plotting brain images was done using <em>FSLeyes</em> <span class="citation">(v0.32; McCarthy, <a href="bibliography.html#ref-McCarthy2019-yt" role="doc-biblioref">2021</a>)</span> and plotting statistical plots was done using the Python packages <em>seaborn</em> <span class="citation">(Waskom et al., <a href="bibliography.html#ref-Waskom2020-qq" role="doc-biblioref">2020</a>)</span> and <em>Matplotlib</em> <span class="citation">(Hunter, <a href="bibliography.html#ref-Hunter2007-at" role="doc-biblioref">2007</a>)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="confounds-decoding.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="morbid-curiosity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"download": "docs/thesis.pdf"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
