<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Supplement to Chapter 2 | Towards prediction</title>
  <meta name="description" content="A Supplement to Chapter 2 | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="A Supplement to Chapter 2 | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Supplement to Chapter 2 | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="general-discussion.html"/>
<link rel="next" href="confounds-decoding-supplement.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>1.1</b> Inference done differently</a></li>
<li class="chapter" data-level="1.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>1.2</b> Towards prediction</a></li>
<li class="chapter" data-level="1.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.2.7</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.3</b> Results</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.3.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.3.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
<li class="chapter" data-level="4.4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.4.6</b> Code availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="5.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>5.2.2</b> Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>5.2.3</b> Materials</a></li>
<li class="chapter" data-level="5.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>5.2.4</b> Procedure</a></li>
<li class="chapter" data-level="5.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>5.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="5.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>5.2.6</b> Imaging details</a></li>
<li class="chapter" data-level="5.2.7" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>5.2.7</b> Data availability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>5.3</b> Results</a><ul>
<li class="chapter" data-level="5.3.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>5.3.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="5.3.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>5.3.3</b> ROI analyses</a></li>
<li class="chapter" data-level="5.3.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>5.3.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>6</b> Explainable models of facial movements predict emotion perception behavior</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#the-prediction-explanation-exploration-framework"><i class="fa fa-check"></i><b>6.1.1</b> The prediction-explanation-exploration framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>6.2.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>6.2.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>6.2.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="6.2.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>6.2.4</b> Evaluated mappings</a></li>
<li class="chapter" data-level="6.2.5" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-dataset"><i class="fa fa-check"></i><b>6.2.5</b> Dataset used to evaluate mappings</a></li>
<li class="chapter" data-level="6.2.6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-code"><i class="fa fa-check"></i><b>6.2.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#prediction"><i class="fa fa-check"></i><b>6.3.1</b> Prediction</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#explanation"><i class="fa fa-check"></i><b>6.3.2</b> Explanation</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#exploration"><i class="fa fa-check"></i><b>6.3.3</b> Exploration</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>7</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="7.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>7.2.1</b> Participants</a></li>
<li class="chapter" data-level="7.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>7.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="7.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>7.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>7.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="7.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>7.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>7.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="7.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>7.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>7.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="7.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>7.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>7.4</b> Discussion</a><ul>
<li class="chapter" data-level="7.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>7.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="7.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>7.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="7.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>7.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="7.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>7.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>8</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a><ul>
<li class="chapter" data-level="E.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supplementary-methods"><i class="fa fa-check"></i><b>E.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="E.1.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-in-detail"><i class="fa fa-check"></i><b>E.1.1</b> Hypothesis kernel analysis (in detail)</a></li>
<li class="chapter" data-level="E.1.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-noise-ceiling-detail"><i class="fa fa-check"></i><b>E.1.2</b> Noise ceiling estimation (in detail)</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supp-fig"><i class="fa fa-check"></i><b>E.2</b> Supplementary figures</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 7</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shared-states-supplement" class="section level1">
<h1><span class="header-section-number">A</span> Supplement to Chapter 2</h1>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-shared-states-S1">Table A.1: </span>Stimuli used for SF-task
</caption>
<thead>
<tr>
<th style="text-align:left;">
Class
</th>
<th style="text-align:left;">
Dutch
</th>
<th style="text-align:left;">
English translation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 5em; ">
Action
</td>
<td style="text-align:left;">
Hard wegrennen
</td>
<td style="text-align:left;">
Running away fast
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Iemand wegduwen
</td>
<td style="text-align:left;">
Pushing someone away
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Iemand stevig vastpakken
</td>
<td style="text-align:left;">
Holding someone tightly
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hoofd schudden
</td>
<td style="text-align:left;">
Shaking your head
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Heftige armgebaren maken
</td>
<td style="text-align:left;">
Making big arm gestures
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Ergens voor terugdeinzen
</td>
<td style="text-align:left;">
Recoiling from something
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je ogen dichtknijpen
</td>
<td style="text-align:left;">
Closing your eyes tightly
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je ogen wijd open sperren
</td>
<td style="text-align:left;">
Opening your eyes widely
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je wenkbrauwen fronsen
</td>
<td style="text-align:left;">
Frowning with your eyebrows
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je schouders ophalen
</td>
<td style="text-align:left;">
Raising your shoulders
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Op de vloer stampen
</td>
<td style="text-align:left;">
Stamping on the floor
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
In elkaar duiken
</td>
<td style="text-align:left;">
Cowering
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je schouders laten hangen
</td>
<td style="text-align:left;">
Slumping your shoulders
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je vuisten ballen
</td>
<td style="text-align:left;">
Tighten your fists
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je borst vooruit duwen
</td>
<td style="text-align:left;">
Push your chest forward
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je tanden op elkaar zetten
</td>
<td style="text-align:left;">
Clench your teeth
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hand voor je mond slaan
</td>
<td style="text-align:left;">
Put your hand in front of your mouth
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Onrustig bewegen
</td>
<td style="text-align:left;">
Moving restlessly
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Heen en weer lopen
</td>
<td style="text-align:left;">
Walking back and forth
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hoofd afkeren
</td>
<td style="text-align:left;">
Turning your head away
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
Interoception
</td>
<td style="text-align:left;">
Een brok in je keel
</td>
<td style="text-align:left;">
A lump in your throat
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Buiten adem zijn
</td>
<td style="text-align:left;">
Being out of breath
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een versnelde hartslag
</td>
<td style="text-align:left;">
A fast beating heart
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hart klopt in de keel
</td>
<td style="text-align:left;">
You heart is beating in your throat
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een benauwd gevoel
</td>
<td style="text-align:left;">
An oppressed feeling
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een misselijk gevoel
</td>
<td style="text-align:left;">
Being nauseous
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Druk op je borst
</td>
<td style="text-align:left;">
A pressure on your chest
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Strak aangespannen spieren
</td>
<td style="text-align:left;">
Tense muscles
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een droge keel
</td>
<td style="text-align:left;">
A dry throat
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Koude rillingen hebben
</td>
<td style="text-align:left;">
Cold shivers
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Bloed stroomt naar je hoofd
</td>
<td style="text-align:left;">
Blood is going to your head
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een verdoofd gevoel
</td>
<td style="text-align:left;">
A numb feeling
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hebt tintelende ledenmaten
</td>
<td style="text-align:left;">
Tingling limbs
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een verlaagde hartslag
</td>
<td style="text-align:left;">
A slow heartbeat
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hebt zware ledematen
</td>
<td style="text-align:left;">
Heavy limbs
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een versnelde ademhaling
</td>
<td style="text-align:left;">
Fast breathing
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hebt hoofdpijn
</td>
<td style="text-align:left;">
Headache
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je hebt buikpijn
</td>
<td style="text-align:left;">
Stomachache
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Zweet staat in je handen
</td>
<td style="text-align:left;">
Sweaty palms
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je maag keert zich om
</td>
<td style="text-align:left;">
Your stomach churns
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
Situation
</td>
<td style="text-align:left;">
Vals beschuldigd worden
</td>
<td style="text-align:left;">
Being falsely accused
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Dierbare overlijdt
</td>
<td style="text-align:left;">
A loved one dies
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Vlees is bedorven
</td>
<td style="text-align:left;">
Meat that has gone off
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je wordt bijna aangereden
</td>
<td style="text-align:left;">
You are almost hit by a car
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Iemand naast je braakt
</td>
<td style="text-align:left;">
Someone next to you vomits
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Huis staat in brand
</td>
<td style="text-align:left;">
House is on fire
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Zonder reden ontslagen worden
</td>
<td style="text-align:left;">
Being fired for no reason
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een ongemakkelijke stilte
</td>
<td style="text-align:left;">
An uncomfortable silence
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Alleen in donker park
</td>
<td style="text-align:left;">
Alone in a dark park
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Inbraak in je huis
</td>
<td style="text-align:left;">
A house burglary
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Een gewond dier zien
</td>
<td style="text-align:left;">
Seeing a wounded animal
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Tentamen verknallen
</td>
<td style="text-align:left;">
Messing up your exam
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je partner bedriegt je
</td>
<td style="text-align:left;">
You partner cheats on you
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Dierbare is vermist
</td>
<td style="text-align:left;">
A loved one is missing
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Belangrijke sollicitatie vergeten
</td>
<td style="text-align:left;">
Forgot a job interview
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Onvoorbereid presentatie geven
</td>
<td style="text-align:left;">
Giving a presentation unprepared
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Je baas beledigt je
</td>
<td style="text-align:left;">
Your boss offends you
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Goede vriend negeert je
</td>
<td style="text-align:left;">
A good friend neglects you
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Slecht nieuws bij arts
</td>
<td style="text-align:left;">
Bad news at the doctor
</td>
</tr>
<tr>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;">
Bommelding in metro
</td>
<td style="text-align:left;">
A bomb alarm in the metro
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> The stimulus materials presented in Table S1 were selected from a pilot study. In this pilot study we asked an independent sample of twenty-four subjects to describe how they would express an emotion in their behavior, body posture or facial expression (action information), what specific sensations they would feel inside their body when they would experience an emotion (interoceptive information), and for what reason or in what situation they would experience an emotion (situational information). These three questions were asked in random order for twenty-eight different negative emotional states, including anger, fear, disgust, sadness, contempt, worry, disappointment, regret and shame. The descriptions generated by these subjects were used as qualitative input in order to create our stimulus set of twenty short sentences that described emotional actions, sensations or situations. With this procedure, we ensured that our stimulus set held sentences that were validated and ecologically appropriate for our sample.
</td>
</tr>
</tfoot>
</table>
<p> 
 </p>
<p><strong>Full instruction for the other-focused emotion understanding task.</strong></p>
<p><em>Translated from Dutch; task presented first.</em></p>
<p>"In this study we are interested in how the brain responds when people understand the emotions of others in different ways. In the scanner you will see images that display emotional situations, sometimes with multiple people. In every image one person will be marked with a red square. While viewing the image we ask you to focus on the emotion of that person in three different ways.</p>
<p>With some images we ask you to focus on HOW this person expresses his or her emotion. Here we ask you to identify expressions in the face or body that are informative about the emotional state that the person is experiencing.</p>
<p>With other images we ask you to focus on WHAT this person may feel in his or her body. Here we ask you to identify sensations, such as a change in heart rate, breathing or other internal feeling, that the person might feel in this situation.</p>
<p>With other images we ask you to focus on WHY this person experiences an emotion. Here we ask you to identify a specific reason or cause that explains why the person feels what he or she feels.</p>
<p>Every image will be presented for six seconds. During this period we ask you to silently focus on HOW this person expresses emotion, WHAT this person feels in his/her body, and WHY this person feels an emotion.</p>
<p>Before you will enter the scanner we will practice. I will show you three images and will ask you to perform each of the three instructions out loud.</p>
<p>It is important to note that there are no correct or incorrect answers, it is about how you interpret the image. For the success of the study it is very important that you apply the HOW, WHAT or WHY instruction for each image. Please do not skip any images and try to apply each instruction with the same motivation. It is also important to treat every image separately, although it is possible that you have similar interpretations for different images.
The three instructions are combined with the images in blocks. In every block you will see five images with the same instruction. Each block will start with a cue that tells you what to focus on in that block.</p>
<p>Each image is combined with all three instructions, so you will see the same image multiple times. In between images you will sometimes see a black screen for a longer period of time.</p>
<p>Do you have any questions?"</p>
<p> 
 </p>
<p><strong>Full instruction for the self-focused emotion imagery task.</strong></p>
<p><em>Translated from Dutch; task presented second.</em></p>
<p>"In this study we are interested in how the brain responds when people imagine different aspects of emotion. In the scanner you will see sentences that describe aspects of emotional experience. We ask you to try to imagine the content of each sentence as rich and detailed as possible.</p>
<p>Some sentences describe actions and expressions. We ask you to imagine that you are performing this action or expression. Other sentences describe sensations or feelings that you can have <em>inside</em> your body. We ask you to imagine that you are experiencing this sensation or feeling. Other sentences describe emotional situations. We ask you to imagine that you are experiencing this specific situation.</p>
<p>We ask you to always imagine that YOU have the experience. Thus, it is about imagining an action or expression of your body, a sensation inside your body, or a situation that you are part of.</p>
<p>I will give some examples now.</p>
<p>For each sentence you have six seconds to imagine the content. All sentences will be presented twice. In between sentences you will sometimes see a black screen for a longer period of time. For this experiment to succeed it is important that you imagine each sentence with the same motivation, even if you have seen the sentence before. Please do not skip sentences.</p>
<p>Do you have any questions?"</p>
<div class="figure"><span id="fig:fig-shared-states-S1"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S1.png" alt="Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, F(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, F(2, 17) = 17.74, p &lt; 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (M = 74.00, SE = 2.10) were significantly less successful (p &lt; 0.001) than both action-trials (M = 85.50, SE = 1.85) and situation trials (M = 90.00, SE = 1.92)."  />
<p class="caption">
Figure A.1: Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, <em>F</em>(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, <em>F</em>(2, 17) = 17.74, <em>p</em> &lt; 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (<em>M</em> = 74.00, <em>SE</em> = 2.10) were significantly less successful (<em>p</em> &lt; 0.001) than both action-trials (<em>M</em> = 85.50, <em>SE</em> = 1.85) and situation trials (<em>M</em> = 90.00, <em>SE</em> = 1.92).
</p>
</div>


<div class="figure"><span id="fig:fig-shared-states-S2"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S2.png" alt="Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e. the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. A) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. B) Classification results of using a low-pass filter (2 seconds) or not. C) Classification results for different numbers of test-trials per class (1 to 5). D) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not." width="90%" height="90%" />
<p class="caption">
Figure A.2: Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e. the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. <strong>A</strong>) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. <strong>B</strong>) Classification results of using a low-pass filter (2 seconds) or not. <strong>C</strong>) Classification results for different numbers of test-trials per class (1 to 5). <strong>D</strong>) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.
</p>
</div>


<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-shared-states-S2">Table A.2: </span>Parameters assessed in the optimization set
</caption>
<thead>
<tr>
<th style="text-align:left;">
Parameter
</th>
<th style="text-align:left;">
Options
</th>
<th style="text-align:left;">
Final choice
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Smoothing kernel
</td>
<td style="text-align:left;">
0 mm, 2 mm, 5 mm, 10 mm
</td>
<td style="text-align:left;">
5 mm
</td>
</tr>
<tr>
<td style="text-align:left;">
Feature selection threshold
</td>
<td style="text-align:left;">
1.5, 1.75, 2, 2.25, 2.5, 2.75, 3
</td>
<td style="text-align:left;">
2.3
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of test-trials
</td>
<td style="text-align:left;">
1, 2, 3, 4, 5
</td>
<td style="text-align:left;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
Low-pass filter
</td>
<td style="text-align:left;">
2 seconds vs. none
</td>
<td style="text-align:left;">
None
</td>
</tr>
<tr>
<td style="text-align:left;">
ICA denoising
</td>
<td style="text-align:left;">
ICA vs. no ICA
</td>
<td style="text-align:left;">
No ICA
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> The first set of parameters we evaluated in the optimization-set were different smoothing factors and feature selection thresholds (see MVPA pipeline section in the main text). On average, across the self- and cross-analysis, a 5 mm smoothing kernel yielded the best results in combination with a feature selection threshold of 2.25, which we rounded up to 2.3 as this number represents a normalized (z-transformed) score, which corresponds to the top 1% scores within a normal distribution. Next, the difference between using a low-pass (of 2 seconds, i.e. 1 TR) versus none was assessed, establishing no low-pass filter as the optimal choice. Next, different numbers of test-trials (1 to 5) per class per iteration were assessed. Four trials yielded the best results. Lastly, the effect of “cleaning” the data with an independent component analysis was examined (FSL: MELODIC and FIX; Salimi-Khorshidi et al., 2014). Not performing ICA yielded the best results. These parameters – 5 mm smoothing kernel, 2.3 feature selection thresholded, no low-pass filter, and four test-trials per iteration – were subsequently used in the analysis of the validation set.
</td>
</tr>
</tfoot>
</table>
<div class="figure"><span id="fig:fig-shared-states-S3"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S3.png" alt="Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent t-test against chance-level classification (i.e. 0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e. 13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all p &lt; 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43% correct) and situation (44% correct) classes scored significantly above chance, p = 0.014 and p = 0.0007 respectively. Interoception was classified at chance level, p = 0.99, which stands in contrast with the results in the validation-set."  />
<p class="caption">
Figure A.3: Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent <em>t</em>-test against chance-level classification (i.e. 0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e. 13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all <em>p</em> &lt; 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43% correct) and situation (44% correct) classes scored significantly above chance, <em>p</em> = 0.014 and <em>p</em> = 0.0007 respectively. Interoception was classified at chance level, <em>p</em> = 0.99, which stands in contrast with the results in the validation-set.
</p>
</div>

<div class="figure"><span id="fig:fig-shared-states-S4"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S4.png" alt="Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial’s final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated."  />
<p class="caption">
Figure A.4: Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial’s final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.
</p>
</div>

<div class="figure"><span id="fig:fig-shared-states-S5"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S5.png" alt="A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all p &lt; 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all p &lt; 0.001); for recall both action and situation were decoded significantly above chance (p = 0.0013 and p &lt; 0.001, respectively), while interoception was decoded below chance. All p-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44%) compared to its recall score (14%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 %) compared to its precision score (44%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59% of the interoception-trials are misclassified as situation-trials."  />
<p class="caption">
Figure A.5: A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all <em>p</em> &lt; 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all <em>p</em> &lt; 0.001); for recall both action and situation were decoded significantly above chance (<em>p</em> = 0.0013 and <em>p</em> &lt; 0.001, respectively), while interoception was decoded below chance. All <em>p</em>-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44%) compared to its recall score (14%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 %) compared to its precision score (44%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59% of the interoception-trials are misclassified as situation-trials.
</p>
</div>

<div class="figure"><span id="fig:fig-shared-states-S6"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S6.png" alt="Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. A) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, r = -0.04, p = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. B) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. C) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4)."  />
<p class="caption">
Figure A.6: Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. <strong>A</strong>) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, <em>r</em> = -0.04, <em>p</em> = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. <strong>B</strong>) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. <strong>C</strong>) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).
</p>
</div>

<div class="figure"><span id="fig:fig-shared-states-S7"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S7.png" alt="Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. P-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, p(action) &lt; 0.001, p(interoception) = 0.008, p(situation) &lt; 0.001. For recall, classification scores for action and interoception were significant (both p &lt; 0.001), but not significant for situation (p = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e. 90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis."  />
<p class="caption">
Figure A.7: Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. <em>P</em>-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, <em>p</em>(action) &lt; 0.001, <em>p</em>(interoception) = 0.008, <em>p</em>(situation) &lt; 0.001. For recall, classification scores for action and interoception were significant (both <em>p</em> &lt; 0.001), but not significant for situation (<em>p</em> = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e. 90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.
</p>
</div>

<div class="figure"><span id="fig:fig-shared-states-S8"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_S8.png" alt="Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p &lt; 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals."  />
<p class="caption">
Figure A.8: Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p &lt; 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.
</p>
</div>

<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-shared-states-S3">Table A.3: </span>Mean general classification scores per subject for the self- and cross-analysis on the validation-set only.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Subject nr.
</th>
<th style="text-align:right;">
Self-analysis precision
</th>
<th style="text-align:right;">
Cross-analysis precision
</th>
<th style="text-align:right;">
Session
</th>
<th style="text-align:left;">
Part of optimization-set?
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.758
</td>
<td style="text-align:right;">
0.445
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.487
</td>
<td style="text-align:right;">
0.336
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.629
</td>
<td style="text-align:right;">
0.316
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.524
</td>
<td style="text-align:right;">
0.577
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.457
</td>
<td style="text-align:right;">
0.492
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.741
</td>
<td style="text-align:right;">
0.296
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.600
</td>
<td style="text-align:right;">
0.542
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
0.431
</td>
<td style="text-align:right;">
0.240
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0.629
</td>
<td style="text-align:right;">
0.497
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.734
</td>
<td style="text-align:right;">
0.268
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.683
</td>
<td style="text-align:right;">
0.386
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.415
</td>
<td style="text-align:right;">
0.525
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
0.623
</td>
<td style="text-align:right;">
0.604
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
y
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
0.810
</td>
<td style="text-align:right;">
0.610
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
0.538
</td>
<td style="text-align:right;">
0.578
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
0.486
</td>
<td style="text-align:right;">
0.455
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
0.549
</td>
<td style="text-align:right;">
0.415
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
<tr>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
0.488
</td>
<td style="text-align:right;">
0.494
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
0.590
</td>
<td style="text-align:right;">
0.289
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
0.600
</td>
<td style="text-align:right;">
0.502
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
n
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Supplementary Table 3 suggests individual variability in the extent to which neural resources are shared between self- and other-focused processes. In the SF-task all subjects demonstrated a mean classification score well above .33 (i.e., score associated with chance). When generalizing the SF-classifier to the OF-task, however, the classification scores appear to be bimodally distributed (see Supplementary Figure 5C). As can be seen in Table 3, some subjects demonstrated a relatively high mean classification score (i.e., &gt; .45), whereas other subjects demonstrated a classification score at chance level or lower. Note that there is no significant difference between the OF classification scores for subjects who participated in the experiment for the first or second time (“Session” column in table; <em>t</em>(18) = 1.73, p = 0.10), nor for subjects who were or were not part of the optimization-set (“Part of optimization-set?” column in table; <em>t</em>(18) = -.95, p = 0.35), suggesting that inclusion in the optimization-set or session ordering is not a confound in the analyses. Regarding individual variability in self-other neural overlap, it is important to note that in the field of embodied cognition, there is increasing attention for the idea that simulation is both individually and contextually dynamic (Oosterwijk &amp; Barrett, 2014; Winkielman, Niedenthal, Wielgosz &amp; Kavanagh, 2015; see also Barrett, 2009). To better distinguish between meaningful individual variation and variation due to other factors (e.g., random noise), future research should test a priori formulated hypotheses about how and when individual variation is expected to occur.
</td>
</tr>
</tfoot>
</table>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-shared-states-S4">Table A.4: </span>Most important voxels in terms of their average weight across iterations and subjects.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Brain region
</th>
<th style="text-align:right;">
k
</th>
<th style="text-align:right;">
Max
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Frontal pole
</td>
<td style="text-align:right;">
1827
</td>
<td style="text-align:right;">
5.05
</td>
<td style="text-align:right;">
2.35
</td>
<td style="text-align:right;">
0.52
</td>
</tr>
<tr>
<td style="text-align:left;">
Occipital pole
</td>
<td style="text-align:right;">
1714
</td>
<td style="text-align:right;">
5.15
</td>
<td style="text-align:right;">
2.45
</td>
<td style="text-align:right;">
0.56
</td>
</tr>
<tr>
<td style="text-align:left;">
Supramarginal gyrus anterior
</td>
<td style="text-align:right;">
1573
</td>
<td style="text-align:right;">
7.48
</td>
<td style="text-align:right;">
2.84
</td>
<td style="text-align:right;">
0.91
</td>
</tr>
<tr>
<td style="text-align:left;">
Lateral occipital cortex superior
</td>
<td style="text-align:right;">
1060
</td>
<td style="text-align:right;">
4.52
</td>
<td style="text-align:right;">
2.18
</td>
<td style="text-align:right;">
0.39
</td>
</tr>
<tr>
<td style="text-align:left;">
Lateral occipital cortex inferior
</td>
<td style="text-align:right;">
923
</td>
<td style="text-align:right;">
4.73
</td>
<td style="text-align:right;">
2.36
</td>
<td style="text-align:right;">
0.49
</td>
</tr>
<tr>
<td style="text-align:left;">
Angular gyrus
</td>
<td style="text-align:right;">
856
</td>
<td style="text-align:right;">
4.52
</td>
<td style="text-align:right;">
2.24
</td>
<td style="text-align:right;">
0.40
</td>
</tr>
<tr>
<td style="text-align:left;">
Supramarginal gyrus posterior
</td>
<td style="text-align:right;">
806
</td>
<td style="text-align:right;">
4.49
</td>
<td style="text-align:right;">
2.29
</td>
<td style="text-align:right;">
0.45
</td>
</tr>
<tr>
<td style="text-align:left;">
Middle temporal gyrus temporo-occipital
</td>
<td style="text-align:right;">
798
</td>
<td style="text-align:right;">
4.00
</td>
<td style="text-align:right;">
2.33
</td>
<td style="text-align:right;">
0.48
</td>
</tr>
<tr>
<td style="text-align:left;">
Temporal pole
</td>
<td style="text-align:right;">
711
</td>
<td style="text-align:right;">
4.38
</td>
<td style="text-align:right;">
2.37
</td>
<td style="text-align:right;">
0.54
</td>
</tr>
<tr>
<td style="text-align:left;">
Precentral gyrus
</td>
<td style="text-align:right;">
568
</td>
<td style="text-align:right;">
3.54
</td>
<td style="text-align:right;">
2.14
</td>
<td style="text-align:right;">
0.31
</td>
</tr>
<tr>
<td style="text-align:left;">
Superior temporal gyrus posterior
</td>
<td style="text-align:right;">
549
</td>
<td style="text-align:right;">
3.64
</td>
<td style="text-align:right;">
2.27
</td>
<td style="text-align:right;">
0.41
</td>
</tr>
<tr>
<td style="text-align:left;">
Superior frontal gyrus
</td>
<td style="text-align:right;">
510
</td>
<td style="text-align:right;">
3.83
</td>
<td style="text-align:right;">
2.18
</td>
<td style="text-align:right;">
0.38
</td>
</tr>
<tr>
<td style="text-align:left;">
Postcentral gyrus
</td>
<td style="text-align:right;">
489
</td>
<td style="text-align:right;">
4.61
</td>
<td style="text-align:right;">
2.43
</td>
<td style="text-align:right;">
0.60
</td>
</tr>
<tr>
<td style="text-align:left;">
Inferior frontal gyrus parstriangularis
</td>
<td style="text-align:right;">
488
</td>
<td style="text-align:right;">
4.22
</td>
<td style="text-align:right;">
2.35
</td>
<td style="text-align:right;">
0.50
</td>
</tr>
<tr>
<td style="text-align:left;">
Inferior frontal gyrus parsopercularis
</td>
<td style="text-align:right;">
441
</td>
<td style="text-align:right;">
3.54
</td>
<td style="text-align:right;">
2.14
</td>
<td style="text-align:right;">
0.31
</td>
</tr>
<tr>
<td style="text-align:left;">
Middle temporal gyrus posterior
</td>
<td style="text-align:right;">
417
</td>
<td style="text-align:right;">
5.68
</td>
<td style="text-align:right;">
2.34
</td>
<td style="text-align:right;">
0.52
</td>
</tr>
<tr>
<td style="text-align:left;">
Occipital fusiform
</td>
<td style="text-align:right;">
400
</td>
<td style="text-align:right;">
4.28
</td>
<td style="text-align:right;">
2.14
</td>
<td style="text-align:right;">
0.37
</td>
</tr>
<tr>
<td style="text-align:left;">
Middle temporal gyrus anterior
</td>
<td style="text-align:right;">
398
</td>
<td style="text-align:right;">
5.68
</td>
<td style="text-align:right;">
2.58
</td>
<td style="text-align:right;">
0.76
</td>
</tr>
<tr>
<td style="text-align:left;">
Middle frontal gyrus
</td>
<td style="text-align:right;">
300
</td>
<td style="text-align:right;">
3.01
</td>
<td style="text-align:right;">
2.06
</td>
<td style="text-align:right;">
0.25
</td>
</tr>
<tr>
<td style="text-align:left;">
Precuneus
</td>
<td style="text-align:right;">
282
</td>
<td style="text-align:right;">
3.34
</td>
<td style="text-align:right;">
2.14
</td>
<td style="text-align:right;">
0.31
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Brain regions were extracted from the Harvard-Oxford (bilateral) Cortical atlas. A minimum threshold for the probabilistic masks of 20 was chosen to minimize overlap between adjacent masks while maximizing coverage of the entire brain. The column <em>k</em> represents the absolute number of above-threshold voxels in the masks. The columns <em>Max</em>, <em>Mean</em>, and <em>Std</em> represent the maximum, mean, and standard deviation from the <em>t</em>-values included in the masks. Note that the <em>t</em>-values, corresponding to the mean weight across subjects normalized by the standard error of the weights across subjects (after correcting for a positive bias when taking the absolute of the weights), were thresholded at a minimum of 1.75, referring to a <em>p</em>-value of 0.05 of a one-sided <em>t</em>-test against zero with 19 degrees of freedom (i.e. <em>n</em> – 1). Note that this <em>t</em>-value map was not corrected for multiple comparisons, and is intended to visualize which regions in the brain were generally involved in our sample of subjects. The <em>X</em>, <em>Y</em>, and <em>Z</em> columns represent the MNI152 (2mm) coordinates of the peak (i.e. max) <em>t</em>-value for each listed brain region.
</td>
</tr>
</tfoot>
</table>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="general-discussion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="confounds-decoding-supplement.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"download": "docs/thesis.pdf"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
