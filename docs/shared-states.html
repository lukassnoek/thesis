<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Towards prediction</title>
  <meta name="description" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="general-introduction.html"/>
<link rel="next" href="confounds-decoding.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>1.1</b> Inference done differently</a></li>
<li class="chapter" data-level="1.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>1.2</b> Towards prediction</a></li>
<li class="chapter" data-level="1.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.2.7</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.3</b> Results</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.3.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.3.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
<li class="chapter" data-level="4.4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.4.6</b> Code availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="5.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>5.2.2</b> Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>5.2.3</b> Materials</a></li>
<li class="chapter" data-level="5.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>5.2.4</b> Procedure</a></li>
<li class="chapter" data-level="5.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>5.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="5.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>5.2.6</b> Imaging details</a></li>
<li class="chapter" data-level="5.2.7" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>5.2.7</b> Data availability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>5.3</b> Results</a><ul>
<li class="chapter" data-level="5.3.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>5.3.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="5.3.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>5.3.3</b> ROI analyses</a></li>
<li class="chapter" data-level="5.3.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>5.3.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>6</b> Explainable models of facial movements predict emotion perception behavior</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#the-prediction-explanation-exploration-framework"><i class="fa fa-check"></i><b>6.1.1</b> The prediction-explanation-exploration framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>6.2.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>6.2.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>6.2.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="6.2.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>6.2.4</b> Evaluated mappings</a></li>
<li class="chapter" data-level="6.2.5" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-dataset"><i class="fa fa-check"></i><b>6.2.5</b> Dataset used to evaluate mappings</a></li>
<li class="chapter" data-level="6.2.6" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-code"><i class="fa fa-check"></i><b>6.2.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#prediction"><i class="fa fa-check"></i><b>6.3.1</b> Prediction</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#explanation"><i class="fa fa-check"></i><b>6.3.2</b> Explanation</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#exploration"><i class="fa fa-check"></i><b>6.3.3</b> Exploration</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>7</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="7.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>7.2.1</b> Participants</a></li>
<li class="chapter" data-level="7.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>7.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="7.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>7.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>7.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="7.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>7.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>7.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="7.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>7.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>7.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="7.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>7.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>7.4</b> Discussion</a><ul>
<li class="chapter" data-level="7.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>7.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="7.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>7.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="7.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>7.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="7.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>7.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>8</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a><ul>
<li class="chapter" data-level="E.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supplementary-methods"><i class="fa fa-check"></i><b>E.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="E.1.1" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-in-detail"><i class="fa fa-check"></i><b>E.1.1</b> Hypothesis kernel analysis (in detail)</a></li>
<li class="chapter" data-level="E.1.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-noise-ceiling-detail"><i class="fa fa-check"></i><b>E.1.2</b> Noise ceiling estimation (in detail)</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html#hka-supp-fig"><i class="fa fa-check"></i><b>E.2</b> Supplementary figures</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 7</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shared-states" class="section level1">
<h1><span class="header-section-number">2</span> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Oosterwijk, S.*, Snoek, L.*, Rotteveel, M., Barrett, L. F., &amp; Scholte, H. S. (2017). Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding. <em>Social cognitive and affective neuroscience, 12</em>(7), 1025-1035.</p>

<p>* Shared first authorship

</p>
<p><p><strong>Abstract</strong></p>

The present study tested whether the neural patterns that support imagining “performing an action”, “feeling a bodily sensation” or “being in a situation” are directly involved in understanding <em>other people’s</em> actions, bodily sensations and situations. Subjects imagined the content of short sentences describing emotional actions, interoceptive sensations and situations (self-focused task), and processed scenes and focused on <em>how</em> the target person was expressing an emotion, <em>what</em> this person was feeling, and <em>why</em> this person was feeling an emotion (other-focused task). Using a linear support vector machine classifier on brain-wide multi-voxel patterns, we accurately decoded each individual class in the self-focused task. When generalizing the classifier from the self-focused task to the other-focused task, we also accurately decoded whether subjects focused on the emotional actions, interoceptive sensations and situations of <em>others</em>. These results show that the neural patterns that underlie self-imagined experience are involved in understanding the experience of other people. This supports the theoretical assumption that the basic components of emotion experience and understanding share resources in the brain.
</p>
<div id="shared-states-introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>To navigate the social world successfully it is crucial to understand other people. But how do people generate meaningful representations of other people’s actions, sensations, thoughts and emotions? The dominant view assumes that representations of other people’s experiences are supported by the same neural systems as those that are involved in generating experience in the self <span class="citation">(e.g., Gallese et al., <a href="bibliography.html#ref-gallese2004unifying" role="doc-biblioref">2004</a>; see for an overview Singer, <a href="bibliography.html#ref-singer2012past" role="doc-biblioref">2012</a>)</span>. We tested this principle of self-other neural overlap directly, using multi-voxel pattern analysis (MVPA), across three different aspects of experience that are central to emotions: actions, sensations from the body and situational knowledge.</p>
<p>In recent years, evidence has accumulated that suggests a similarity between the neural patterns representing the self and others. For example, a great variety of studies have shown that observing actions and sensations in other people engages similar neural circuits as acting and feeling in the self <span class="citation">(see for an overview Bastiaansen et al., <a href="bibliography.html#ref-bastiaansen2009evidence" role="doc-biblioref">2009</a>)</span>. Moreover, an extensive research program on pain has demonstrated an overlap between the experience of physical pain and the observation of pain in other people, utilizing both neuroimaging techniques <span class="citation">(e.g., Lamm et al., <a href="bibliography.html#ref-lamm2011meta" role="doc-biblioref">2011</a>)</span> and analgesic interventions <span class="citation">(e.g., Rütgen et al., <a href="bibliography.html#ref-rutgen2015placebo" role="doc-biblioref">2015</a>; Mischkowski et al., <a href="bibliography.html#ref-mischkowski2016painkiller" role="doc-biblioref">2016</a>)</span>. This process of “vicarious experience” or “simulation” is viewed as an important component of empathy <span class="citation">(Carr et al., <a href="bibliography.html#ref-carr2003neural" role="doc-biblioref">2003</a>; Decety, <a href="bibliography.html#ref-decety2011dissecting" role="doc-biblioref">2011</a>; Keysers &amp; Gazzola, <a href="bibliography.html#ref-keysers2014dissociating" role="doc-biblioref">2014</a>)</span>. In addition, it is argued that mentalizing (e.g. understanding the mental states of other people) involves the same brain networks as those involved in self-generated thoughts <span class="citation">(Uddin et al., <a href="bibliography.html#ref-uddin2007self" role="doc-biblioref">2007</a>; Waytz &amp; Mitchell, <a href="bibliography.html#ref-waytz2011two" role="doc-biblioref">2011</a>)</span>. Specifying this idea further, a constructionist view on emotion proposes that both emotion experience and interpersonal emotion understanding are produced by the same large-scale distributed brain networks that support the processing of sensorimotor, interoceptive and situationally relevant information <span class="citation">(Barrett &amp; Satpute, <a href="bibliography.html#ref-barrett2013large" role="doc-biblioref">2013</a>; Oosterwijk &amp; Barrett, <a href="bibliography.html#ref-oosterwijk2014embodiment" role="doc-biblioref">2014</a>)</span>. An implication of these views is that the representation of self- and other-focused emotional actions, interoceptive sensations and situations overlap in the brain.</p>
<p>Although there is experimental and theoretical support for the idea of self-other neural overlap, the present study is the first to directly test this process using MVPA across three different aspects of experience (i.e. actions, interoceptive sensations and situational knowledge). Our experimental design consisted of two different tasks aimed at generating self- and other-focused representations with a relatively large weight given to either action information, interoceptive information or situational information.</p>
<p>In the <em>self-focused</em> emotion imagery task (SF-task) subjects imagined performing or experiencing actions (e.g., <em>pushing someone away</em>), interoceptive sensations (e.g., <em>increased heart rate</em>) and situations (e.g., <em>alone in a park at night</em>) associated with emotion. Previous research has demonstrated that processing linguistic descriptions of (emotional) actions and feeling states can result in neural patterns of activation associated with, respectively, the representation and generation of actions and internal states <span class="citation">(Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2015concepts" role="doc-biblioref">2015</a>; Pulvermüller &amp; Fadiga, <a href="bibliography.html#ref-pulvermuller2010active" role="doc-biblioref">2010</a>)</span>. Furthermore, imagery-based inductions of emotion have been successfully used in the MRI scanner before <span class="citation">(Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2012states" role="doc-biblioref">2012</a>; Wilson-Mendenhall et al., <a href="bibliography.html#ref-wilson2011grounding" role="doc-biblioref">2011</a>)</span>, and are seen as robust inducers of emotional experience <span class="citation">(Lench et al., <a href="bibliography.html#ref-lench2011discrete" role="doc-biblioref">2011</a>)</span>. In the <em>other-focused</em> emotion understanding task (OF-task), subjects viewed images of people in emotional situations and focused on actions (i.e., <em>How</em> does this person express his/her emotions?), interoceptive sensations (i.e., <em>What</em> does this person feel in his/her body) or the situation (i.e., <em>Why</em> does this person feel an emotion?). This task is based on previous research studying the neural basis of emotion oriented mentalizing <span class="citation">(Spunt &amp; Lieberman, <a href="bibliography.html#ref-spunt2012integrative" role="doc-biblioref">2012</a>)</span>.</p>
<p>With MVPA, we examined to what extent the SF- and OF-task evoked similar neural patterns. MVPA allows researchers to assess whether the neural pattern associated with one set of experimental conditions can be used to distinguish between another set of experimental conditions. This relatively novel technique has been successfully applied to the field of social neuroscience in general <span class="citation">(e.g., Gilbert et al., <a href="bibliography.html#ref-gilbert2012evaluative" role="doc-biblioref">2012</a>; Brosch et al., <a href="bibliography.html#ref-brosch2013implicit" role="doc-biblioref">2013</a>; Parkinson et al., <a href="bibliography.html#ref-parkinson2014common" role="doc-biblioref">2014</a>)</span>, and the field of self-other neural overlap in particular. For example, several MVPA studies recently assessed whether experiencing pain and observing pain in others involved similar neural patterns <span class="citation">(Corradi-Dell’Acqua et al., <a href="bibliography.html#ref-corradi2016cross" role="doc-biblioref">2016</a>; Krishnan et al., <a href="bibliography.html#ref-krishnan2016somatic" role="doc-biblioref">2016</a>)</span>. Although there is an ongoing discussion about the specifics of shared representation in pain based on these MVPA results <span class="citation">(see for an overview Zaki et al., <a href="bibliography.html#ref-zaki2016anatomy" role="doc-biblioref">2016</a>)</span>, many authors emphasize the importance of this technique in the scientific study of self-other neural overlap <span class="citation">(e.g., Corradi-Dell’Acqua et al., <a href="bibliography.html#ref-corradi2016cross" role="doc-biblioref">2016</a>; Krishnan et al., <a href="bibliography.html#ref-krishnan2016somatic" role="doc-biblioref">2016</a>)</span>.</p>
<p>MVPA is an analysis technique that decodes latent categories from fMRI data in terms of multi-voxel patterns of activity <span class="citation">(Norman et al., <a href="bibliography.html#ref-norman2006beyond" role="doc-biblioref">2006</a>)</span>. This technique is particularly suited for our research question for several reasons. First of all, although univariate techniques can demonstrate that tasks activate the same brain regions, only MVPA can statistically test for shared representation <span class="citation">(Lamm &amp; Majdandžić, <a href="bibliography.html#ref-lamm2015role" role="doc-biblioref">2015</a>)</span>. We will evaluate whether multivariate brain patterns that distinguish between mental events in the SF-task can be used to distinguish, above chance level, between mental events in the OF-task. Second, MVPA analyses are particularly useful in research that is aimed at examining distributed representations <span class="citation">(Singer, <a href="bibliography.html#ref-singer2012past" role="doc-biblioref">2012</a>)</span>. Based on our constructionist framework, we indeed hypothesize that the neural patterns that will represent self- and other focused mental events are distributed across large-scale brain networks. To capture these distributed patterns, we used MVPA in combination with data-driven univariate feature selection on whole-brain voxel patterns, instead of limiting our analysis to specific regions-of-interest <span class="citation">(Haynes, <a href="bibliography.html#ref-haynes2015primer" role="doc-biblioref">2015</a>)</span>. And third, in contrast to univariate analyses that aggregate data across subjects, MVPA can be performed within-subjects and is thus able to incorporate individual variation in the representational content of multivariate brain patterns. In that aspect within-subject MVPA is sensitive to individual differences in how people imagine actions, sensations and situations, and how they understand others. In short, for our purpose to explicitly test the assumption that self and other focused processes share neural resources, MVPA is the designated method.</p>
<p>We tested the following two hypotheses. First, we tested whether we could classify <em>self-imagined</em> actions, interoceptive sensations and situations above chance level. Second, we tested whether the multivariate pattern underlying this classification could also be used to classify the how, what and why condition in the <em>other-focused</em> task.</p>
</div>
<div id="shared-states-methods" class="section level2">
<h2><span class="header-section-number">2.2</span> Methods</h2>
<div id="shared-states-methods-subjects" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Subjects</h3>
<p>In total, we tested 22 Dutch undergraduate students from the University of Amsterdam (14 females; M<sub>age</sub> = 21.48, s.d.<sub>age</sub> = 1.75). Of those 22 subjects, 13 subjects were tested twice in 2 sessions about 1 week apart. Half of those sessions were used for the model optimization procedure. The other half of the sessions, combined with an additional nine subjects (who were tested only once), constituted the model validation set (see Model optimization procedure section). In total, two subjects were excluded from the model validation dataset: one subject was excluded because there was not enough time to complete the experimental protocol and another subject was excluded due to excessive movement (&gt;3 mm within data acquisition runs).</p>
<p>All subjects signed informed consent prior to the experiment. The experiment was approved by the University of Amsterdam’s ethical review board. Subjects received 22.50 euro per session. Standard exclusion criteria regarding MRI safety were applied and people who were on psychopharmacological medication were excluded a priori.</p>
</div>
<div id="shared-states-methods-experimental-design" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Experimental design</h3>
<div id="shared-states-methods-experimental-design-sf-task" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> Self-focused emotion imagery task</h4>
<p>The self-focused emotion imagery task (SF-task) was created to preferentially elicit <em>self-focused</em> processing of action, interoceptive or situational information associated with emotion. Subjects processed short linguistic cues that described actions (e.g., <em>pushing someone away</em>; <em>making a fist</em>), interoceptive sensations (e.g., <em>being out of breath</em>; <em>an increased heart rate</em>), or situations (e.g., <em>alone in a park at night</em>; <em>being falsely accused</em>) and were instructed to imagine performing or experiencing the content. The complete instruction is presented in the Supplementary Materials; all stimuli used in the SF-task are presented in Supplementary Table <a href="shared-states-supplement.html#tab:tab-shared-states-S1">A.1</a>. Linguistic cues were selected from a pilot study performed on an independent sample of subjects (<em>n</em> = 24). Details about this pilot study are available on request. The descriptions generated in this pilot study were used as qualitative input to create short sentences that described actions, sensations or situations that were associated with negative emotions, without including discrete emotion terms. The cues did not differ in number of words, nor in number of characters (<em>F</em> &lt; 1).</p>
<p>The SF-task was performed in two runs subsequent to the other-focused task using the software package Presentation (Version 16.4, <a href="www.neurobs.com">www.neurobs.com</a>). Each run presented 60 sentences on a black background (20 per condition) in a fully randomized event-related fashion, with a different randomization for each subject. Note that implementing a separate randomization for each subject prevents inflated false positive pattern correlations between trials of the same condition, which may occur in single-trial designs with short inter-stimulus intervals <span class="citation">(Mumford et al., <a href="bibliography.html#ref-mumford2014impact" role="doc-biblioref">2014</a>)</span>. A fixed inter-trial–interval of 2 seconds separating trials; 12 null-trials (i.e. a black screen for 8 seconds) were mixed with the experimental trials at random positions during each run (see Figure <a href="shared-states.html#fig:fig-shared-states-1">2.1</a>).</p>
<div class="figure"><span id="fig:fig-shared-states-1"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_1.png" alt="Overview of the self-focused and other-focused task."  />
<p class="caption">
Figure 2.1: Overview of the self-focused and other-focused task.
</p>
</div>
</div>
<div id="shared-states-methods-experimental-design-of-task" class="section level4">
<h4><span class="header-section-number">2.2.2.2</span> Other-focused emotion understanding task</h4>
<p>The other-focused emotion understanding task (OF-task) was created to preferentially elicit <em>other-focused</em> processing of action, interoceptive or situational information associated with emotion. Subjects viewed images of people in negative situations (e.g. a woman screaming at a man, a man held at gunpoint). A red rectangle highlighted the face of the person that the subjects should focus on to avoid ambiguity in images depicting more than one person. Image blocks were preceded by a cue indicating the strategy subjects should use in perceiving the emotional state of the people in the images <span class="citation">(Spunt &amp; Lieberman, <a href="bibliography.html#ref-spunt2012integrative" role="doc-biblioref">2012</a>)</span>. The cue <em>How</em> instructed the subjects to identify actions that were informative about the person’s emotional state (i.e., <em>How</em> does this person express his/her emotions?). The cue <em>What</em> instructed subjects to identify interoceptive sensations that the person could experience (i.e., <em>What</em> does this person feel in his/her body). The cue <em>Why</em> instructed subjects to identify reasons or explanations for the person’s emotional state (i.e., <em>Why</em> does this person feel an emotion?). The complete instruction is presented in the <a href="shared-states-supplement.html#shared-states-supplement">Supplementary Materials</a>.</p>
<p>Stimuli for the OF-task were selected from the International Affective Picture System database <span class="citation">(IAPS; Lang, <a href="bibliography.html#ref-lang2005international" role="doc-biblioref">2005</a>; Lang et al., <a href="bibliography.html#ref-lang1997international" role="doc-biblioref">1997</a>)</span>, the image set developed by the Kveraga lab <span class="citation">(<a href="http://www.kveragalab.org/stimuli.html" role="doc-biblioref">http://www.kveragalab.org/stimuli.html</a>; Kveraga et al., <a href="bibliography.html#ref-kveraga2015if" role="doc-biblioref">2015</a>)</span> and the internet (Google images). We selected images based on a pilot study, performed on an independent sample of subjects (<em>n</em> = 22). Details about this pilot study are available on request.</p>
<p>The OF-task was presented using the software package Presentation. The task presented thirty images on a black background in blocked fashion, with each block starting with a what, why or how cue (see Figure <a href="shared-states.html#fig:fig-shared-states-1">2.1</a>). Each image was shown three times, once for each cue type. Images were presented in blocks of six, each lasting 6 seconds, followed by a fixed inter trial interval of 2 seconds. Null-trials were inserted at random positions within the blocks. Both the order of the blocks and the specific stimuli within and across blocks were fully randomized, with a different randomization for each subject.</p>
</div>
</div>
<div id="shared-states-methods-procedure" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Procedure</h3>
<p>Each experimental session lasted about 2 hours. Subjects who underwent two sessions had them on different days within a time span of 1 week. On arrival, subjects gave informed consent and received thorough task instructions, including practice trials (see the <a href="shared-states-supplement.html#shared-states-supplement">Supplementary Materials</a> for a translation of the task instructions). The actual time in the scanner was 55 minutes, and included a rough 3D scout image, shimming sequence, 3-min structural T1-weighted scan, one functional run for the OF-task and two functional runs for the SF-task. We deliberately chose to present the SF-task after the OF-task to exclude the possibility that the SF-task affected the OF-task, thereby influencing the success of the decoding procedure.</p>
<p>After each scanning session, subjects rated their success rate for the SF-task and OF-task (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S1">A.1</a>). In the second session, subjects filled out three personality questionnaires that will not be further discussed in this paper and were debriefed about the purpose of the study.</p>
</div>
<div id="shared-states-methods-image-acquisition" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Image acquisition</h3>
<p>Subjects were tested using a Philips Achieva 3T MRI scanner and a 32-channel SENSE headcoil. A survey scan was made for spatial planning of the subsequent scans. Following the survey scan, a 3-min structural T1-weighted scan was acquired using 3D fast field echo (TR: 82 ms, TE: 38 ms, flip angle: 8°, FOV: 240 × 188 mm, 220 slices acquired using single-shot ascending slice order and a voxel size of 1.0 × 1.0 × 1.0 mm). After the T1-weighted scan, functional T2*-weighted sequences were acquired using single shot gradient echo, echo planar imaging (TR = 2000 ms, TE = 27.63 ms, flip angle: 76.1°, FOV: 240 × 240 mm, in-plane resolution 64 × 64, 37 slices (with ascending acquisition), slice thickness 3 mm, slice gap 0.3 mm, voxel size 3 × 3 × 3 mm), covering the entire brain. For the SF-task, 301 volumes were acquired; for the OF-task 523 volumes were acquired.</p>
</div>
<div id="shared-states-methods-model-optimization-procedure" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Model optimization procedure</h3>
<p>As MVPA is a fairly novel technique, no consistent, optimal MVPA pipeline has been established <span class="citation">(Etzel et al., <a href="bibliography.html#ref-etzel2011impact" role="doc-biblioref">2011</a>)</span>. Therefore, we adopted a validation strategy in the present study that is advised in the pattern classification field <span class="citation">(Kay et al., <a href="bibliography.html#ref-kay2008identifying" role="doc-biblioref">2008</a>; Kriegeskorte et al., <a href="bibliography.html#ref-kriegeskorte2009circular" role="doc-biblioref">2009</a>)</span>. This strategy entailed that we separated our data into an optimization dataset to find the most optimal parameters for preprocessing and analysis, and a validation dataset to independently verify classification success with those optimal parameters. We generated an optimization and validation dataset by running the SF-task and OF-task twice, in two identical experimental sessions for a set of thirteen subjects. The sessions were equally split between the optimization and validation set (see Figure <a href="shared-states.html#fig:fig-shared-states-2">2.2</a>A); first and second sessions were counterbalanced between the two sets. Based on a request received during the review process, we added nine new subjects to the validation dataset. Ultimately, the optimization-set held 13 sessions and the validation-set, after exclusion of 2 subjects (see <a href="shared-states.html#shared-states-methods-subjects">Subjects</a> section), held 20 sessions.</p>
<div class="figure"><span id="fig:fig-shared-states-2"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_2.png" alt="Schematic overview of the cross-validation procedures. A) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. B) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90% of the self-data trials (i.e. train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e. scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials."  />
<p class="caption">
Figure 2.2: Schematic overview of the cross-validation procedures. <strong>A</strong>) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. <strong>B</strong>) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90% of the self-data trials (i.e. train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e. scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.
</p>
</div>

<p>In the optimization-set, we explored how different preprocessing options and the so-called ‘hyperparameters’ in the MVPA pipeline affected the performance of the (multivariate) analyses (visualized in Figure <a href="shared-states.html#fig:fig-shared-states-2">2.2</a>B; see <a href="shared-states.html#shared-states-methods-mvpa-pipeline">MVPA pipeline</a> subsection for more details). Thus, we performed the self- and cross-analyses <em>on the data of the optimization set</em> multiple times with different preprocessing options (i.e., smoothing kernel, low-pass filter and ICA-based denoising strategies) and MVPA hyperparameter values (i.e., univariate feature selection <em>threshold</em> and train/test size ratio during cross-validation). We determined the optimal parameters on the basis of classification performance, which was operationalized as the mean precision value after a repeated random subsampling procedure with 1000 iterations. A list with the results from the optimization procedure can be found in Supplementary Table <a href="shared-states-supplement.html#tab:tab-shared-states-S2">A.2</a> and Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S2">A.2</a>. The optimal parameters were then used for preprocessing and the self- and cross-analysis within the validation-set, in which the findings from the optimization-set were replicated. All findings discussed in the <a href="shared-states.html#shared-states-results">2.3</a> section follow from the validation-set (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S3">A.3</a> for an overview of the findings from the optimization-set).</p>
</div>
<div id="shared-states-methods-preprocessing" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Preprocessing and single-trial modeling</h3>
<p>Functional and structural data were preprocessed and analyzed using FSL 5.0 <span class="citation">(Jenkinson et al., <a href="bibliography.html#ref-Jenkinson2012-ui" role="doc-biblioref">2012</a>)</span> and MATLAB (2012b; <a href="www.mathworks.com/products/matlab">www.mathworks.com/products/matlab</a>), using an in-house developed preprocessing pipeline and the parameters established in the optimization procedure. Functional data were corrected for motion (using FSL MCFLIRT) and slice timing and was spatially smoothed (5 mm isotropic kernel). After preprocessing, individual time series were modeled using a double-gamma hemodynamic response function in a single-trial GLM design using FSL’s FEAT. Resulting beta values were converted to <em>t</em>-values <span class="citation">(Misaki et al., <a href="bibliography.html#ref-misaki2010comparison" role="doc-biblioref">2010</a>)</span>, constituting a whole-brain pattern of <em>t</em>-values per trial. Subsequently, the data were indexed by a gray-matter mask (excluding most white-matter, CSF and brainstem voxels). Thus, the data points for the MVPA consist of whole-brain (gray matter) <em>t</em>-value patterns per trial. For the optimization analyses, the data were transformed to standard space (MNI152, 2 mm) using FSL’s FNIRT. To reduce computation time for the validation data, and in particular its corresponding permutation analysis, analyses on the validation dataset were performed on data in native (functional) space.</p>
</div>
<div id="shared-states-methods-mvpa" class="section level3">
<h3><span class="header-section-number">2.2.7</span> Multi-voxel pattern analysis</h3>
<div id="shared-states-methods-mvpa-pipeline" class="section level4">
<h4><span class="header-section-number">2.2.7.1</span> MVPA pipeline</h4>
<p>Within the optimization and validation dataset, we implemented an iterated cross-validation scheme that separated the data into a train-set and a test-set (this procedure is described in more detail in the next section). Before fitting the classifier on the train-set in each iteration of the cross-validation scheme, standardization and voxel selection were estimated and applied to the train-set. Standardization ensured that each feature (i.e., voxel) had zero mean and unit variance across trials. After standardization, voxel selection was performed in each iteration on the train-set by extracting the voxels with the highest average pairwise Euclidian distance across classes, which will be subsequently referred to as a voxel’s differentiation score. More specifically, differentiation scores were calculated by subtracting the mean value across trials per class from each other (i.e., action—interoception, action—situation, interoception—situation), normalizing these values across voxels (yielding “z-scores”), and taking their absolute value. The three resulting values per voxel were averaged and the most differentiating voxels (z-score threshold: 2.3, as determined by the optimization procedure; see <a href="shared-states.html#shared-states-methods-model-optimization-procedure">Model optimization procedure</a> section) were extracted and used as features when fitting the classifier. Importantly, the standardization parameters (voxel mean and variance) and voxel indices (i.e. which voxels had differentiation scores above threshold) were estimated from the train-set only and subsequently applied to the test-set to ensure independence between the train- and test-set (see Figure 2B). After standardization and voxel selection in each iteration, a support vector classifier (SVC) was fit on the train-set and cross-validated on the test-set, generating a class probability for each trial in the test-set. Our classifier of choice was the SVC implementation from the scikit-learn <code>svm</code> module <span class="citation">(Pedregosa et al., <a href="bibliography.html#ref-pedregosa2011scikit" role="doc-biblioref">2011</a>)</span> with a linear kernel, fixed regularization parameter (<em>C</em>) of 1.0, one-vs-one multiclass strategy, estimation of class probability output (instead of discrete class prediction) and otherwise default parameters.</p>
</div>
<div id="shared-states-methods-mvpa-cv-and-bagging" class="section level4">
<h4><span class="header-section-number">2.2.7.2</span> Cross-validation scheme and bagging procedure</h4>
<p>Cross-validation of the classification analysis was implemented using a repeated random subsampling cross-validation scheme (also known as Monte Carlo cross-validation), meaning that, for each iteration of the analysis, the classification pipeline (i.e., standardization, voxel selection and SVM fitting) was applied on a random subset of data points (i.e., the train-set) and cross-validated on the remaining data (i.e., the test-set). Each trial belonged to one out of three classes: action, interoception or situation. Following the results from the parameter optimization process, we selected four trials per class for testing, amounting to 12 test-trials per iteration.</p>
<p>Per iteration, the classifier was fit on the train-set from the SF-data. Subsequently, this classifier was cross-validated on 12 test SF-trials (test-set “self-analysis”) and 12 test OF-trials (test-set “cross-analysis”; see Figure 2B). This process was subsequently iterated 100 000 times to generate a set of class distributions for each trial. After all iterations, the final predicted class of each trial was determined by its highest summed class probability across iterations (also known as “soft voting”; see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S4">A.4</a>). This strategy of a random sub-sampling cross-validation scheme in combination with majority (soft) voting is more commonly known as “bagging” <span class="citation">(Breiman, <a href="bibliography.html#ref-breiman1996bagging" role="doc-biblioref">1996</a>)</span>. An important advantage of bagging is that it reduces model overfitting by averaging over an ensemble of models, which is especially useful for multi-voxel pattern analyses because fMRI data is known to display high variance <span class="citation">(Varoquaux, <a href="bibliography.html#ref-Varoquaux2018-uo" role="doc-biblioref">2018</a>)</span>.</p>
<p>After generating a final prediction for all trials using the soft voting method, we constructed confusion matrices for both the self- and cross-analysis. In each raw confusion matrix with prediction counts per class, cells were normalized by dividing prediction counts by the sum over rows (i.e., the total amount of predictions per class), yielding precision-scores (also known as positive predictive value). In other words, this metric represents the ratio of true positives to the sum of true positives and false positives (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S5">A.5</a> for a description of the results expressed as <em>recall</em> estimates, or the ratio of true positives to the total number of samples in that class). This classification pipeline generated subject-specific confusion matrices that were subsequently averaged to generate the final classification scores.</p>
</div>
<div id="shared-states-methods-mvpa-statistical-evaluation" class="section level4">
<h4><span class="header-section-number">2.2.7.3</span> Statistical evaluation</h4>
<p>To evaluate the statistical significance of the observed average precision-scores in the confusion matrices, we permuted the original self- and cross-analysis 1300 times per subject with randomly shuffled class labels, yielding 1300 confusion matrices (with precision-scores). We then averaged the confusion matrices across subjects, yielding 1300 permuted confusion matrices reflecting the null-distribution of each cell of the matrix (which is centered around chance level classification, i.e., 33%). For each cell in the diagonal of the observed confusion matrix, <em>p</em>-values were calculated as the proportion of instances of values in the permuted matrix which were higher than the values in the observed matrix <span class="citation">(Nichols &amp; Holmes, <a href="bibliography.html#ref-nichols2002nonparametric" role="doc-biblioref">2002</a>)</span>. To correct for multiple comparisons, <em>p</em>-values were tested against a Bonferroni-corrected threshold. The distribution of precision-scores and the relationship between precision-scores in the self- and cross-analysis is reported in Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S6">A.6</a>.</p>
</div>
<div id="shared-states-methods-mvpa-spatial-representation" class="section level4">
<h4><span class="header-section-number">2.2.7.4</span> Spatial representation</h4>
<p>To visualize the classifier feature weights, we plotted the absolute feature weights averaged over iterations, subjects and pairwise classifiers (action <em>vs</em> interoception, action <em>vs</em> situation, interoception <em>vs</em> situation) that underlie our multiclass classification analysis. We chose to visualize the spatial representation of our model by plotting the average absolute feature weights, because the absolute value of feature weights in linear SVMs can be interpreted as how important the weights are in constructing the model’s decision hyperplane <span class="citation">(Ethofer et al., <a href="bibliography.html#ref-ethofer2009decoding" role="doc-biblioref">2009</a>; Guyon et al., <a href="bibliography.html#ref-guyon2002gene" role="doc-biblioref">2002</a>; Stelzer et al., <a href="bibliography.html#ref-stelzer2014prioritizing" role="doc-biblioref">2014</a>)</span>. To correct for a positive bias in plotting absolute weights, we ran the main classification analysis again with permuted labels to extract the average absolute feature weights that one would expect by chance. Subsequently, a voxel-wise independent <em>t</em>-test was performed for all feature weights across subjects, using the average permuted feature weights as the null-hypothesis, yielding an interpretable <em>t</em>-value map (see the supplementary code notebook on our Github repository for computational details).</p>
</div>
</div>
<div id="shared-states-methods-additional-analyses" class="section level3">
<h3><span class="header-section-number">2.2.8</span> Additional analyses</h3>
<p>In addition to the self-analysis and the self-to-other cross-analysis presented in the main text, we also performed a within-subjects other-to-self cross-analysis <span class="citation">(see for a similar approach Corradi-Dell’Acqua et al., <a href="bibliography.html#ref-corradi2016cross" role="doc-biblioref">2016</a>)</span> and a between-subjects self-analysis and self-to-other cross-analysis. These analyses forward largely similar results as the analyses presented in the main text. Due to space constraints, we present these additional analyses in the <a href="shared-states-supplement.html#shared-states-supplement">Supplementary Materials</a>. Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S7">A.7</a> represents confusion matrices with precision and recall estimates for the other-to-self cross-analysis. Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S8">A.8</a> presents the results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects.</p>
</div>
<div id="shared-states-methods-univariate-analysis" class="section level3">
<h3><span class="header-section-number">2.2.9</span> Univariate analysis</h3>
<p>To be complete, we also report a set of univariate analyses performed on the SF-task and the OF-task data. The univariate analyses were performed on the validation dataset, and were subject to the same preprocessing steps as the MVPA analysis, except that we did not model each trial, but each condition as a separate regressor. The group-level analysis was performed with FSL’s FLAME1 option. To examine differences in neural activity between conditions, we calculated contrasts between the three classes in the SF-task (self-action <em>vs</em> self-interoception; self-action <em>vs</em> self-situation and self-interoception <em>vs</em> self-situation) and the three classes in the OF-task (other-action <em>vs</em> other-interoception; other-action <em>vs</em> other-situation and other-interoception <em>vs</em> other-situation). We report clusters that were corrected using cluster-correction with a voxel-wise threshold of 0.005 (<em>z</em> = 2.7) and a cluster-wise <em>p</em>-value threshold of 0.05.</p>
</div>
<div id="shared-states-methods-code-availability" class="section level3">
<h3><span class="header-section-number">2.2.10</span> Code availability</h3>
<p>The MVPA-analysis and subsequent (statistical) analyses were implemented using custom Python scripts, which depend heavily on the skbold package, a set of tools for machine learning analyses of fMRI data developed in-house (see <a href="https://github.com/lukassnoek/skbold">https://github.com/lukassnoek/skbold</a>). The original scripts were documented and are hosted at the following Github repository: <a href="https://github.com/lukassnoek/SharedStates">https://github.com/lukassnoek/SharedStates</a>.</p>
</div>
</div>
<div id="shared-states-results" class="section level2">
<h2><span class="header-section-number">2.3</span> Results</h2>
<div id="shared-states-results-mvpa" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Multi-voxel pattern analysis</h3>
<p>The analyses of the SF-task demonstrated that voxel patterns reflecting imagined self-focused actions, interoceptive sensations and situations associated with emotion could be decoded accurately for each individual class (all <em>p</em> &lt; 0.001, see Figure <a href="shared-states.html#fig:fig-shared-states-3">2.3</a>). Furthermore, when we generalized the classifier based on the SF-task to the data from the OF-task (i.e. cross-analysis), we found that neural representations of emotional actions, interoceptive sensations and situations of others could also be reliably decoded above chance (all <em>p</em> &lt; 0.001; see Figure <a href="shared-states.html#fig:fig-shared-states-3">2.3</a>). Supplementary Table <a href="shared-states-supplement.html#tab:tab-shared-states-S3">A.3</a> presents mean precision-scores across classes for each subject separately. As predicted, our findings demonstrate that <em>self-imagined</em> actions, interoceptive sensations and situations are associated with distinct neural patterns. Furthermore, and as predicted, our findings demonstrate that the patterns associated with self-imagined actions, sensations and situations can be used to decode <em>other-focused</em> actions, interoceptive sensations and situations (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S7">A.7</a> for the complementary other-to-self cross-analysis).</p>
<div class="figure"><span id="fig:fig-shared-states-3"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_3.png" alt="Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero."  />
<p class="caption">
Figure 2.3: Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.
</p>
</div>

<p>To visualize which neural regions were involved in the successful decoding of the three classes in the OF-task and SF-task, we display in Figure <a href="shared-states.html#fig:fig-shared-states-4">2.4</a> the averaged absolute values of the SVM feature weights. Note that Figure <a href="shared-states.html#fig:fig-shared-states-4">2.4</a> only displays one feature map, as both the self and cross-analysis depend on the same model. Regions displaying high and consistent feature weights across subjects were frontal pole (including parts of the dorsomedial prefrontal cortex and ventromedial prefrontal cortex), orbitofrontal cortex (OFC), inferior frontal gyrus (IFG), superior frontal gyrus (SFG), middle frontal gyrus (MFG), insular cortex, precentral gyrus, postcentral gyrus, posterior cingulate cortex/precuneus, superior parietal lobule (SPL), supramarginal gyrus (SMG), angular gyrus (AG), middle temporal gyrus (MTG), temporal pole (TP), lateral occipital cortex (lOC) and occipital pole (see Supplementary Table <a href="shared-states-supplement.html#tab:tab-shared-states-S4">A.4</a> for an overview of all involved regions).</p>
<div class="figure"><span id="fig:fig-shared-states-4"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_4.png" alt="Uncorrected t-value map of average feature weights across subjects; t-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown."  />
<p class="caption">
Figure 2.4: Uncorrected <em>t</em>-value map of average feature weights across subjects; <em>t</em>-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.
</p>
</div>

</div>
<div id="shared-states-results-univariate" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Univariate analyses</h3>
<p>Figure <a href="shared-states.html#fig:fig-shared-states-5">2.5</a> displays the pattern of neural activity revealed by univariate contrasts between the three different classes in the SF-task and the OF-task. For the sake of brevity, we summarize the most relevant univariate results here. Please see the <a href="shared-states-supplement.html#shared-states-supplement">Supplementary Materials</a> and the study’s Github repository for an overview of all clusters.</p>
<div class="figure"><span id="fig:fig-shared-states-5"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_5.png" alt="Univariate contrasts for the self-focused and other-focused task."  />
<p class="caption">
Figure 2.5: Univariate contrasts for the self-focused and other-focused task.
</p>
</div>
<p>In the SF-task, action was associated with increased involvement of the MFG, SFG, AG, SMG, lOC and middle temporal gyrus (temporo-occipital) when compared with interoception, and increased involvement of the IFG, MFG, SFG, anterior cingulate cortex (ACC), supplementary motor area (SMA), precentral gyrus, postcentral gyrus, insular cortex, SMG, SPL, lOC and middle temporal gyrus (temporo-occipital) when compared with situation. Interoception was associated with increased involvement of the insular cortex, precentral gyrus, postcentral gyrus and central operculum when compared with action, and increased involvement of the insular cortex, central operculum, parietal operculum, IFG, frontal pole, ACC, SMA, precentral gyrus, postcentral gyrus, SMG, SPL and putamen when compared with situation. The situation <em>vs</em> action contrast and the situation <em>vs</em> interoception contrast forwarded clusters in similar regions, including the temporal pole, superior/middle temporal gyrus, IFG, SFG, frontal pole, medial prefrontal cortex (mPFC), OFC, precuneus, posterior cingulate cortex (PCC), lOC, fusiform gyrus, hippocampus and lingual gyrus.</p>
<p>In the OF-task, action was associated with increased involvement of the IFG, MFG, SFG, precentral gyrus, postcentral gyrus, SMG, SPL, middle/inferior temporal gyrus (temporo-occipital), lOC and fusiform gyrus, when compared with interoception, and increased involvement of the IFG, MFG, SFG, frontal pole, precentral gyrus, postcentral gyrus, SMG, SPL, middle/inferior temporal gyrus (temporo-occipital) and lOC, when compared with situation. Interoception was associated with increased involvement of the left frontal pole when compared with action, and increased involvement of the SMG, SPL, precentral gyrus, postcentral gyrus, PCC, IFG and frontal pole, when compared with situation. The situation <em>vs</em> action contrast and the situation <em>vs</em> interoception contrast forwarded clusters in similar regions, including the temporal pole, superior/middle temporal gyrus, frontal pole, mPFC, PCC, precuneus, AG, lOC, occipital pole, fusiform gyrus and lingual gyrus.</p>
</div>
</div>
<div id="shared-states-discussion" class="section level2">
<h2><span class="header-section-number">2.4</span> Discussion</h2>
<p>In this study, we investigated the neural overlap between self-focused emotion imagery and other-focused emotion understanding using a decoding approach. The results confirmed our hypothesis that other-focused representations of emotion-related actions, bodily sensations and situations can be decoded from neural patterns associated with accessing similar sources of information in a self-focused task. This cross-classification was successful even though the tasks employed different stimulus materials and instructions. Thus, the observed neural overlap between the underlying processes in the SF-task and OF-task cannot be attributed to similarities in stimulus dimensions or task instructions. Rather, we conclude from our findings that emotion experience and emotion understanding have basic psychological processes in common.</p>
<p>Although we could successfully classify the interoception class in the SF-task (across both datasets), and in the OF-task in the validation dataset, we were not able to successfully classify the interoception class in the OF-task in the optimization dataset. Furthermore, although precision and recall metrics demonstrated similar results for the action and situation cross-classification in the validation dataset, these metrics demonstrated different results for the classification of the interoception class (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S5">A.5</a>). This difference was partly driven by the fact that trials were very infrequently classified as interoception in the cross-classification analysis. The finding that subjects reported lower success rates for the <em>what</em> trials in which they were asked to identify interoceptive sensations in other people than for the <em>how</em> (action) and <em>why</em> (situation) trials may point to a possible explanation for the inconsistent findings regarding interoception. Although speculative, it may be relatively easy to recognize (and represent) interoceptive sensations when they are described in words (as in the SF-task), but relatively hard to deduce these sensations when only diffuse cues about someone’s internal state are available (e.g. posture, frowning facial expression, as in the OF-task).</p>
<p>An exploration of the spatial characteristics of the distributed neural pattern associated with successful decoding of the SF-task and OF-task revealed regions that are commonly active during self- and other-focused processing. First, we found that successful classification was associated with voxels in the precentral gyrus, IFG, SMA and SPL. These same regions were also revealed by the univariate analyses, in particular for the action and interoception classes. These regions are part of the so-called “mirror” network, which is argued to support both action planning and action understanding <span class="citation">(Bastiaansen et al., <a href="bibliography.html#ref-bastiaansen2009evidence" role="doc-biblioref">2009</a>; Gallese et al., <a href="bibliography.html#ref-gallese2004unifying" role="doc-biblioref">2004</a>; Spunt &amp; Lieberman, <a href="bibliography.html#ref-spunt2012integrative" role="doc-biblioref">2012</a>; Van Overwalle &amp; Baetens, <a href="bibliography.html#ref-van2009understanding" role="doc-biblioref">2009</a>)</span>. Furthermore, we found that successful classification was associated with voxels in the lateral occipital cortex and fusiform gyrus, which have been linked in the literature to the processing of both concrete and abstract action <span class="citation">(Wurm &amp; Lingnau, <a href="bibliography.html#ref-wurm2015decoding" role="doc-biblioref">2015</a>)</span> and the (visual) processing of emotional scenes, faces and bodies <span class="citation">(Gelder et al., <a href="bibliography.html#ref-de2010standing" role="doc-biblioref">2010</a>; Sabatinelli et al., <a href="bibliography.html#ref-sabatinelli2011emotional" role="doc-biblioref">2011</a>)</span>. The univariate analyses demonstrated activity in the lOC and the fusiform gyrus in particular for the situation class, both when subjects viewed images of other people in emotional situations, and when subjects imagined being in an emotional situation themselves.</p>
<p>Second, we found that successful classification was associated with voxels in regions associated with somatosensory processing (postcentral gyrus) and the representation of interoceptive sensations <span class="citation">(insular cortex, see Craig &amp; Craig, <a href="bibliography.html#ref-craig2009you" role="doc-biblioref">2009</a>; Medford &amp; Critchley, <a href="bibliography.html#ref-medford2010conjoint" role="doc-biblioref">2010</a>)</span>. Univariate analyses of the SF-task also demonstrated involvement of these regions for both the action and interoception classes. This pattern of activation is consistent with embodied cognition views that propose that thinking about or imagining bodily states is grounded in simulations of somatosensory and interoceptive sensations <span class="citation">(Barsalou, <a href="bibliography.html#ref-barsalou2009simulation" role="doc-biblioref">2009</a>)</span>. In contrast to previous work on interoceptive simulation when observing pain or disgust in other people <span class="citation">(cf. Bastiaansen et al., <a href="bibliography.html#ref-bastiaansen2009evidence" role="doc-biblioref">2009</a>; Lamm et al., <a href="bibliography.html#ref-lamm2011meta" role="doc-biblioref">2011</a>)</span>, the univariate analyses of the OF-task did not demonstrate insular cortex activation for the interoception class.</p>
<p>And third, we found that successful classification was associated with voxels in the middle temporal gyrus (including the temporal pole), PCC/precuneus, dmPFC and vmPFC. These regions are part of the so-called “mentalizing” network (or “default” network). This same network was also revealed by the univariate analyses, in particular for the situation class. Meta-analyses have demonstrated that the mentalizing network is commonly active during tasks involving emotion experience and perception <span class="citation">(Lindquist et al., <a href="bibliography.html#ref-lindquist2012brain" role="doc-biblioref">2012</a>)</span>, mentalizing/theory of mind <span class="citation">(Spreng et al., <a href="bibliography.html#ref-spreng2009common" role="doc-biblioref">2009</a>; Van Overwalle &amp; Baetens, <a href="bibliography.html#ref-van2009understanding" role="doc-biblioref">2009</a>)</span>, judgments about the self and others <span class="citation">(Denny et al., <a href="bibliography.html#ref-denny2012meta" role="doc-biblioref">2012</a>)</span> and semantic/conceptual processing in general <span class="citation">(Binder et al., <a href="bibliography.html#ref-binder2009semantic" role="doc-biblioref">2009</a>)</span>. Moreover, this network contributes to the representation of emotion knowledge <span class="citation">(Peelen et al., <a href="bibliography.html#ref-peelen2010supramodal" role="doc-biblioref">2010</a>)</span> and is involved in both empathy <span class="citation">(Keysers &amp; Gazzola, <a href="bibliography.html#ref-keysers2014dissociating" role="doc-biblioref">2014</a>; Zaki &amp; Ochsner, <a href="bibliography.html#ref-zaki2012neuroscience" role="doc-biblioref">2012</a>)</span> and self-generated thought <span class="citation">(Andrews-Hanna et al., <a href="bibliography.html#ref-andrews2014default" role="doc-biblioref">2014</a>)</span>. We propose that this network supports the implementation of situated knowledge and personal experience that is necessary to generate rich mental models of emotional situations, both when experienced individually, and when understood in someone else <span class="citation">(cf. Barrett &amp; Satpute, <a href="bibliography.html#ref-barrett2013large" role="doc-biblioref">2013</a>; Oosterwijk &amp; Barrett, <a href="bibliography.html#ref-oosterwijk2014embodiment" role="doc-biblioref">2014</a>)</span>.</p>
<p>The most important contribution of our study is that it provides direct evidence for the idea of shared neural resources between self-and other focused processes. It is important, however, to specify what we think this “sharedness” entails. In research on pain, there is an ongoing discussion about whether experiencing pain and observing pain in others are distinct processes <span class="citation">(Krishnan et al., <a href="bibliography.html#ref-krishnan2016somatic" role="doc-biblioref">2016</a>)</span>, or whether experiencing and observing pain involve a shared domain-specific representation <span class="citation">(e.g., a discrete pain-specific brain state; Corradi-Dell’Acqua et al., <a href="bibliography.html#ref-corradi2016cross" role="doc-biblioref">2016</a>)</span> and/or the sharing of domain-general processes <span class="citation">(e.g. general negative affect; Zaki et al., <a href="bibliography.html#ref-zaki2016anatomy" role="doc-biblioref">2016</a>)</span>. Connecting to this discussion, we think that it is unlikely that our decoding success reflects the sharing of discrete experiential states between the SF-task and OF-task. After all, unlike in studies on pain, the stimuli in our tasks referred to a large variety of different actions, sensations and situations. Instead, decoding success in our study is most likely due to shared brain state configurations, reflecting the similar engagement of domain-general processes evoked by self- and other-focused instances of action (or interoceptive sensation or situation). This interpretation is consistent with views that suggests that global processes are shared between pain experience and pain observation <span class="citation">(Lamm et al., <a href="bibliography.html#ref-lamm2011meta" role="doc-biblioref">2011</a>; Zaki et al., <a href="bibliography.html#ref-zaki2016anatomy" role="doc-biblioref">2016</a>)</span> or between self- and other-focused tasks in general <span class="citation">(e.g., Legrand &amp; Ruby, <a href="bibliography.html#ref-legrand2009self" role="doc-biblioref">2009</a>)</span>. Moreover, this interpretation is consistent with the suggestion that neural re-use is a general principle of brain functioning <span class="citation">(e.g., Anderson, <a href="bibliography.html#ref-anderson2016precis" role="doc-biblioref">2016</a>)</span>.</p>
<p>In our constructionist view, we posit that emotion imagery and understanding share basic psychological processes <span class="citation">(cf. Oosterwijk &amp; Barrett, <a href="bibliography.html#ref-oosterwijk2014embodiment" role="doc-biblioref">2014</a>)</span>. More specifically, both emotion imagery and understanding are “conceptual acts” in which the brain generates predictions based on concept knowledge (including sensorimotor and interoceptive predictions) that are meaningful within a particular situational context <span class="citation">(Barrett, <a href="bibliography.html#ref-barrett2012emotions" role="doc-biblioref">2012</a>; Barrett &amp; Simmons, <a href="bibliography.html#ref-barrett2015interoceptive" role="doc-biblioref">2015</a>)</span>. Based on accumulating evidence, we propose that these predictions are implemented in domain-general brain networks <span class="citation">(cf. Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2012states" role="doc-biblioref">2012</a>; Barrett &amp; Satpute, <a href="bibliography.html#ref-barrett2013large" role="doc-biblioref">2013</a>)</span>. The relative contribution of these networks depends on the demands of the situational context. Specifically, in contexts where people are focused on actions and expressions (their own or someone else’s) a network that supports the representation of sensorimotor states (i.e., the mirror system) may contribute relatively heavily; in contexts where people are focused on bodily states (their own or someone else’s) a network that supports the representation of interoceptive states (i.e., the salience network) may contribute relatively heavily; and in contexts where people are focused on interpreting a situation (their own or someone else’s) a network that supports a general inferential meaning-making function (i.e., the mentalizing network) may contribute relatively heavily <span class="citation">(see also Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2015concepts" role="doc-biblioref">2015</a>)</span>. We believe that it is likely that our ability to successfully distinguish between classes in the self-task relies on the relatively <em>different</em> patterns of activity across these networks for actions, interoceptive sensations and situations. Regarding our ability to successfully generalize from the self- to the other-focused task, we believe that this relies on the relatively <em>similar</em> pattern of activity across these networks when people generate self-focused or other-focused instances of action (or interoceptive sensation or situation).</p>
<p>Our explicit manipulation of the weight of action, interoceptive and situational information in the SF-task and the OF-task tests the possibility of shared representation in a novel way. Although this procedure may seem artificial, social neuroscience studies support the notion that there is contextual variety in the contribution of action, interoceptive, and situation information when understanding other people <span class="citation">(Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2015concepts" role="doc-biblioref">2015</a>; Van Overwalle &amp; Baetens, <a href="bibliography.html#ref-van2009understanding" role="doc-biblioref">2009</a>)</span>. Moreover, this weighting may mimic the variability with which these sources of information contribute to different instances of subjective emotional experience in reality <span class="citation">(Barrett, <a href="bibliography.html#ref-barrett2012emotions" role="doc-biblioref">2012</a>)</span>. In future directions, it may be relevant to apply the current paradigm to the study of individuals in which access to these sources of information is disturbed (e.g., individuals with different types of psychopathology) or facilitated (e.g., individuals with high interoceptive sensitivity).</p>
<p>In short, the present study demonstrates that the neural patterns that support imagining “performing an action”, “feeling a bodily sensation” or “being in a situation” are directly involved in understanding other people’s actions, sensations and situations. This supports our prediction that self- and other-focused emotion processes share resources in the brain.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="general-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="confounds-decoding.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"download": "thesis.pdf"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
