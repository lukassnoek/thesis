% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Tjur2009-dp,
  title     = "Coefficients of Determination in Logistic Regression
               {Models---A} New Proposal: The Coefficient of Discrimination",
  author    = "Tjur, Tue",
  abstract  = "Many analogues to the coefficient of determination R2 in
               ordinary regression models have been proposed in the context of
               logistic regression. Our starting point is a study of three
               definitions related to quadratic measures of variation. We
               discuss the properties of these statistics, and show that the
               family can be extended in a natural way by a fourth statistic
               with an even simpler interpretation, namely the difference
               between the averages of fitted values for successes and
               failures, respectively. We propose the name ?the coefficient of
               discrimination? for this statistic, and recommend its use as a
               standard measure of explanatory power. In its intuitive
               interpretation, this quantity has no immediate relation to the
               classical versions of R2, but it turns out to be related to
               these by two exact relations, which imply that all these
               statistics are asymptotically equivalent.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  63,
  number    =  4,
  pages     = "366--372",
  month     =  nov,
  year      =  2009
}

@ARTICLE{Pedregosa2011-bp,
  title    = "Scikit-learn: Machine Learning in Python",
  author   = "Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre
              and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and
              Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and
              Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and
              Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and
              Duchesnay, {\'E}douard",
  abstract = "Abstract Scikit - learn is a Python module integrating a wide
              range of state-of-the-art machine learning algorithms for
              medium-scale supervised and unsupervised problems. This package
              focuses on bringing machine learning to non-specialists using a
              general-purpose high-level",
  journal  = "J. Mach. Learn. Res.",
  volume   =  12,
  number   = "Oct",
  pages    = "2825--2830",
  year     =  2011
}

@ARTICLE{Kriegeskorte2009-yz,
  title     = "Circular analysis in systems neuroscience: the dangers of double
               dipping",
  author    = "Kriegeskorte, Nikolaus and Simmons, W Kyle and Bellgowan,
               Patrick S F and Baker, Chris I",
  abstract  = "A neuroscientific experiment typically generates a large amount
               of data, of which only a small fraction is analyzed in detail
               and presented in a publication. However, selection among noisy
               measurements can render circular an otherwise appropriate
               analysis and invalidate results. Here we argue that systems
               neuroscience needs to adjust some widespread practices to avoid
               the circularity that can arise from selection. In particular,
               'double dipping', the use of the same dataset for selection and
               selective analysis, will give distorted descriptive statistics
               and invalid statistical inference whenever the results
               statistics are not inherently independent of the selection
               criteria under the null hypothesis. To demonstrate the problem,
               we apply widely used analyses to noise data known to not contain
               the experimental effects in question. Spurious effects can
               appear in the context of both univariate activation analysis and
               multivariate pattern-information analysis. We suggest a policy
               for avoiding circularity.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  12,
  number    =  5,
  pages     = "535--540",
  month     =  may,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Betancourt2017-rj,
  title         = "A Conceptual Introduction to Hamiltonian Monte Carlo",
  author        = "Betancourt, Michael",
  abstract      = "Hamiltonian Monte Carlo has proven a remarkable empirical
                   success, but only recently have we begun to develop a
                   rigorous understanding of why it performs so well on
                   difficult problems and how it is best applied in practice.
                   Unfortunately, that understanding is confined within the
                   mathematics of differential geometry which has limited its
                   dissemination, especially to the applied communities for
                   which it is particularly important. In this review I provide
                   a comprehensive conceptual account of these theoretical
                   foundations, focusing on developing a principled intuition
                   behind the method and its optimal implementations rather of
                   any exhaustive rigor. Whether a practitioner or a
                   statistician, the dedicated reader will acquire a solid
                   grasp of how Hamiltonian Monte Carlo works, when it
                   succeeds, and, perhaps most importantly, when it fails.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1701.02434"
}

@ARTICLE{Ringach2004-nn,
  title     = "Reverse correlation in neurophysiology",
  author    = "Ringach, Dario and Shapley, Robert",
  abstract  = "Abstract This article presents a review of reverse correlation
               in neurophysiology. We discuss the basis of reverse correlation
               in linear transducers and in spiking neurons. The application of
               reverse correlation to measure the receptive fields of visual
               neurons using white noise and m-sequences, and classical
               findings about spatial and color processing in the cortex
               resulting from such measurements, are emphasized. Finally, we
               describe new developments in reverse correlation, including
               ?sub-space? and categorical reverse-correlation. Recent results
               obtained by applying such methods in the orientation,
               spatial-frequency and Fourier domains have revealed the
               importance of cortical inhibition in the establishment of sharp
               tuning selectivity in single neurons.",
  journal   = "Cogn. Sci.",
  publisher = "Wiley",
  volume    =  28,
  number    =  2,
  pages     = "147--166",
  month     =  mar,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Jack2015-sh,
  title     = "The Human Face as a Dynamic Tool for Social Communication",
  author    = "Jack, Rachael E and Schyns, Philippe G",
  abstract  = "As a highly social species, humans frequently exchange social
               information to support almost all facets of life. One of the
               richest and most powerful tools in social communication is the
               face, from which observers can quickly and easily make a number
               of inferences - about identity, gender, sex, age, race,
               ethnicity, sexual orientation, physical health, attractiveness,
               emotional state, personality traits, pain or physical pleasure,
               deception, and even social status. With the advent of the
               digital economy, increasing globalization and cultural
               integration, understanding precisely which face information
               supports social communication and which produces
               misunderstanding is central to the evolving needs of modern
               society (for example, in the design of socially interactive
               digital avatars and companion robots). Doing so is challenging,
               however, because the face can be thought of as comprising a
               high-dimensional, dynamic information space, and this impacts
               cognitive science and neuroimaging, and their broader
               applications in the digital economy. New opportunities to
               address this challenge are arising from the development of new
               methods and technologies, coupled with the emergence of a modern
               scientific culture that embraces cross-disciplinary approaches.
               Here, we briefly review one such approach that combines
               state-of-the-art computer graphics, psychophysics and vision
               science, cultural psychology and social cognition, and highlight
               the main knowledge advances it has generated. In the light of
               current developments, we provide a vision of the future
               directions in the field of human facial communication within and
               across cultures.",
  journal   = "Curr. Biol.",
  publisher = "Elsevier",
  volume    =  25,
  number    =  14,
  pages     = "R621--34",
  month     =  jul,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Said2009-tf,
  title     = "Structural resemblance to emotional expressions predicts
               evaluation of emotionally neutral faces",
  author    = "Said, Christopher P and Sebe, Nicu and Todorov, Alexander",
  abstract  = "People make trait inferences based on facial appearance despite
               little evidence that these inferences accurately reflect
               personality. The authors tested the hypothesis that these
               inferences are driven in part by structural resemblance to
               emotional expressions. The authors first had participants judge
               emotionally neutral faces on a set of trait dimensions. The
               authors then submitted the face images to a Bayesian network
               classifier trained to detect emotional expressions. By using a
               classifier, the authors can show that neutral faces perceived to
               possess various personality traits contain objective resemblance
               to emotional expression. In general, neutral faces that are
               perceived to have positive valence resemble happiness, faces
               that are perceived to have negative valence resemble disgust and
               fear, and faces that are perceived to be threatening resemble
               anger. These results support the idea that trait inferences are
               in part the result of an overgeneralization of emotion
               recognition systems. Under this hypothesis, emotion recognition
               systems, which typically extract accurate information about a
               person's emotional state, are engaged during the perception of
               neutral faces that bear subtle resemblance to emotional
               expressions. These emotions could then be misattributed as
               traits.",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  9,
  number    =  2,
  pages     = "260--264",
  month     =  apr,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Franklin2019-qo,
  title     = "Reading the lines in the face: The contribution of angularity
               and roundness to perceptions of facial anger and joy",
  author    = "Franklin, Robert G and Adams, Reginald B and Steiner, Troy G and
               Zebrowitz, Leslie A",
  abstract  = "Through 3 studies, we investigated whether angularity and
               roundness present in faces contributes to the perception of
               anger and joyful expressions, respectively. First, in Study 1 we
               found that angry expressions naturally contain more
               inward-pointing lines, whereas joyful expressions contain more
               outward-pointing lines. Then, using image-processing techniques
               in Studies 2 and 3, we filtered images to contain only
               inward-pointing or outward-pointing lines as a way to
               approximate angularity and roundness. We found that filtering
               images to be more angular increased how threatening and angry a
               neutral face was rated, increased how intense angry expressions
               were rated, and enhanced the recognition of anger. Conversely,
               filtering images to be rounder increased how warm and joyful a
               neutral face was rated, increased the intensity of joyful
               expressions, and enhanced recognition of joy. Together these
               findings show that angularity and roundness play a direct role
               in the recognition of angry and joyful expressions. Given
               evidence that angularity and roundness may play a biological
               role in indicating threat and safety in the environment, this
               suggests that angularity and roundness represent primitive
               facial cues used to signal threat-anger and warmth-joy pairings.
               (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  19,
  number    =  2,
  pages     = "209--218",
  month     =  mar,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Oosterhof2009-mf,
  title     = "Shared perceptual basis of emotional expressions and
               trustworthiness impressions from faces",
  author    = "Oosterhof, Nikolaas N and Todorov, Alexander",
  abstract  = "Using a dynamic stimuli paradigm, in which faces expressed
               either happiness or anger, the authors tested the hypothesis
               that perceptions of trustworthiness are related to these
               expressions. Although the same emotional intensity was added to
               both trustworthy and untrustworthy faces, trustworthy faces who
               expressed happiness were perceived as happier than untrustworthy
               faces, and untrustworthy faces who expressed anger were
               perceived as angrier than trustworthy faces. The authors also
               manipulated changes in face trustworthiness simultaneously with
               the change in expression. Whereas transitions in face
               trustworthiness in the direction of the expressed emotion (e.g.,
               high-to-low trustworthiness and anger) increased the perceived
               intensity of the emotion, transitions in the opposite direction
               decreased this intensity. For example, changes from high to low
               trustworthiness increased the intensity of perceived anger but
               decreased the intensity of perceived happiness. These findings
               support the hypothesis that changes along the trustworthiness
               dimension correspond to subtle changes resembling expressions
               signaling whether the person displaying the emotion should be
               avoided or approached.",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  9,
  number    =  1,
  pages     = "128--133",
  month     =  feb,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Schoppe2016-bu,
  title     = "Measuring the Performance of Neural Models",
  author    = "Schoppe, Oliver and Harper, Nicol S and Willmore, Ben D B and
               King, Andrew J and Schnupp, Jan W H",
  abstract  = "Good metrics of the performance of a statistical or
               computational model are essential for model comparison and
               selection. Here, we address the design of performance metrics
               for models that aim to predict neural responses to sensory
               inputs. This is particularly difficult because the responses of
               sensory neurons are inherently variable, even in response to
               repeated presentations of identical stimuli. In this situation,
               standard metrics (such as the correlation coefficient) fail
               because they do not distinguish between explainable variance
               (the part of the neural response that is systematically
               dependent on the stimulus) and response variability (the part of
               the neural response that is not systematically dependent on the
               stimulus, and cannot be explained by modeling the
               stimulus-response relationship). As a result, models which
               perfectly describe the systematic stimulus-response relationship
               may appear to perform poorly. Two metrics have previously been
               proposed which account for this inherent variability: Signal
               Power Explained (SPE, Sahani and Linden, 2003), and the
               normalized correlation coefficient (CC norm , Hsu et al., 2004).
               Here, we analyze these metrics, and show that they are
               intimately related. However, SPE has no lower bound, and we show
               that, even for good models, SPE can yield negative values that
               are difficult to interpret. CC norm is better behaved in that it
               is effectively bounded between -1 and 1, and values below zero
               are very rare in practice and easy to interpret. However, it was
               hitherto not possible to calculate CC norm directly; instead, it
               was estimated using imprecise and laborious resampling
               techniques. Here, we identify a new approach that can calculate
               CC norm quickly and accurately. As a result, we argue that it is
               now a better choice of metric than SPE to accurately evaluate
               the performance of neural models.",
  journal   = "Front. Comput. Neurosci.",
  publisher = "frontiersin.org",
  volume    =  10,
  pages     = "10",
  month     =  feb,
  year      =  2016,
  keywords  = "model selection; neural coding; receptive field; sensory neuron;
               signal power; statistical modeling",
  language  = "en"
}

@INCOLLECTION{Del_Sole2018-ms,
  title     = "Introducing Microsoft Cognitive Services",
  booktitle = "Microsoft Computer Vision {APIs} Distilled : Getting Started
               with Cognitive Services",
  author    = "Del Sole, Alessandro",
  editor    = "Del Sole, Alessandro",
  abstract  = "Without a doubt, artificial intelligence (AI) is an important
               part of information technology today. It certainly will be more
               and more important in the future, but it's already being used in
               many ways, so you, as a developer, should learn about what tools
               and services are available to build next-generation
               applications.",
  publisher = "Apress",
  pages     = "1--4",
  year      =  2018,
  address   = "Berkeley, CA"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sato2007-ah,
  title     = "Enhanced experience of emotional arousal in response to dynamic
               facial expressions",
  author    = "Sato, Wataru and Yoshikawa, Sakiko",
  journal   = "J. Nonverbal Behav.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  31,
  number    =  2,
  pages     = "119--135",
  month     =  mar,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Lefevre2014-vo,
  title     = "Facial width-to-height ratio predicts self-reported dominance
               and aggression in males and females, but a measure of
               masculinity does not",
  author    = "Lefevre, Carmen E and Etchells, Peter J and Howell, Emma C and
               Clark, Andrew P and Penton-Voak, Ian S",
  abstract  = "Recently, associations between facial structure and aggressive
               behaviour have been reported. Specifically, the facial
               width-to-height ratio (fWHR) is thought to link to aggression,
               although it is unclear whether this association is related to a
               specific dimension of aggression, or to a more generalized
               concept of dominance behaviour. Similarly, an association has
               been proposed between facial masculinity and dominant and
               aggressive behaviour, but, to date, this has not been formally
               tested. Because masculinity and fWHR are negatively correlated,
               it is unlikely that both signal similar behaviours. Here, we
               thus tested these associations and show that: (i) fWHR is
               related to both self-reported dominance and aggression; (ii)
               physical aggression, verbal aggression and anger, but not
               hostility are associated with fWHR; (iii) there is no evidence
               for a sex difference in associations between fWHR and
               aggression; and (iv) the facial masculinity index does not
               predict dominance or aggression. Taken together, these results
               indicate that fWHR, but not a measure of facial masculinity,
               cues dominance and specific types of aggression in both sexes.",
  journal   = "Biol. Lett.",
  publisher = "royalsocietypublishing.org",
  volume    =  10,
  number    =  10,
  pages     = "20140729",
  month     =  oct,
  year      =  2014,
  keywords  = "aggression; dominance; facial morphology; facial width-to-height
               ratio; masculinity",
  language  = "en"
}

@ARTICLE{Barman2019-af,
  title     = "Facial expression recognition using distance and texture
               signature relevant features",
  author    = "Barman, Asit and Dutta, Paramartha",
  abstract  = "Distance and texture characteristics among the landmark points
               reflected in human faces are important features in so far as the
               recognition of human faces is concerned. In this article we
               consider (i) normalized distance signature obtained from Active
               Appearance Model (AAM) based grid, (ii) normalized texture
               signature derived from salient landmarks within the grid, (iii)
               stability indices arising out of these signatures and (iv)
               relevant statistical measures as the set of features for
               training of artificial neural models such as Multilayer
               Perceptron (MLP), Radial Basis Function Network (RBF), Nonlinear
               AutoRegressive with eXogenous input (NARX) and Convolutional
               Neural Network (CNN) to achieve the task of recognition of
               facial expressions. The Cohn-Kanade (CK+), Japanese Female
               Facial Expression (JAFFE), MMI and MUG benchmark databases are
               used to conduct the experiments and the results obtained justify
               the effectiveness of the proposed procedure. The combined
               distance-texture (D-T) signature is found to perform
               convincingly better than the distance signature and texture
               signature individually. The effectiveness of the proposed
               technique based on combined D-T signature is substantiated by
               its extremely encouraging performance compared to other existing
               arts.",
  journal   = "Appl. Soft Comput.",
  publisher = "Elsevier",
  volume    =  77,
  pages     = "88--105",
  month     =  apr,
  year      =  2019,
  keywords  = "Distance signature; Texture signature; Grid; Feature selection;
               Facial expression recognition; MLP; RBF; NARX; CNN"
}

@ARTICLE{Craig2017-db,
  title     = "The influence of social category cues on the happy
               categorisation advantage depends on expression valence",
  author    = "Craig, Belinda M and Koch, Severine and Lipp, Ottmar V",
  abstract  = "Facial race and sex cues can influence the magnitude of the
               happy categorisation advantage. It has been proposed that
               implicit race or sex based evaluations drive this influence.
               Within this account a uniform influence of social category cues
               on the happy categorisation advantage should be observed for all
               negative expressions. Support has been shown with angry and sad
               expressions but evidence to the contrary has been found for
               fearful expressions. To determine the generality of the
               evaluative congruence account, participants categorised
               happiness with either sadness, fear, or surprise displayed on
               White male as well as White female, Black male, or Black female
               faces across three experiments. Faster categorisation of happy
               than negative expressions was observed for female faces when
               presented among White male faces, and for White male faces when
               presented among Black male faces. These results support the
               evaluative congruence account when both positive and negative
               expressions are presented.",
  journal   = "Cogn. Emot.",
  publisher = "Taylor \& Francis",
  volume    =  31,
  number    =  7,
  pages     = "1493--1501",
  month     =  nov,
  year      =  2017,
  keywords  = "Emotion recognition; person construal; race; sex; social
               categorisation",
  language  = "en"
}

@ARTICLE{Allefeld2016-xp,
  title     = "Valid population inference for information-based imaging: From
               the second-level t-test to prevalence inference",
  author    = "Allefeld, Carsten and G{\"o}rgen, Kai and Haynes, John-Dylan",
  abstract  = "In multivariate pattern analysis of neuroimaging data,
               'second-level' inference is often performed by entering
               classification accuracies into a t-test vs chance level across
               subjects. We argue that while the random-effects analysis
               implemented by the t-test does provide population inference if
               applied to activation differences, it fails to do so in the case
               of classification accuracy or other 'information-like' measures,
               because the true value of such measures can never be below
               chance level. This constraint changes the meaning of the
               population-level null hypothesis being tested, which becomes
               equivalent to the global null hypothesis that there is no effect
               in any subject in the population. Consequently, rejecting it
               only allows to infer that there are some subjects in which there
               is an information effect, but not that it generalizes, rendering
               it effectively equivalent to fixed-effects analysis. This
               statement is supported by theoretical arguments as well as
               simulations. We review possible alternative approaches to
               population inference for information-based imaging, converging
               on the idea that it should not target the mean, but the
               prevalence of the effect in the population. One method to do so,
               'permutation-based information prevalence inference using the
               minimum statistic', is described in detail and applied to
               empirical data.",
  journal   = "Neuroimage",
  publisher = "Elsevier",
  volume    =  141,
  pages     = "378--392",
  month     =  nov,
  year      =  2016,
  keywords  = "Effect prevalence; Information-based imaging; Multivariate
               pattern analysis; Population inference; t-Test",
  language  = "en"
}

@article{waskom2021seaborn,
  title={Seaborn: statistical data visualization},
  author={Waskom, Michael L},
  journal={Journal of Open Source Software},
  volume={6},
  number={60},
  pages={3021},
  year={2021}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{McKinney2011-kl,
  title     = "pandas: a foundational Python library for data analysis and
               statistics",
  author    = "McKinney, Wes and {Others}",
  journal   = "Python for High Performance and Scientific Computing",
  publisher = "Seattle",
  volume    =  14,
  number    =  9,
  year      =  2011
}

@ARTICLE{Barrett2019-bc,
  title     = "Emotional Expressions Reconsidered: Challenges to Inferring
               Emotion From Human Facial Movements",
  author    = "Barrett, Lisa Feldman and Adolphs, Ralph and Marsella, Stacy and
               Martinez, Aleix M and Pollak, Seth D",
  abstract  = "It is commonly assumed that a person's emotional state can be
               readily inferred from his or her facial movements, typically
               called emotional expressions or facial expressions. This
               assumption influences legal judgments, policy decisions,
               national security protocols, and educational practices; guides
               the diagnosis and treatment of psychiatric illness, as well as
               the development of commercial applications; and pervades
               everyday social interactions as well as research in other
               scientific fields such as artificial intelligence, neuroscience,
               and computer vision. In this article, we survey examples of this
               widespread assumption, which we refer to as the common view, and
               we then examine the scientific evidence that tests this view,
               focusing on the six most popular emotion categories used by
               consumers of emotion research: anger, disgust, fear, happiness,
               sadness, and surprise. The available scientific evidence
               suggests that people do sometimes smile when happy, frown when
               sad, scowl when angry, and so on, as proposed by the common
               view, more than what would be expected by chance. Yet how people
               communicate anger, disgust, fear, happiness, sadness, and
               surprise varies substantially across cultures, situations, and
               even across people within a single situation. Furthermore,
               similar configurations of facial movements variably express
               instances of more than one emotion category. In fact, a given
               configuration of facial movements, such as a scowl, often
               communicates something other than an emotional state. Scientists
               agree that facial movements convey a range of information and
               are important for social communication, emotional or otherwise.
               But our review suggests an urgent need for research that
               examines how people actually move their faces to express
               emotions and other social information in the variety of contexts
               that make up everyday life, as well as careful study of the
               mechanisms by which people perceive instances of emotion in one
               another. We make specific research recommendations that will
               yield a more valid picture of how people move their faces to
               express emotions and how they infer emotional meaning from
               facial movements in situations of everyday life. This research
               is crucial to provide consumers of emotion research with the
               translational information they require.",
  journal   = "Psychol. Sci. Public Interest",
  publisher = "journals.sagepub.com",
  volume    =  20,
  number    =  1,
  pages     = "1--68",
  month     =  jul,
  year      =  2019,
  keywords  = "emotion perception; emotion recognition; emotional expression",
  language  = "en"
}

@ARTICLE{Brooks2018-ao,
  title     = "Stereotypes Bias Visual Prototypes for Sex and Emotion
               Categories",
  author    = "Brooks, Jeffrey A and Stolier, Ryan M and Freeman, Jonathan B",
  abstract  = "Recent models suggest that social categories are not perceived
               independently, but that they can facilitate or bias each other's
               perception due to incidentally shared stereotypes. To address
               the role of visual prediction in driving these intersectional
               effects in the domains of sex and emotion perception, three
               studies were conducted. Participants categorized visually
               obscured faces by sex and emotion, from which we produced
               reverse-correlated prototype faces for each social category.
               These prototype faces were found to exhibit systematic biases in
               their visual appearance (Male-Angry, Female-Happy), as judged by
               independent raters. Moreover, this biased appearance in sex and
               emotion prototypes was related to the extent of a participant's
               stereotypical associations linking men to anger and women to
               happiness. Together, the findings suggest that stereotypes can
               bind social categories together at the level of visual
               prediction and offer new insights into current theoretical
               models of social perception.",
  journal   = "Soc. Cogn.",
  publisher = "Guilford Publications Inc.",
  volume    =  36,
  number    =  5,
  pages     = "481--493",
  month     =  oct,
  year      =  2018
}

@INPROCEEDINGS{Bryant2019-sg,
  title     = "A Comparative Analysis of {Emotion-Detecting} {AI} Systems with
               Respect to Algorithm Performance and Dataset Diversity",
  booktitle = "Proceedings of the 2019 {AAAI/ACM} Conference on {AI}, Ethics,
               and Society",
  author    = "Bryant, De'aira and Howard, Ayanna",
  abstract  = "In recent news, organizations have been considering the use of
               facial and emotion recognition for applications involving youth
               such as tackling surveillance and security in schools. However,
               the majority of efforts on facial emotion recognition research
               have focused on adults. Children, particularly in their early
               years, have been shown to express emotions quite differently
               than adults. Thus, before such algorithms are deployed in
               environments that impact the wellbeing and circumstance of
               youth, a careful examination should be made on their accuracy
               with respect to appropriateness for this target demographic. In
               this work, we utilize several datasets that contain facial
               expressions of children linked to their emotional state to
               evaluate eight different commercial emotion classification
               systems. We compare the ground truth labels provided by the
               respective datasets to the labels given with the highest
               confidence by the classification systems and assess the results
               in terms of matching score (TPR), positive predictive value, and
               failure to compute rate. Overall results show that the emotion
               recognition systems displayed subpar performance on the datasets
               of children's expressions compared to prior work with adult
               datasets and initial human ratings. We then identify limitations
               associated with automated recognition of emotions in children
               and provide suggestions on directions with enhancing recognition
               accuracy through data diversification, dataset accountability,
               and algorithmic regulation.",
  publisher = "Association for Computing Machinery",
  pages     = "377--382",
  series    = "AIES '19",
  month     =  jan,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "affective computing, emotion detection, dataset accountability,
               children, algorithm transparency, emotion recognition, emotion
               classification",
  location  = "Honolulu, HI, USA"
}

@ARTICLE{Benitez-Quiroz2018-vr,
  title     = "Facial color is an efficient mechanism to visually transmit
               emotion",
  author    = "Benitez-Quiroz, Carlos F and Srinivasan, Ramprakash and
               Martinez, Aleix M",
  abstract  = "Facial expressions of emotion in humans are believed to be
               produced by contracting one's facial muscles, generally called
               action units. However, the surface of the face is also
               innervated with a large network of blood vessels. Blood flow
               variations in these vessels yield visible color changes on the
               face. Here, we study the hypothesis that these visible facial
               colors allow observers to successfully transmit and visually
               interpret emotion even in the absence of facial muscle
               activation. To study this hypothesis, we address the following
               two questions. Are observable facial colors consistent within
               and differential between emotion categories and positive vs.
               negative valence? And does the human visual system use these
               facial colors to decode emotion from faces? These questions
               suggest the existence of an important, unexplored mechanism of
               the production of facial expressions of emotion by a sender and
               their visual interpretation by an observer. The results of our
               studies provide evidence in favor of our hypothesis. We show
               that people successfully decode emotion using these color
               features, even in the absence of any facial muscle activation.
               We also demonstrate that this color signal is independent from
               that provided by facial muscle movements. These results support
               a revised model of the production and perception of facial
               expressions of emotion where facial color is an effective
               mechanism to visually transmit and decode emotion.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  115,
  number    =  14,
  pages     = "3581--3586",
  month     =  apr,
  year      =  2018,
  keywords  = "affect; categorization; computer vision; face perception",
  language  = "en"
}

@ARTICLE{Jack2017-gt,
  title     = "Toward a Social Psychophysics of Face Communication",
  author    = "Jack, Rachael E and Schyns, Philippe G",
  abstract  = "As a highly social species, humans are equipped with a powerful
               tool for social communication-the face. Although seemingly
               simple, the human face can elicit multiple social perceptions
               due to the rich variations of its movements, morphology, and
               complexion. Consequently, identifying precisely what face
               information elicits different social perceptions is a complex
               empirical challenge that has largely remained beyond the reach
               of traditional methods. In the past decade, the emerging field
               of social psychophysics has developed new methods to address
               this challenge, with the potential to transfer psychophysical
               laws of social perception to the digital economy via avatars and
               social robots. At this exciting juncture, it is timely to review
               these new methodological developments. In this article, we
               introduce and review the foundational methodological
               developments of social psychophysics, present work done in the
               past decade that has advanced understanding of the face as a
               tool for social communication, and discuss the major challenges
               that lie ahead.",
  journal   = "Annu. Rev. Psychol.",
  publisher = "annualreviews.org",
  volume    =  68,
  pages     = "269--297",
  month     =  jan,
  year      =  2017,
  keywords  = "culture; facial expressions; reverse correlation; social
               communication; social psychophysics",
  language  = "en"
}

@ARTICLE{Todorov2008-kn,
  title     = "Evaluating faces on trustworthiness: an extension of systems for
               recognition of emotions signaling approach/avoidance behaviors",
  author    = "Todorov, Alexander",
  abstract  = "People routinely make various trait judgments from facial
               appearance, and such judgments affect important social outcomes.
               These judgments are highly correlated with each other,
               reflecting the fact that valence evaluation permeates trait
               judgments from faces. Trustworthiness judgments best approximate
               this evaluation, consistent with evidence about the involvement
               of the amygdala in the implicit evaluation of face
               trustworthiness. Based on computer modeling and behavioral
               experiments, I argue that face evaluation is an extension of
               functionally adaptive systems for understanding the
               communicative meaning of emotional expressions. Specifically, in
               the absence of diagnostic emotional cues, trustworthiness
               judgments are an attempt to infer behavioral intentions
               signaling approach/avoidance behaviors. Correspondingly, these
               judgments are derived from facial features that resemble
               emotional expressions signaling such behaviors: happiness and
               anger for the positive and negative ends of the trustworthiness
               continuum, respectively. The emotion overgeneralization
               hypothesis can explain highly efficient but not necessarily
               accurate trait judgments from faces, a pattern that appears
               puzzling from an evolutionary point of view and also generates
               novel predictions about brain responses to faces. Specifically,
               this hypothesis predicts a nonlinear response in the amygdala to
               face trustworthiness, confirmed in functional magnetic resonance
               imaging (fMRI) studies, and dissociations between processing of
               facial identity and face evaluation, confirmed in studies with
               developmental prosopagnosics. I conclude with some
               methodological implications for the study of face evaluation,
               focusing on the advantages of formally modeling representation
               of faces on social dimensions.",
  journal   = "Ann. N. Y. Acad. Sci.",
  publisher = "Wiley Online Library",
  volume    =  1124,
  pages     = "208--224",
  month     =  mar,
  year      =  2008,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Punitha2013-jy,
  title     = "Texture based emotion recognition from facial expressions using
               support vector machine",
  author    = "Punitha, A and Geetha, M Kalaiselvi",
  journal   = "Int. J. Comput. Appl.",
  publisher = "Citeseer",
  volume    =  80,
  number    =  5,
  pages     = "1--5",
  year      =  2013
}

@ARTICLE{Craig2018-jm,
  title     = "The influence of multiple social categories on emotion
               perception",
  author    = "Craig, Belinda M and Lipp, Ottmar V",
  abstract  = "Although the human face provides multiple sources of social
               information concurrently (race, sex, age, etc.), the majority of
               studies investigating how social category cues influence
               emotional expression perception have investigated the influence
               of only one social category at a time. Only a couple of studies
               have investigated how race and sex cues concurrently influence
               emotion perception and these studies have produced mixed
               results. In addition, the concurrent influence of age and sex
               cues on emotion perception has not been investigated. To address
               this, participants categorized happy and angry expressions on
               faces varying in race (Black and White) and sex (Experiments 1a
               and 1b) or age (older adult and young adult) and sex (Experiment
               2). In Experiments 1a and 1b, results indicated that sex but not
               race influenced emotion categorization. Participants were, on
               average, faster to categorize happiness than anger on female,
               but not on male faces. In Experiment 2, both the age and the sex
               of the face independently influenced emotion categorization.
               Participants were faster to categorize happiness than anger on
               female and young adult faces, but not on male or older adult
               faces. Bayesian ANOVAs provided additional evidence that the sex
               of the face had the strongest influence on emotion
               categorization speeds in Experiment 1a and 1b, but both age and
               sex cues had an equal influence on emotion categorization in
               Experiment 2.",
  journal   = "J. Exp. Soc. Psychol.",
  publisher = "Elsevier",
  volume    =  75,
  pages     = "27--35",
  month     =  mar,
  year      =  2018,
  keywords  = "Emotion recognition; Social categorization; Race; Sex; Age"
}

@ARTICLE{Wegrzyn2017-ke,
  title     = "Mapping the emotional face. How individual face parts contribute
               to successful emotion recognition",
  author    = "Wegrzyn, Martin and Vogt, Maria and Kireclioglu, Berna and
               Schneider, Julia and Kissler, Johanna",
  abstract  = "Which facial features allow human observers to successfully
               recognize expressions of emotion? While the eyes and mouth have
               been frequently shown to be of high importance, research on
               facial action units has made more precise predictions about the
               areas involved in displaying each emotion. The present research
               investigated on a fine-grained level, which physical features
               are most relied on when decoding facial expressions. In the
               experiment, individual faces expressing the basic emotions
               according to Ekman were hidden behind a mask of 48 tiles, which
               was sequentially uncovered. Participants were instructed to stop
               the sequence as soon as they recognized the facial expression
               and assign it the correct label. For each part of the face, its
               contribution to successful recognition was computed, allowing to
               visualize the importance of different face areas for each
               expression. Overall, observers were mostly relying on the eye
               and mouth regions when successfully recognizing an emotion.
               Furthermore, the difference in the importance of eyes and mouth
               allowed to group the expressions in a continuous space, ranging
               from sadness and fear (reliance on the eyes) to disgust and
               happiness (mouth). The face parts with highest diagnostic value
               for expression identification were typically located in areas
               corresponding to action units from the facial action coding
               system. A similarity analysis of the usefulness of different
               face parts for expression recognition demonstrated that faces
               cluster according to the emotion they express, rather than by
               low-level physical features. Also, expressions relying more on
               the eyes or mouth region were in close proximity in the
               constructed similarity space. These analyses help to better
               understand how human observers process expressions of emotion,
               by delineating the mapping from facial features to psychological
               representation.",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  12,
  number    =  5,
  pages     = "e0177239",
  month     =  may,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Cowen2017-vz,
  title     = "Self-report captures 27 distinct categories of emotion bridged
               by continuous gradients",
  author    = "Cowen, Alan S and Keltner, Dacher",
  abstract  = "Emotions are centered in subjective experiences that people
               represent, in part, with hundreds, if not thousands, of semantic
               terms. Claims about the distribution of reported emotional
               states and the boundaries between emotion categories-that is,
               the geometric organization of the semantic space of emotion-have
               sparked intense debate. Here we introduce a conceptual framework
               to analyze reported emotional states elicited by 2,185 short
               videos, examining the richest array of reported emotional
               experiences studied to date and the extent to which reported
               experiences of emotion are structured by discrete and
               dimensional geometries. Across self-report methods, we find that
               the videos reliably elicit 27 distinct varieties of reported
               emotional experience. Further analyses revealed that categorical
               labels such as amusement better capture reports of subjective
               experience than commonly measured affective dimensions (e.g.,
               valence and arousal). Although reported emotional experiences
               are represented within a semantic space best captured by
               categorical labels, the boundaries between categories of emotion
               are fuzzy rather than discrete. By analyzing the distribution of
               reported emotional states we uncover gradients of emotion-from
               anxiety to fear to horror to disgust, calmness to aesthetic
               appreciation to awe, and others-that correspond to smooth
               variation in affective dimensions such as valence and dominance.
               Reported emotional states occupy a complex, high-dimensional
               categorical space. In addition, our library of videos and an
               interactive map of the emotional states they elicit
               (https://s3-us-west-1.amazonaws.com/emogifs/map.html) are made
               available to advance the science of emotion.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  114,
  number    =  38,
  pages     = "E7900--E7909",
  month     =  sep,
  year      =  2017,
  keywords  = "dimensions; discrete emotion; emotional experience; semantic
               space",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Jaeger2020-bn,
  title     = "The accuracy and meta-accuracy of personality impressions from
               faces",
  author    = "Jaeger, Bastian and Sleegers, Willem and Stern, Julia and Penke,
               Lars and Jones, Alex",
  publisher = "PsyArXiv",
  year      =  2020
}

@ARTICLE{Deska2018-hx,
  title     = "The face of fear and anger: Facial width-to-height ratio biases
               recognition of angry and fearful expressions",
  author    = "Deska, Jason C and Lloyd, E Paige and Hugenberg, Kurt",
  abstract  = "The ability to rapidly and accurately decode facial expressions
               is adaptive for human sociality. Although judgments of emotion
               are primarily determined by musculature, static face structure
               can also impact emotion judgments. The current work investigates
               how facial width-to-height ratio (fWHR), a stable feature of all
               faces, influences perceivers' judgments of expressive displays
               of anger and fear (Studies 1a, 1b, \& 2), and anger and
               happiness (Study 3). Across 4 studies, we provide evidence
               consistent with the hypothesis that perceivers more readily see
               anger on faces with high fWHR compared with those with low fWHR,
               which instead facilitates the recognition of fear and happiness.
               This bias emerges when participants are led to believe that
               targets displaying otherwise neutral faces are attempting to
               mask an emotion (Studies 1a \& 1b), and is evident when faces
               display an emotion (Studies 2 \& 3). Together, these studies
               suggest that target facial width-to-height ratio biases
               ascriptions of emotion with consequences for emotion recognition
               speed and accuracy. (PsycINFO Database Record",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  18,
  number    =  3,
  pages     = "453--464",
  month     =  apr,
  year      =  2018,
  language  = "en"
}



@ARTICLE{Calvo2016-la,
  title     = "Perceptual and affective mechanisms in facial expression
               recognition: An integrative review",
  author    = "Calvo, Manuel G and Nummenmaa, Lauri",
  abstract  = "Facial expressions of emotion involve a physical component of
               morphological changes in a face and an affective component
               conveying information about the expresser's internal feelings.
               It remains unresolved how much recognition and discrimination of
               expressions rely on the perception of morphological patterns or
               the processing of affective content. This review of research on
               the role of visual and emotional factors in expression
               recognition reached three major conclusions. First, behavioral,
               neurophysiological, and computational measures indicate that
               basic expressions are reliably recognized and discriminated from
               one another, albeit the effect may be inflated by the use of
               prototypical expression stimuli and forced-choice responses.
               Second, affective content along the dimensions of valence and
               arousal is extracted early from facial expressions, although
               this coarse affective representation contributes minimally to
               categorical recognition of specific expressions. Third, the
               physical configuration and visual saliency of facial features
               contribute significantly to expression recognition, with
               ``emotionless'' computational models being able to reproduce
               some of the basic phenomena demonstrated in human observers. We
               conclude that facial expression recognition, as it has been
               investigated in conventional laboratory tasks, depends to a
               greater extent on perceptual than affective information and
               mechanisms.",
  journal   = "Cogn. Emot.",
  publisher = "Taylor \& Francis",
  volume    =  30,
  number    =  6,
  pages     = "1081--1106",
  month     =  sep,
  year      =  2016,
  keywords  = "Affective priming; Emotion; Facial expression; Perception;
               Recognition",
  language  = "en"
}

@ARTICLE{Hess2009-jz,
  title     = "Face gender and emotion expression: are angry women more like
               men?",
  author    = "Hess, Ursula and Adams, Jr, Reginald B and Grammer, Karl and
               Kleck, Robert E",
  abstract  = "Certain features of facial appearance perceptually resemble
               expressive cues related to facial displays of emotion. We
               hypothesized that because expressive markers of anger (such as
               lowered eyebrows) overlap with perceptual markers of male sex,
               perceivers would identify androgynous angry faces as more likely
               to be a man than a woman (Study 1) and would be slower to
               classify an angry woman as a woman than an angry man as a man
               (Study 2). Conversely, we hypothesized that because perceptual
               features of fear (raised eyebrows) and happiness (a rounded
               smiling face) overlap with female sex markers, perceivers would
               be more likely to identify an androgynous face showing these
               emotions as a woman than as a man (Study 1) and would be slower
               to identify happy and fearful men as men than happy and fearful
               women as women (Study 2). The results of the two studies showed
               that happiness and fear expressions bias sex discrimination
               toward the female, whereas anger expressions bias sex perception
               toward the male.",
  journal   = "J. Vis.",
  publisher = "iovs.arvojournals.org",
  volume    =  9,
  number    =  12,
  pages     = "19.1--8",
  month     =  nov,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Brinkman2017-hg,
  title     = "Visualising mental representations: A primer on noise-based
               reverse correlation in social psychology",
  author    = "Brinkman, L and Todorov, A and Dotsch, R",
  abstract  = "ABSTRACTWith the introduction of the psychophysical method of
               reverse correlation, a holy grail of social psychology appears
               to be within reach ? visualising mental representations. Reverse
               correlation is a data-driven method that yields visual proxies
               of mental representations, based on judgements of randomly
               varying stimuli. This review is a primer to an influential
               reverse correlation approach in which stimuli vary by applying
               random noise to the pixels of images. Our review suggests that
               the technique is an invaluable tool in the investigation of
               social perception (e.g., in the perception of race, gender and
               personality traits), with ample potential applications. However,
               it is unclear how these visual proxies are best interpreted.
               Building on advances in cognitive neuroscience, we suggest that
               these proxies are visual reflections of the internal
               representations that determine how social stimuli are perceived.
               In addition, we provide a tutorial on how to perform reverse
               correlation experiments using R.",
  journal   = "European Review of Social Psychology",
  publisher = "Routledge",
  volume    =  28,
  number    =  1,
  pages     = "333--361",
  month     =  jan,
  year      =  2017,
  keywords  = "FEED"
}

@ARTICLE{Darwin1872-nv,
  title   = "The expression of the emotions in man and animals, New York: {D}",
  author  = "Darwin, Charles",
  journal = "Appleton and Company",
  year    =  1872
}

@ARTICLE{Xie2009-fp,
  title     = "Facial expression recognition based on shape and texture",
  author    = "Xie, Xudong and Lam, Kin-Man",
  abstract  = "In this paper, an efficient method for human facial expression
               recognition is presented. We first propose a representation
               model for facial expressions, namely the spatially maximum
               occurrence model (SMOM), which is based on the statistical
               characteristics of training facial images and has a powerful
               representation capability. Then the elastic shape--texture
               matching (ESTM) algorithm is used to measure the similarity
               between images based on the shape and texture information. By
               combining SMOM and ESTM, the algorithm, namely SMOM--ESTM, can
               achieve a higher recognition performance level. The recognition
               rates of the SMOM--ESTM algorithm based on the AR database and
               the Yale database are 94.5\% and 94.7\%, respectively.",
  journal   = "Pattern Recognit.",
  publisher = "Elsevier",
  volume    =  42,
  number    =  5,
  pages     = "1003--1011",
  month     =  may,
  year      =  2009,
  keywords  = "Face recognition; Facial expression recognition; Elastic
               shape--texture matching; Spatially maximum occurrence model;
               Gabor wavelets"
}

@ARTICLE{Todorov2008-eb,
  title     = "Evaluating face trustworthiness: a model based approach",
  author    = "Todorov, Alexander and Baron, Sean G and Oosterhof, Nikolaas N",
  abstract  = "Judgments of trustworthiness from faces determine basic
               approach/avoidance responses and approximate the valence
               evaluation of faces that runs across multiple person judgments.
               Here, based on trustworthiness judgments and using a computer
               model for face representation, we built a model for representing
               face trustworthiness (study 1). Using this model, we generated
               novel faces with an increased range of trustworthiness and used
               these faces as stimuli in a functional Magnetic Resonance
               Imaging study (study 2). Although participants did not engage in
               explicit evaluation of the faces, the amygdala response changed
               as a function of face trustworthiness. An area in the right
               amygdala showed a negative linear response-as the
               untrustworthiness of faces increased so did the amygdala
               response. Areas in the left and right putamen, the latter area
               extended into the anterior insula, showed a similar negative
               linear response. The response in the left amygdala was
               quadratic--strongest for faces on both extremes of the
               trustworthiness dimension. The medial prefrontal cortex and
               precuneus also showed a quadratic response, but their response
               was strongest to faces in the middle range of the
               trustworthiness dimension.",
  journal   = "Soc. Cogn. Affect. Neurosci.",
  publisher = "academic.oup.com",
  volume    =  3,
  number    =  2,
  pages     = "119--127",
  month     =  jun,
  year      =  2008,
  keywords  = "FEED",
  language  = "en"
}

@ARTICLE{Hess2009-br,
  title     = "The Categorical Perception of Emotions and Traits",
  author    = "Hess, Ursula and Adams, Reginald B and Kleck, Robert E",
  abstract  = "Facial expressions of happiness and anger have been suggested to
               share morphological features with certain personality markers in
               the face. A study was conducted to assess the hypothesis that
               angry and dominant faces on one hand and happy, fearful, and
               affiliative faces on the other hand would be categorized
               together based on the features they share. A total of 89
               participants (22 men) completed a double oddball task. Reaction
               time data confirmed the hypothesis for angry/dominant and
               happy/affiliative faces. This supports the notion that the
               perceptual markers for anger and dominance as well as happiness
               and affiliation have some morphological characteristics in
               common.",
  journal   = "Soc. Cogn.",
  publisher = "Guilford Publications Inc.",
  volume    =  27,
  number    =  2,
  pages     = "320--326",
  month     =  apr,
  year      =  2009
}

@ARTICLE{Brooks2018-gm,
  title    = "Conceptual knowledge predicts the representational structure of
              facial emotion perception",
  author   = "Brooks, Jeffrey A and Freeman, Jonathan B",
  abstract = "Recent theoretical accounts argue that conceptual knowledge
              dynamically interacts with processing of facial cues,
              fundamentally influencing visual perception of social and emotion
              categories. Evidence is accumulating for the idea that a
              perceiver's conceptual knowledge about emotion is involved in
              emotion perception, even when stereotypic facial expressions are
              presented in isolation1--4. However, existing methods have not
              allowed a comprehensive assessment of the relationship between
              conceptual knowledge and emotion perception across individuals
              and emotion categories. Here we use a representational similarity
              analysis approach to show that conceptual knowledge predicts the
              representational structure of facial emotion perception. We
              conducted three studies using computer mouse-tracking5 and
              reverse-correlation6 paradigms. Overall, we found that when
              individuals believed two emotions to be conceptually more
              similar, faces from those categories were perceived with a
              corresponding similarity, even when controlling for any physical
              similarity in the stimuli themselves. When emotions were rated
              conceptually more similar, computer-mouse trajectories during
              emotion perception exhibited a greater simultaneous attraction to
              both category responses (despite only one emotion being depicted;
              studies 1 and 2), and reverse-correlated face prototypes
              exhibited a greater visual resemblance (study 3). Together, our
              findings suggest that differences in conceptual knowledge are
              reflected in the perceptual processing of facial emotion.",
  journal  = "Nature Human Behaviour",
  volume   =  2,
  number   =  8,
  pages    = "581--591",
  month    =  aug,
  year     =  2018
}

@ARTICLE{Hess2009-xo,
  title     = "The face is not an empty canvas: how facial expressions interact
               with facial appearance",
  author    = "Hess, Ursula and Adams, Jr, Reginald B and Kleck, Robert E",
  abstract  = "Faces are not simply blank canvases upon which facial
               expressions write their emotional messages. In fact, facial
               appearance and facial movement are both important social
               signalling systems in their own right. We here provide multiple
               lines of evidence for the notion that the social signals derived
               from facial appearance on the one hand and facial movement on
               the other interact in a complex manner, sometimes reinforcing
               and sometimes contradicting one another. Faces provide
               information on who a person is. Sex, age, ethnicity, personality
               and other characteristics that can define a person and the
               social group the person belongs to can all be derived from the
               face alone. The present article argues that faces interact with
               the perception of emotion expressions because this information
               informs a decoder's expectations regarding an expresser's
               probable emotional reactions. Facial appearance also interacts
               more directly with the interpretation of facial movement because
               some of the features that are used to derive personality or sex
               information are also features that closely resemble certain
               emotional expressions, thereby enhancing or diluting the
               perceived strength of particular expressions.",
  journal   = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  publisher = "royalsocietypublishing.org",
  volume    =  364,
  number    =  1535,
  pages     = "3497--3504",
  month     =  dec,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Cohn2007-xe,
  title     = "Use of automated facial image analysis for measurement of
               emotion expression",
  author    = "Cohn, Jeffrey and Kanade, Takeo",
  journal   = "Handbook of emotion elicitation and assessment",
  publisher = "Oxford University Press New York, NY",
  pages     = "222--238",
  year      =  2007
}

@UNPUBLISHED{Jaeger2020-sr,
  title    = "Can people detect the trustworthiness of strangers based on their
              facial appearance?",
  author   = "Jaeger, Bastian and Oud, Bastiaan and Williams, Tony and
              Krumhuber, Eva and Fehr, Ernst and Engelmann, Jan B",
  abstract = "Although cooperation can lead to mutually beneficial outcomes,
              cooperating only pays off for the individual if others can be
              trusted to cooperate as well. How do people detect trustworthy
              interaction partners? While people readily rely on the facial
              appearance of strangers to judge their trustworthiness, the
              question of whether these judgments are accurate remains debated.
              The present research examines whether having access to the facial
              appearance of counterparts provides a strategic advantage to
              participants when making trust decisions. Furthermore, we
              investigated whether people show above-chance accuracy in
              trustworthiness detection (a) when they make trust decisions vs.
              provide explicit trustworthiness ratings, (b) when judging male
              vs. female counterparts, and (c) when rating cropped images (with
              non-facial features removed) vs. uncropped images. Results showed
              that incentivized trust decisions (Study 1, n = 131) and
              predictions of counterparts' trustworthiness (Study 2, n = 266)
              were unrelated to actual trustworthiness. Moreover, accuracy was
              not moderated by stimulus type (cropped vs. uncropped faces) or
              counterparts' gender. Overall, these findings suggest that people
              are unable to detect the trustworthiness of strangers based on
              their facial appearance.",
  month    =  oct,
  year     =  2020,
  keywords = "accuracy; face perception; impression formation; trust;
              trustworthiness"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Montepare2003-hy,
  title     = "The contribution of emotion perceptions and their
               overgeneralizations to trait impressions",
  author    = "Montepare, Joann M and Dobish, Heidi",
  journal   = "J. Nonverbal Behav.",
  publisher = "Springer Nature",
  volume    =  27,
  number    =  4,
  pages     = "237--254",
  year      =  2003
}

@ARTICLE{Toisoul2021-yc,
  title     = "Estimation of continuous valence and arousal levels from faces
               in naturalistic conditions",
  author    = "Toisoul, Antoine and Kossaifi, Jean and Bulat, Adrian and
               Tzimiropoulos, Georgios and Pantic, Maja",
  abstract  = "Facial affect analysis aims to create new types of
               human--computer interactions by enabling computers to better
               understand a person's emotional state in order to provide ad hoc
               help and interactions. Since discrete emotional classes (such as
               anger, happiness, sadness and so on) are not representative of
               the full spectrum of emotions displayed by humans on a daily
               basis, psychologists typically rely on dimensional measures,
               namely valence (how positive the emotional display is) and
               arousal (how calming or exciting the emotional display looks
               like). However, while estimating these values from a face is
               natural for humans, it is extremely difficult for computer-based
               systems and automatic estimation of valence and arousal in
               naturalistic conditions is an open problem. Additionally, the
               subjectivity of these measures makes it hard to obtain good
               quality data. Here we introduce a novel deep neural network
               architecture to analyse facial affect in naturalistic conditions
               with a high level of accuracy. The proposed network integrates
               face alignment and jointly estimates both categorical and
               continuous emotions in a single pass, making it suitable for
               real-time applications. We test our method on three challenging
               datasets collected in naturalistic conditions and show that our
               approach outperforms all previous methods. We also discuss
               caveats regarding the use of this tool, and ethical aspects that
               must be considered in its application. The annotation of the
               visual signs of emotions can be important for psychological
               studies and even human--computer interactions. Instead of only
               ascribing discrete emotions, Toisoul and colleagues use a single
               neural network that predicts emotional labels on a spectrum of
               valence and arousal without separate face-alignment steps.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  3,
  number    =  1,
  pages     = "42--50",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Rosenblatt2014-az,
  title     = "Revisiting multi-subject random effects in {fMRI}: advocating
               prevalence estimation",
  author    = "Rosenblatt, J D and Vink, M and Benjamini, Y",
  abstract  = "Random effect analysis has been introduced into fMRI research in
               order to generalize findings from the study group to the whole
               population. Generalizing findings is obviously harder than
               detecting activation within the study group since in order to be
               significant, an activation has to be larger than the
               inter-subject variability. Indeed, detected regions are smaller
               when using random effect analysis versus fixed effects. The
               statistical assumptions behind the classic random effect model
               are that the effect in each location is normally distributed
               over subjects, and ``activation'' refers to a non-null mean
               effect. We argue that this model is unrealistic compared to the
               true population variability, where due to function-anatomy
               inconsistencies and registration anomalies, some of the subjects
               are active and some are not at each brain location. We propose a
               Gaussian-mixture-random-effect that amortizes between-subject
               spatial disagreement and quantifies it using the prevalence of
               activation at each location. We present a formal definition and
               an estimation procedure of this prevalence. The end result of
               the proposed analysis is a map of the prevalence at locations
               with significant activation, highlighting activation regions
               that are common over many brains. Prevalence estimation has
               several desirable properties: (a) It is more informative than
               the typical active/inactive paradigm. (b) In contrast to the
               usual display of p-values in activated regions - which trivially
               converge to 0 for large sample sizes - prevalence estimates
               converge to the true prevalence.",
  journal   = "Neuroimage",
  publisher = "Elsevier",
  volume    =  84,
  pages     = "113--121",
  month     =  jan,
  year      =  2014,
  keywords  = "Gaussian mixture; Group studies; Localization; Random effects;
               Statistical inference; fMRI",
  language  = "en"
}

@ARTICLE{Folster2014-zy,
  title     = "Facial age affects emotional expression decoding",
  author    = "F{\"o}lster, Mara and Hess, Ursula and Werheid, Katja",
  abstract  = "Facial expressions convey important information on emotional
               states of our interaction partners. However, in interactions
               between younger and older adults, there is evidence for a
               reduced ability to accurately decode emotional facial
               expressions. Previous studies have often followed up this
               phenomenon by examining the effect of the observers' age.
               However, decoding emotional faces is also likely to be
               influenced by stimulus features, and age-related changes in the
               face such as wrinkles and folds may render facial expressions of
               older adults harder to decode. In this paper, we review
               theoretical frameworks and empirical findings on age effects on
               decoding emotional expressions, with an emphasis on age-of-face
               effects. We conclude that the age of the face plays an important
               role for facial expression decoding. Lower expressivity,
               age-related changes in the face, less elaborated emotion schemas
               for older faces, negative attitudes toward older adults, and
               different visual scan patterns and neural processing of older
               than younger faces may lower decoding accuracy for older faces.
               Furthermore, age-related stereotypes and age-related changes in
               the face may bias the attribution of specific emotions such as
               sadness to older faces.",
  journal   = "Front. Psychol.",
  publisher = "frontiersin.org",
  volume    =  5,
  pages     = "30",
  month     =  feb,
  year      =  2014,
  keywords  = "aging; emotional facial expressions; expressivity; facial
               expression decoding; older face; own-age advantage; response
               bias",
  language  = "en"
}

@ARTICLE{Thorstenson2018-io,
  title     = "Emotion-color associations in the context of the face",
  author    = "Thorstenson, Christopher A and Elliot, Andrew J and Pazda, Adam
               D and Perrett, David I and Xiao, Dengke",
  abstract  = "Facial expressions of emotion contain important information that
               is perceived and used by observers to understand others'
               emotional state. While there has been considerable research into
               perceptions of facial musculature and emotion, less work has
               been conducted to understand perceptions of facial coloration
               and emotion. The current research examined emotion-color
               associations in the context of the face. Across 4 experiments,
               participants were asked to manipulate the color of face, or
               shape, stimuli along 2 color axes (i.e., red-green, yellow-blue)
               for 6 target emotions (i.e., anger, disgust, fear, happiness,
               sadness, surprise). The results yielded a pattern that is
               consistent with physiological and psychological models of
               emotion. (PsycINFO Database Record (c) 2018 APA, all rights
               reserved).",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  18,
  number    =  7,
  pages     = "1032--1042",
  month     =  oct,
  year      =  2018,
  language  = "en"
}

@BOOK{McElreath2020-pz,
  title     = "Statistical Rethinking: A Bayesian Course with Examples in {R}
               and {STAN}",
  author    = "McElreath, Richard",
  abstract  = "Statistical Rethinking: A Bayesian Course with Examples in R and
               Stan builds your knowledge of and confidence in making
               inferences from data. Reflecting the need for scripting in
               today's model-based statistics, the book pushes you to perform
               step-by-step calculations that are usually automated. This
               unique computational approach ensures that you understand enough
               of the details to make reasonable choices and interpretations in
               your own modeling work. The text presents causal inference and
               generalized linear multilevel models from a simple Bayesian
               perspective that builds on information theory and maximum
               entropy. The core material ranges from the basics of regression
               to advanced multilevel models. It also presents measurement
               error, missing data, and Gaussian process models for spatial and
               phylogenetic confounding. The second edition emphasizes the
               directed acyclic graph (DAG) approach to causal inference,
               integrating DAGs into many examples. The new edition also
               contains new material on the design of prior distributions,
               splines, ordered categorical predictors, social relations
               models, cross-validation, importance sampling, instrumental
               variables, and Hamiltonian Monte Carlo. It ends with an entirely
               new chapter that goes beyond generalized linear modeling,
               showing how domain-specific scientific models can be built into
               statistical analyses. Features Integrates working code into the
               main text Illustrates concepts through worked data analysis
               examples Emphasizes understanding assumptions and how
               assumptions are reflected in code Offers more detailed
               explanations of the mathematics in optional sections Presents
               examples of using the dagitty R package to analyze causal graphs
               Provides the rethinking R package on the author's website and on
               GitHub",
  publisher = "CRC Press",
  month     =  mar,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Varoquaux2017-fj,
  title     = "Assessing and tuning brain decoders: Cross-validation, caveats,
               and guidelines",
  author    = "Varoquaux, Ga{\"e}l and Raamana, Pradeep Reddy and Engemann,
               Denis A and Hoyos-Idrobo, Andr{\'e}s and Schwartz, Yannick and
               Thirion, Bertrand",
  abstract  = "Decoding, i.e. prediction from brain images or signals, calls
               for empirical evaluation of its predictive power. Such
               evaluation is achieved via cross-validation, a method also used
               to tune decoders' hyper-parameters. This paper is a review on
               cross-validation procedures for decoding in neuroimaging. It
               includes a didactic overview of the relevant theoretical
               considerations. Practical aspects are highlighted with an
               extensive empirical study of the common decoders in within- and
               across-subject predictions, on multiple datasets -anatomical and
               functional MRI and MEG- and simulations. Theory and experiments
               outline that the popular ``leave-one-out'' strategy leads to
               unstable and biased estimates, and a repeated random splits
               method should be preferred. Experiments outline the large error
               bars of cross-validation in neuroimaging settings: typical
               confidence intervals of 10\%. Nested cross-validation can tune
               decoders' parameters while avoiding circularity bias. However we
               find that it can be favorable to use sane defaults, in
               particular for non-sparse decoders.",
  journal   = "Neuroimage",
  publisher = "Elsevier",
  volume    =  145,
  number    = "Pt B",
  pages     = "166--179",
  month     =  jan,
  year      =  2017,
  keywords  = "Bagging; Cross-validation; Decoding; FMRI; MVPA; Model
               selection; Sparse",
  language  = "en"
}

@UNPUBLISHED{Ince2020-mr,
  title    = "Bayesian inference of population prevalence",
  author   = "Ince, Robin A A and Kay, Jim W and Schyns, Philippe G",
  abstract = "Within neuroscience, psychology and neuroimaging, the most
              frequently used statistical approach is null-hypothesis
              significance testing (NHST) of the population mean. An
              alternative approach is to perform NHST within individual
              participants and then infer, from the proportion of participants
              showing an effect, the prevalence of that effect in the
              population. We propose a novel Bayesian method to estimate such
              population prevalence that offers several advantages over
              population mean NHST. This method provides a population-level
              inference that is currently missing from study designs with small
              participant numbers, such as in traditional psychophysics and in
              precision imaging. It delivers a quantitative estimate with
              associated uncertainty instead of reducing an experiment to a
              binary inference on a population mean. Bayesian prevalence is
              widely applicable to a broad range of studies in neuroscience,
              psychology and neuroimaging. Its emphasis on detecting effects
              within individual participants could also help address
              replicability issues in these fields. \#\#\# Competing Interest
              Statement The authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2020.07.08.191106",
  month    =  jul,
  year     =  2020,
  language = "en"
}

@ARTICLE{Jack2014-ku,
  title     = "Dynamic facial expressions of emotion transmit an evolving
               hierarchy of signals over time",
  author    = "Jack, Rachael E and Garrod, Oliver G B and Schyns, Philippe G",
  abstract  = "Designed by biological and social evolutionary pressures, facial
               expressions of emotion comprise specific facial movements to
               support a near-optimal system of signaling and decoding.
               Although highly dynamical, little is known about the form and
               function of facial expression temporal dynamics. Do facial
               expressions transmit diagnostic signals simultaneously to
               optimize categorization of the six classic emotions, or
               sequentially to support a more complex communication system of
               successive categorizations over time? Our data support the
               latter. Using a combination of perceptual expectation modeling,
               information theory, and Bayesian classifiers, we show that
               dynamic facial expressions of emotion transmit an evolving
               hierarchy of ``biologically basic to socially specific''
               information over time. Early in the signaling dynamics, facial
               expressions systematically transmit few, biologically rooted
               face signals supporting the categorization of fewer elementary
               categories (e.g., approach/avoidance). Later transmissions
               comprise more complex signals that support categorization of a
               larger number of socially specific categories (i.e., the six
               classic emotions). Here, we show that dynamic facial expressions
               of emotion provide a sophisticated signaling system, questioning
               the widely accepted notion that emotion communication is
               comprised of six basic (i.e., psychologically irreducible)
               categories, and instead suggesting four.",
  journal   = "Curr. Biol.",
  publisher = "Elsevier",
  volume    =  24,
  number    =  2,
  pages     = "187--192",
  month     =  jan,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Ekman1976-hm,
  title     = "Measuring facial movement",
  author    = "Ekman, Paul and Friesen, Wallace V",
  abstract  = "A procedure has been developed for measuring visibly different
               facial movements. The Facial Action Code was derived from an
               analysis of the anatomical basis of facial movement. The method
               can be used to describe any facial movement (observed in
               photographs, motion picture film or videotape) in terms of
               anatomically based action units. The development of the method
               is explained, contrasting it to other methods of measuring
               facial behavior. An example of how facial behavior is measured
               is provided, and ideas about research applications are
               discussed.",
  journal   = "Environmental psychology and nonverbal behavior",
  publisher = "Springer",
  volume    =  1,
  number    =  1,
  pages     = "56--75",
  month     =  sep,
  year      =  1976
}

@ARTICLE{Hunter2007-at,
  title     = "Matplotlib: A {2D} Graphics Environment",
  author    = "{Hunter}",
  abstract  = "Matplotlib is a 2D graphics package for Python for application
               development, interactive scripting, and publication-quality
               image generation across user interfaces and operating systems.",
  journal   = "IEEE Ann. Hist. Comput.",
  publisher = "computer.org",
  volume    =  9,
  pages     = "90--95",
  month     =  may,
  year      =  2007
}

@INPROCEEDINGS{Lien1998-bg,
  title     = "Automated facial expression recognition based on {FACS} action
               units",
  booktitle = "Proceedings Third {IEEE} International Conference on Automatic
               Face and Gesture Recognition",
  author    = "Lien, J J and Kanade, T and Cohn, J F and {Ching-Chung Li}",
  abstract  = "Automated recognition of facial expression is an important
               addition to computer vision research because of its relevance to
               the study of psychological phenomena and the development of
               human-computer interaction (HCI). We developed a computer vision
               system that automatically recognizes individual action units or
               action unit combinations in the upper face using hidden Markov
               models (HMMs). Our approach to facial expression recognition is
               based an the Facial Action Coding System (FACS), which separates
               expressions into upper and lower face action. We use three
               approaches to extract facial expression information: (1) facial
               feature point tracking; (2) dense flow tracking with principal
               component analysis (PCA); and (3) high gradient component
               detection (i.e. furrow detection). The recognition results of
               the upper face expressions using feature point tracking, dense
               flow tracking, and high gradient component detection are 85\%,
               93\% and 85\%, respectively.",
  publisher = "ieeexplore.ieee.org",
  pages     = "390--395",
  month     =  apr,
  year      =  1998,
  keywords  = "Face recognition;Face detection;Computational Intelligence
               Society;Computer vision;Neutron spin echo;Principal component
               analysis;Humans;Image recognition;Image motion
               analysis;Psychology"
}

@ARTICLE{Krumhuber2013-qi,
  title     = "Effects of Dynamic Aspects of Facial Expressions: A Review",
  author    = "Krumhuber, Eva G and Kappas, Arvid and Manstead, Antony S R",
  abstract  = "A key feature of facial behavior is its dynamic quality.
               However, most previous research has been limited to the use of
               static images of prototypical expressive patterns. This article
               explores the role of facial dynamics in the perception of
               emotions, reviewing relevant empirical evidence demonstrating
               that dynamic information improves coherence in the
               identification of affect (particularly for degraded and subtle
               stimuli), leads to higher emotion judgments (i.e., intensity and
               arousal), and helps to differentiate between genuine and fake
               expressions. The findings underline that using static
               expressions not only poses problems of ecological validity, but
               also limits our understanding of what facial activity does.
               Implications for future research on facial activity,
               particularly for social neuroscience and affective computing,
               are discussed.",
  journal   = "Emot. Rev.",
  publisher = "SAGE Publications",
  volume    =  5,
  number    =  1,
  pages     = "41--46",
  month     =  jan,
  year      =  2013
}

@ARTICLE{Mileva2014-ld,
  title     = "In the face of dominance: Self-perceived and other-perceived
               dominance are positively associated with facial-width-to-height
               ratio in men",
  author    = "Mileva, V R and Cowan, M L and Cobey, K D and Knowles, K K and
               Little, A C",
  abstract  = "In recent research, facial width-to-height ratio (fWHR) has
               garnered considerable attention because it has been linked with
               different behavioural characteristics (e.g., achievement drive,
               deception, aggression). Here we examined whether
               other-perceptions and self-perceptions of dominance are related
               to fWHR. In study 1, we found that other-perceived dominance was
               positively associated with fWHR, but only in men. In studies 2
               and 3, using two different self-perceived dominance scales, and
               two different samples of participants, we found that fWHR was
               positively related to self-perceived dominance, again only in
               men. There was no relationship between fWHR and self-perceived
               prestige scores. Consistent with previous work, we also found
               that there was no sexual dimorphism in fWHR across all three
               studies. Together these results suggest that fWHR may be a
               reliable cue to dominant social behaviour in men.",
  journal   = "Pers. Individ. Dif.",
  publisher = "Elsevier",
  volume    =  69,
  pages     = "115--118",
  month     =  oct,
  year      =  2014,
  keywords  = "Facial width-to-height ratio; Dominance; Prestige; Sexual
               dimorphism; Facial metrics"
}

@INPROCEEDINGS{Xu2020-jd,
  title     = "Investigating Bias and Fairness in Facial Expression Recognition",
  booktitle = "Computer Vision -- {ECCV} 2020 Workshops",
  author    = "Xu, Tian and White, Jennifer and Kalkan, Sinan and Gunes, Hatice",
  abstract  = "Recognition of expressions of emotions and affect from facial
               images is a well-studied research problem in the fields of
               affective computing and computer vision with a large number of
               datasets available containing facial images and corresponding
               expression labels. However, virtually none of these datasets
               have been acquired with consideration of fair distribution
               across the human population. Therefore, in this work, we
               undertake a systematic investigation of bias and fairness in
               facial expression recognition by comparing three different
               approaches, namely a baseline, an attribute-aware and a
               disentangled approach, on two well-known datasets, RAF-DB and
               CelebA. Our results indicate that: (i) data augmentation
               improves the accuracy of the baseline model, but this alone is
               unable to mitigate the bias effect; (ii) both the
               attribute-aware and the disentangled approaches equipped with
               data augmentation perform better than the baseline approach in
               terms of accuracy and fairness; (iii) the disentangled approach
               is the best for mitigating demographic bias; and (iv) the bias
               mitigation strategies are more suitable in the existence of
               uneven attribute distribution or imbalanced number of subgroup
               data.",
  publisher = "Springer International Publishing",
  pages     = "506--523",
  year      =  2020
}

@ARTICLE{Gill2014-hx,
  title     = "Facial movements strategically camouflage involuntary social
               signals of face morphology",
  author    = "Gill, Daniel and Garrod, Oliver G B and Jack, Rachael E and
               Schyns, Philippe G",
  abstract  = "Animals use social camouflage as a tool of deceit to increase
               the likelihood of survival and reproduction. We tested whether
               humans can also strategically deploy transient facial movements
               to camouflage the default social traits conveyed by the
               phenotypic morphology of their faces. We used the responses of
               12 observers to create models of the dynamic facial signals of
               dominance, trustworthiness, and attractiveness. We applied these
               dynamic models to facial morphologies differing on perceived
               dominance, trustworthiness, and attractiveness to create a set
               of dynamic faces; new observers rated each dynamic face
               according to the three social traits. We found that specific
               facial movements camouflage the social appearance of a face by
               modulating the features of phenotypic morphology. A comparison
               of these facial expressions with those similarly derived for
               facial emotions showed that social-trait expressions, rather
               than being simple one-to-one overgeneralizations of emotional
               expressions, are a distinct set of signals composed of movements
               from different emotions. Our generative face models represent
               novel psychophysical laws for social sciences; these laws
               predict the perception of social traits on the basis of dynamic
               face identities.",
  journal   = "Psychol. Sci.",
  publisher = "journals.sagepub.com",
  volume    =  25,
  number    =  5,
  pages     = "1079--1086",
  month     =  may,
  year      =  2014,
  keywords  = "face perception; facial expressions; social cognition",
  language  = "en"
}

@ARTICLE{Yarkoni2017-om,
  title     = "Choosing Prediction Over Explanation in Psychology: Lessons From
               Machine Learning",
  author    = "Yarkoni, Tal and Westfall, Jacob",
  abstract  = "Psychology has historically been concerned, first and foremost,
               with explaining the causal mechanisms that give rise to
               behavior. Randomized, tightly controlled experiments are
               enshrined as the gold standard of psychological research, and
               there are endless investigations of the various mediating and
               moderating variables that govern various behaviors. We argue
               that psychology's near-total focus on explaining the causes of
               behavior has led much of the field to be populated by research
               programs that provide intricate theories of psychological
               mechanism but that have little (or unknown) ability to predict
               future behaviors with any appreciable accuracy. We propose that
               principles and techniques from the field of machine learning can
               help psychology become a more predictive science. We review some
               of the fundamental concepts and tools of machine learning and
               point out examples where these concepts have been used to
               conduct interesting and important psychological research that
               focuses on predictive research questions. We suggest that an
               increased focus on prediction, rather than explanation, can
               ultimately lead us to greater understanding of behavior.",
  journal   = "Perspect. Psychol. Sci.",
  publisher = "pilab.psy.utexas.edu",
  pages     = "1745691617693393",
  month     =  aug,
  year      =  2017,
  keywords  = "explanation; machine learning; prediction;NeuroimagingMethods",
  language  = "en"
}

@ARTICLE{Adams2016-tz,
  title     = "What Facial Appearance Reveals Over Time: When Perceived
               Expressions in Neutral Faces Reveal Stable Emotion Dispositions",
  author    = "Adams, Jr, Reginald B and Garrido, Carlos O and Albohn, Daniel N
               and Hess, Ursula and Kleck, Robert E",
  abstract  = "It might seem a reasonable assumption that when we are not
               actively using our faces to express ourselves (i.e., when we
               display nonexpressive, or neutral faces), those around us will
               not be able to read our emotions. Herein, using a variety of
               expression-related ratings, we examined whether age-related
               changes in the face can accurately reveal one's innermost
               affective dispositions. In each study, we found that expressive
               ratings of neutral facial displays predicted self-reported
               positive/negative dispositional affect, but only for elderly
               women, and only for positive affect. These findings meaningfully
               replicate and extend earlier work examining age-related emotion
               cues in the face of elderly women (Malatesta et al., 1987a). We
               discuss these findings in light of evidence that women are
               expected to, and do, smile more than men, and that the quality
               of their smiles predicts their life satisfaction. Although
               ratings of old male faces did not significantly predict
               self-reported affective dispositions, the trend was similar to
               that found for old female faces. A plausible explanation for
               this gender difference is that in the process of attenuating
               emotional expressions over their lifetimes, old men reveal less
               evidence of their total emotional experiences in their faces
               than do old women.",
  journal   = "Front. Psychol.",
  publisher = "frontiersin.org",
  volume    =  7,
  pages     = "986",
  month     =  jun,
  year      =  2016,
  keywords  = "aging; appearance; emotional expression; face perception; person
               perception",
  language  = "en"
}

@ARTICLE{Zebrowitz2017-qe,
  title     = "First Impressions From Faces",
  author    = "Zebrowitz, Leslie A",
  abstract  = "Although cultural wisdom warns 'don't judge a book by its
               cover,' we seem unable to inhibit this tendency even though it
               can produce inaccurate impressions of people's psychological
               traits and has significant social consequences. One explanation
               for this paradox is that first impressions of faces
               overgeneralize our adaptive impressions of categories of people
               that those faces resemble (including babies, familiar or
               unfamiliar people, unfit people, emotional people). Research
               testing these 'overgeneralization' hypotheses elucidates why we
               form first impressions from faces, what impressions we form, and
               what cues influence these impressions. This article focuses on
               commonalities in impressions across diverse perceivers. However,
               brief attention is given to individual differences in
               impressions and impression accuracy.",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "journals.sagepub.com",
  volume    =  26,
  number    =  3,
  pages     = "237--242",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@INPROCEEDINGS{Lucey2010-at,
  title     = "The Extended {Cohn-Kanade} Dataset ({CK+)}: A complete dataset
               for action unit and emotion-specified expression",
  booktitle = "2010 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition - Workshops",
  author    = "Lucey, Patrick and Cohn, Jeffrey F and Kanade, Takeo and
               Saragih, Jason and Ambadar, Zara and Matthews, Iain",
  abstract  = "In 2000, the Cohn-Kanade (CK) database was released for the
               purpose of promoting research into automatically detecting
               individual facial expressions. Since then, the CK database has
               become one of the most widely used test-beds for algorithm
               development and evaluation. During this period, three
               limitations have become apparent: 1) While AU codes are well
               validated, emotion labels are not, as they refer to what was
               requested rather than what was actually performed, 2) The lack
               of a common performance metric against which to evaluate new
               algorithms, and 3) Standard protocols for common databases have
               not emerged. As a consequence, the CK database has been used for
               both AU and emotion detection (even though labels for the latter
               have not been validated), comparison with benchmark algorithms
               is missing, and use of random subsets of the original database
               makes meta-analyses difficult. To address these and other
               concerns, we present the Extended Cohn-Kanade (CK+) database.
               The number of sequences is increased by 22\% and the number of
               subjects by 27\%. The target expression for each sequence is
               fully FACS coded and emotion labels have been revised and
               validated. In addition to this, non-posed sequences for several
               types of smiles and their associated metadata have been added.
               We present baseline results using Active Appearance Models
               (AAMs) and a linear support vector machine (SVM) classifier
               using a leave-one-out subject cross-validation for both AU and
               emotion detection for the posed data. The emotion and AU labels,
               along with the extended image data and tracked landmarks will be
               made available July 2010.",
  publisher = "ieeexplore.ieee.org",
  pages     = "94--101",
  month     =  jun,
  year      =  2010,
  keywords  = "Databases;Gold;Active appearance model;Support vector
               machines;Support vector machine classification;Face
               detection;Testing;Performance evaluation;Measurement;Code
               standards"
}

@ARTICLE{Kunz2019-uh,
  title     = "Facial muscle movements encoding pain---a systematic review",
  author    = "Kunz, Miriam and Meixner, Doris and Lautenbacher, Stefan",
  abstract  = "that used the Facial Action Coding System to analyze facial
               activity during pain in adults, and that report on distinct
               facial responses (action units [AUs]). Twenty-seven studies
               using experimental pain and 10 clinical pain studies were
               included. We synthesized the data by taking into consideration
               (1) the criteria used to define whether an AU is pain-related;
               (2) types of pain; and (3) the cognitive status of the
               individuals. When AUs were selected as being pain-related based
               on a ``pain > baseline'' increase, a consistent subset of
               pain-related AUs emerged across studies: lowering the brows
               (AU4), cheek raise/lid tightening (AUs6\_7), nose
               wrinkling/raising the upper lip (AUs9\_10), and opening of the
               mouth (AUs25\_26\_27). This subset was found independently of
               the cognitive status of the individuals and was stable across
               clinical and experimental pain with only one variation, namely
               that eye closure (AU43) occurred more frequently during clinical
               pain. This subset of pain-related facial responses seems to
               encode the essential information about pain available in the
               face. However, given that these pain-related AUs are most often
               not displayed all at once, but are differently combined, health
               care professionals should use a more individualized approach,
               determining which pain-related facial responses an individual
               combines and aggregates to express pain, instead of erroneously
               searching for a uniform expression of pain....",
  journal   = "Pain",
  publisher = "journals.lww.com",
  volume    =  160,
  number    =  3,
  pages     = "535",
  month     =  mar,
  year      =  2019
}

@article{chen2018distinct,
  title={Distinct facial expressions represent pain and pleasure across cultures},
  author={Chen, Chaona and Crivelli, Carlos and Garrod, Oliver GB and Schyns, Philippe G and Fern{\'a}ndez-Dols, Jos{\'e}-Miguel and Jack, Rachael E},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={43},
  pages={E10013--E10021},
  year={2018},
  publisher={National Acad Sciences}
}

@ARTICLE{Barrett2011-gv,
  title     = "Was Darwin Wrong About Emotional Expressions?",
  author    = "Barrett, Lisa Feldman",
  abstract  = "Emotional expressions have endured as a topic of profound
               scientific interest for over a century, in part due to Darwin?s
               classic volume, The Expression of Emotions in Man and Animals.
               Since its publication, there has been a strong, spirited debate
               over the origin, nature, and function of emotional expressions.
               In this article, I consider two basic questions: What did Darwin
               really write about emotional expressions, and how well does his
               account match the modern, conventional, ?basic emotion? account?
               And does the scientific evidence specifically support the modern
               account of Darwin?s view, or are there alternative hypotheses
               that provide good (or even better) interpretations for the data
               at hand? I discuss the various ways that Darwin might be correct
               (and incorrect) about how emotions and their manifestations have
               been sculpted by natural selection.",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  20,
  number    =  6,
  pages     = "400--406",
  month     =  dec,
  year      =  2011
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dinga2019-mh,
  title     = "Beyond accuracy: measures for assessing machine learning models,
               pitfalls and guidelines",
  author    = "Dinga, R and Penninx, Bwjh and Veltman, D J and Schmaal, L and
               {others}",
  journal   = "bioRxiv",
  publisher = "biorxiv.org",
  year      =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ojala2010-rc,
  title     = "Permutation Tests for Studying Classifier Performance",
  author    = "Ojala, Markus and Garriga, Gemma C",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  11,
  number    = "Jun",
  pages     = "1833--1863",
  year      =  2010
}

@ARTICLE{Van_Bergen2015-kl,
  title     = "Sensory uncertainty decoded from visual cortex predicts behavior",
  author    = "van Bergen, Ruben S and Ma, Wei Ji and Pratte, Michael S and
               Jehee, Janneke F M",
  abstract  = "Bayesian theories of neural coding propose that sensory
               uncertainty is represented by a probability distribution encoded
               in neural population activity, but direct neural evidence
               supporting this hypothesis is currently lacking. Using fMRI in
               combination with a generative model-based analysis, we found
               that probability distributions reflecting sensory uncertainty
               could reliably be estimated from human visual cortex and,
               moreover, that observers appeared to use knowledge of this
               uncertainty in their perceptual decisions.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  18,
  number    =  12,
  pages     = "1728--1730",
  month     =  dec,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Murugappan2021-yj,
  title     = "Facial geometric feature extraction based emotional expression
               classification using machine learning algorithms",
  author    = "Murugappan, M and Mutawa, A",
  abstract  = "Emotion plays a significant role in interpersonal communication
               and also improving social life. In recent years, facial emotion
               recognition is highly adopted in developing human-computer
               interfaces (HCI) and humanoid robots. In this work, a
               triangulation method for extracting a novel set of geometric
               features is proposed to classify six emotional expressions
               (sadness, anger, fear, surprise, disgust, and happiness) using
               computer-generated markers. The subject's face is recognized by
               using Haar-like features. A mathematical model has been applied
               to positions of eight virtual markers in a defined location on
               the subject's face in an automated way. Five triangles are
               formed by manipulating eight markers' positions as an edge of
               each triangle. Later, these eight markers are uninterruptedly
               tracked by Lucas- Kanade optical flow algorithm while subjects'
               articulating facial expressions. The movement of the markers
               during facial expression directly changes the property of each
               triangle. The area of the triangle (AoT), Inscribed circle
               circumference (ICC), and the Inscribed circle area of a triangle
               (ICAT) are extracted as features to classify the facial
               emotions. These features are used to distinguish six different
               facial emotions using various types of machine learning
               algorithms. The inscribed circle area of the triangle (ICAT)
               feature gives a maximum mean classification rate of 98.17\%
               using a Random Forest (RF) classifier compared to other features
               and classifiers in distinguishing emotional expressions.",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  16,
  number    =  2,
  pages     = "e0247131",
  month     =  feb,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Ko2018-rv,
  title     = "A Brief Review of Facial Emotion Recognition Based on Visual
               Information",
  author    = "Ko, Byoung Chul",
  abstract  = "Facial emotion recognition (FER) is an important topic in the
               fields of computer vision and artificial intelligence owing to
               its significant academic and commercial potential. Although FER
               can be conducted using multiple sensors, this review focuses on
               studies that exclusively use facial images, because visual
               expressions are one of the main information channels in
               interpersonal communication. This paper provides a brief review
               of researches in the field of FER conducted over the past
               decades. First, conventional FER approaches are described along
               with a summary of the representative categories of FER systems
               and their main algorithms. Deep-learning-based FER approaches
               using deep networks enabling ``end-to-end'' learning are then
               presented. This review also focuses on an up-to-date hybrid
               deep-learning approach combining a convolutional neural network
               (CNN) for the spatial features of an individual frame and long
               short-term memory (LSTM) for temporal features of consecutive
               frames. In the later part of this paper, a brief review of
               publicly available evaluation metrics is given, and a comparison
               with benchmark results, which are a standard for a quantitative
               comparison of FER researches, is described. This review can
               serve as a brief guidebook to newcomers in the field of FER,
               providing basic knowledge and a general understanding of the
               latest state-of-the-art studies, as well as to experienced
               researchers looking for productive directions for future work.",
  journal   = "Sensors",
  publisher = "mdpi.com",
  volume    =  18,
  number    =  2,
  month     =  jan,
  year      =  2018,
  keywords  = "conventional FER; convolutional neural networks; deep
               learning-based FER; facial action coding system; facial action
               unit; facial emotion recognition; long short term memory",
  language  = "en"
}

@ARTICLE{Jack2012-eq,
  title     = "Facial expressions of emotion are not culturally universal",
  author    = "Jack, Rachael E and Garrod, Oliver G B and Yu, Hui and Caldara,
               Roberto and Schyns, Philippe G",
  abstract  = "Since Darwin's seminal works, the universality of facial
               expressions of emotion has remained one of the longest standing
               debates in the biological and social sciences. Briefly stated,
               the universality hypothesis claims that all humans communicate
               six basic internal emotional states (happy, surprise, fear,
               disgust, anger, and sad) using the same facial movements by
               virtue of their biological and evolutionary origins [Susskind
               JM, et al. (2008) Nat Neurosci 11:843-850]. Here, we refute this
               assumed universality. Using a unique computer graphics platform
               that combines generative grammars [Chomsky N (1965) MIT Press,
               Cambridge, MA] with visual perception, we accessed the mind's
               eye of 30 Western and Eastern culture individuals and
               reconstructed their mental representations of the six basic
               facial expressions of emotion. Cross-cultural comparisons of the
               mental representations challenge universality on two separate
               counts. First, whereas Westerners represent each of the six
               basic emotions with a distinct set of facial movements common to
               the group, Easterners do not. Second, Easterners represent
               emotional intensity with distinctive dynamic eye activity. By
               refuting the long-standing universality hypothesis, our data
               highlight the powerful influence of culture on shaping basic
               behaviors once considered biologically hardwired. Consequently,
               our data open a unique nature-nurture debate across broad fields
               from evolutionary psychology and social neuroscience to social
               networking via digital avatars.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  109,
  number    =  19,
  pages     = "7241--7244",
  month     =  may,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Jack2009-yy,
  title     = "Cultural confusions show that facial expressions are not
               universal",
  author    = "Jack, Rachael E and Blais, Caroline and Scheepers, Christoph and
               Schyns, Philippe G and Caldara, Roberto",
  abstract  = "Central to all human interaction is the mutual understanding of
               emotions, achieved primarily by a set of biologically rooted
               social signals evolved for this purpose-facial expressions of
               emotion. Although facial expressions are widely considered to be
               the universal language of emotion, some negative facial
               expressions consistently elicit lower recognition levels among
               Eastern compared to Western groups (see [4] for a meta-analysis
               and [5, 6] for review). Here, focusing on the decoding of facial
               expression signals, we merge behavioral and computational
               analyses with novel spatiotemporal analyses of eye movements,
               showing that Eastern observers use a culture-specific decoding
               strategy that is inadequate to reliably distinguish universal
               facial expressions of ``fear'' and ``disgust.'' Rather than
               distributing their fixations evenly across the face as
               Westerners do, Eastern observers persistently fixate the eye
               region. Using a model information sampler, we demonstrate that
               by persistently fixating the eyes, Eastern observers sample
               ambiguous information, thus causing significant confusion. Our
               results question the universality of human facial expressions of
               emotion, highlighting their true complexity, with critical
               consequences for cross-cultural communication and globalization.",
  journal   = "Curr. Biol.",
  publisher = "Elsevier",
  volume    =  19,
  number    =  18,
  pages     = "1543--1548",
  month     =  sep,
  year      =  2009,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Guan2018-hq,
  title     = "Computational modeling of social face perception in humans:
               Leveraging the active appearance model",
  author    = "Guan, J and Ryali, C K and Angela, J Y",
  journal   = "bioRxiv",
  publisher = "biorxiv.org",
  year      =  2018
}

@ARTICLE{Zebrowitz2010-vr,
  title     = "Facial resemblance to emotions: group differences, impression
               effects, and race stereotypes",
  author    = "Zebrowitz, Leslie A and Kikuchi, Masako and Fellous, Jean-Marc",
  abstract  = "The authors used connectionist modeling to extend previous
               research on emotion overgeneralization effects. Study 1
               demonstrated that neutral expression male faces objectively
               resemble angry expressions more than female faces do, female
               faces objectively resemble surprise expressions more than male
               faces do, White faces objectively resemble angry expressions
               more than Black or Korean faces do, and Black faces objectively
               resemble happy and surprise expressions more than White faces
               do. Study 2 demonstrated that objective resemblance to emotion
               expressions influences trait impressions even when statistically
               controlling possible confounding influences of attractiveness
               and babyfaceness. It further demonstrated that emotion
               overgeneralization is moderated by face race and that racial
               differences in emotion resemblance contribute to White
               perceivers' stereotypes of Blacks and Asians. These results
               suggest that intergroup relations may be strained not only by
               cultural stereotypes but also by adaptive responses to emotion
               expressions that are overgeneralized to groups whose faces
               subtly resemble particular emotions.",
  journal   = "J. Pers. Soc. Psychol.",
  publisher = "psycnet.apa.org",
  volume    =  98,
  number    =  2,
  pages     = "175--189",
  month     =  feb,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Harris2020-en,
  title     = "Array programming with {NumPy}",
  author    = "Harris, Charles R and Millman, K Jarrod and van der Walt,
               St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and
               Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg,
               Sebastian and Smith, Nathaniel J and Kern, Robert and Picus,
               Matti and Hoyer, Stephan and van Kerkwijk, Marten H and Brett,
               Matthew and Haldane, Allan and Del R{\'\i}o, Jaime Fern{\'a}ndez
               and Wiebe, Mark and Peterson, Pearu and G{\'e}rard-Marchant,
               Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser,
               Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant,
               Travis E",
  abstract  = "Array programming provides a powerful, compact and expressive
               syntax for accessing, manipulating and operating on data in
               vectors, matrices and higher-dimensional arrays. NumPy is the
               primary array programming library for the Python language. It
               has an essential role in research analysis pipelines in fields
               as diverse as physics, chemistry, astronomy, geoscience,
               biology, psychology, materials science, engineering, finance and
               economics. For example, in astronomy, NumPy was an important
               part of the software stack used in the discovery of
               gravitational waves1 and in the first imaging of a black hole2.
               Here we review how a few fundamental array concepts lead to a
               simple and powerful programming paradigm for organizing,
               exploring and analysing scientific data. NumPy is the foundation
               upon which the scientific Python ecosystem is constructed. It is
               so pervasive that several projects, targeting audiences with
               specialized needs, have developed their own NumPy-like
               interfaces and array objects. Owing to its central position in
               the ecosystem, NumPy increasingly acts as an interoperability
               layer between such array computation libraries and, together
               with its application programming interface (API), provides a
               flexible framework to support the next decade of scientific and
               industrial analysis.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  585,
  number    =  7825,
  pages     = "357--362",
  month     =  sep,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Windhager2011-ik,
  title     = "Geometric morphometrics of male facial shape in relation to
               physical strength and perceived attractiveness, dominance, and
               masculinity",
  author    = "Windhager, Sonja and Schaefer, Katrin and Fink, Bernhard",
  abstract  = "OBJECTIVES: Evolutionary psychologists claim that women have
               adaptive preferences for specific male physical traits. Physical
               strength may be one of those traits, because recent research
               suggests that women rate faces of physically strong men as more
               masculine, dominant, and attractive. Yet, previous research has
               been limited in its ability to statistically map specific male
               facial shapes and features to corresponding physical measures
               (e.g., strength) and ratings (e.g., attractiveness). METHODS:
               The association of handgrip strength (together with measures of
               shoulder width, body height, and body fat) and women's ratings
               of male faces (concerning dominance, masculinity, and
               attractiveness) were studied in a sample of 26 Caucasian men
               (aged 18-32 years). Geometric morphometrics was used to
               statistically assess the covariation of male facial shape with
               these measures. Statistical results were visualized with
               thin-plate spline deformation grids along with image unwarping
               and image averaging. RESULTS: Handgrip strength together with
               shoulder width, body fat, dominance, and masculinity loaded
               positively on the first dimension of covariation with facial
               shape (explaining 72.6\%, P < 0.05). These measures were related
               to rounder faces with wider eyebrows and a prominent jaw outline
               while highly attractive and taller men had longer, narrower jaws
               and wider/fuller lips. CONCLUSIONS: Male physical strength was
               more strongly associated with changes in face shape that relate
               to perceived masculinity and dominance than to attractiveness.
               Our study adds to the growing evidence that attractiveness and
               dominance/masculinity may reflect different aspects of male mate
               quality.",
  journal   = "Am. J. Hum. Biol.",
  publisher = "Wiley Online Library",
  volume    =  23,
  number    =  6,
  pages     = "805--814",
  month     =  nov,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Haines2019-iq,
  title     = "Using automated computer vision and machine learning to code
               facial expressions of affect and arousal: Implications for
               emotion dysregulation research",
  author    = "Haines, Nathaniel and Bell, Ziv and Crowell, Sheila and Hahn,
               Hunter and Kamara, Dana and McDonough-Caplan, Heather and
               Shader, Tiffany and Beauchaine, Theodore P",
  abstract  = "As early as infancy, caregivers' facial expressions shape
               children's behaviors, help them regulate their emotions, and
               encourage or dissuade their interpersonal agency. In childhood
               and adolescence, proficiencies in producing and decoding facial
               expressions promote social competence, whereas deficiencies
               characterize several forms of psychopathology. To date, however,
               studying facial expressions has been hampered by the
               labor-intensive, time-consuming nature of human coding. We
               describe a partial solution: automated facial expression coding
               (AFEC), which combines computer vision and machine learning to
               code facial expressions in real time. Although AFEC cannot
               capture the full complexity of human emotion, it codes positive
               affect, negative affect, and arousal-core Research Domain
               Criteria constructs-as accurately as humans, and it
               characterizes emotion dysregulation with greater specificity
               than other objective measures such as autonomic responding. We
               provide an example in which we use AFEC to evaluate emotion
               dynamics in mother-daughter dyads engaged in conflict. Among
               other findings, AFEC (a) shows convergent validity with a
               validated human coding scheme, (b) distinguishes among risk
               groups, and (c) detects developmental increases in positive
               dyadic affect correspondence as teen daughters age. Although
               more research is needed to realize the full potential of AFEC,
               findings demonstrate its current utility in research on emotion
               dysregulation.",
  journal   = "Dev. Psychopathol.",
  publisher = "ncbi.nlm.nih.gov",
  volume    =  31,
  number    =  3,
  pages     = "871--886",
  month     =  aug,
  year      =  2019,
  keywords  = "arousal; emotion dysregulation; facial expression; negative
               valence system; positive valence system",
  language  = "en"
}

@ARTICLE{Jack2017-qp,
  title    = "{Data-Driven} Methods to Diversify Knowledge of Human Psychology",
  author   = "Jack, Rachael and Crivelli, Carlos and Wheatley, Thalia",
  abstract = "Psychology aims to understand real human behavior. However,
              cultural biases in the scientific process can constrain
              knowledge. We describe here how data-driven methods can relax
              these constraints to reveal new insights that theories can
              overlook. To advance knowledge we advocate a symbiotic approach
              that better combines data-driven methods with theory.",
  journal  = "Trends Cogn. Sci.",
  month    =  nov,
  year     =  2017,
  keywords = "cultural psychology; data-driven methods; diversity;
              generalizability; human universals",
  language = "en"
}

@ARTICLE{Adams2012-dl,
  title     = "Emotion in the neutral face: a mechanism for impression
               formation?",
  author    = "Adams, Jr, Reginald B and Nelson, Anthony J and Soto, Jos{\'e} A
               and Hess, Ursula and Kleck, Robert E",
  abstract  = "The current work examined contributions of emotion-resembling
               facial cues to impression formation. There exist common facial
               cues that make people look emotional, male or female, and from
               which we derive personality inferences. We first conducted a
               Pilot Study to assess these effects. We found that neutral
               female versus neutral male faces were rated as more submissive,
               affiliative, na{\"\i}ve, honest, cooperative, babyish, fearful,
               happy, and less angry than neutral male faces. In our Primary
               Study, we then ``warped'' these same neutral faces over their
               corresponding anger and fear displays so the resultant facial
               appearance cues now structurally resembled emotion while
               retaining a neutral visage (e.g., no wrinkles, furrows, creases,
               etc.). The gender effects found in the Pilot Study were
               replicated in the Primary Study, suggesting clear
               stereotype-driven impressions. Critically, ratings of the
               neutral-over-fear warps versus neutral-over-anger warps also
               revealed a profile similar to the gender-based ratings,
               revealing perceptually driven impressions directly attributable
               to emotion overgeneralisation.",
  journal   = "Cogn. Emot.",
  publisher = "Taylor \& Francis",
  volume    =  26,
  number    =  3,
  pages     = "431--441",
  year      =  2012,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sahani2003-kk,
  title     = "How linear are auditory cortical responses?",
  author    = "Sahani, Maneesh and Linden, Jennifer F",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "MIT; 1998",
  pages     = "125--132",
  year      =  2003
}

@UNPUBLISHED{Liu2020-vo,
  title    = "Facial Expressions of Emotion Categories are Embedded within a
              Dimensional Space of Valence-arousal",
  author   = "Liu, Meng and Duan, Yaocong and Ince, Robin A A and Chen, Chaona
              and Garrod, Oliver G B and Schyns, Philippe and Jack, Rachael",
  abstract = "One of the longest standing debates in the emotion sciences is
              whether emotions are represented as discrete categories such as
              happy or sad or as continuous fundamental dimensions such as
              valence and arousal. Theories of communication make specific
              predictions about the facial expression signals that would
              represent emotions as either discrete or dimensional messages.
              Here, we address this debate by testing whether facial
              expressions of emotion categories are embedded in a dimensional
              space of affective signals, leading to multiplexed communication
              of affective information. Using a data-driven method based on
              human perception, we modelled the facial expressions representing
              the six classic emotion categories -- happy, surprise, fear,
              disgust, anger and sad -- and those representing the dimensions
              of valence and arousal. We then evaluated their embedding by
              mapping and validating the facial expressions categories onto the
              valence-arousal space. Results showed that facial expressions of
              these six classic emotion categories formed dissociable clusters
              within the valence-arousal space, each located in semantically
              congruent regions (e.g., happy facial expressions distributed in
              positively valenced regions). Crucially, we further demonstrated
              the generalization of the embedding beyond the six classic
              categories, using a broader set of 19 complex emotion categories
              (e.g., delighted, fury, and terrified). Together, our results
              show that facial expressions of emotion categories comprise
              specific combinations of valence and arousal related face
              movements, suggesting a multiplexed signalling of categorical and
              dimensional affective information. Our results unite current
              theories of emotion representation to form the basis of a new
              framework of multiplexed communication of affective information.",
  month    =  nov,
  year     =  2020,
  keywords = "affect; categorical; dimensional; Facial expressions;
              reverse-correlation"
}

@ARTICLE{Neth2009-eh,
  title     = "Emotion perception in emotionless face images suggests a
               norm-based representation",
  author    = "Neth, Donald and Martinez, Aleix M",
  abstract  = "Perception of facial expressions of emotion is generally assumed
               to correspond to underlying muscle movement. However, it is
               often observed that some individuals have sadder or angrier
               faces, even for neutral, motionless faces. Here, we report on
               one such effect caused by simple static configural changes. In
               particular, we show four variations in the relative vertical
               position of the nose, mouth, eyes, and eyebrows that affect the
               perception of emotion in neutral faces. The first two
               configurations make the vertical distance between the eyes and
               mouth shorter than average, resulting in the perception of an
               angrier face. The other two configurations make this distance
               larger than average, resulting in the perception of sadness.
               These perceptions increase with the amount of configural change,
               suggesting a representation based on variations from a norm
               (prototypical) face.",
  journal   = "J. Vis.",
  publisher = "jov.arvojournals.org",
  volume    =  9,
  number    =  1,
  pages     = "5.1--11",
  month     =  jan,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Wu2006-qs,
  title     = "Complete functional characterization of sensory neurons by
               system identification",
  author    = "Wu, Michael C-K and David, Stephen V and Gallant, Jack L",
  abstract  = "System identification is a growing approach to sensory
               neurophysiology that facilitates the development of quantitative
               functional models of sensory processing. This approach provides
               a clear set of guidelines for combining experimental data with
               other knowledge about sensory function to obtain a description
               that optimally predicts the way that neurons process sensory
               information. This prediction paradigm provides an objective
               method for evaluating and comparing computational models. In
               this chapter we review many of the system identification
               algorithms that have been used in sensory neurophysiology, and
               we show how they can be viewed as variants of a single
               statistical inference problem. We then review many of the
               practical issues that arise when applying these methods to
               neurophysiological experiments: stimulus selection, behavioral
               control, model visualization, and validation. Finally we discuss
               several problems to which system identification has been applied
               recently, including one important long-term goal of sensory
               neuroscience: developing models of sensory systems that
               accurately predict neuronal responses under completely natural
               conditions.",
  journal   = "Annu. Rev. Neurosci.",
  publisher = "annualreviews.org",
  volume    =  29,
  pages     = "477--505",
  year      =  2006,
  language  = "en"
}

@ARTICLE{Varoquaux2018-uo,
  title     = "Cross-validation failure: Small sample sizes lead to large error
               bars",
  author    = "Varoquaux, Ga{\"e}l",
  abstract  = "Predictive models ground many state-of-the-art developments in
               statistical brain image analysis: decoding, MVPA, searchlight,
               or extraction of biomarkers. The principled approach to
               establish their validity and usefulness is cross-validation,
               testing prediction on unseen data. Here, I would like to raise
               awareness on error bars of cross-validation, which are often
               underestimated. Simple experiments show that sample sizes of
               many neuroimaging studies inherently lead to large error bars,
               eg$\pm$10\% for 100 samples. The standard error across folds
               strongly underestimates them. These large error bars compromise
               the reliability of conclusions drawn with predictive models,
               such as biomarkers or methods developments where, unlike with
               cognitive neuroimaging MVPA approaches, more samples cannot be
               acquired by repeating the experiment across many subjects.
               Solutions to increase sample size must be investigated, tackling
               possible increases in heterogeneity of the data.",
  journal   = "Neuroimage",
  publisher = "Elsevier",
  volume    =  180,
  number    = "Pt A",
  pages     = "68--77",
  month     =  oct,
  year      =  2018,
  keywords  = "Biomarkers; Cross-validation; Decoding; MVPA; Model selection;
               Statistics; fMRI",
  language  = "en"
}

@ARTICLE{Jain2019-ob,
  title     = "Extended deep neural network for facial emotion recognition",
  author    = "Jain, Deepak Kumar and Shamsolmoali, Pourya and Sehdev, Paramjit",
  abstract  = "Humans use facial expressions to show their emotional states.
               However, facial expression recognition has remained a
               challenging and interesting problem in computer vision. In this
               paper we present our approach which is the extension of our
               previous work for facial emotion recognition [1]. The aim of
               this work is to classify each image into one of six facial
               emotion classes. The proposed model is based on single Deep
               Convolutional Neural Networks (DNNs), which contain convolution
               layers and deep residual blocks. In the proposed model, firstly
               the image label to all faces has been set for the training.
               Secondly, the images go through proposed DNN model. This model
               trained on two datasets Extended Cohn--Kanade (CK+) and Japanese
               Female Facial Expression (JAFFE) Dataset. The overall results
               show that, the proposed DNN model can outperform the recent
               state-of-the-art approaches for emotion recognition. Even the
               proposed model has accuracy improvement in comparison with our
               previous model.",
  journal   = "Pattern Recognit. Lett.",
  publisher = "Elsevier",
  volume    =  120,
  pages     = "69--74",
  month     =  apr,
  year      =  2019,
  keywords  = "Facial emotion recognition; Deep neural network; Fully
               convolution network"
}

@ARTICLE{Hofling2020-mk,
  title     = "Read My Face: Automatic Facial Coding Versus Psychophysiological
               Indicators of Emotional Valence and Arousal",
  author    = "H{\"o}fling, T Tim A and Gerdes, Antje B M and F{\"o}hl, Ulrich
               and Alpers, Georg W",
  abstract  = "Facial expressions provide insight into a person's emotional
               experience. To automatically decode these expressions has been
               made possible by tremendous progress in the field of computer
               vision. Researchers are now able to decode emotional facial
               expressions with impressive accuracy in standardized images of
               prototypical basic emotions. We tested the sensitivity of a
               well-established automatic facial coding software program to
               detect spontaneous emotional reactions in individuals responding
               to emotional pictures. We compared automatically generated
               scores for valence and arousal of the Facereader (FR; Noldus
               Information Technology) with the current psychophysiological
               gold standard of measuring emotional valence (Facial
               Electromyography, EMG) and arousal (Skin Conductance, SC). We
               recorded physiological and behavioral measurements of 43 healthy
               participants while they looked at pleasant, unpleasant, or
               neutral scenes. When viewing pleasant pictures, FR Valence and
               EMG were both comparably sensitive. However, for unpleasant
               pictures, FR Valence showed an expected negative shift, but the
               signal differentiated not well between responses to neutral and
               unpleasant stimuli, that were distinguishable with EMG.
               Furthermore, FR Arousal values had a stronger correlation with
               self-reported valence than with arousal while SC was sensitive
               and specifically associated with self-reported arousal. This is
               the first study to systematically compare FR measurement of
               spontaneous emotional reactions to standardized emotional images
               with established psychophysiological measurement tools. This
               novel technology has yet to make strides to surpass the
               sensitivity of established psychophysiological measures.
               However, it provides a promising new measurement technique for
               non-contact assessment of emotional responses.",
  journal   = "Front. Psychol.",
  publisher = "frontiersin.org",
  volume    =  11,
  pages     = "1388",
  month     =  jun,
  year      =  2020,
  keywords  = "automatic facial coding; emotion; facial electromyography;
               facial expression; skin conductance",
  language  = "en"
}

@ARTICLE{Neri1999-rj,
  title     = "Probing the human stereoscopic system with reverse correlation",
  author    = "Neri, P and Parker, A J and Blakemore, C",
  abstract  = "Our two eyes obtain slightly different views of the world. The
               resulting differences in the two retinal images, called
               binocular disparities, provide us with a stereoscopic sense of
               depth. The primary visual cortex (V1) contains neurons that are
               selective for the disparity of individual elements in an image,
               but this information must be further analysed to complete the
               stereoscopic process. Here we apply the psychophysical technique
               of reverse correlation to investigate disparity processing in
               human vision. Observers viewed binocular random-dot patterns,
               with 'signal' dots in a specific depth plane plus 'noise' dots
               with randomly assigned disparities. By examining the correlation
               between the observers' ability to detect the plane and the
               particular sample of 'noise' disparities presented on each
               trial, we revealed detection 'filters', whose disparity
               selectivity was remarkably similar to that of individual neurons
               in monkey V1. Moreover, if the noise dots were of opposite
               contrast in the two eyes, the tuning inverted, just like the
               response patterns of V1 neurons. Reverse correlation appears to
               probe disparity processing at the earliest stages of binocular
               combination, prior to the generation of a full stereoscopic
               depth percept.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  401,
  number    =  6754,
  pages     = "695--698",
  month     =  oct,
  year      =  1999,
  language  = "en"
}

@ARTICLE{Peirce2019-rj,
  title     = "{PsychoPy2}: Experiments in behavior made easy",
  author    = "Peirce, Jonathan and Gray, Jeremy R and Simpson, Sol and
               MacAskill, Michael and H{\"o}chenberger, Richard and Sogo,
               Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer",
  abstract  = "PsychoPy is an application for the creation of experiments in
               behavioral science (psychology, neuroscience, linguistics, etc.)
               with precise spatial control and timing of stimuli. It now
               provides a choice of interface; users can write scripts in
               Python if they choose, while those who prefer to construct
               experiments graphically can use the new Builder interface. Here
               we describe the features that have been added over the last 10
               years of its development. The most notable addition has been
               that Builder interface, allowing users to create studies with
               minimal or no programming, while also allowing the insertion of
               Python code for maximal flexibility. We also present some of the
               other new features, including further stimulus options,
               asynchronous time-stamped hardware polling, and better support
               for open science and reproducibility. Tens of thousands of users
               now launch PsychoPy every month, and more than 90 people have
               contributed to the code. We discuss the current state of the
               project, as well as plans for the future.",
  journal   = "Behav. Res. Methods",
  publisher = "Springer",
  volume    =  51,
  number    =  1,
  pages     = "195--203",
  month     =  feb,
  year      =  2019,
  keywords  = "Experiment; Open science; Open-source; Psychology; Reaction
               time; Software; Timing",
  language  = "en"
}

@ARTICLE{Yu2012-ag,
  title     = "Perception-driven facial expression synthesis",
  author    = "Yu, Hui and Garrod, Oliver G B and Schyns, Philippe G",
  abstract  = "We propose a novel platform to flexibly synthesize any arbitrary
               meaningful facial expression in the absence of actor performance
               data for that expression. With techniques from computer
               graphics, we synthesized random arbitrary dynamic facial
               expression animations. The synthesis was controlled by
               parametrically modulating Action Units (AUs) taken from the
               Facial Action Coding System (FACS). We presented these to human
               observers and instructed them to categorize the animations
               according to one of six possible facial expressions. With
               techniques from human psychophysics, we modeled the internal
               representation of these expressions for each observer, by
               extracting from the random noise the perceptually relevant
               expression parameters. We validated these models of facial
               expressions with naive observers.",
  journal   = "Comput. Graph.",
  publisher = "Elsevier",
  volume    =  36,
  number    =  3,
  pages     = "152--162",
  month     =  may,
  year      =  2012,
  keywords  = "Face; Facial expression; Emotion; Reverse correlation; 3D
               graphics; 3D animation"
}

@ARTICLE{Ekman1969-pu,
  title     = "Pan-cultural elements in facial displays of emotion",
  author    = "Ekman, P and Sorenson, E R and Friesen, W V",
  abstract  = "Observers in both literate and preliterate cultures chose the
               predicted emotion for photographs of the face, although
               agreement was higher in the literate samples. These findings
               suggest that the pan-cultural element in facial displays of
               emotion is the association between facial muscular movements and
               discrete primary emotions, although cultures may still differ in
               what evokes an emotion, in rules for controlling the display of
               emotion, and in behavioral consequences.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  164,
  number    =  3875,
  pages     = "86--88",
  month     =  apr,
  year      =  1969,
  language  = "en"
}

@ARTICLE{Salvatier2016-ko,
  title     = "Probabilistic programming in Python using {PyMC3}",
  author    = "Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher",
  abstract  = "Probabilistic programming allows for automatic Bayesian
               inference on user-defined probabilistic models. Recent advances
               in Markov chain Monte Carlo (MCMC) sampling allow inference on
               increasingly complex models. This class of MCMC, known as
               Hamiltonian Monte Carlo, requires gradient information which is
               often not readily available. PyMC3 is a new open source
               probabilistic programming framework written in Python that uses
               Theano to compute gradients via automatic differentiation as
               well as compile probabilistic programs on-the-fly to C for
               increased speed. Contrary to other probabilistic programming
               languages, PyMC3 allows model specification directly in Python
               code. The lack of a domain specific language allows for great
               flexibility and direct interaction with the model. This paper is
               a tutorial-style introduction to this software package.",
  journal   = "PeerJ Comput. Sci.",
  publisher = "PeerJ Inc.",
  volume    =  2,
  pages     = "e55",
  month     =  apr,
  year      =  2016,
  keywords  = "Bayesian statistic; Probabilistic Programming; Python; Markov
               chain Monte Carlo; Statistical modeling",
  language  = "en"
}

@ARTICLE{Jack2016-jq,
  title     = "Four not six: Revealing culturally common facial expressions of
               emotion",
  author    = "Jack, Rachael E and Sun, Wei and Delis, Ioannis and Garrod,
               Oliver G B and Schyns, Philippe G",
  abstract  = "As a highly social species, humans generate complex facial
               expressions to communicate a diverse range of emotions. Since
               Darwin's work, identifying among these complex patterns which
               are common across cultures and which are culture-specific has
               remained a central question in psychology, anthropology,
               philosophy, and more recently machine vision and social
               robotics. Classic approaches to addressing this question
               typically tested the cross-cultural recognition of theoretically
               motivated facial expressions representing 6 emotions, and
               reported universality. Yet, variable recognition accuracy across
               cultures suggests a narrower cross-cultural communication
               supported by sets of simpler expressive patterns embedded in
               more complex facial expressions. We explore this hypothesis by
               modeling the facial expressions of over 60 emotions across 2
               cultures, and segregating out the latent expressive patterns.
               Using a multidisciplinary approach, we first map the conceptual
               organization of a broad spectrum of emotion words by building
               semantic networks in 2 cultures. For each emotion word in each
               culture, we then model and validate its corresponding dynamic
               facial expression, producing over 60 culturally valid facial
               expression models. We then apply to the pooled models a
               multivariate data reduction technique, revealing 4 latent and
               culturally common facial expression patterns that each
               communicates specific combinations of valence, arousal, and
               dominance. We then reveal the face movements that accentuate
               each latent expressive pattern to create complex facial
               expressions. Our data questions the widely held view that 6
               facial expression patterns are universal, instead suggesting 4
               latent expressive patterns with direct implications for emotion
               communication, social psychology, cognitive neuroscience, and
               social robotics. (PsycINFO Database Record",
  journal   = "J. Exp. Psychol. Gen.",
  publisher = "psycnet.apa.org",
  volume    =  145,
  number    =  6,
  pages     = "708--730",
  month     =  jun,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Cowen2021-ld,
  title     = "Sixteen facial expressions occur in similar contexts worldwide",
  author    = "Cowen, Alan S and Keltner, Dacher and Schroff, Florian and Jou,
               Brendan and Adam, Hartwig and Prasad, Gautam",
  abstract  = "Understanding the degree to which human facial expressions
               co-vary with specific social contexts across cultures is central
               to the theory that emotions enable adaptive responses to
               important challenges and opportunities1-6. Concrete evidence
               linking social context to specific facial expressions is sparse
               and is largely based on survey-based approaches, which are often
               constrained by language and small sample sizes7-13. Here, by
               applying machine-learning methods to real-world, dynamic
               behaviour, we ascertain whether naturalistic social contexts
               (for example, weddings or sporting competitions) are associated
               with specific facial expressions14 across different cultures. In
               two experiments using deep neural networks, we examined the
               extent to which 16 types of facial expression occurred
               systematically in thousands of contexts in 6 million videos from
               144 countries. We found that each kind of facial expression had
               distinct associations with a set of contexts that were 70\%
               preserved across 12 world regions. Consistent with these
               associations, regions varied in how frequently different facial
               expressions were produced as a function of which contexts were
               most salient. Our results reveal fine-grained patterns in human
               facial expressions that are preserved across the modern world.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  589,
  number    =  7841,
  pages     = "251--257",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@article{snoek-submitted,
  title = "Explainable models of facial movements predict emotion perception behavior",
  author  = "Snoek, Lukas and Mittenbhler, Maximilian and Jack, Rachael and Schyns, Philippe and Fischer, Agneta and Scholte, H. Steven",
  year    = "\noop{3001}submitted"
}

@article{lage2019methods,
  title={Methods for computing the maximum performance of computational models of fMRI responses},
  author={Lage-Castellanos, Agustin and Valente, Giancarlo and Formisano, Elia and De Martino, Federico},
  journal={PLoS computational biology},
  volume={15},
  number={3},
  pages={e1006397},
  year={2019},
  publisher={Public Library of Science}
}