% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Cowen2020-pi,
  title    = "What the face displays: Mapping 28 emotions conveyed by
              naturalistic expression",
  author   = "Cowen, Alan S and Keltner, Dacher",
  abstract = "What emotions do the face and body express? Guided by new
              conceptual and quantitative approaches (Cowen, Elfenbein, Laukka,
              \& Keltner, 2018; Cowen \& Keltner, 2017, 2018), we explore the
              taxonomy of emotion recognized in facial-bodily expression.
              Participants (N = 1,794; 940 female, ages 18-76 years) judged the
              emotions captured in 1,500 photographs of facial-bodily
              expression in terms of emotion categories, appraisals, free
              response, and ecological validity. We find that facial-bodily
              expressions can reliably signal at least 28 distinct categories
              of emotion that occur in everyday life. Emotion categories, more
              so than appraisals such as valence and arousal, organize emotion
              recognition. However, categories of emotion recognized in
              naturalistic facial and bodily behavior are not discrete but
              bridged by smooth gradients that correspond to continuous
              variations in meaning. Our results support a novel view that
              emotions occupy a high-dimensional space of categories bridged by
              smooth gradients of meaning. They offer an approximation of a
              taxonomy of facial-bodily expressions, visualized within an
              online interactive map. (PsycInfo Database Record (c) 2020 APA,
              all rights reserved).",
  journal  = "Am. Psychol.",
  volume   =  75,
  number   =  3,
  pages    = "349--364",
  month    =  apr,
  year     =  2020,
  language = "en"
}

@ARTICLE{Oosterhof2009-mf,
  title     = "Shared perceptual basis of emotional expressions and
               trustworthiness impressions from faces",
  author    = "Oosterhof, Nikolaas N and Todorov, Alexander",
  abstract  = "Using a dynamic stimuli paradigm, in which faces expressed
               either happiness or anger, the authors tested the hypothesis
               that perceptions of trustworthiness are related to these
               expressions. Although the same emotional intensity was added to
               both trustworthy and untrustworthy faces, trustworthy faces who
               expressed happiness were perceived as happier than untrustworthy
               faces, and untrustworthy faces who expressed anger were
               perceived as angrier than trustworthy faces. The authors also
               manipulated changes in face trustworthiness simultaneously with
               the change in expression. Whereas transitions in face
               trustworthiness in the direction of the expressed emotion (e.g.,
               high-to-low trustworthiness and anger) increased the perceived
               intensity of the emotion, transitions in the opposite direction
               decreased this intensity. For example, changes from high to low
               trustworthiness increased the intensity of perceived anger but
               decreased the intensity of perceived happiness. These findings
               support the hypothesis that changes along the trustworthiness
               dimension correspond to subtle changes resembling expressions
               signaling whether the person displaying the emotion should be
               avoided or approached.",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  9,
  number    =  1,
  pages     = "128--133",
  month     =  feb,
  year      =  2009,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ekman1988-bi,
  title     = "Who knows what about contempt: A reply to Izard and Haynes",
  author    = "Ekman, Paul and Friesen, Wallace V",
  abstract  = "… or across cultures; does the expression we have identified as
               signaling contempt across cultures signal any other emotional
               state as clear- ly; is the relationship between expression and
               emotional state one-to-one, many - to - one , or one-to … The
               face of emotion … Human emotions …",
  journal   = "Motiv. Emot.",
  publisher = "Springer",
  volume    =  12,
  number    =  1,
  pages     = "17--22",
  year      =  1988
}

@ARTICLE{Friesen1978-tp,
  title     = "Facial action coding system: a technique for the measurement of
               facial movement",
  author    = "Friesen, Wallace and Ekman, Paul",
  journal   = "Palo Alto",
  publisher = "Consulting Psychologists Press",
  volume    =  3,
  year      =  1978
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hess1997-oa,
  title     = "The intensity of emotional facial expressions and decoding
               accuracy",
  author    = "Hess, Ursula and Blairy, Sylvie and Kleck, Robert E",
  abstract  = "The influence of the physical intensity of emotional facial
               expressions on perceived intensity and emotion category decoding
               accuracy was assessed for expressions of anger, disgust,
               sadness, and happiness. The facial expressions of two men and
               two women posing each of …",
  journal   = "J. Nonverbal Behav.",
  publisher = "Springer",
  volume    =  21,
  number    =  4,
  pages     = "241--257",
  year      =  1997
}

@ARTICLE{Wingenbach2018-wv,
  title     = "Sex differences in facial emotion recognition across varying
               expression intensity levels from videos",
  author    = "Wingenbach, Tanja S H and Ashwin, Chris and Brosnan, Mark",
  abstract  = "There has been much research on sex differences in the ability
               to recognise facial expressions of emotions, with results
               generally showing a female advantage in reading emotional
               expressions from the face. However, most of the research to date
               has used static images and/or 'extreme' examples of facial
               expressions. Therefore, little is known about how expression
               intensity and dynamic stimuli might affect the commonly reported
               female advantage in facial emotion recognition. The current
               study investigated sex differences in accuracy of response (Hu;
               unbiased hit rates) and response latencies for emotion
               recognition using short video stimuli (1sec) of 10 different
               facial emotion expressions (anger, disgust, fear, sadness,
               surprise, happiness, contempt, pride, embarrassment, neutral)
               across three variations in the intensity of the emotional
               expression (low, intermediate, high) in an adolescent and adult
               sample (N = 111; 51 male, 60 female) aged between 16 and 45 (M =
               22.2, SD = 5.7). Overall, females showed more accurate facial
               emotion recognition compared to males and were faster in
               correctly recognising facial emotions. The female advantage in
               reading expressions from the faces of others was unaffected by
               expression intensity levels and emotion categories used in the
               study. The effects were specific to recognition of emotions, as
               males and females did not differ in the recognition of neutral
               faces. Together, the results showed a robust sex difference
               favouring females in facial emotion recognition using video
               stimuli of a wide range of emotions and expression intensity
               variations.",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  13,
  number    =  1,
  pages     = "e0190634",
  month     =  jan,
  year      =  2018,
  language  = "en"
}

@INPROCEEDINGS{Greche2017-ps,
  title     = "Comparison between Euclidean and Manhattan distance measure for
               facial expressions classification",
  booktitle = "2017 International Conference on Wireless Technologies, Embedded
               and Intelligent Systems ({WITS})",
  author    = "Greche, L and Jazouli, M and Es-Sbai, N and Majda, A and
               Zarghili, A",
  abstract  = "In this paper, we compare classification results, of six facial
               expressions including joy, surprise, sadness, anger, disgust,
               and fear, relying on two different methods of distance computing
               between 121 landmark points on the face. Facial features were
               computed using L1 norm (Manhattan distance) in the first case
               and L2 norm (Euclidean distance) in the second case. Training
               and test data have been collected using kinect sensor. Labelled
               dataset contains sequences of 121 landmark points extracted from
               the face of each subject while displaying six facial expressions
               including joy, surprise, sadness, anger, disgust, and fear.
               Classification has been realized using multi-layer feed forward
               neural network with one hidden layer. Good recognition rates
               have been achieved in the early stages of training regarding
               Euclidean facial distances.",
  publisher = "ieeexplore.ieee.org",
  pages     = "1--4",
  month     =  apr,
  year      =  2017,
  keywords  = "face recognition;feature extraction;image classification;image
               sensors;Manhattan distance measure;Euclidean distance
               measure;facial expressions classification;distance
               computing;landmark points;facial features;kinect sensor;Face
               recognition;Neurons;Face;Biological neural
               networks;Training;Emotion recognition;Euclidean distance;facial
               expression recognition;Euclidean distance;Manhattan
               distance;neural network;kinect sensor"
}

@ARTICLE{Wiggers1982-na,
  title    = "Judgments of facial expressions of emotion predicted from facial
              behavior",
  author   = "Wiggers, Michiel",
  abstract = "An experiment was performed to assess whether Ekman and Friesen's
              Facial Action Coding System (FACS) could be used to construct
              facial expressions that portrayed with varying intensities each
              of the eight emotions of happiness, fear, disgust, sadness,
              surprise, shame, anger, and contempt. Based on detailed
              instructions from FACS, seven adults posed facial expressions
              that presumably varied in the conveyed emotion and emotion
              intensity. Thirty-nine college student observers then viewed each
              of the videotaped facial expressions. Ratings were made of
              whether each expression connoted one of the eight emotions or no
              emotion and of the intensity of the perceived emotion. Observers'
              emotion classification and intensity ratings agreed with
              FACS-based predictions regarding the facial action units involved
              in expressing each of the emotions. Most perceived-predicted
              emotion discrepancies could be accounted for by facial action
              units shared by the different emotions. Moreover, except for
              disgust, observers' intensity judgments reflected a reliance on
              only one or two action units for each emotion. These findings
              corroborate the descriptive and predictive utility of FACS for
              studies on perception of emotions.",
  journal  = "J. Nonverbal Behav.",
  volume   =  7,
  number   =  2,
  pages    = "101--116",
  month    =  dec,
  year     =  1982
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Punitha2013-jy,
  title     = "Texture based emotion recognition from facial expressions using
               support vector machine",
  author    = "Punitha, A and Geetha, M Kalaiselvi",
  abstract  = "… Texture analysis aims in finding a distinctive way of
               representing the essential characteristics of textures and
               represent them in some simpler but unique form, so that they can
               be used for robust, accurate classification.The texture features
               of face contain significant …",
  journal   = "Int. J. Comput. Appl.",
  publisher = "Citeseer",
  volume    =  80,
  number    =  5,
  pages     = "1--5",
  year      =  2013
}

@ARTICLE{Beer2009-hv,
  title     = "{EMOTION} {RECOGNITION} {OF} {VIRTUAL} {AGENTS} {FACIAL}
               {EXPRESSIONS}: {THE} {EFFECTS} {OF} {AGE} {AND} {EMOTION}
               {INTENSITY}",
  author    = "Beer, Jenay M and Fisk, Arthur D and Rogers, Wendy A",
  abstract  = "People make determinations about the social characteristics of
               an agent (e.g., robot or virtual agent) by interpreting social
               cues displayed by the agent, such as facial expressions.
               Although a considerable amount of research has been conducted
               investigating age-related differences in emotion recognition of
               human faces (e.g., Sullivan, \& Ruffman, 2004), the effect of
               age on emotion identification of virtual agent facial
               expressions has been largely unexplored. Age-related differences
               in emotion recognition of facial expressions are an important
               factor to consider in the design of agents that may assist older
               adults in a recreational or healthcare setting. The purpose of
               the current research was to investigate whether age-related
               differences in facial emotion recognition can extend to
               emotion-expressive virtual agents. Younger and older adults
               performed a recognition task with a virtual agent expressing six
               basic emotions. Larger age-related differences were expected for
               virtual agents displaying negative emotions, such as anger,
               sadness, and fear. In fact, the results indicated that older
               adults showed a decrease in emotion recognition accuracy for a
               virtual agent's emotions of anger, fear, and happiness.",
  journal   = "Proc. Hum. Factors Ergon. Soc. Annu. Meet.",
  publisher = "journals.sagepub.com",
  volume    =  53,
  number    =  2,
  pages     = "131--135",
  month     =  oct,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Westfall2016-wf,
  title     = "Fixing the stimulus-as-fixed-effect fallacy in task {fMRI}",
  author    = "Westfall, Jacob and Nichols, Thomas E and Yarkoni, Tal",
  abstract  = "Most functional magnetic resonance imaging (fMRI) experiments
               record the brain's responses to samples of stimulus materials
               (e.g., faces or words). Yet the statistical modeling approaches
               used in fMRI research universally fail to model stimulus
               variability in a manner that affords population generalization,
               meaning that researchers' conclusions technically apply only to
               the precise stimuli used in each study, and cannot be
               generalized to new stimuli. A direct consequence of this
               stimulus-as-fixed-effect fallacy is that the majority of
               published fMRI studies have likely overstated the strength of
               the statistical evidence they report. Here we develop a Bayesian
               mixed model (the random stimulus model; RSM) that addresses this
               problem, and apply it to a range of fMRI datasets. Results
               demonstrate considerable inflation (50-200\% in most of the
               studied datasets) of test statistics obtained from standard
               ``summary statistics''-based approaches relative to the
               corresponding RSM models. We demonstrate how RSMs can be used to
               improve parameter estimates, properly control false positive
               rates, and test novel research hypotheses about stimulus-level
               variability in human brain responses.",
  journal   = "Wellcome Open Res",
  publisher = "ncbi.nlm.nih.gov",
  volume    =  1,
  pages     = "23",
  month     =  dec,
  year      =  2016,
  keywords  = "Bayesian modeling; experimental design; functional magnetic
               resonance imaging; mixed-effect modeling; statistical modeling;
               stimulus-as-fixed-effect fallacy",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{McKinney2011-kl,
  title     = "pandas: a foundational Python library for data analysis and
               statistics",
  author    = "McKinney, Wes and {Others}",
  abstract  = "In this paper we will discuss pandas , a Python library of rich
               data structures and tools for working with structured data sets
               common to statistics, finance, social sciences, and many other
               fields. The library provides integrated, intuitive routines for
               performing common data …",
  journal   = "Python for High Performance and Scientific Computing",
  publisher = "Seattle",
  volume    =  14,
  number    =  9,
  year      =  2011
}

@ARTICLE{Orgeta2008-nn,
  title     = "Effects of age and emotional intensity on the recognition of
               facial emotion",
  author    = "Orgeta, Vasiliki and Phillips, Louise H",
  abstract  = "Older adults have a specific deficit in their ability to
               identify some negative facial emotions. The present study
               investigated the influence of intensity of expression on 40
               young and 40 older adults' recognition of facial expressions of
               emotion. Older adults showed no impairment in the perception of
               low-intensity subtle expressions of happiness, surprise, and
               disgust. However, older adults were worse at recognizing all
               intensities of sadness, anger, and fear, with the greatest
               impairment at 50\% intensity. Observed age differences were not
               influenced by covarying general facial processing skills, but
               were substantially reduced when a measure of general cognitive
               functioning was covaried. The current study suggests that age
               differences in identifying facial expressions of emotion are not
               caused by decreasing visual perceptual abilities, but may
               partially overlap with general cognitive changes.",
  journal   = "Exp. Aging Res.",
  publisher = "Taylor \& Francis",
  volume    =  34,
  number    =  1,
  pages     = "63--79",
  month     =  jan,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Neth2009-eh,
  title     = "Emotion perception in emotionless face images suggests a
               norm-based representation",
  author    = "Neth, Donald and Martinez, Aleix M",
  abstract  = "Perception of facial expressions of emotion is generally assumed
               to correspond to underlying muscle movement. However, it is
               often observed that some individuals have sadder or angrier
               faces, even for neutral, motionless faces. Here, we report on
               one such effect caused by simple static configural changes. In
               particular, we show four variations in the relative vertical
               position of the nose, mouth, eyes, and eyebrows that affect the
               perception of emotion in neutral faces. The first two
               configurations make the vertical distance between the eyes and
               mouth shorter than average, resulting in the perception of an
               angrier face. The other two configurations make this distance
               larger than average, resulting in the perception of sadness.
               These perceptions increase with the amount of configural change,
               suggesting a representation based on variations from a norm
               (prototypical) face.",
  journal   = "J. Vis.",
  publisher = "jov.arvojournals.org",
  volume    =  9,
  number    =  1,
  pages     = "5.1--11",
  month     =  jan,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Huth2012-yc,
  title     = "A continuous semantic space describes the representation of
               thousands of object and action categories across the human brain",
  author    = "Huth, Alexander G and Nishimoto, Shinji and Vu, An T and
               Gallant, Jack L",
  abstract  = "Humans can see and name thousands of distinct object and action
               categories, so it is unlikely that each category is represented
               in a distinct brain area. A more efficient scheme would be to
               represent categories as locations in a continuous semantic space
               mapped smoothly across the cortical surface. To search for such
               a space, we used fMRI to measure human brain activity evoked by
               natural movies. We then used voxelwise models to examine the
               cortical representation of 1,705 object and action categories.
               The first few dimensions of the underlying semantic space were
               recovered from the fit models by principal components analysis.
               Projection of the recovered semantic space onto cortical flat
               maps shows that semantic selectivity is organized into smooth
               gradients that cover much of visual and nonvisual cortex.
               Furthermore, both the recovered semantic space and the cortical
               organization of the space are shared across different
               individuals.",
  journal   = "Neuron",
  publisher = "Elsevier",
  volume    =  76,
  number    =  6,
  pages     = "1210--1224",
  month     =  dec,
  year      =  2012,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cohn2007-az,
  title     = "Observer-based measurement of facial expression with the Facial
               Action Coding System",
  author    = "Cohn, Jeffrey F and Ambadar, Zara and Ekman, Paul",
  abstract  = "Facial expression has been a focus of emotion research for over
               a hundred years (Darwin, 1872/1998). It is central to several
               leading theories of emotion (Ekman, 1992; Izard, 1977; Tomkins,
               1962) and has been the focus of at times heated debate about
               issues in emotion …",
  journal   = "The handbook of emotion elicitation and assessment",
  publisher = "Oxford University Press New York, Oxford",
  volume    =  1,
  number    =  3,
  pages     = "203--221",
  year      =  2007
}

@ARTICLE{Aviezer2008-fi,
  title     = "Angry, disgusted, or afraid? Studies on the malleability of
               emotion perception",
  author    = "Aviezer, Hillel and Hassin, Ran R and Ryan, Jennifer and Grady,
               Cheryl and Susskind, Josh and Anderson, Adam and Moscovitch,
               Morris and Bentin, Shlomo",
  abstract  = "Current theories of emotion perception posit that basic facial
               expressions signal categorically discrete emotions or affective
               dimensions of valence and arousal. In both cases, the
               information is thought to be directly ``read out'' from the face
               in a way that is largely immune to context. In contrast, the
               three studies reported here demonstrated that identical facial
               configurations convey strikingly different emotions and
               dimensional values depending on the affective context in which
               they are embedded. This effect is modulated by the similarity
               between the target facial expression and the facial expression
               typically associated with the context. Moreover, by monitoring
               eye movements, we demonstrated that characteristic fixation
               patterns previously thought to be determined solely by the
               facial expression are systematically modulated by emotional
               context already at very early stages of visual processing, even
               by the first time the face is fixated. Our results indicate that
               the perception of basic facial expressions is not context
               invariant and can be categorically altered by context at early
               perceptual levels.",
  journal   = "Psychol. Sci.",
  publisher = "journals.sagepub.com",
  volume    =  19,
  number    =  7,
  pages     = "724--732",
  month     =  jul,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Hess2009-xo,
  title     = "The face is not an empty canvas: how facial expressions interact
               with facial appearance",
  author    = "Hess, Ursula and Adams, Jr, Reginald B and Kleck, Robert E",
  abstract  = "Faces are not simply blank canvases upon which facial
               expressions write their emotional messages. In fact, facial
               appearance and facial movement are both important social
               signalling systems in their own right. We here provide multiple
               lines of evidence for the notion that the social signals derived
               from facial appearance on the one hand and facial movement on
               the other interact in a complex manner, sometimes reinforcing
               and sometimes contradicting one another. Faces provide
               information on who a person is. Sex, age, ethnicity, personality
               and other characteristics that can define a person and the
               social group the person belongs to can all be derived from the
               face alone. The present article argues that faces interact with
               the perception of emotion expressions because this information
               informs a decoder's expectations regarding an expresser's
               probable emotional reactions. Facial appearance also interacts
               more directly with the interpretation of facial movement because
               some of the features that are used to derive personality or sex
               information are also features that closely resemble certain
               emotional expressions, thereby enhancing or diluting the
               perceived strength of particular expressions.",
  journal   = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  publisher = "royalsocietypublishing.org",
  volume    =  364,
  number    =  1535,
  pages     = "3497--3504",
  month     =  dec,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Kay2013-ch,
  title     = "Compressive spatial summation in human visual cortex",
  author    = "Kay, Kendrick N and Winawer, Jonathan and Mezer, Aviv and
               Wandell, Brian A",
  abstract  = "Neurons within a small (a few cubic millimeters) region of
               visual cortex respond to stimuli within a restricted region of
               the visual field. Previous studies have characterized the
               population response of such neurons using a model that sums
               contrast linearly across the visual field. In this study, we
               tested linear spatial summation of population responses using
               blood oxygenation level-dependent (BOLD) functional MRI. We
               measured BOLD responses to a systematic set of contrast patterns
               and discovered systematic deviation from linearity: the data are
               more accurately explained by a model in which a compressive
               static nonlinearity is applied after linear spatial summation.
               We found that the nonlinearity is present in early visual areas
               (e.g., V1, V2) and grows more pronounced in relatively anterior
               extrastriate areas (e.g., LO-2, VO-2). We then analyzed the
               effect of compressive spatial summation in terms of changes in
               the position and size of a viewed object. Compressive spatial
               summation is consistent with tolerance to changes in position
               and size, an important characteristic of object representation.",
  journal   = "J. Neurophysiol.",
  publisher = "journals.physiology.org",
  volume    =  110,
  number    =  2,
  pages     = "481--494",
  month     =  jul,
  year      =  2013,
  keywords  = "fMRI; human visual cortex; population receptive field; spatial
               nonlinearity; spatial summation",
  language  = "en"
}

@ARTICLE{Jack2017-qp,
  title    = "{Data-Driven} Methods to Diversify Knowledge of Human Psychology",
  author   = "Jack, Rachael E and Crivelli, Carlos and Wheatley, Thalia",
  abstract = "Psychology aims to understand real human behavior. However,
              cultural biases in the scientific process can constrain
              knowledge. We describe here how data-driven methods can relax
              these constraints to reveal new insights that theories can
              overlook. To advance knowledge we advocate a symbiotic approach
              that better combines data-driven methods with theory.",
  journal  = "Trends Cogn. Sci.",
  month    =  nov,
  year     =  2017,
  keywords = "cultural psychology; data-driven methods; diversity;
              generalizability; human universals",
  language = "en"
}

@ARTICLE{Said2009-tf,
  title     = "Structural resemblance to emotional expressions predicts
               evaluation of emotionally neutral faces",
  author    = "Said, Christopher P and Sebe, Nicu and Todorov, Alexander",
  abstract  = "People make trait inferences based on facial appearance despite
               little evidence that these inferences accurately reflect
               personality. The authors tested the hypothesis that these
               inferences are driven in part by structural resemblance to
               emotional expressions. The authors first had participants judge
               emotionally neutral faces on a set of trait dimensions. The
               authors then submitted the face images to a Bayesian network
               classifier trained to detect emotional expressions. By using a
               classifier, the authors can show that neutral faces perceived to
               possess various personality traits contain objective resemblance
               to emotional expression. In general, neutral faces that are
               perceived to have positive valence resemble happiness, faces
               that are perceived to have negative valence resemble disgust and
               fear, and faces that are perceived to be threatening resemble
               anger. These results support the idea that trait inferences are
               in part the result of an overgeneralization of emotion
               recognition systems. Under this hypothesis, emotion recognition
               systems, which typically extract accurate information about a
               person's emotional state, are engaged during the perception of
               neutral faces that bear subtle resemblance to emotional
               expressions. These emotions could then be misattributed as
               traits.",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  9,
  number    =  2,
  pages     = "260--264",
  month     =  apr,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Jack2015-sh,
  title     = "The Human Face as a Dynamic Tool for Social Communication",
  author    = "Jack, Rachael E and Schyns, Philippe G",
  abstract  = "As a highly social species, humans frequently exchange social
               information to support almost all facets of life. One of the
               richest and most powerful tools in social communication is the
               face, from which observers can quickly and easily make a number
               of inferences - about identity, gender, sex, age, race,
               ethnicity, sexual orientation, physical health, attractiveness,
               emotional state, personality traits, pain or physical pleasure,
               deception, and even social status. With the advent of the
               digital economy, increasing globalization and cultural
               integration, understanding precisely which face information
               supports social communication and which produces
               misunderstanding is central to the evolving needs of modern
               society (for example, in the design of socially interactive
               digital avatars and companion robots). Doing so is challenging,
               however, because the face can be thought of as comprising a
               high-dimensional, dynamic information space, and this impacts
               cognitive science and neuroimaging, and their broader
               applications in the digital economy. New opportunities to
               address this challenge are arising from the development of new
               methods and technologies, coupled with the emergence of a modern
               scientific culture that embraces cross-disciplinary approaches.
               Here, we briefly review one such approach that combines
               state-of-the-art computer graphics, psychophysics and vision
               science, cultural psychology and social cognition, and highlight
               the main knowledge advances it has generated. In the light of
               current developments, we provide a vision of the future
               directions in the field of human facial communication within and
               across cultures.",
  journal   = "Curr. Biol.",
  publisher = "Elsevier",
  volume    =  25,
  number    =  14,
  pages     = "R621--34",
  month     =  jul,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Barrett2019-bc,
  title     = "Emotional Expressions Reconsidered: Challenges to Inferring
               Emotion From Human Facial Movements",
  author    = "Barrett, Lisa Feldman and Adolphs, Ralph and Marsella, Stacy and
               Martinez, Aleix M and Pollak, Seth D",
  abstract  = "It is commonly assumed that a person's emotional state can be
               readily inferred from his or her facial movements, typically
               called emotional expressions or facial expressions. This
               assumption influences legal judgments, policy decisions,
               national security protocols, and educational practices; guides
               the diagnosis and treatment of psychiatric illness, as well as
               the development of commercial applications; and pervades
               everyday social interactions as well as research in other
               scientific fields such as artificial intelligence, neuroscience,
               and computer vision. In this article, we survey examples of this
               widespread assumption, which we refer to as the common view, and
               we then examine the scientific evidence that tests this view,
               focusing on the six most popular emotion categories used by
               consumers of emotion research: anger, disgust, fear, happiness,
               sadness, and surprise. The available scientific evidence
               suggests that people do sometimes smile when happy, frown when
               sad, scowl when angry, and so on, as proposed by the common
               view, more than what would be expected by chance. Yet how people
               communicate anger, disgust, fear, happiness, sadness, and
               surprise varies substantially across cultures, situations, and
               even across people within a single situation. Furthermore,
               similar configurations of facial movements variably express
               instances of more than one emotion category. In fact, a given
               configuration of facial movements, such as a scowl, often
               communicates something other than an emotional state. Scientists
               agree that facial movements convey a range of information and
               are important for social communication, emotional or otherwise.
               But our review suggests an urgent need for research that
               examines how people actually move their faces to express
               emotions and other social information in the variety of contexts
               that make up everyday life, as well as careful study of the
               mechanisms by which people perceive instances of emotion in one
               another. We make specific research recommendations that will
               yield a more valid picture of how people move their faces to
               express emotions and how they infer emotional meaning from
               facial movements in situations of everyday life. This research
               is crucial to provide consumers of emotion research with the
               translational information they require.",
  journal   = "Psychol. Sci. Public Interest",
  publisher = "journals.sagepub.com",
  volume    =  20,
  number    =  1,
  pages     = "1--68",
  month     =  jul,
  year      =  2019,
  keywords  = "emotion perception; emotion recognition; emotional expression",
  language  = "en"
}

@ARTICLE{Du2014-sy,
  title     = "Compound facial expressions of emotion",
  author    = "Du, Shichuan and Tao, Yong and Martinez, Aleix M",
  abstract  = "Understanding the different categories of facial expressions of
               emotion regularly used by us is essential to gain insights into
               human cognition and affect as well as for the design of
               computational models and perceptual interfaces. Past research on
               facial expressions of emotion has focused on the study of six
               basic categories--happiness, surprise, anger, sadness, fear, and
               disgust. However, many more facial expressions of emotion exist
               and are used regularly by humans. This paper describes an
               important group of expressions, which we call compound emotion
               categories. Compound emotions are those that can be constructed
               by combining basic component categories to create new ones. For
               instance, happily surprised and angrily surprised are two
               distinct compound emotion categories. The present work defines
               21 distinct emotion categories. Sample images of their facial
               expressions were collected from 230 human subjects. A Facial
               Action Coding System analysis shows the production of these 21
               categories is different but consistent with the subordinate
               categories they represent (e.g., a happily surprised expression
               combines muscle movements observed in happiness and surprised).
               We show that these differences are sufficient to distinguish
               between the 21 defined categories. We then use a computational
               model of face perception to demonstrate that most of these
               categories are also visually discriminable from one another.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  111,
  number    =  15,
  pages     = "E1454--62",
  month     =  apr,
  year      =  2014,
  keywords  = "action units; categorization; face recognition",
  language  = "en"
}

@ARTICLE{Hebart2020-wp,
  title     = "Revealing the multidimensional mental representations of natural
               objects underlying human similarity judgements",
  author    = "Hebart, Martin N and Zheng, Charles Y and Pereira, Francisco and
               Baker, Chris I",
  abstract  = "Objects can be characterized according to a vast number of
               possible criteria (such as animacy, shape, colour and function),
               but some dimensions are more useful than others for making sense
               of the objects around us. To identify these core dimensions of
               object representations, we developed a data-driven computational
               model of similarity judgements for real-world images of 1,854
               objects. The model captured most explainable variance in
               similarity judgements and produced 49 highly reproducible and
               meaningful object dimensions that reflect various conceptual and
               perceptual properties of those objects. These dimensions
               predicted external categorization behaviour and reflected
               typicality judgements of those categories. Furthermore, humans
               can accurately rate objects along these dimensions, highlighting
               their interpretability and opening up a way to generate
               similarity estimates from object dimensions alone. Collectively,
               these results demonstrate that human similarity judgements can
               be captured by a fairly low-dimensional, interpretable embedding
               that generalizes to external behaviour.",
  journal   = "Nat Hum Behav",
  publisher = "nature.com",
  volume    =  4,
  number    =  11,
  pages     = "1173--1185",
  month     =  nov,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Aviezer2017-wa,
  title     = "The inherently contextualized nature of facial emotion
               perception",
  author    = "Aviezer, Hillel and Ensenberg, Noga and Hassin, Ran R",
  abstract  = "According to mainstream views of emotion perception, facial
               expressions are powerful signals conveying specific emotional
               states. This approach, which endorsed the use of
               stereotypical-posed faces as stimuli, has typically ignored the
               role of context in emotion perception. We argue that this
               methodological tradition is flawed. Real-life facial expressions
               are often highly ambiguous, heavily relying on contextual
               information. We review recent work suggesting that context is an
               inherent part of real-life emotion perception, often leading to
               radical categorical changes. Contextual effects are not an
               obscurity at the fringe of facial emotion perception, rather,
               they are part of emotion perception itself.",
  journal   = "Curr Opin Psychol",
  publisher = "Elsevier",
  volume    =  17,
  pages     = "47--54",
  month     =  oct,
  year      =  2017,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Delis2016-zl,
  title     = "Space-by-time manifold representation of dynamic facial
               expressions for emotion categorization",
  author    = "Delis, Ioannis and Chen, Chaona and Jack, Rachael E and Garrod,
               Oliver G B and Panzeri, Stefano and Schyns, Philippe G",
  abstract  = "Visual categorization is the brain computation that reduces
               high-dimensional information in the visual environment into a
               smaller set of meaningful categories. An important problem in
               visual neuroscience is to identify the visual information that
               the brain must represent and …",
  journal   = "J. Vis.",
  publisher = "The Association for Research in Vision and Ophthalmology",
  volume    =  16,
  number    =  8,
  pages     = "14--14",
  year      =  2016
}

@ARTICLE{Matsumoto2008-qk,
  title     = "Facial expressions of emotion",
  author    = "Matsumoto, David and Keltner, Dacher and Shiota, Michelle N and
               O'Sullivan, Maureen and Frank, Mark",
  abstract  = "Within the field of emotion, the study of facial expressions has
               been notable both for empirical advances and for theoretical
               controversy. In this chapter, we draw upon an ``evolutionist''
               approach to emotion, inspired by Charles Darwin, to draw
               together recent studies of facial expression. This literature
               indicates that facial expressions of emotion, as described by
               Darwin over 135 years ago, (1) include universal, reliable
               markers of discrete emotions when emotions are aroused and there
               is no reason to modify or manage the expression; (2) covary with
               distinct subjective experience; (3) are part of a coherent
               package of emotion responses that includes appraisals,
               physiological reactions, other nonverbal behaviors, and
               subsequent actions, as well as individual differences and mental
               and physical health; (4) are judged as discrete categories; and
               (5) as such, serve many interpersonal and social regulatory
               functions. (PsycINFO Database Record (c) 2019 APA, all rights
               reserved)",
  journal   = "Handbook of emotions., 3rd ed.",
  publisher = "The Guilford Press, xvi",
  volume    =  3,
  pages     = "211--234",
  year      =  2008,
  address   = "New York, NY, US"
}

@ARTICLE{Craig2018-jm,
  title     = "The influence of multiple social categories on emotion
               perception",
  author    = "Craig, Belinda M and Lipp, Ottmar V",
  abstract  = "Although the human face provides multiple sources of social
               information concurrently (race, sex, age, etc.), the majority of
               studies investigating how social category cues influence
               emotional expression perception have investigated the influence
               of only one social category at a time. Only a couple of studies
               have investigated how race and sex cues concurrently influence
               emotion perception and these studies have produced mixed
               results. In addition, the concurrent influence of age and sex
               cues on emotion perception has not been investigated. To address
               this, participants categorized happy and angry expressions on
               faces varying in race (Black and White) and sex (Experiments 1a
               and 1b) or age (older adult and young adult) and sex (Experiment
               2). In Experiments 1a and 1b, results indicated that sex but not
               race influenced emotion categorization. Participants were, on
               average, faster to categorize happiness than anger on female,
               but not on male faces. In Experiment 2, both the age and the sex
               of the face independently influenced emotion categorization.
               Participants were faster to categorize happiness than anger on
               female and young adult faces, but not on male or older adult
               faces. Bayesian ANOVAs provided additional evidence that the sex
               of the face had the strongest influence on emotion
               categorization speeds in Experiment 1a and 1b, but both age and
               sex cues had an equal influence on emotion categorization in
               Experiment 2.",
  journal   = "J. Exp. Soc. Psychol.",
  publisher = "Elsevier",
  volume    =  75,
  pages     = "27--35",
  month     =  mar,
  year      =  2018,
  keywords  = "Emotion recognition; Social categorization; Race; Sex; Age"
}

@ARTICLE{Ruch1995-fi,
  title     = "Will the real relationship between facial expression and
               affective experience please stand up: The case of exhilaration",
  author    = "Ruch, Willibald",
  abstract  = "Abstract It is hypothesised that the empirical correlation
               between facial expression and affective experience varies as a
               function of the correlational design used to compute the
               coefficients. Predictions about the rank order of five designs
               were derived based on two assumptions. Female subjects were
               placed into one of three alcohol conditions (no ethanol, low
               dose, high dose) and were exposed to 30 slides containing jokes
               or cartoons. The degree of rated funniness and overt behaviour
               were intercorrelated using five different designs to analyse the
               same set of data. The results show that within-subject analyses
               yielded higher coefficients than between-subjects analyses.
               Aggregation of data increased the coefficients for
               within-subject analyses, but not for between-subject analyses. A
               cheerful mood was associated with hyper-expressiveness, i.e. the
               occurrence of smiling and laughter at relatively low levels of
               perceived funniness. It was demonstrated that low correlations
               between facial expression and affective experience may be based
               on several method artefacts.",
  journal   = "Cognition and Emotion",
  publisher = "Routledge",
  volume    =  9,
  number    =  1,
  pages     = "33--58",
  month     =  jan,
  year      =  1995
}

@article{lage2019methods,
  title={Methods for computing the maximum performance of computational models of fMRI responses},
  author={Lage-Castellanos, Agustin and Valente, Giancarlo and Formisano, Elia and De Martino, Federico},
  journal={PLoS computational biology},
  volume={15},
  number={3},
  pages={e1006397},
  year={2019},
  publisher={Public Library of Science}
}

@ARTICLE{Kriegeskorte2009-yz,
  title     = "Circular analysis in systems neuroscience: the dangers of double
               dipping",
  author    = "Kriegeskorte, Nikolaus and Simmons, W Kyle and Bellgowan,
               Patrick S F and Baker, Chris I",
  abstract  = "A neuroscientific experiment typically generates a large amount
               of data, of which only a small fraction is analyzed in detail
               and presented in a publication. However, selection among noisy
               measurements can render circular an otherwise appropriate
               analysis and invalidate results. Here we argue that systems
               neuroscience needs to adjust some widespread practices to avoid
               the circularity that can arise from selection. In particular,
               'double dipping', the use of the same dataset for selection and
               selective analysis, will give distorted descriptive statistics
               and invalid statistical inference whenever the results
               statistics are not inherently independent of the selection
               criteria under the null hypothesis. To demonstrate the problem,
               we apply widely used analyses to noise data known to not contain
               the experimental effects in question. Spurious effects can
               appear in the context of both univariate activation analysis and
               multivariate pattern-information analysis. We suggest a policy
               for avoiding circularity.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  12,
  number    =  5,
  pages     = "535--540",
  month     =  may,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Craig2018-jm,
  title     = "The influence of multiple social categories on emotion
               perception",
  author    = "Craig, Belinda M and Lipp, Ottmar V",
  abstract  = "Although the human face provides multiple sources of social
               information concurrently (race, sex, age, etc.), the majority of
               studies investigating how social category cues influence
               emotional expression perception have investigated the influence
               of only one social category at a time. Only a couple of studies
               have investigated how race and sex cues concurrently influence
               emotion perception and these studies have produced mixed
               results. In addition, the concurrent influence of age and sex
               cues on emotion perception has not been investigated. To address
               this, participants categorized happy and angry expressions on
               faces varying in race (Black and White) and sex (Experiments 1a
               and 1b) or age (older adult and young adult) and sex (Experiment
               2). In Experiments 1a and 1b, results indicated that sex but not
               race influenced emotion categorization. Participants were, on
               average, faster to categorize happiness than anger on female,
               but not on male faces. In Experiment 2, both the age and the sex
               of the face independently influenced emotion categorization.
               Participants were faster to categorize happiness than anger on
               female and young adult faces, but not on male or older adult
               faces. Bayesian ANOVAs provided additional evidence that the sex
               of the face had the strongest influence on emotion
               categorization speeds in Experiment 1a and 1b, but both age and
               sex cues had an equal influence on emotion categorization in
               Experiment 2.",
  journal   = "J. Exp. Soc. Psychol.",
  publisher = "Elsevier",
  volume    =  75,
  pages     = "27--35",
  month     =  mar,
  year      =  2018,
  keywords  = "Emotion recognition; Social categorization; Race; Sex; Age"
}

@ARTICLE{Hess2004-al,
  title     = "Facial appearance, gender, and emotion expression",
  author    = "Hess, Ursula and Adams, Jr, Reginald B and Kleck, Robert E",
  abstract  = "Western gender stereotypes describe women as affiliative and
               more likely to show happiness and men as dominant and more
               likely to show anger. The authors assessed the hypothesis that
               the gender-stereotypic effects on perceptions of anger and
               happiness are partially mediated by facial appearance markers of
               dominance and affiliation by equating men's and women's faces
               for these cues. In 2 studies, women were rated as more angry and
               men as more happy-a reversal of the stereotype. Ratings of
               sadness, however, were not systematically affected. It is
               posited that markers of affiliation and dominance, themselves
               confounded with gender, interact with the expressive cues for
               anger and happiness to produce emotional perceptions that have
               been viewed as simple gender stereotypes.",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  4,
  number    =  4,
  pages     = "378--388",
  month     =  dec,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Azari2020-fi,
  title     = "Comparing supervised and unsupervised approaches to emotion
               categorization in the human brain, body, and subjective
               experience",
  author    = "Azari, Bahar and Westlin, Christiana and Satpute, Ajay B and
               Hutchinson, J Benjamin and Kragel, Philip A and Hoemann, Katie
               and Khan, Zulqarnain and Wormwood, Jolie B and Quigley, Karen S
               and Erdogmus, Deniz and Dy, Jennifer and Brooks, Dana H and
               Barrett, Lisa Feldman",
  abstract  = "Machine learning methods provide powerful tools to map physical
               measurements to scientific categories. But are such methods
               suitable for discovering the ground truth about psychological
               categories? We use the science of emotion as a test case to
               explore this question. In studies of emotion, researchers use
               supervised classifiers, guided by emotion labels, to attempt to
               discover biomarkers in the brain or body for the corresponding
               emotion categories. This practice relies on the assumption that
               the labels refer to objective categories that can be discovered.
               Here, we critically examine this approach across three distinct
               datasets collected during emotional episodes-measuring the human
               brain, body, and subjective experience-and compare supervised
               classification solutions with those from unsupervised clustering
               in which no labels are assigned to the data. We conclude with a
               set of recommendations to guide researchers towards meaningful,
               data-driven discoveries in the science of emotion and beyond.",
  journal   = "Sci. Rep.",
  publisher = "nature.com",
  volume    =  10,
  number    =  1,
  pages     = "20284",
  month     =  nov,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Jack2014-ku,
  title     = "Dynamic facial expressions of emotion transmit an evolving
               hierarchy of signals over time",
  author    = "Jack, Rachael E and Garrod, Oliver G B and Schyns, Philippe G",
  abstract  = "Designed by biological and social evolutionary pressures, facial
               expressions of emotion comprise specific facial movements to
               support a near-optimal system of signaling and decoding.
               Although highly dynamical, little is known about the form and
               function of facial expression temporal dynamics. Do facial
               expressions transmit diagnostic signals simultaneously to
               optimize categorization of the six classic emotions, or
               sequentially to support a more complex communication system of
               successive categorizations over time? Our data support the
               latter. Using a combination of perceptual expectation modeling,
               information theory, and Bayesian classifiers, we show that
               dynamic facial expressions of emotion transmit an evolving
               hierarchy of ``biologically basic to socially specific''
               information over time. Early in the signaling dynamics, facial
               expressions systematically transmit few, biologically rooted
               face signals supporting the categorization of fewer elementary
               categories (e.g., approach/avoidance). Later transmissions
               comprise more complex signals that support categorization of a
               larger number of socially specific categories (i.e., the six
               classic emotions). Here, we show that dynamic facial expressions
               of emotion provide a sophisticated signaling system, questioning
               the widely accepted notion that emotion communication is
               comprised of six basic (i.e., psychologically irreducible)
               categories, and instead suggesting four.",
  journal   = "Curr. Biol.",
  publisher = "Elsevier",
  volume    =  24,
  number    =  2,
  pages     = "187--192",
  month     =  jan,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Jack2017-gt,
  title     = "Toward a Social Psychophysics of Face Communication",
  author    = "Jack, Rachael E and Schyns, Philippe G",
  abstract  = "As a highly social species, humans are equipped with a powerful
               tool for social communication-the face. Although seemingly
               simple, the human face can elicit multiple social perceptions
               due to the rich variations of its movements, morphology, and
               complexion. Consequently, identifying precisely what face
               information elicits different social perceptions is a complex
               empirical challenge that has largely remained beyond the reach
               of traditional methods. In the past decade, the emerging field
               of social psychophysics has developed new methods to address
               this challenge, with the potential to transfer psychophysical
               laws of social perception to the digital economy via avatars and
               social robots. At this exciting juncture, it is timely to review
               these new methodological developments. In this article, we
               introduce and review the foundational methodological
               developments of social psychophysics, present work done in the
               past decade that has advanced understanding of the face as a
               tool for social communication, and discuss the major challenges
               that lie ahead.",
  journal   = "Annu. Rev. Psychol.",
  publisher = "annualreviews.org",
  volume    =  68,
  pages     = "269--297",
  month     =  jan,
  year      =  2017,
  keywords  = "culture; facial expressions; reverse correlation; social
               communication; social psychophysics",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mauss2009-hm,
  title     = "Measures of emotion: A review",
  author    = "Mauss, I B and Robinson, M D",
  abstract  = "A consensual, componential model of emotions conceptualises them
               as experiential, physiological, and behavioural responses to
               personally meaningful stimuli. The present review examines this
               model in terms of whether different types of emotion -evocative
               stimuli …",
  journal   = "Cognition and emotion",
  publisher = "Taylor \& Francis",
  volume    =  23,
  pages     = "209--237",
  year      =  2009
}

@ARTICLE{Harris2020-en,
  title     = "Array programming with {NumPy}",
  author    = "Harris, Charles R and Millman, K Jarrod and van der Walt,
               St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and
               Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg,
               Sebastian and Smith, Nathaniel J and Kern, Robert and Picus,
               Matti and Hoyer, Stephan and van Kerkwijk, Marten H and Brett,
               Matthew and Haldane, Allan and Del R{\'\i}o, Jaime Fern{\'a}ndez
               and Wiebe, Mark and Peterson, Pearu and G{\'e}rard-Marchant,
               Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser,
               Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant,
               Travis E",
  abstract  = "Array programming provides a powerful, compact and expressive
               syntax for accessing, manipulating and operating on data in
               vectors, matrices and higher-dimensional arrays. NumPy is the
               primary array programming library for the Python language. It
               has an essential role in research analysis pipelines in fields
               as diverse as physics, chemistry, astronomy, geoscience,
               biology, psychology, materials science, engineering, finance and
               economics. For example, in astronomy, NumPy was an important
               part of the software stack used in the discovery of
               gravitational waves1 and in the first imaging of a black hole2.
               Here we review how a few fundamental array concepts lead to a
               simple and powerful programming paradigm for organizing,
               exploring and analysing scientific data. NumPy is the foundation
               upon which the scientific Python ecosystem is constructed. It is
               so pervasive that several projects, targeting audiences with
               specialized needs, have developed their own NumPy-like
               interfaces and array objects. Owing to its central position in
               the ecosystem, NumPy increasingly acts as an interoperability
               layer between such array computation libraries and, together
               with its application programming interface (API), provides a
               flexible framework to support the next decade of scientific and
               industrial analysis.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  585,
  number    =  7825,
  pages     = "357--362",
  month     =  sep,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Yu2012-ag,
  title     = "Perception-driven facial expression synthesis",
  author    = "Yu, Hui and Garrod, Oliver G B and Schyns, Philippe G",
  abstract  = "We propose a novel platform to flexibly synthesize any arbitrary
               meaningful facial expression in the absence of actor performance
               data for that expression. With techniques from computer
               graphics, we synthesized random arbitrary dynamic facial
               expression animations. The synthesis was controlled by
               parametrically modulating Action Units (AUs) taken from the
               Facial Action Coding System (FACS). We presented these to human
               observers and instructed them to categorize the animations
               according to one of six possible facial expressions. With
               techniques from human psychophysics, we modeled the internal
               representation of these expressions for each observer, by
               extracting from the random noise the perceptually relevant
               expression parameters. We validated these models of facial
               expressions with naive observers.",
  journal   = "Comput. Graph.",
  publisher = "Elsevier",
  volume    =  36,
  number    =  3,
  pages     = "152--162",
  month     =  may,
  year      =  2012,
  keywords  = "Face; Facial expression; Emotion; Reverse correlation; 3D
               graphics; 3D animation"
}

@ARTICLE{Parmley2014-nj,
  title     = "She looks sad, but he looks mad: the effects of age, gender, and
               ambiguity on emotion perception",
  author    = "Parmley, Maria and Cunningham, Joseph G",
  abstract  = "This study investigated how target sex, target age, and
               expressive ambiguity influence emotion perception. Undergraduate
               participants (N = 192) watched morphed video clips of eight
               child and eight adult facial expressions shifting from neutral
               to either sadness or anger. Participants were asked to stop the
               video clip when they first saw an emotion appear (perceptual
               sensitivity) and were asked to identify the emotion that they
               saw (accuracy). Results indicate that female participants
               identified sad expressions sooner in female targets than in male
               targets. Participants were also more accurate identifying angry
               facial expressions by male children than by female children.
               Findings are discussed in terms of the effects of ambiguity,
               gender, and age on the perception of emotional expressions.",
  journal   = "J. Soc. Psychol.",
  publisher = "Taylor \& Francis",
  volume    =  154,
  number    =  4,
  pages     = "323--338",
  month     =  jul,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Yarkoni2017-om,
  title     = "Choosing Prediction Over Explanation in Psychology: Lessons From
               Machine Learning",
  author    = "Yarkoni, Tal and Westfall, Jacob",
  abstract  = "Psychology has historically been concerned, first and foremost,
               with explaining the causal mechanisms that give rise to
               behavior. Randomized, tightly controlled experiments are
               enshrined as the gold standard of psychological research, and
               there are endless investigations of the various mediating and
               moderating variables that govern various behaviors. We argue
               that psychology's near-total focus on explaining the causes of
               behavior has led much of the field to be populated by research
               programs that provide intricate theories of psychological
               mechanism but that have little (or unknown) ability to predict
               future behaviors with any appreciable accuracy. We propose that
               principles and techniques from the field of machine learning can
               help psychology become a more predictive science. We review some
               of the fundamental concepts and tools of machine learning and
               point out examples where these concepts have been used to
               conduct interesting and important psychological research that
               focuses on predictive research questions. We suggest that an
               increased focus on prediction, rather than explanation, can
               ultimately lead us to greater understanding of behavior.",
  journal   = "Perspect. Psychol. Sci.",
  publisher = "pilab.psy.utexas.edu",
  pages     = "1745691617693393",
  month     =  aug,
  year      =  2017,
  keywords  = "explanation; machine learning; prediction;NeuroimagingMethods",
  language  = "en"
}

@ARTICLE{Rosenberg1994-oz,
  title     = "Coherence between expressive and experiential systems in emotion",
  author    = "Rosenberg, Erika L and Ekman, Paul",
  abstract  = "Abstract In order to assess the extent of coherence in emotional
               response systems, we examined the relationship between facial
               expression and self-report of emotion at multiple points in time
               during an affective episode. We showed subjects brief films that
               were selected for their ability to elicit disgust and fear, and
               we asked them to report on their emotions using a new reporting
               procedure. This procedure, called cued-review, allows subjects
               to rate the degree to which they experienced each of several
               categories of emotion for many locations over the time interval
               of a stimulus period. When facial expressions and reports of
               emotion were analysed for specific moments in film time, there
               was a high degree of temporal linkage and categorical agreement
               between facial expression and self-report, as predicted.
               Coherence was even stronger for more intense emotional events.
               This is the first evidence of linkage between facial expression
               and self-report of emotion on a momentary basis.",
  journal   = "null",
  publisher = "Routledge",
  volume    =  8,
  number    =  3,
  pages     = "201--229",
  month     =  may,
  year      =  1994
}

@ARTICLE{Fabiano2020-qv,
  title         = "Impact of multiple modalities on emotion recognition:
                   investigation into 3d facial landmarks, action units, and
                   physiological data",
  author        = "Fabiano, Diego and Jaishanker, Manikandan and Canavan, Shaun",
  abstract      = "To fully understand the complexities of human emotion, the
                   integration of multiple physical features from different
                   modalities can be advantageous. Considering this, we present
                   an analysis of 3D facial data, action units, and
                   physiological data as it relates to their impact on emotion
                   recognition. We analyze each modality independently, as well
                   as the fusion of each for recognizing human emotion. This
                   analysis includes which features are most important for
                   specific emotions (e.g. happy). Our analysis indicates that
                   both 3D facial landmarks and physiological data are
                   encouraging for expression/emotion recognition. On the other
                   hand, while action units can positively impact emotion
                   recognition when fused with other modalities, the results
                   suggest it is difficult to detect emotion using them in a
                   unimodal fashion.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2005.08341"
}

@ARTICLE{Izard1994-ca,
  title     = "Innate and universal facial expressions: evidence from
               developmental and cross-cultural research",
  author    = "Izard, C E",
  abstract  = "The idea of innate and universal facial expressions that have
               links with human emotions was given the status of scientific
               hypothesis by Darwin (1872/1965). Substantial evidence, old and
               new, supports his hypothesis. Much of the evidence is
               independent of language, but Russell's (1994) criticisms of the
               hypothesis focus on language-dependent data. In this article, it
               is argued that Russell's critique was off target in that his
               arguments relate only to a hypothesis of the universality of
               semantic attributions and overstated in that he used
               questionable logic in designing studies to support his claims.
               It is also argued that Russell misinterpreted the relation
               between the universality hypothesis and differential emotions
               theory. Finally, new evidence is presented that supports the
               Darwinian hypothesis of the innateness and universality of the
               facial expressions of a limited set of emotions and the efficacy
               of the most commonly used method of testing it.",
  journal   = "Psychol. Bull.",
  publisher = "psycnet.apa.org",
  volume    =  115,
  number    =  2,
  pages     = "288--299",
  month     =  mar,
  year      =  1994,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ekman1997-bk,
  title     = "Universal facial expressions of emotion",
  author    = "Ekman, Paul and Keltner, Dacher",
  abstract  = "The culture speciﬁc view, that facial behaviors are associated
               with emotion through culturally variable learning, received
               support from Klineberg's (1938) descriptions of how the facial
               behaviors described in Chinese literature diﬁered from the
               facial behaviors associated with …",
  journal   = "Segerstrale U, P. Molnar P, eds. Nonverbal communication: Where
               nature meets culture",
  publisher = "paulekman.com",
  pages     = "27--46",
  year      =  1997
}

@ARTICLE{Greenaway2018-rk,
  title     = "Context is Everything (in Emotion Research)",
  author    = "Greenaway, Katharine H and Kalokerinos, Elise K and Williams,
               Lisa A",
  abstract  = "Abstract As in many areas of psychological inquiry, context
               matters for how emotion is experienced, expressed, perceived,
               and regulated. While this may sound like a truism, emotion
               research does not always directly theorize, manipulate, or
               measure emotion with context in mind. To facilitate this
               process, we present a framework of contextual features that
               shape emotion-related processes, and highlight several key
               factors that have been shown to matter in emotion research. We
               make four recommendations which we believe will help to better
               integrate context in emotion science. We argue that a deeper
               collective understanding, interrogation, and integration of
               context will propel the field forward theoretically and
               methodologically, and enhance researchers' ability to probe the
               mechanisms of human psychological experience. While our focus is
               on emotion research, we believe that the context framework and
               associated recommendations will also be useful to other fields
               of social psychological and personality science.",
  journal   = "Soc. Personal. Psychol. Compass",
  publisher = "Wiley",
  volume    =  12,
  number    =  6,
  pages     = "e12393",
  month     =  jun,
  year      =  2018,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Chang2017-km,
  title     = "The Code for Facial Identity in the Primate Brain",
  author    = "Chang, Le and Tsao, Doris Y",
  abstract  = "Primates recognize complex objects such as faces with remarkable
               speed and reliability. Here, we reveal the brain's code for
               facial identity. Experiments in macaques demonstrate an
               extraordinarily simple transformation between faces and
               responses of cells in face patches. By formatting faces as
               points in a high-dimensional linear space, we discovered that
               each face cell's firing rate is proportional to the projection
               of an incoming face stimulus onto a single axis in this space,
               allowing a face cell ensemble to encode the location of any face
               in the space. Using this code, we could precisely decode faces
               from neural population responses and predict neural firing rates
               to faces. Furthermore, this code disavows the long-standing
               assumption that face cells encode specific facial identities,
               confirmed by engineering faces with drastically different
               appearance that elicited identical responses in single face
               cells. Our work suggests that other objects could be encoded by
               analogous metric coordinate systems. PAPERCLIP.",
  journal   = "Cell",
  publisher = "Elsevier",
  volume    =  169,
  number    =  6,
  pages     = "1013--1028.e14",
  month     =  jun,
  year      =  2017,
  keywords  = "decoding; electrophysiology; face processing; inferior temporal
               cortex; primate vision;FEED",
  language  = "en"
}

@ARTICLE{Hsu2004-hs,
  title     = "Quantifying variability in neural responses and its application
               for the validation of model predictions",
  author    = "Hsu, Anne and Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric
               E",
  abstract  = "A rate code assumes that a neuron's response is completely
               characterized by its time-varying mean firing rate. This
               assumption has successfully described neural responses in many
               systems. The noise in rate coding neurons can be quantified by
               the coherence function or the correlation coefficient between
               the neuron's deterministic time-varying mean rate and noise
               corrupted single spike trains. Because of the finite data size,
               the mean rate cannot be known exactly and must be approximated.
               We introduce novel unbiased estimators for the measures of
               coherence and correlation which are based on the extrapolation
               of the signal to noise ratio in the neural response to infinite
               data size. We then describe the application of these estimates
               to the validation of the class of stimulus-response models that
               assume that the mean firing rate captures all the information
               embedded in the neural response. We explain how these
               quantifiers can be used to separate response prediction errors
               that are due to inaccurate model assumptions from errors due to
               noise inherent in neuronal spike trains.",
  journal   = "Network",
  publisher = "Taylor \& Francis",
  volume    =  15,
  number    =  2,
  pages     = "91--109",
  month     =  may,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Cordaro2018-xm,
  title     = "Universals and cultural variations in 22 emotional expressions
               across five cultures",
  author    = "Cordaro, Daniel T and Sun, Rui and Keltner, Dacher and Kamble,
               Shanmukh and Huddar, Niranjan and McNeil, Galen",
  abstract  = "We collected and Facial Action Coding System (FACS) coded over
               2,600 free-response facial and body displays of 22 emotions in
               China, India, Japan, Korea, and the United States to test 5
               hypotheses concerning universals and cultural variants in
               emotional expression. New techniques enabled us to identify
               cross-cultural core patterns of expressive behaviors for each of
               the 22 emotions. We also documented systematic cultural
               variations of expressive behaviors within each culture that were
               shaped by the cultural resemblance in values, and identified a
               gradient of universality for the 22 emotions. Our discussion
               focused on the science of new expressions and how the evidence
               from this investigation identifies the extent to which emotional
               displays vary across cultures. (PsycINFO Database Record",
  journal   = "Emotion",
  publisher = "psycnet.apa.org",
  volume    =  18,
  number    =  1,
  pages     = "75--93",
  month     =  feb,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Ekman1976-hm,
  title     = "Measuring facial movement",
  author    = "Ekman, Paul and Friesen, Wallace V",
  abstract  = "A procedure has been developed for measuring visibly different
               facial movements. The Facial Action Code was derived from an
               analysis of the anatomical basis of facial movement. The method
               can be used to describe any facial movement (observed in
               photographs, motion picture film or videotape) in terms of
               anatomically based action units. The development of the method
               is explained, contrasting it to other methods of measuring
               facial behavior. An example of how facial behavior is measured
               is provided, and ideas about research applications are
               discussed.",
  journal   = "Environmental psychology and nonverbal behavior",
  publisher = "Springer",
  volume    =  1,
  number    =  1,
  pages     = "56--75",
  month     =  sep,
  year      =  1976
}

@ARTICLE{Jack2012-eq,
  title     = "Facial expressions of emotion are not culturally universal",
  author    = "Jack, Rachael E and Garrod, Oliver G B and Yu, Hui and Caldara,
               Roberto and Schyns, Philippe G",
  abstract  = "Since Darwin's seminal works, the universality of facial
               expressions of emotion has remained one of the longest standing
               debates in the biological and social sciences. Briefly stated,
               the universality hypothesis claims that all humans communicate
               six basic internal emotional states (happy, surprise, fear,
               disgust, anger, and sad) using the same facial movements by
               virtue of their biological and evolutionary origins [Susskind
               JM, et al. (2008) Nat Neurosci 11:843-850]. Here, we refute this
               assumed universality. Using a unique computer graphics platform
               that combines generative grammars [Chomsky N (1965) MIT Press,
               Cambridge, MA] with visual perception, we accessed the mind's
               eye of 30 Western and Eastern culture individuals and
               reconstructed their mental representations of the six basic
               facial expressions of emotion. Cross-cultural comparisons of the
               mental representations challenge universality on two separate
               counts. First, whereas Westerners represent each of the six
               basic emotions with a distinct set of facial movements common to
               the group, Easterners do not. Second, Easterners represent
               emotional intensity with distinctive dynamic eye activity. By
               refuting the long-standing universality hypothesis, our data
               highlight the powerful influence of culture on shaping basic
               behaviors once considered biologically hardwired. Consequently,
               our data open a unique nature-nurture debate across broad fields
               from evolutionary psychology and social neuroscience to social
               networking via digital avatars.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  109,
  number    =  19,
  pages     = "7241--7244",
  month     =  may,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Keltner2019-tm,
  title     = "Emotional Expression: Advances in Basic Emotion Theory",
  author    = "Keltner, Dacher and Sauter, Disa and Tracy, Jessica and Cowen,
               Alan",
  abstract  = "In this article, we review recent developments in the study of
               emotional expression within a basic emotion framework. Dozens of
               new studies find that upwards of 20 emotions are signaled in
               multimodal and dynamic patterns of expressive behavior. Moving
               beyond word to stimulus matching paradigms, new studies are
               detailing the more nuanced and complex processes involved in
               emotion recognition and the structure of how people perceive
               emotional expression. Finally, we consider new studies
               documenting contextual influences upon emotion recognition. We
               conclude by extending these recent findings to questions about
               emotion-related physiology and the mammalian precursors of human
               emotion.",
  journal   = "J. Nonverbal Behav.",
  publisher = "Springer",
  volume    =  43,
  number    =  2,
  pages     = "133--160",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Calvo2016-la,
  title     = "Perceptual and affective mechanisms in facial expression
               recognition: An integrative review",
  author    = "Calvo, Manuel G and Nummenmaa, Lauri",
  abstract  = "Facial expressions of emotion involve a physical component of
               morphological changes in a face and an affective component
               conveying information about the expresser's internal feelings.
               It remains unresolved how much recognition and discrimination of
               expressions rely on the perception of morphological patterns or
               the processing of affective content. This review of research on
               the role of visual and emotional factors in expression
               recognition reached three major conclusions. First, behavioral,
               neurophysiological, and computational measures indicate that
               basic expressions are reliably recognized and discriminated from
               one another, albeit the effect may be inflated by the use of
               prototypical expression stimuli and forced-choice responses.
               Second, affective content along the dimensions of valence and
               arousal is extracted early from facial expressions, although
               this coarse affective representation contributes minimally to
               categorical recognition of specific expressions. Third, the
               physical configuration and visual saliency of facial features
               contribute significantly to expression recognition, with
               ``emotionless'' computational models being able to reproduce
               some of the basic phenomena demonstrated in human observers. We
               conclude that facial expression recognition, as it has been
               investigated in conventional laboratory tasks, depends to a
               greater extent on perceptual than affective information and
               mechanisms.",
  journal   = "Cogn. Emot.",
  publisher = "Taylor \& Francis",
  volume    =  30,
  number    =  6,
  pages     = "1081--1106",
  month     =  sep,
  year      =  2016,
  keywords  = "Affective priming; Emotion; Facial expression; Perception;
               Recognition",
  language  = "en"
}

@ARTICLE{Ekman1980-of,
  title     = "Facial signs of emotional experience",
  author    = "Ekman, Paul and Freisen, Wallace V and Ancoli, Sonia",
  abstract  = "35 right-handed White females (18--35 yrs) viewed positive and
               stress-inducing motion picture films and then reported on their
               subjective experience. Spontaneous facial expressions provided
               accurate information about more specific aspects of emotional
               experience than just the pleasant vs unpleasant distinction. The
               facial action coding system (P. Ekman and W. V. Friesen, 1978)
               isolated a particular type of smile that was related to
               differences in reported happiness between Ss who showed this
               action and Ss who did not, to the intensity of happiness, and to
               which of 2 happy experiences was reported as happiest. Ss who
               showed a set of facial actions hypothesized to be signs of
               various negative affects reported experiencing more negative
               emotion than Ss who did not show these actions. How much these
               facial actions were shown was related to the reported intensity
               of negative affect. Specific facial actions associated with the
               experience of disgust are identified. (38 ref) (PsycINFO
               Database Record (c) 2016 APA, all rights reserved)",
  journal   = "J. Pers. Soc. Psychol.",
  publisher = "psycnet.apa.org",
  volume    =  39,
  number    =  6,
  pages     = "1125--1134",
  month     =  dec,
  year      =  1980
}

@ARTICLE{Folster2014-zy,
  title     = "Facial age affects emotional expression decoding",
  author    = "F{\"o}lster, Mara and Hess, Ursula and Werheid, Katja",
  abstract  = "Facial expressions convey important information on emotional
               states of our interaction partners. However, in interactions
               between younger and older adults, there is evidence for a
               reduced ability to accurately decode emotional facial
               expressions. Previous studies have often followed up this
               phenomenon by examining the effect of the observers' age.
               However, decoding emotional faces is also likely to be
               influenced by stimulus features, and age-related changes in the
               face such as wrinkles and folds may render facial expressions of
               older adults harder to decode. In this paper, we review
               theoretical frameworks and empirical findings on age effects on
               decoding emotional expressions, with an emphasis on age-of-face
               effects. We conclude that the age of the face plays an important
               role for facial expression decoding. Lower expressivity,
               age-related changes in the face, less elaborated emotion schemas
               for older faces, negative attitudes toward older adults, and
               different visual scan patterns and neural processing of older
               than younger faces may lower decoding accuracy for older faces.
               Furthermore, age-related stereotypes and age-related changes in
               the face may bias the attribution of specific emotions such as
               sadness to older faces.",
  journal   = "Front. Psychol.",
  publisher = "frontiersin.org",
  volume    =  5,
  pages     = "30",
  month     =  feb,
  year      =  2014,
  keywords  = "aging; emotional facial expressions; expressivity; facial
               expression decoding; older face; own-age advantage; response
               bias",
  language  = "en"
}

@ARTICLE{Ko2018-rv,
  title     = "A Brief Review of Facial Emotion Recognition Based on Visual
               Information",
  author    = "Ko, Byoung Chul",
  abstract  = "Facial emotion recognition (FER) is an important topic in the
               fields of computer vision and artificial intelligence owing to
               its significant academic and commercial potential. Although FER
               can be conducted using multiple sensors, this review focuses on
               studies that exclusively use facial images, because visual
               expressions are one of the main information channels in
               interpersonal communication. This paper provides a brief review
               of researches in the field of FER conducted over the past
               decades. First, conventional FER approaches are described along
               with a summary of the representative categories of FER systems
               and their main algorithms. Deep-learning-based FER approaches
               using deep networks enabling ``end-to-end'' learning are then
               presented. This review also focuses on an up-to-date hybrid
               deep-learning approach combining a convolutional neural network
               (CNN) for the spatial features of an individual frame and long
               short-term memory (LSTM) for temporal features of consecutive
               frames. In the later part of this paper, a brief review of
               publicly available evaluation metrics is given, and a comparison
               with benchmark results, which are a standard for a quantitative
               comparison of FER researches, is described. This review can
               serve as a brief guidebook to newcomers in the field of FER,
               providing basic knowledge and a general understanding of the
               latest state-of-the-art studies, as well as to experienced
               researchers looking for productive directions for future work.",
  journal   = "Sensors",
  publisher = "mdpi.com",
  volume    =  18,
  number    =  2,
  month     =  jan,
  year      =  2018,
  keywords  = "conventional FER; convolutional neural networks; deep
               learning-based FER; facial action coding system; facial action
               unit; facial emotion recognition; long short term memory",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dinga2019-mh,
  title     = "Beyond accuracy: measures for assessing machine learning models,
               pitfalls and guidelines",
  author    = "Dinga, R and Penninx, Bwjh and Veltman, D J and Schmaal, L and
               {others}",
  abstract  = "Beyond accuracy : Measures for assessing machine learning
               models, pitfalls and guidelines … a Department of Psychiatry,
               Amsterdam UMC, Amsterdam, the Netherlands b Donders Institute
               for Brain, Cognition and Behaviour, Radboud University,
               Nijmegen, the Netherlands c …",
  journal   = "bioRxiv",
  publisher = "biorxiv.org",
  year      =  2019
}

@ARTICLE{Brooks2019-zm,
  title     = "The neural representation of facial-emotion categories reflects
               conceptual structure",
  author    = "Brooks, Jeffrey A and Chikazoe, Junichi and Sadato, Norihiro and
               Freeman, Jonathan B",
  abstract  = "Humans reliably categorize configurations of facial actions into
               specific emotion categories, leading some to argue that this
               process is invariant between individuals and cultures. However,
               growing behavioral evidence suggests that factors such as
               emotion-concept knowledge may shape the way emotions are
               visually perceived, leading to variability-rather than
               universality-in facial-emotion perception. Understanding
               variability in emotion perception is only emerging, and the
               neural basis of any impact from the structure of emotion-concept
               knowledge remains unknown. In a neuroimaging study, we used a
               representational similarity analysis (RSA) approach to measure
               the correspondence between the conceptual, perceptual, and
               neural representational structures of the six emotion categories
               Anger, Disgust, Fear, Happiness, Sadness, and Surprise. We found
               that subjects exhibited individual differences in their
               conceptual structure of emotions, which predicted their own
               unique perceptual structure. When viewing faces, the
               representational structure of multivoxel patterns in the right
               fusiform gyrus was significantly predicted by a subject's unique
               conceptual structure, even when controlling for potential
               physical similarity in the faces themselves. Finally,
               cross-cultural differences in emotion perception were also
               observed, which could be explained by individual differences in
               conceptual structure. Our results suggest that the
               representational structure of emotion expressions in visual
               face-processing regions may be shaped by idiosyncratic
               conceptual understanding of emotion categories.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  116,
  number    =  32,
  pages     = "15861--15870",
  month     =  aug,
  year      =  2019,
  keywords  = "conceptual knowledge; emotion perception; facial expressions;
               functional magnetic resonance imaging; fusiform gyrus",
  language  = "en"
}

@ARTICLE{Friesen1983-ft,
  title   = "{EMFACS-7}: Emotional facial action coding system",
  author  = "Friesen, Wallace V and {Ekman}",
  journal = "Unpublished manuscript, University of California at San Francisco",
  volume  =  2,
  number  =  36,
  pages   = "1",
  year    =  1983
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Keltner2003-yn,
  title     = "Facial expression of emotion",
  author    = "Keltner, D and Ekman, P and Gonzaga, G C and Beer, J",
  abstract  = "The study of facial expression of emotion has long been the
               focus of theoretical controversy and empirical research (eg,
               Allport, 1924; Birdwhistell, 1963; Coleman, 1949; Darwin,
               1872/1998; Ekman, 1973, 1994; Fridlund, 1991; Hunt, 1941;
               Landis, 1924; Mead, 1975; …",
  publisher = "psycnet.apa.org",
  year      =  2003
}

@ARTICLE{Nili2014-ar,
  title     = "A toolbox for representational similarity analysis",
  author    = "Nili, Hamed and Wingfield, Cai and Walther, Alexander and Su, Li
               and Marslen-Wilson, William and Kriegeskorte, Nikolaus",
  abstract  = "Neuronal population codes are increasingly being investigated
               with multivariate pattern-information analyses. A key challenge
               is to use measured brain-activity patterns to test computational
               models of brain information processing. One approach to this
               problem is representational similarity analysis (RSA), which
               characterizes a representation in a brain or computational model
               by the distance matrix of the response patterns elicited by a
               set of stimuli. The representational distance matrix
               encapsulates what distinctions between stimuli are emphasized
               and what distinctions are de-emphasized in the representation. A
               model is tested by comparing the representational distance
               matrix it predicts to that of a measured brain region. RSA also
               enables us to compare representations between stages of
               processing within a given brain or model, between brain and
               behavioral data, and between individuals and species. Here, we
               introduce a Matlab toolbox for RSA. The toolbox supports an
               analysis approach that is simultaneously data- and
               hypothesis-driven. It is designed to help integrate a wide range
               of computational models into the analysis of multichannel
               brain-activity measurements as provided by modern functional
               imaging and neuronal recording techniques. Tools for
               visualization and inference enable the user to relate sets of
               models to sets of brain regions and to statistically test and
               compare the models using nonparametric inference methods. The
               toolbox supports searchlight-based RSA, to continuously map a
               measured brain volume in search of a neuronal population code
               with a specific geometry. Finally, we introduce the
               linear-discriminant t value as a measure of representational
               discriminability that bridges the gap between linear decoding
               analyses and RSA. In order to demonstrate the capabilities of
               the toolbox, we apply it to both simulated and real fMRI data.
               The key functions are equally applicable to other modalities of
               brain-activity measurement. The toolbox is freely available to
               the community under an open-source license agreement
               (http://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/license/).",
  journal   = "PLoS Comput. Biol.",
  publisher = "journals.plos.org",
  volume    =  10,
  number    =  4,
  pages     = "e1003553",
  month     =  apr,
  year      =  2014,
  language  = "en"
}

@INPROCEEDINGS{Dhall2011-gl,
  title     = "Emotion recognition using {PHOG} and {LPQ} features",
  booktitle = "2011 {IEEE} International Conference on Automatic Face Gesture
               Recognition ({FG})",
  author    = "Dhall, A and Asthana, A and Goecke, R and Gedeon, T",
  abstract  = "We propose a method for automatic emotion recognition as part of
               the FERA 2011 competition. The system extracts pyramid of
               histogram of gradients (PHOG) and local phase quantisation (LPQ)
               features for encoding the shape and appearance information. For
               selecting the key frames, K-means clustering is applied to the
               normalised shape vectors derived from constraint local model
               (CLM) based face tracking on the image sequences. Shape vectors
               closest to the cluster centers are then used to extract the
               shape and appearance features. We demonstrate the results on the
               SSPNET GEMEP-FERA dataset. It comprises of both person specific
               and person independent partitions. For emotion classification we
               use support vector machine (SVM) and largest margin nearest
               neighbour (LMNN) and compare our results to the pre-computed
               FERA 2011 emotion challenge baseline.",
  publisher = "ieeexplore.ieee.org",
  pages     = "878--883",
  month     =  mar,
  year      =  2011,
  keywords  = "emotion recognition;feature extraction;image
               classification;image sequences;pattern clustering;support vector
               machines;PHOG;LPQ features;automatic emotion recognition;pyramid
               of histogram of gradient extraction;local phase quantisation
               features;k-means clustering;constraint local model;face
               tracking;image sequences;shape vectors;appearance features
               extraction;SSPNET GEMEP-FERA dataset;emotion
               classification;support vector machine;largest margin nearest
               neighbour;Face;Shape;Feature extraction;Image sequences;Support
               vector machines;Databases;Accuracy"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Fabian_Benitez-Quiroz2016-eo,
  title     = "Emotionet: An accurate, real-time algorithm for the automatic
               annotation of a million facial expressions in the wild",
  booktitle = "Proceedings of the {IEEE} conference on computer vision and
               pattern recognition",
  author    = "Fabian Benitez-Quiroz, C and Srinivasan, Ramprakash and
               Martinez, Aleix M",
  abstract  = "Research in face perception and emotion theory requires very
               large annotated databases of images of facial expressions of
               emotion. Annotations should include Action Units (AUs) and their
               intensities as well as emotion category. This goal cannot be
               readily achieved manually …",
  publisher = "cv-foundation.org",
  pages     = "5562--5570",
  year      =  2016
}

@ARTICLE{Yan2018-aw,
  title     = "Multi-cue fusion for emotion recognition in the wild",
  author    = "Yan, Jingwei and Zheng, Wenming and Cui, Zhen and Tang, Chuangao
               and Zhang, Tong and Zong, Yuan",
  abstract  = "Emotion recognition has become a hot research topic in the past
               several years due to the large demand of this technology in many
               practical situations. One challenging task in this topic is to
               recognize emotion types in a given video clip collected in the
               wild. In order to solve this problem we propose a multi-cue
               fusion emotion recognition (MCFER) framework by modeling human
               emotions from three complementary cues, i.e., facial texture,
               facial landmark action and audio signal, and then fusing them
               together. To capture the dynamic change of facial texture we
               employ a cascaded convolutional neutral network (CNN) and
               bidirectional recurrent neutral network (BRNN) architecture
               where facial image from each frame is first fed into CNN to
               extract high-level texture feature, and then the feature
               sequence is traversed into BRNN to learn the changes within it.
               Facial landmark action models the movement of facial muscles
               explicitly. SVM and CNN are deployed to explore the emotion
               related patterns in it. Audio signal is also modeled with CNN by
               extracting low-level acoustic features from segmented clips and
               then stacking them as an image-like matrix. We fuse these models
               at both feature level and decision level to further boost the
               overall performance. Experimental results on two challenging
               databases demonstrate the effectiveness and superiority of our
               proposed MCFER framework.",
  journal   = "Neurocomputing",
  publisher = "Elsevier",
  volume    =  309,
  pages     = "27--35",
  month     =  oct,
  year      =  2018,
  keywords  = "Emotion recognition; Convolutional neural network (CNN); Facial
               landmark action; Multi-cue fusion"
}

@ARTICLE{Merlhiot2021-eg,
  title     = "Facial width-to-height ratio underlies perceived dominance on
               facial emotional expressions",
  author    = "Merlhiot, Ga{\"e}tan and Mondillon, Laurie and M{\'e}ot, Alain
               and Dutheil, Frederic and Mermillod, Martial",
  abstract  = "The facial width-to-height ratio (fWHR) is a perceptual cue that
               affects the perception of psychological traits such as
               dominance. The current research examined whether the fWHR would
               impact the perception of dominance and emotional intensity when
               expressing emotions. In study one, we examined whether the
               emotional facial expressions (EFEs) modify the visually
               perceivable fWHR by following a specific pattern reflecting the
               perception of dominance associated with basic EFEs. We found
               that EFEs differed from neutral poses following the expected
               pattern: high dominance EFEs (anger, disgust, happiness)
               increased the fWHR, whereas low dominance EFEs (fear, sadness,
               surprise) decreased the fWHR. In study two, we investigated
               whether manipulating the fWHR (low, average, high) would affect
               the perception of dominance and emotional intensity. We obtained
               that the fWHR influenced the perception of dominance and
               emotional intensity but its effect on dominance was only present
               with high dominance EFEs. One social implication of this effect
               is that individuals for which expressing dominant emotions lead
               to high increase of their fWHR would be perceived highly
               dominant. We discuss that such effect could participate in the
               development of individuals' dominance and further researches are
               still needed to determine its social impact in interaction with
               other factors.",
  journal   = "Pers. Individ. Dif.",
  publisher = "Elsevier",
  volume    =  172,
  pages     = "110583",
  month     =  apr,
  year      =  2021,
  keywords  = "Facial width-to-height ratio; Emotional facial expression;
               Dominance; Appraisal; Gender bias"
}

@ARTICLE{Hess2005-sq,
  title     = "Who may frown and who should smile? Dominance, affiliation, and
               the display of happiness and anger",
  author    = "Hess, Ursula and Adams, Reginald and Kleck, Robert",
  abstract  = "Three experiments were conducted to test the hypothesis that the
               social stereotype that anger displays are more appropriate for
               men and smiling is requisite for women is based on the
               perception of men and women as more or less dominant or
               affiliative. The first study tested the mediation model that men
               are rated as more dominant and women as more affiliative and
               that expectations for men to show more anger and for women to
               smile more are partially mediated by this difference in
               perception. Second, a vignette approach was used to test the
               notion that these expectations translate into prescriptive
               social norms that are based on levels of perceived dominance and
               affiliation rather than sex per se. The results strongly support
               this hypothesis for dominance and provide partial confirmation
               for affiliation.",
  journal   = "Cognition and Emotion",
  publisher = "Routledge",
  volume    =  19,
  number    =  4,
  pages     = "515--536",
  month     =  jun,
  year      =  2005
}

@ARTICLE{Hugenberg2003-om,
  title     = "Facing prejudice: implicit prejudice and the perception of
               facial threat",
  author    = "Hugenberg, Kurt and Bodenhausen, Galen V",
  abstract  = "We propose that social attitudes, and in particular implicit
               prejudice, bias people's perceptions of the facial emotion
               displayed by others. To test this hypothesis, we employed a
               facial emotion change-detection task in which European American
               participants detected the offset (Study 1) or onset (Study 2) of
               facial anger in both Black and White targets. Higher implicit
               (but not explicit) prejudice was associated with a greater
               readiness to perceive anger in Black faces, but neither explicit
               nor implicit prejudice predicted anger perceptions regarding
               similar White faces. This pattern indicates that European
               Americans high in implicit racial prejudice are biased to
               perceive threatening affect in Black but not White faces,
               suggesting that the deleterious effects of stereotypes may take
               hold extremely early in social interaction.",
  journal   = "Psychol. Sci.",
  publisher = "journals.sagepub.com",
  volume    =  14,
  number    =  6,
  pages     = "640--643",
  month     =  nov,
  year      =  2003,
  language  = "en"
}

@article{westfall2016fixing,
  title={Fixing the stimulus-as-fixed-effect fallacy in task fMRI},
  author={Westfall, Jacob and Nichols, Thomas E and Yarkoni, Tal},
  journal={Wellcome open research},
  volume={1},
  year={2016},
  publisher={The Wellcome Trust}
}
