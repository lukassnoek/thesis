```{r setup-hypothesis-kernel-analysis, include=FALSE}
knitr::opts_chunk$set(results = 'hide', echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(kableExtra)
```

# Using predictive modeling to quantify the importance and limitations of action units in emotion perception {#hypothesis-kernel-analysis}
\chaptermark{Predictive modelling of action units}

\newpage
\normalsize

`<p><strong>Abstract</strong></p>`{=html}
`\begin{abstract}`{=latex}
Bla bla
`\end{abstract} \newpage`{=latex}

## Methods {#hka-methods}

### Hypothesis kernel analysis

To formalize AU-emotion mappings as predictive models, we propose a novel method which we call “hypothesis kernel analysis”. In the context of the current study, we use this method to reframe AU-emotion mappings as classification models that predict the probability of an emotion given a set of AUs [analogous to how people attempt to infer the emotion from others’ facial emotion expressions; @Jack2015-sh]. In what follows, we conceptually explain how the method works. For a detailed and more mathematical description of the method, we refer the reader to the [Supplementary Methods](#hypothesis-kernel-analysis-supplement).

The underlying idea of the hypothesis kernel analysis is to predict a categorical dependent variable (e.g., the perceived emotion) based on the similarity between an *observation* with a particular set of features (e.g., a face with a particular set of AUs; the independent variables) and statements of a *hypothesis* (e.g., “happiness is expressed by AUs 6 and 12”). This prediction can then be compared to real observations to evaluate the accuracy of the hypothesis. The three methodological challenges of this approach are how to measure the similarity between an observation and a hypothesis statement, how to derive a prediction based on this similarity, and how to compare the predictions to real data. Figure \@ref(fig:fig-hka-2) outlines how we have solved these challenges in five steps, which we will describe in turn.

```{r fig-hka-2, fig.cap='(ref:caption-fig-hka-2)', results='show'}
knitr::include_graphics("_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_2.png", auto_pdf = TRUE)
```

(ref:caption-fig-hka-2) Schematic visualization of the proposed method using a set of hypothetical AU-emotion mappings ($\mathbf{M}$) and stimuli ($\mathbf{S}$) based on a small set of AUs (five in total). The variable $P$ represents the number of variables (here: AUs), $Q$ represents the number of classes (here: emotions), and $N$ represents the number of trials (here: facial expression stimuli). Note that the AU space technically may contain any number of ($P$) dimensions, but is shown here in two dimensions for convenience.

To quantify the similarity between an observation and a hypothesis statement, we embed both in a multidimensional space that is spanned by a particular set of variables (e.g., different AUs). In this space, we start by representing each class of the dependent variable (corresponding to the statements of the hypothesis) as a separate point. In the current study, this amounts to embedding the different hypothesized AU configurations (e.g., “happiness = AU12 + 6”; $\mathbf{M}$ in Figure \@ref(fig:fig-hka-2)) as points in “AU space”, separately for each categorical emotion (see step 1 in Figure \@ref(fig:fig-hka-2)). The coordinates of each point are determined by the hypothesized “importance” of each independent variable for that given class of the target variable. For example, the coordinates of each point in AU space represents the hypothesized relative intensity of each AU for a given emotion. As the AU-emotion mappings evaluated in the current study only specify whether an AU is included or excluded within a particular emotional configuration, we specify the coordinates of their embedding to be binary (0: excluded, 1: included). A different interpretation of the class embeddings described here is that they represent the location of a typical facial expression for this emotion in “AU space” according to a particular hypothesis. 

As a second step, we embed each data point in the same space as the hypotheses. This means, the data used for this purpose should contain the same variables as were used to embed the hypotheses. For example, in this study, we use emotion ratings (i.e., the target variable) in response to dynamic facial expression stimuli with random configurations of AUs (i.e., the independent variables; $\mathbf{S}$ in Figure \@ref(fig:fig-hka-2)) to test the hypothesized AU-emotion mappings (see [Dataset used to evaluate mappings](#hka-dataset)).

With the hypotheses and the data in the same space, the next step in our method is to compute, for each observation separately, the “similarity” between the data and each class of the target. For this purpose, we use *kernel functions* (step 3 in Figure \@ref(fig:fig-hka-2)), a technique that quantifies the similarity of two vectors. Any kernel function that computes a measure of similarity can be used, but in our analyses we use the cosine similarity as it normalizes the similarity by the magnitude (specifically, the L2 norm) of the data and hypothesis embeddings (but see Supplementary Figure \@ref(fig:fig-hka-S3) for a comparison of model performance across different similarity and distance metrics). 

As a fourth step, we interpret the similarity between the data and a given class embedding as being proportional to the evidence for a given class. In other words, the more similar a data point is to the statement of a hypothesis the stronger the prediction for the associated class. To produce a probabilistic prediction of the classes given a particular observation and hypothesis, we normalize the similarity values to the 0-1 range using the *softmax* function (step 4 in Figure \@ref(fig:fig-hka-2)).^[Readers familiar with machine learning algorithms may recognize this as a specific implementation of a K-nearest neighbor classification model with *K* = 1, which is fit on the embedded hypotheses ($\mathbf{M}$) and cross-validated on the data ($\mathbf{S}$).]

Finally, the accuracy of the model can be summarized by comparing its predictions to the actual values of the target variable in the dataset (see step 5 in Figure \@ref(fig:fig-hka-2)). In this study, this means that the predictions are compared to the actual emotion ratings from participants. Although any model performance metric can be used, we use the “Area under the Receiver operating curve” (AUROC) as our model performance metric, because it is insensitive to class imbalance, allows for class-specific scores, and can handle probabilistic predictions [@Dinga2019-mh]. We report class-specific scores, which means that each class of the categorical dependent variable (i.e., different emotions) gets a separate score with a chance level of 0.5 and a theoretical maximum score of 1.

### Ablation and follow-up exploration analyses

To gain a better understanding of why some mappings perform better than others, we performed an “ablation analysis”, which entails removing (or “ablating”) AUs one by one from each configuration for each evaluated mapping and subsequently rerunning the kernel analysis to observe how this impacts model performance. If ablating a particular AU *decreases* model performance for a given emotion, it means that this AU is important for perceiving this emotion. If on the other hand ablating an AU *increases* performance for a given emotion, it could mean that the inclusion of this AU in a given mapping is incorrect.

Using the results from the ablation analyses, we explored strategies to enhance existing mappings. Specifically, we computed for each emotion which AUs, on average across mappings, led to a decrease in model performance after being ablated. We then ran follow-up analyses in which we enhanced existing mappings with the AUs that were found to decrease model performance in the ablation analyses and reran the predictive analysis. This was done for each AU and emotion separately. For example, if ablation of AU4 was found to decrease model performance for “fear” on average across mappings that included AU4 (i.e., all but Cordaro et al., 2018; ICP), we would rerun the predictive analysis for the mapping(s) that did not contain AU4 (i.e., Cordaro et al., 2018; ICP) using an “enhanced” mapping with AU4 appended.

Finally, we constructed “optimized” models by, for each mapping separately, adding all AUs that led to a *decrease* in model performance after ablation and removing all AUs that led to an *increase* in model performance after ablation. Then, the predictive analysis was rerun and the “optimized” model performance was compared to the original model performance.

### Noise ceiling estimation {#hka-noise-ceiling}

Instead of interpreting model performance relative to the theoretical optimum performance, we propose to interpret model performance relative to the *noise ceiling*, an estimate of the in principle explainable portion of the target variable. The noise ceiling is a concept often used in systems neuroscience to correct model performance for noise in the measured brain data [@Hsu2004-hs; @Huth2012-yc; @Kay2013-ch]. Traditionally, noise ceilings in neuroscience are applied in the context of within-subject regression models [@Lage-Castellanos2019-dm]. Here, we develop a method to derive noise ceilings for classification models, i.e., models with a categorical target variable (such as categorical emotion ratings) that are applicable to both within-subject and between-subject models [see also @Hebart2020-wp]. In this section, we explain our derivation of noise ceilings for classification models conceptually; the [Supplementary Methods](#hypothesis-kernel-analysis-supplement) outline a more detailed and formal description.

Noise ceiling estimation is a method that adjusts the theoretical maximum performance of a predictive model for the presence of irreducible noise in the data. As such, like the theoretical maximum, the noise ceiling imposes an upper bound on model performance. Another way to think about noise ceilings is that they split the variance of the data into three portions: the explained variance, the unexplained variance, and the "irreducible" noise (see Figure \@ref(fig:fig-hka-3)). "Irreducible" is put in quotes because this proportion of noise can, in fact, be explained in principle as will be discussed in the [Discussion](#kha-discussion) (see also the [Supplementary Methods](#hypothesis-kernel-analysis-supplement). Importantly, the noise ceiling thus indicates how much improvement in terms of model performance can be gained for a given dataset (i.e., unexplained variance) and how much cannot be explained by the model (i.e., the "irreducible" noise).

```{r fig-hka-3, fig.cap='(ref:caption-fig-hka-3)', results='show'}
knitr::include_graphics("_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_3.png", auto_pdf = TRUE)
```

(ref:caption-fig-hka-3) The noise ceiling partitions the variance into *explained variance*, *unexplained variance*, and *"irreducible" noise* for any given model ($\mathbf{M}$). Here, AUROC is used as the metric of model performance, but the noise ceiling can be estimated using any metric.

In the context of the current study, we use the variance (or “inconsistency”) in emotion ratings across participants in response to the same set of facial expression stimuli to estimate a noise ceiling for the different AU-based models. The noise ceiling gives us insight into whether the evaluated set of AU-based models are sufficiently accurate to explain variance that can in principle be explained by AUs or whether we may need differently parameterized AU-based models. This way, the importance and limitations of AUs can be estimated empirically.

### Evaluated mappings

Many different AU-emotion mappings have been put forward, but in this study we assess those summarized in @Barrett2019-bc (Table 1). Additionally, we included the AU-emotion mappings from the “emotional FACS” (EMFACS) manual [@Friesen1983-ft]. So, in total, we evaluated six hypothesized AU-emotion mappings, which are summarized in Table \@ref(tab:tab-hka-1) (and an additional data-driven AU-emotion mapping, see below).

\newpage
\pagestyle{empty}
\blandscape

```{r tab-hka-1, results='asis'}
data = read_csv('_bookdown_files/hypothesis-kernel-analysis-files/table_1_data.csv')
options(knitr.kable.NA = '')
kbl(data, booktabs = T, longtable = T, escape = F, linesep="", caption = 'Evaluated AU-emotion mappings in our study') %>%
    kable_styling(full_width = T, font_size = 7, latex_options = "repeat_header") %>%
    column_spec(column = 1, width = "4em") %>%
    column_spec(column = 3, width = "10em") %>%
    footnote(footnote_as_chunk = T, title_format = "italic", threeparttable = T, escape = T, 
             general = 'Mappings evaluated in the current study. The mappings from Darwin (1872) were taken from Matsumoto et al. (2018). Both the “reference configuration” (ref.) and the “international core pattern” (ICP) from Cordaro et al. (2018) are included. The + symbol means that AUs occur together. AUs following a comma represent optional AUs. The inverted ^ symbol represents “or”. When multiple configurations are explicitly proposed for a given emotion (i.e., a “many-to-one” mapping), they are represented as separate bullet points.')
```

\elandscape
\newpage
\pagestyle{empty}

All of these mappings propose that a number of AUs must occur together to communicate a particular emotion. However, the comparison between them is complicated by the fact that not all of them posit a single, consistent set of AUs per emotion. First, some contain multiple sets, such as the EMFACS manual [@Friesen1983-ft] proposing that “sadness” can be expressed with AUs 1 + 4 or AUs 6 + 15. Second, some offer optional AUs for a set, such as @Matsumoto2008-qk proposing that “sadness” is associated with AUs 1 + 15 and optionally with AUs 4 and/or 17. Thirdly, some describe *mutually exclusive* options of AUs for a set, such as @Matsumoto2008-qk proposing that “surprise” can be communicated with AUs 1 + 2 + 5 in combination with *either* AU25 *or* AU26.

We address this issue by explicitly formulating all possible AU configurations that communicate a particular emotion for each mapping. For example, Matsumoto et al. (2008) propose that “disgust” is associated with AU 9 or 10 and, optionally, AU 25 or 26, which yields six different possible configurations (9; 10; 9 + 25; 9 + 26; 10 + 25; 10 + 26). The specific configurations for each emotion derived from each evaluated mapping can be viewed in the study’s code repository (in `mappings.py`; see [Code Availability](#hka-code) section). In our analysis framework, we deal with multiple configurations per emotion (within a particular mapping), for each prediction separately, by using the configuration with the largest similarity to the stimulus under consideration (which occurs in between steps 3 and 4 in Figure \@ref(fig:fig-hka-2)). We demonstrate that this procedure does not give an unfair advantage to mappings with more configurations using a simulation analysis (see Supplementary Figure \@ref(fig:fig-hka-S4)).

In addition to evaluating existing mappings from the literature, we also constructed a mapping based on a data-driven analysis of the relationship between the AUs and emotion ratings from the dataset we use to evaluate the mappings. Importantly, to avoid circularity in our data-driven analysis [“double dipping”; @Kriegeskorte2009-yz], we performed the mapping estimation and evaluation on different partitions of the data (i.e., cross-validation). Specifically, we estimated the mapping on approximately 50% of the trials from 50% of the participants (the “train set”) and evaluated the mapping on the other 50% of trials from the other 50% of the participants (the “test set”). Importantly, the train and test set contained unique facial expressions and unique face identities, thus effectively treating both subject and stimulus as a random effect [@westfall2016fixing].

To estimate the data-driven mapping, we followed the procedure specified in @Yu2012-ag. For each AU and emotion, we computed the Pearson correlation between the binary activation values (1 if active, 0 otherwise) and the binary emotion rating (1 if this emotion was rated, 0 otherwise) for each participant in the train set. The raw correlations were averaged across the participants and binarized based on whether the correlation was statistically significant at $\alpha = 0.05$ (1 if significant, 0 otherwise; uncorrected for multiple comparisons), which resulted in a binary 6 (emotion) × 33 (AU) mapping matrix.

### Dataset used to evaluate mappings {#hka-dataset}

We use data from an existing dataset [@@Yu2012-ag] which contains emotion ratings in response to 2400 dynamic facial expressions (with a duration of 1.25 seconds) with random AU configurations from 60 subjects. Each stimulus was composed of one of eight “base faces” and a random number of activated AUs drawn from a set of 42 AUs. Per stimulus, the number of AUs was drawn from a binomial distribution with parameters $n = 6$ and $p = 0.5$. The selected AUs varied in amplitude from 0 (not activated) to 1 (fully activated) in steps of 0.25 and a set of temporal parameters which determined the exact time course of each AU (see for details Yu et al., 2012). The original set of 42 AUs contained both compound AUs (such as AU25-12 and AU1-2) and AUs that could be activated both unilaterally (left or right) and bilaterally (such as AU12). In order to encode these AUs into independent variables, we recoded the compound AUs (e.g., activation of AU1-2 was recoded as activation of both AU1 and AU2) and bilateral AUs (e.g., activation of AU12 was recoded as activation of both AU12L and AU12R), yielding a total of 33 AUs: 1, 2L, 2R, 4, 5, 6L, 6R, 7L, 7R, 9, 10L, 10R, 11L, 11R, 12L, 12R, 13, 14L, 14R, 15, 16, 17, 20L, 20R, 22, 23, 24, 25, 26, 27, 38, 39, 43 (where L = left, R = right). 

The emotion ratings were collected in a 7 alternative forced-choice facial expression categorization task in which participants were instructed to label the stimuli using one of the six universal basic emotions (“anger”, “disgust”, “fear”, “happiness”, “sadness”, and “surprise) or, when the stimulus matched none of the emotion categories, “other”. In addition, participants rated the “intensity” of the perceived emotion, which ranged from 1 (not intense at all) to 5 (very intense). Trials in which the stimulus was rated as “other” were removed from the dataset (because the evaluated mappings do not contain hypotheses about this category) leaving a grand total of 121,902 trials (average per subject: 2031.7 trials, *SD*: 311.5) for our analysis. This grand total contains 4660 repeated observations with an average of 26.16 (*SD*: 14.92) repetitions.

### Code availability {#hka-code}

All code used for this study’s analysis and visualization of results is publicly available from Github: <https://github.com/lukassnoek/hypothesis-kernel-analysis>. The analyses were implemented in the Python programming language (version 3.7) and use several third-party packages, including *numpy* [@Harris2020-en], *pandas* [@McKinney2011-kl], *scikit-learn* [@pedregosa2011scikit], and *seaborn* [@waskom2021seaborn]. A Python package to compute noise ceilings as described in the current study can be found on Github: <https://github.com/lukassnoek/noiseceiling>.

## Results {#hka-results}

## Discussion {#hka-discussion}