\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\catcode `"\active 
\HyPL@Entry{0<</P(\376\377\000c\000o\000v\000e\000r)>>}
\selectlanguage *[variant=american]{english}
\@writefile{toc}{\selectlanguage *[variant=american]{english}}
\@writefile{lof}{\selectlanguage *[variant=american]{english}}
\@writefile{lot}{\selectlanguage *[variant=american]{english}}
\HyPL@Entry{1<</S/r>>}
\@writefile{toc}{\contentsline {chapter}{Contents}{vi}{section*.1}\protected@file@percent }
\HyPL@Entry{9<</S/D>>}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\newlabel{general-introduction}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Inference done differently}{3}{section.1.1}\protected@file@percent }
\newlabel{inference-done-differently}{{\M@TitleReference {1.1}{Inference done differently}}{3}{Inference done differently}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Towards prediction}{8}{section.1.2}\protected@file@percent }
\newlabel{towards-prediction}{{\M@TitleReference {1.2}{Towards prediction}}{8}{Towards prediction}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Outline of this thesis}{13}{section.1.3}\protected@file@percent }
\newlabel{outline-of-this-thesis}{{\M@TitleReference {1.3}{Outline of this thesis}}{13}{Outline of this thesis}{section.1.3}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}{14}{chapter.2}\protected@file@percent }
\newlabel{shared-states}{{\M@TitleReference {2}{Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}}{14}{Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{16}{section.2.1}\protected@file@percent }
\newlabel{shared-states-introduction}{{\M@TitleReference {2.1}{Introduction}}{16}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methods}{19}{section.2.2}\protected@file@percent }
\newlabel{shared-states-methods}{{\M@TitleReference {2.2}{Methods}}{19}{Methods}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{Subjects}{19}{section*.2}\protected@file@percent }
\newlabel{shared-states-methods-subjects}{{\M@TitleReference {2.2}{Subjects}}{19}{Subjects}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Experimental design}{19}{section*.3}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design}{{\M@TitleReference {2.2}{Experimental design}}{19}{Experimental design}{section*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Self-focused emotion imagery task}{19}{section*.4}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design-sf-task}{{\M@TitleReference {2.2}{Self-focused emotion imagery task}}{19}{Self-focused emotion imagery task}{section*.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Other-focused emotion understanding task}{20}{section*.6}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design-of-task}{{\M@TitleReference {2.2}{Other-focused emotion understanding task}}{20}{Other-focused emotion understanding task}{section*.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of the self-focused and other-focused task.\relax }}{21}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig-shared-states-1}{{\M@TitleReference {2.1}{Overview of the self-focused and other-focused task.\relax }}{21}{Overview of the self-focused and other-focused task.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{Procedure}{22}{section*.7}\protected@file@percent }
\newlabel{shared-states-methods-procedure}{{\M@TitleReference {2.2}{Procedure}}{22}{Procedure}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{Image acquisition}{22}{section*.8}\protected@file@percent }
\newlabel{shared-states-methods-image-acquisition}{{\M@TitleReference {2.2}{Image acquisition}}{22}{Image acquisition}{section*.8}{}}
\@writefile{toc}{\contentsline {subsection}{Model optimization procedure}{23}{section*.9}\protected@file@percent }
\newlabel{shared-states-methods-model-optimization-procedure}{{\M@TitleReference {2.2}{Model optimization procedure}}{23}{Model optimization procedure}{section*.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematic overview of the cross-validation procedures. \textbf  {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf  {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }}{24}{figure.caption.10}\protected@file@percent }
\newlabel{fig:fig-shared-states-2}{{\M@TitleReference {2.2}{Schematic overview of the cross-validation procedures. \textbf  {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf  {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }}{24}{Schematic overview of the cross-validation procedures. \textbf {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{Preprocessing and single-trial modeling}{24}{section*.11}\protected@file@percent }
\newlabel{shared-states-methods-preprocessing}{{\M@TitleReference {2.2}{Preprocessing and single-trial modeling}}{24}{Preprocessing and single-trial modeling}{section*.11}{}}
\@writefile{toc}{\contentsline {subsection}{Multi-voxel pattern analysis}{25}{section*.12}\protected@file@percent }
\newlabel{shared-states-methods-mvpa}{{\M@TitleReference {2.2}{Multi-voxel pattern analysis}}{25}{Multi-voxel pattern analysis}{section*.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{MVPA pipeline}{25}{section*.13}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-pipeline}{{\M@TitleReference {2.2}{MVPA pipeline}}{25}{MVPA pipeline}{section*.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation scheme and bagging procedure}{26}{section*.14}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-cv-and-bagging}{{\M@TitleReference {2.2}{Cross-validation scheme and bagging procedure}}{26}{Cross-validation scheme and bagging procedure}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Statistical evaluation}{27}{section*.15}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-statistical-evaluation}{{\M@TitleReference {2.2}{Statistical evaluation}}{27}{Statistical evaluation}{section*.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial representation}{28}{section*.16}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-spatial-representation}{{\M@TitleReference {2.2}{Spatial representation}}{28}{Spatial representation}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{Additional analyses}{28}{section*.17}\protected@file@percent }
\newlabel{shared-states-methods-additional-analyses}{{\M@TitleReference {2.2}{Additional analyses}}{28}{Additional analyses}{section*.17}{}}
\@writefile{toc}{\contentsline {subsection}{Univariate analysis}{28}{section*.18}\protected@file@percent }
\newlabel{shared-states-methods-univariate-analysis}{{\M@TitleReference {2.2}{Univariate analysis}}{28}{Univariate analysis}{section*.18}{}}
\@writefile{toc}{\contentsline {subsection}{Code availability}{29}{section*.19}\protected@file@percent }
\newlabel{shared-states-methods-code-availability}{{\M@TitleReference {2.2}{Code availability}}{29}{Code availability}{section*.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Results}{29}{section.2.3}\protected@file@percent }
\newlabel{shared-states-results}{{\M@TitleReference {2.3}{Results}}{29}{Results}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{Multi-voxel pattern analysis}{29}{section*.20}\protected@file@percent }
\newlabel{shared-states-results-mvpa}{{\M@TitleReference {2.3}{Multi-voxel pattern analysis}}{29}{Multi-voxel pattern analysis}{section*.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }}{30}{figure.caption.21}\protected@file@percent }
\newlabel{fig:fig-shared-states-3}{{\M@TitleReference {2.3}{Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }}{30}{Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Uncorrected \emph  {t}-value map of average feature weights across subjects; \emph  {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }}{31}{figure.caption.22}\protected@file@percent }
\newlabel{fig:fig-shared-states-4}{{\M@TitleReference {2.4}{Uncorrected \emph  {t}-value map of average feature weights across subjects; \emph  {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }}{31}{Uncorrected \emph {t}-value map of average feature weights across subjects; \emph {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{Univariate analyses}{31}{section*.23}\protected@file@percent }
\newlabel{shared-states-results-univariate}{{\M@TitleReference {2.3}{Univariate analyses}}{31}{Univariate analyses}{section*.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Univariate contrasts for the self-focused and other-focused task.\relax }}{32}{figure.caption.24}\protected@file@percent }
\newlabel{fig:fig-shared-states-5}{{\M@TitleReference {2.5}{Univariate contrasts for the self-focused and other-focused task.\relax }}{32}{Univariate contrasts for the self-focused and other-focused task.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Discussion}{33}{section.2.4}\protected@file@percent }
\newlabel{shared-states-discussion}{{\M@TitleReference {2.4}{Discussion}}{33}{Discussion}{section.2.4}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}How to control for confounds in decoding analyses of neuroimaging data}{38}{chapter.3}\protected@file@percent }
\newlabel{confounds-decoding}{{\M@TitleReference {3}{How to control for confounds in decoding analyses of neuroimaging data}}{38}{How to control for confounds in decoding analyses of neuroimaging data}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{40}{section.3.1}\protected@file@percent }
\newlabel{confounds-decoding-introduction}{{\M@TitleReference {3.1}{Introduction}}{40}{Introduction}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{Partitioning effects into \emph  {true} signal and \emph  {confounded} signal}{42}{section*.25}\protected@file@percent }
\newlabel{confounds-decoding-introduction-true-vs-confounded}{{\M@TitleReference {3.1}{Partitioning effects into \emph  {true} signal and \emph  {confounded} signal}}{42}{\texorpdfstring {Partitioning effects into \emph {true} signal and \emph {confounded} signal}{Partitioning effects into true signal and confounded signal}}{section*.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }}{44}{figure.caption.26}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-1}{{\M@TitleReference {3.1}{Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }}{44}{Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }{figure.caption.26}{}}
\gdef \LT@i {\LT@entry 
    {1}{41.52026pt}\LT@entry 
    {1}{41.52026pt}\LT@entry 
    {1}{214.95947pt}}
\@writefile{toc}{\contentsline {subsection}{Methods for confound control}{45}{section*.27}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods}{{\M@TitleReference {3.1}{Methods for confound control}}{45}{Methods for confound control}{section*.27}{}}
\newlabel{tab:tab-confounds-decoding-1}{{\M@TitleReference {3.1}{Methods for confound control}}{45}{Methods for confound control}{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Notation.\relax }}{45}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{A priori counterbalancing}{46}{section*.28}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-apriori-counterbalancing}{{\M@TitleReference {3.1}{A priori counterbalancing}}{46}{A priori counterbalancing}{section*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{Include confounds in the data}{48}{section*.29}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-include-in-data}{{\M@TitleReference {3.1}{Include confounds in the data}}{48}{Include confounds in the data}{section*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{Control for confounds during pattern estimation}{49}{section*.30}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-pattern-estimation}{{\M@TitleReference {3.1}{Control for confounds during pattern estimation}}{49}{Control for confounds during pattern estimation}{section*.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Post hoc counterbalancing of confounds}{50}{section*.31}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-posthoc-counterbalancing}{{\M@TitleReference {3.1}{Post hoc counterbalancing of confounds}}{50}{Post hoc counterbalancing of confounds}{section*.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }}{51}{figure.caption.32}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-2}{{\M@TitleReference {3.2}{A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }}{51}{A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{51}{section*.33}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-confound-regression}{{\M@TitleReference {3.1}{Confound regression}}{51}{Confound regression}{section*.33}{}}
\@writefile{toc}{\contentsline {subsection}{Current study}{53}{section*.34}\protected@file@percent }
\newlabel{confounds-decoding-introduction-current-study}{{\M@TitleReference {3.1}{Current study}}{53}{Current study}{section*.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Methods}{55}{section.3.2}\protected@file@percent }
\newlabel{confounds-decoding-methods}{{\M@TitleReference {3.2}{Methods}}{55}{Methods}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{Data}{55}{section*.35}\protected@file@percent }
\newlabel{confounds-decoding-methods-data}{{\M@TitleReference {3.2}{Data}}{55}{Data}{section*.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{VBM acquisition \& analysis}{55}{section*.36}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-vbm}{{\M@TitleReference {3.2}{VBM acquisition \& analysis}}{55}{VBM acquisition \& analysis}{section*.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{TBSS acquisition \& analysis}{56}{section*.37}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-tbss}{{\M@TitleReference {3.2}{TBSS acquisition \& analysis}}{56}{TBSS acquisition \& analysis}{section*.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Brain size estimation}{56}{section*.38}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-brainsize}{{\M@TitleReference {3.2}{Brain size estimation}}{56}{Brain size estimation}{section*.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data and code availability}{56}{section*.39}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-data-and-code}{{\M@TitleReference {3.2}{Data and code availability}}{56}{Data and code availability}{section*.39}{}}
\@writefile{toc}{\contentsline {subsection}{Decoding pipeline}{57}{section*.40}\protected@file@percent }
\newlabel{confounds-decoding-methods-pipeline}{{\M@TitleReference {3.2}{Decoding pipeline}}{57}{Decoding pipeline}{section*.40}{}}
\@writefile{toc}{\contentsline {subsection}{Evaluated methods for confound control}{58}{section*.41}\protected@file@percent }
\newlabel{confounds-decoding-methods-evaluated-methods}{{\M@TitleReference {3.2}{Evaluated methods for confound control}}{58}{Evaluated methods for confound control}{section*.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{Post hoc counterbalancing}{58}{section*.42}\protected@file@percent }
\newlabel{confounds-decoding-methods-evaluated-methods-counterbalancing}{{\M@TitleReference {3.2}{Post hoc counterbalancing}}{58}{Post hoc counterbalancing}{section*.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{59}{section*.43}\protected@file@percent }
\newlabel{confound-regression}{{\M@TitleReference {3.2}{Confound regression}}{59}{Confound regression}{section*.43}{}}
\newlabel{eq:cvcr-train}{{3.8}{60}{Confound regression}{equation.3.2.8}{}}
\newlabel{eq:cvcr-test}{{3.9}{61}{Confound regression}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Control for confounds during pattern estimation}{61}{section*.44}\protected@file@percent }
\newlabel{control-for-confounds-during-pattern-estimation}{{\M@TitleReference {3.2}{Control for confounds during pattern estimation}}{61}{Control for confounds during pattern estimation}{section*.44}{}}
\@writefile{toc}{\contentsline {subsection}{Analyses of simulated data}{61}{section*.45}\protected@file@percent }
\newlabel{analyses-of-simulated-data}{{\M@TitleReference {3.2}{Analyses of simulated data}}{61}{Analyses of simulated data}{section*.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analyses}{62}{section*.46}\protected@file@percent }
\newlabel{efficacy-analyses}{{\M@TitleReference {3.2}{Efficacy analyses}}{62}{Efficacy analyses}{section*.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of positive bias after post hoc counterbalancing}{64}{section*.47}\protected@file@percent }
\newlabel{confounds-decoding-methods-counterbalancing-bias}{{\M@TitleReference {3.2}{Analysis of positive bias after post hoc counterbalancing}}{64}{Analysis of positive bias after post hoc counterbalancing}{section*.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }}{66}{figure.caption.48}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-3}{{\M@TitleReference {3.3}{Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }}{66}{Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of negative bias after WDCR}{66}{section*.49}\protected@file@percent }
\newlabel{analysis-of-negative-bias-after-wdcr}{{\M@TitleReference {3.2}{Analysis of negative bias after WDCR}}{66}{Analysis of negative bias after WDCR}{section*.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Results}{68}{section.3.3}\protected@file@percent }
\newlabel{results}{{\M@TitleReference {3.3}{Results}}{68}{Results}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{Influence of brain size}{68}{section*.50}\protected@file@percent }
\newlabel{influence-of-brain-size}{{\M@TitleReference {3.3}{Influence of brain size}}{68}{Influence of brain size}{section*.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \({"ρ}= 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{69}{figure.caption.51}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-4}{{\M@TitleReference {3.4}{A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \({"ρ}= 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{69}{A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \(\rho = 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{Baseline model: no confound control}{69}{section*.52}\protected@file@percent }
\newlabel{baseline-model-no-confound-control}{{\M@TitleReference {3.3}{Baseline model: no confound control}}{69}{Baseline model: no confound control}{section*.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{70}{figure.caption.53}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-5}{{\M@TitleReference {3.5}{Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{70}{Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{Post hoc counterbalancing}{70}{section*.54}\protected@file@percent }
\newlabel{post-hoc-counterbalancing}{{\M@TitleReference {3.3}{Post hoc counterbalancing}}{70}{Post hoc counterbalancing}{section*.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{70}{section*.55}\protected@file@percent }
\newlabel{empirical-results}{{\M@TitleReference {3.3}{Empirical results}}{70}{Empirical results}{section*.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{71}{figure.caption.56}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-6}{{\M@TitleReference {3.6}{Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{71}{Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{72}{figure.caption.57}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-7}{{\M@TitleReference {3.7}{Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{72}{Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{72}{section*.58}\protected@file@percent }
\newlabel{confounds-decoding-results-cb-efficacy}{{\M@TitleReference {3.3}{Efficacy analysis}}{72}{Efficacy analysis}{section*.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.\relax }}{73}{figure.caption.59}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-8}{{\M@TitleReference {3.8}{Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.\relax }}{73}{Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm  {ACC}_{\mathrm  {CB}} - \mathrm  {ACC}_{\mathrm  {baseline}}\)).\relax }}{74}{figure.caption.60}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-9}{{\M@TitleReference {3.9}{The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm  {ACC}_{\mathrm  {CB}} - \mathrm  {ACC}_{\mathrm  {baseline}}\)).\relax }}{74}{The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm {ACC}_{\mathrm {CB}} - \mathrm {ACC}_{\mathrm {baseline}}\)).\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of positive bias after post hoc counterbalancing}{74}{section*.61}\protected@file@percent }
\newlabel{analysis-of-positive-bias-after-post-hoc-counterbalancing}{{\M@TitleReference {3.3}{Analysis of positive bias after post hoc counterbalancing}}{74}{Analysis of positive bias after post hoc counterbalancing}{section*.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{76}{figure.caption.62}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-10}{{\M@TitleReference {3.10}{Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{76}{Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces \textbf  {A}) The proportion of samples classified correctly, separately for the ``retained'' samples (blue line) and ``rejected'' samples (green line); the dashed line represents chance level (0.5). \textbf  {B}) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the ``wrong'' side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{77}{figure.caption.63}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-11}{{\M@TitleReference {3.11}{\textbf  {A}) The proportion of samples classified correctly, separately for the ``retained'' samples (blue line) and ``rejected'' samples (green line); the dashed line represents chance level (0.5). \textbf  {B}) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the ``wrong'' side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{77}{\textbf {A}) The proportion of samples classified correctly, separately for the ``retained'' samples (blue line) and ``rejected'' samples (green line); the dashed line represents chance level (0.5). \textbf {B}) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the ``wrong'' side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{Whole-dataset confound regression (WDCR)}{78}{section*.64}\protected@file@percent }
\newlabel{whole-dataset-confound-regression-wdcr}{{\M@TitleReference {3.3}{Whole-dataset confound regression (WDCR)}}{78}{Whole-dataset confound regression (WDCR)}{section*.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{78}{section*.65}\protected@file@percent }
\newlabel{empirical-results-1}{{\M@TitleReference {3.3}{Empirical results}}{78}{Empirical results}{section*.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{79}{figure.caption.66}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-12}{{\M@TitleReference {3.12}{Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{79}{Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{79}{section*.67}\protected@file@percent }
\newlabel{efficacy-analysis}{{\M@TitleReference {3.3}{Efficacy analysis}}{79}{Efficacy analysis}{section*.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of negative bias after WDCR}{79}{section*.68}\protected@file@percent }
\newlabel{confounds-decoding-results-wdcr-bias}{{\M@TitleReference {3.3}{Analysis of negative bias after WDCR}}{79}{Analysis of negative bias after WDCR}{section*.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces \textbf  {A}) The relationship between the standard deviation of the distribution of feature-target correlations, \emph  {sd}(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \({"ρ}= 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure \ref  {fig:fig-confounds-decoding-S15} for the same plot with different values for \(N\)). \textbf  {B}) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) ``flips'' between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50\% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. \textbf  {C}) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dataset-shift}\unskip \@@italiccorr )}}) and accuracy.\relax }}{81}{figure.caption.69}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-13}{{\M@TitleReference {3.13}{\textbf  {A}) The relationship between the standard deviation of the distribution of feature-target correlations, \emph  {sd}(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \({"ρ}= 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure \ref  {fig:fig-confounds-decoding-S15} for the same plot with different values for \(N\)). \textbf  {B}) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) ``flips'' between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50\% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. \textbf  {C}) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dataset-shift}\unskip \@@italiccorr )}}) and accuracy.\relax }}{81}{\textbf {A}) The relationship between the standard deviation of the distribution of feature-target correlations, \emph {sd}(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \(\rho = 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure \ref {fig:fig-confounds-decoding-S15} for the same plot with different values for \(N\)). \textbf {B}) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) ``flips'' between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50\% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. \textbf {C}) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation \eqref {eq:dataset-shift}) and accuracy.\relax }{figure.caption.69}{}}
\newlabel{eq:dataset-shift}{{3.14}{82}{Analysis of negative bias after WDCR}{equation.3.3.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces \textbf  {A}) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). \textbf  {B}) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. \textbf  {C}) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. \textbf  {D}) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure \ref  {fig:fig-confounds-decoding-S18} for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data.\relax }}{83}{figure.caption.70}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-14}{{\M@TitleReference {3.14}{\textbf  {A}) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). \textbf  {B}) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. \textbf  {C}) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. \textbf  {D}) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure \ref  {fig:fig-confounds-decoding-S18} for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data.\relax }}{83}{\textbf {A}) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). \textbf {B}) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. \textbf {C}) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, \emph {sd}(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. \textbf {D}) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure \ref {fig:fig-confounds-decoding-S18} for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data.\relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {subsection}{Cross-validated confound regression (CVCR)}{83}{section*.71}\protected@file@percent }
\newlabel{cross-validated-confound-regression-cvcr}{{\M@TitleReference {3.3}{Cross-validated confound regression (CVCR)}}{83}{Cross-validated confound regression (CVCR)}{section*.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{83}{section*.72}\protected@file@percent }
\newlabel{empirical-results-2}{{\M@TitleReference {3.3}{Empirical results}}{83}{Empirical results}{section*.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{84}{figure.caption.73}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-15}{{\M@TitleReference {3.15}{Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{84}{Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{84}{section*.74}\protected@file@percent }
\newlabel{efficacy-analysis-1}{{\M@TitleReference {3.3}{Efficacy analysis}}{84}{Efficacy analysis}{section*.74}{}}
\@writefile{toc}{\contentsline {subsection}{Summary methods for confound control}{84}{section*.75}\protected@file@percent }
\newlabel{summary-methods-for-confound-control}{{\M@TitleReference {3.3}{Summary methods for confound control}}{84}{Summary methods for confound control}{section*.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.\relax }}{85}{figure.caption.76}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-16}{{\M@TitleReference {3.16}{An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.\relax }}{85}{An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Discussion}{85}{section.3.4}\protected@file@percent }
\newlabel{confounds-decoding-discussion}{{\M@TitleReference {3.4}{Discussion}}{85}{Discussion}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{Relevance and consequences for previous and future research}{87}{section*.77}\protected@file@percent }
\newlabel{relevance-and-consequences-for-previous-and-future-research}{{\M@TitleReference {3.4}{Relevance and consequences for previous and future research}}{87}{Relevance and consequences for previous and future research}{section*.77}{}}
\@writefile{toc}{\contentsline {subsubsection}{A priori and post hoc counterbalancing}{87}{section*.78}\protected@file@percent }
\newlabel{a-priori-and-post-hoc-counterbalancing}{{\M@TitleReference {3.4}{A priori and post hoc counterbalancing}}{87}{A priori and post hoc counterbalancing}{section*.78}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{88}{section*.79}\protected@file@percent }
\newlabel{confound-regression-1}{{\M@TitleReference {3.4}{Confound regression}}{88}{Confound regression}{section*.79}{}}
\@writefile{toc}{\contentsline {subsubsection}{Relevance to other analysis methods}{89}{section*.80}\protected@file@percent }
\newlabel{relevance-to-other-analysis-methods}{{\M@TitleReference {3.4}{Relevance to other analysis methods}}{89}{Relevance to other analysis methods}{section*.80}{}}
\@writefile{toc}{\contentsline {subsubsection}{Importance for gender decoding studies}{90}{section*.81}\protected@file@percent }
\newlabel{importance-for-gender-decoding-studies}{{\M@TitleReference {3.4}{Importance for gender decoding studies}}{90}{Importance for gender decoding studies}{section*.81}{}}
\@writefile{toc}{\contentsline {subsection}{Choosing a confound model: linear vs.~nonlinear models}{91}{section*.82}\protected@file@percent }
\newlabel{choosing-a-confound-model-linear-vs.-nonlinear-models}{{\M@TitleReference {3.4}{Choosing a confound model: linear vs.~nonlinear models}}{91}{Choosing a confound model: linear vs.~nonlinear models}{section*.82}{}}
\@writefile{toc}{\contentsline {subsection}{Practical recommendations}{92}{section*.83}\protected@file@percent }
\newlabel{practical-recommendations}{{\M@TitleReference {3.4}{Practical recommendations}}{92}{Practical recommendations}{section*.83}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusion}{93}{section.3.5}\protected@file@percent }
\newlabel{conclusion}{{\M@TitleReference {3.5}{Conclusion}}{93}{Conclusion}{section.3.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}{95}{chapter.4}\protected@file@percent }
\newlabel{aomic}{{\M@TitleReference {4}{The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}}{95}{The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Background \& summary}{97}{section.4.1}\protected@file@percent }
\newlabel{background-summary}{{\M@TitleReference {4.1}{Background \& summary}}{97}{Background \& summary}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces General overview of AOMIC's contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of ``derivatives'', i.e., data derived from the original ``raw'' data through state-of-the-art preprocessing pipelines.\relax }}{98}{figure.caption.84}\protected@file@percent }
\newlabel{fig:fig-aomic-1}{{\M@TitleReference {4.1}{General overview of AOMIC's contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of ``derivatives'', i.e., data derived from the original ``raw'' data through state-of-the-art preprocessing pipelines.\relax }}{98}{General overview of AOMIC's contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of ``derivatives'', i.e., data derived from the original ``raw'' data through state-of-the-art preprocessing pipelines.\relax }{figure.caption.84}{}}
\gdef \LT@ii {\LT@entry 
    {1}{61.20044pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}\LT@entry 
    {1}{39.30908pt}}
\newlabel{tab:tab-aomic-1}{{\M@TitleReference {4.1}{Background \& summary}}{99}{Background \& summary}{table.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Overview of the number of subjects per dataset and tasks.\relax }}{99}{table.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Methods}{100}{section.4.2}\protected@file@percent }
\newlabel{methods}{{\M@TitleReference {4.2}{Methods}}{100}{Methods}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{Scanner details and general scanning protocol (all datasets)}{100}{section*.85}\protected@file@percent }
\newlabel{scanner-details-and-general-scanning-protocol-all-datasets}{{\M@TitleReference {4.2}{Scanner details and general scanning protocol (all datasets)}}{100}{Scanner details and general scanning protocol (all datasets)}{section*.85}{}}
\@writefile{toc}{\contentsline {subsection}{ID1000 specifics}{102}{section*.86}\protected@file@percent }
\newlabel{id1000-specifics}{{\M@TitleReference {4.2}{ID1000 specifics}}{102}{ID1000 specifics}{section*.86}{}}
\@writefile{toc}{\contentsline {subsubsection}{Subjects}{102}{section*.87}\protected@file@percent }
\newlabel{subjects}{{\M@TitleReference {4.2}{Subjects}}{102}{Subjects}{section*.87}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data collection protocol}{102}{section*.88}\protected@file@percent }
\newlabel{data-collection-protocol}{{\M@TitleReference {4.2}{Data collection protocol}}{102}{Data collection protocol}{section*.88}{}}
\@writefile{toc}{\contentsline {subsubsection}{Functional MRI paradigm}{103}{section*.89}\protected@file@percent }
\newlabel{functional-mri-paradigm}{{\M@TitleReference {4.2}{Functional MRI paradigm}}{103}{Functional MRI paradigm}{section*.89}{}}
\@writefile{toc}{\contentsline {subsubsection}{Previous analyses}{104}{section*.90}\protected@file@percent }
\newlabel{previous-analyses}{{\M@TitleReference {4.2}{Previous analyses}}{104}{Previous analyses}{section*.90}{}}
\@writefile{toc}{\contentsline {subsection}{PIOP1 and PIOP2 specifics}{104}{section*.91}\protected@file@percent }
\newlabel{piop1-and-piop2-specifics}{{\M@TitleReference {4.2}{PIOP1 and PIOP2 specifics}}{104}{PIOP1 and PIOP2 specifics}{section*.91}{}}
\@writefile{toc}{\contentsline {subsubsection}{Subjects}{105}{section*.92}\protected@file@percent }
\newlabel{subjects-1}{{\M@TitleReference {4.2}{Subjects}}{105}{Subjects}{section*.92}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data collection protocol}{105}{section*.93}\protected@file@percent }
\newlabel{data-collection-protocol-1}{{\M@TitleReference {4.2}{Data collection protocol}}{105}{Data collection protocol}{section*.93}{}}
\@writefile{toc}{\contentsline {subsubsection}{Functional MRI paradigms}{106}{section*.94}\protected@file@percent }
\newlabel{functional-mri-paradigms}{{\M@TitleReference {4.2}{Functional MRI paradigms}}{106}{Functional MRI paradigms}{section*.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.\relax }}{107}{figure.caption.95}\protected@file@percent }
\newlabel{fig:fig-aomic-2}{{\M@TitleReference {4.2}{A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.\relax }}{107}{A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.\relax }{figure.caption.95}{}}
\@writefile{toc}{\contentsline {subsubsection}{Previous analyses}{114}{section*.96}\protected@file@percent }
\newlabel{previous-analyses-1}{{\M@TitleReference {4.2}{Previous analyses}}{114}{Previous analyses}{section*.96}{}}
\@writefile{toc}{\contentsline {subsubsection}{Differences between PIOP1 and PIOP2}{114}{section*.97}\protected@file@percent }
\newlabel{differences-between-piop1-and-piop2}{{\M@TitleReference {4.2}{Differences between PIOP1 and PIOP2}}{114}{Differences between PIOP1 and PIOP2}{section*.97}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Descriptive statistics for biological sex, age, and education level for all three datasets.\relax }}{115}{table.caption.100}\protected@file@percent }
\newlabel{tab:tab-aomic-2}{{\M@TitleReference {4.2}{Descriptive statistics for biological sex, age, and education level for all three datasets.\relax }}{115}{Descriptive statistics for biological sex, age, and education level for all three datasets.\relax }{table.caption.100}{}}
\@writefile{toc}{\contentsline {subsection}{Subject variables (all datasets)}{115}{section*.98}\protected@file@percent }
\newlabel{subject-variables-all-datasets}{{\M@TitleReference {4.2}{Subject variables (all datasets)}}{115}{Subject variables (all datasets)}{section*.98}{}}
\@writefile{toc}{\contentsline {subsubsection}{Age}{115}{section*.99}\protected@file@percent }
\newlabel{age}{{\M@TitleReference {4.2}{Age}}{115}{Age}{section*.99}{}}
\@writefile{toc}{\contentsline {subsubsection}{Biological sex and gender identity}{115}{section*.101}\protected@file@percent }
\newlabel{biological-sex-and-gender-identity}{{\M@TitleReference {4.2}{Biological sex and gender identity}}{115}{Biological sex and gender identity}{section*.101}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sexual orientation}{116}{section*.102}\protected@file@percent }
\newlabel{sexual-orientation}{{\M@TitleReference {4.2}{Sexual orientation}}{116}{Sexual orientation}{section*.102}{}}
\@writefile{toc}{\contentsline {subsubsection}{BMI}{116}{section*.103}\protected@file@percent }
\newlabel{bmi}{{\M@TitleReference {4.2}{BMI}}{116}{BMI}{section*.103}{}}
\@writefile{toc}{\contentsline {subsubsection}{Handedness}{116}{section*.104}\protected@file@percent }
\newlabel{handedness}{{\M@TitleReference {4.2}{Handedness}}{116}{Handedness}{section*.104}{}}
\@writefile{toc}{\contentsline {subsubsection}{Educational level / category}{116}{section*.105}\protected@file@percent }
\newlabel{educational-level-category}{{\M@TitleReference {4.2}{Educational level / category}}{116}{Educational level / category}{section*.105}{}}
\gdef \LT@iii {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\newlabel{tab:tab-aomic-3}{{\M@TitleReference {4.3}{Educational level / category}}{117}{Educational level / category}{table.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Distribution of educational level in the Dutch population (in 2010) and in ID1000.\relax }}{117}{table.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Distribution of background SES.\relax }}{118}{table.caption.106}\protected@file@percent }
\newlabel{tab:tab-aomic-4}{{\M@TitleReference {4.4}{Distribution of background SES.\relax }}{118}{Distribution of background SES.\relax }{table.caption.106}{}}
\@writefile{toc}{\contentsline {subsubsection}{Religion (PIOP1 and ID1000 only)}{118}{section*.107}\protected@file@percent }
\newlabel{religion-piop1-and-id1000-only}{{\M@TitleReference {4.2}{Religion (PIOP1 and ID1000 only)}}{118}{Religion (PIOP1 and ID1000 only)}{section*.107}{}}
\@writefile{toc}{\contentsline {subsection}{Psychometric variables (all datasets)}{119}{section*.108}\protected@file@percent }
\newlabel{psychometric-variables-all-datasets}{{\M@TitleReference {4.2}{Psychometric variables (all datasets)}}{119}{Psychometric variables (all datasets)}{section*.108}{}}
\@writefile{toc}{\contentsline {subsection}{Data standardization, preprocessing, and derivatives}{120}{section*.109}\protected@file@percent }
\newlabel{aomic-derivatives}{{\M@TitleReference {4.2}{Data standardization, preprocessing, and derivatives}}{120}{Data standardization, preprocessing, and derivatives}{section*.109}{}}
\@writefile{toc}{\contentsline {subsubsection}{Raw data standardization}{120}{section*.111}\protected@file@percent }
\newlabel{raw-data-standardization}{{\M@TitleReference {4.2}{Raw data standardization}}{120}{Raw data standardization}{section*.111}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Overview of the types of data and ``derivatives'' included in AOMIC and the software packages used to preprocess and analyze them.\relax }}{121}{figure.caption.110}\protected@file@percent }
\newlabel{fig:fig-aomic-3}{{\M@TitleReference {4.3}{Overview of the types of data and ``derivatives'' included in AOMIC and the software packages used to preprocess and analyze them.\relax }}{121}{Overview of the types of data and ``derivatives'' included in AOMIC and the software packages used to preprocess and analyze them.\relax }{figure.caption.110}{}}
\@writefile{toc}{\contentsline {subsubsection}{Anatomical and functional MRI preprocessing}{122}{section*.112}\protected@file@percent }
\newlabel{anatomical-and-functional-mri-preprocessing}{{\M@TitleReference {4.2}{Anatomical and functional MRI preprocessing}}{122}{Anatomical and functional MRI preprocessing}{section*.112}{}}
\@writefile{toc}{\contentsline {subsubsection}{Diffusion MRI (pre)processing}{124}{section*.113}\protected@file@percent }
\newlabel{diffusion-mri-preprocessing}{{\M@TitleReference {4.2}{Diffusion MRI (pre)processing}}{124}{Diffusion MRI (pre)processing}{section*.113}{}}
\@writefile{toc}{\contentsline {subsubsection}{Freesurfer morphological statistics}{125}{section*.114}\protected@file@percent }
\newlabel{freesurfer-morphological-statistics}{{\M@TitleReference {4.2}{Freesurfer morphological statistics}}{125}{Freesurfer morphological statistics}{section*.114}{}}
\@writefile{toc}{\contentsline {subsubsection}{Voxel-based morphology}{125}{section*.115}\protected@file@percent }
\newlabel{voxel-based-morphology}{{\M@TitleReference {4.2}{Voxel-based morphology}}{125}{Voxel-based morphology}{section*.115}{}}
\@writefile{toc}{\contentsline {subsubsection}{Physiological noise processing}{126}{section*.116}\protected@file@percent }
\newlabel{physiological-noise-processing}{{\M@TitleReference {4.2}{Physiological noise processing}}{126}{Physiological noise processing}{section*.116}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Data records}{127}{section.4.3}\protected@file@percent }
\newlabel{data-records}{{\M@TitleReference {4.3}{Data records}}{127}{Data records}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{Data formats and types}{127}{section*.117}\protected@file@percent }
\newlabel{data-formats-and-types}{{\M@TitleReference {4.3}{Data formats and types}}{127}{Data formats and types}{section*.117}{}}
\gdef \LT@iv {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\@writefile{toc}{\contentsline {subsection}{Data repositories used}{128}{section*.118}\protected@file@percent }
\newlabel{data-repositories-used}{{\M@TitleReference {4.3}{Data repositories used}}{128}{Data repositories used}{section*.118}{}}
\newlabel{tab:tab-aomic-5}{{\M@TitleReference {4.5}{Data repositories used}}{128}{Data repositories used}{table.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Data repository identifiers for subject data (OpenNeuro) and group-level data (NeuroVault).\relax }}{128}{table.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data anonymization}{129}{section*.119}\protected@file@percent }
\newlabel{data-anonymization}{{\M@TitleReference {4.3}{Data anonymization}}{129}{Data anonymization}{section*.119}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Technical validation}{129}{section.4.4}\protected@file@percent }
\newlabel{technical-validation}{{\M@TitleReference {4.4}{Technical validation}}{129}{Technical validation}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{T1-weighted scans}{130}{section*.120}\protected@file@percent }
\newlabel{t1-weighted-scans}{{\M@TitleReference {4.4}{T1-weighted scans}}{130}{T1-weighted scans}{section*.120}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., \hyperlink {ref-Magnotta2006-zs}{2006}); CJV: coefficient of joint variation (Ganzetti et al., \hyperlink {ref-Ganzetti2016-yy}{2016}), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., \hyperlink {ref-Atkinson1997-eu}{1997}), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95\% percentile of all signal intensities; low values may lead to problems with tissue segmentation.\relax }}{131}{figure.caption.121}\protected@file@percent }
\newlabel{fig:fig-aomic-4}{{\M@TitleReference {4.4}{Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., \hyperlink {ref-Magnotta2006-zs}{2006}); CJV: coefficient of joint variation (Ganzetti et al., \hyperlink {ref-Ganzetti2016-yy}{2016}), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., \hyperlink {ref-Atkinson1997-eu}{1997}), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95\% percentile of all signal intensities; low values may lead to problems with tissue segmentation.\relax }}{131}{Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., \protect \hyperlink {ref-Magnotta2006-zs}{2006}); CJV: coefficient of joint variation (Ganzetti et al., \protect \hyperlink {ref-Ganzetti2016-yy}{2016}), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., \protect \hyperlink {ref-Atkinson1997-eu}{1997}), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95\% percentile of all signal intensities; low values may lead to problems with tissue segmentation.\relax }{figure.caption.121}{}}
\@writefile{toc}{\contentsline {subsection}{Functional (BOLD) scans}{131}{section*.122}\protected@file@percent }
\newlabel{functional-bold-scans}{{\M@TitleReference {4.4}{Functional (BOLD) scans}}{131}{Functional (BOLD) scans}{section*.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., \hyperlink {ref-Power2012-kt}{2012}), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., \hyperlink {ref-Saad2013-zd}{2013}); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.\relax }}{132}{figure.caption.123}\protected@file@percent }
\newlabel{fig:fig-aomic-5}{{\M@TitleReference {4.5}{Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., \hyperlink {ref-Power2012-kt}{2012}), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., \hyperlink {ref-Saad2013-zd}{2013}); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.\relax }}{132}{Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., \protect \hyperlink {ref-Power2012-kt}{2012}), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., \protect \hyperlink {ref-Saad2013-zd}{2013}); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.\relax }{figure.caption.123}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.\relax }}{133}{figure.caption.124}\protected@file@percent }
\newlabel{fig:fig-aomic-6}{{\M@TitleReference {4.6}{Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.\relax }}{133}{Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.\relax }{figure.caption.124}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Results from task-specific group-level analyses. Brain maps show uncorrected effects (\emph  {p} \textless {} 0.00001, two-sided) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{135}{figure.caption.125}\protected@file@percent }
\newlabel{fig:fig-aomic-7}{{\M@TitleReference {4.7}{Results from task-specific group-level analyses. Brain maps show uncorrected effects (\emph  {p} \textless {} 0.00001, two-sided) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{135}{Results from task-specific group-level analyses. Brain maps show uncorrected effects (\emph {p} \textless {} 0.00001, two-sided) and were linearly interpolated for visualization in \emph {FSLeyes}. Unthresholded whole-brain \emph {z}-value maps are available on NeuroVault. Unthresholded whole-brain \emph {z}-value maps are available on NeuroVault.\relax }{figure.caption.125}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded \emph  {z}-value maps are available on NeuroVault.\relax }}{136}{figure.caption.126}\protected@file@percent }
\newlabel{fig:fig-aomic-8}{{\M@TitleReference {4.8}{Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded \emph  {z}-value maps are available on NeuroVault.\relax }}{136}{Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded \emph {z}-value maps are available on NeuroVault.\relax }{figure.caption.126}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.\relax }}{136}{figure.caption.127}\protected@file@percent }
\newlabel{fig:fig-aomic-9}{{\M@TitleReference {4.9}{Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.\relax }}{136}{Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.\relax }{figure.caption.127}{}}
\@writefile{toc}{\contentsline {subsection}{Diffusion-weighted scans}{137}{section*.128}\protected@file@percent }
\newlabel{diffusion-weighted-scans}{{\M@TitleReference {4.4}{Diffusion-weighted scans}}{137}{Diffusion-weighted scans}{section*.128}{}}
\@writefile{toc}{\contentsline {subsection}{Physiological data}{137}{section*.131}\protected@file@percent }
\newlabel{physiological-data}{{\M@TitleReference {4.4}{Physiological data}}{137}{Physiological data}{section*.131}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.\relax }}{138}{figure.caption.129}\protected@file@percent }
\newlabel{fig:fig-aomic-10}{{\M@TitleReference {4.10}{Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.\relax }}{138}{Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.\relax }{figure.caption.129}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.\relax }}{139}{figure.caption.130}\protected@file@percent }
\newlabel{fig:fig-aomic-11}{{\M@TitleReference {4.11}{Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.\relax }}{139}{Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.\relax }{figure.caption.130}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at \emph  {z} \textgreater {} 6) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{140}{figure.caption.132}\protected@file@percent }
\newlabel{fig:fig-aomic-12}{{\M@TitleReference {4.12}{Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at \emph  {z} \textgreater {} 6) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{140}{Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at \emph {z} \textgreater {} 6) and were linearly interpolated for visualization in \emph {FSLeyes}. Unthresholded whole-brain \emph {z}-value maps are available on NeuroVault.\relax }{figure.caption.132}{}}
\gdef \LT@v {\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}}
\@writefile{toc}{\contentsline {subsection}{Psychometric data}{141}{section*.133}\protected@file@percent }
\newlabel{psychometric-data}{{\M@TitleReference {4.4}{Psychometric data}}{141}{Psychometric data}{section*.133}{}}
\@writefile{toc}{\contentsline {subsubsection}{Intelligence Structure Test (IST)}{141}{section*.134}\protected@file@percent }
\newlabel{intelligence-structure-test-ist}{{\M@TitleReference {4.4}{Intelligence Structure Test (IST)}}{141}{Intelligence Structure Test (IST)}{section*.134}{}}
\newlabel{tab:tab-aomic-6}{{\M@TitleReference {4.6}{Intelligence Structure Test (IST)}}{141}{Intelligence Structure Test (IST)}{table.4.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Correlations between total score, and subscales of the IST and relevant external variables.\relax }}{141}{table.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Personality: NEO-FFI}{141}{section*.135}\protected@file@percent }
\newlabel{personality-neo-ffi}{{\M@TitleReference {4.4}{Personality: NEO-FFI}}{141}{Personality: NEO-FFI}{section*.135}{}}
\gdef \LT@vi {\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}}
\newlabel{tab:tab-aomic-7}{{\M@TitleReference {4.7}{Personality: NEO-FFI}}{142}{Personality: NEO-FFI}{table.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Cross-correlations for the subscales of the NEO-FFI for the ID1000, PIOP1 and PIOP2 samples.\relax }}{142}{table.4.7}\protected@file@percent }
\gdef \LT@vii {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {subsubsection}{BIS/BAS}{143}{section*.136}\protected@file@percent }
\newlabel{bisbas}{{\M@TitleReference {4.4}{BIS/BAS}}{143}{BIS/BAS}{section*.136}{}}
\newlabel{tab:tab-aomic-8}{{\M@TitleReference {4.8}{BIS/BAS}}{143}{BIS/BAS}{table.4.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Cross-correlations for the subscales of the BIS/BAS for the ID1000 sample.\relax }}{143}{table.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{STAI-T}{143}{section*.137}\protected@file@percent }
\newlabel{stai-t}{{\M@TitleReference {4.4}{STAI-T}}{143}{STAI-T}{section*.137}{}}
\@writefile{toc}{\contentsline {subsection}{Code availability}{144}{section*.138}\protected@file@percent }
\newlabel{aomic-code-availability}{{\M@TitleReference {4.4}{Code availability}}{144}{Code availability}{section*.138}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Choosing to view morbid information involves reward circuitry}{146}{chapter.5}\protected@file@percent }
\newlabel{morbid-curiosity}{{\M@TitleReference {5}{Choosing to view morbid information involves reward circuitry}}{146}{Choosing to view morbid information involves reward circuitry}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{148}{section.5.1}\protected@file@percent }
\newlabel{morbid-curiosity-introduction}{{\M@TitleReference {5.1}{Introduction}}{148}{Introduction}{section.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Overview of paradigm. (\textbf  {A}) The setup of the trials in the choice-condition and passive-viewing condition. Note that in the active-choice condition, participants chose whether they wanted to see the image corresponding to the description during the yes/no response event. In the passive-viewing condition participants did not choose, but confirmed the decision seemingly determined by the computer during the yes/no response event. (\textbf  {B}) An example of a negative description and the consequence of a yes response (either given by the participant, or determined by the computer). (\textbf  {C}) An example of a positive description and the consequence of a no response (either given by the participant, or determined by the computer).\relax }}{153}{figure.caption.139}\protected@file@percent }
\newlabel{fig:fig-morbid-curiosity-1}{{\M@TitleReference {5.1}{Overview of paradigm. (\textbf  {A}) The setup of the trials in the choice-condition and passive-viewing condition. Note that in the active-choice condition, participants chose whether they wanted to see the image corresponding to the description during the yes/no response event. In the passive-viewing condition participants did not choose, but confirmed the decision seemingly determined by the computer during the yes/no response event. (\textbf  {B}) An example of a negative description and the consequence of a yes response (either given by the participant, or determined by the computer). (\textbf  {C}) An example of a positive description and the consequence of a no response (either given by the participant, or determined by the computer).\relax }}{153}{Overview of paradigm. (\textbf {A}) The setup of the trials in the choice-condition and passive-viewing condition. Note that in the active-choice condition, participants chose whether they wanted to see the image corresponding to the description during the yes/no response event. In the passive-viewing condition participants did not choose, but confirmed the decision seemingly determined by the computer during the yes/no response event. (\textbf {B}) An example of a negative description and the consequence of a yes response (either given by the participant, or determined by the computer). (\textbf {C}) An example of a positive description and the consequence of a no response (either given by the participant, or determined by the computer).\relax }{figure.caption.139}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Methods}{154}{section.5.2}\protected@file@percent }
\newlabel{morbid-curiosity-methods}{{\M@TitleReference {5.2}{Methods}}{154}{Methods}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{Participants}{154}{section*.140}\protected@file@percent }
\newlabel{morbid-curiosity-methods-participants}{{\M@TitleReference {5.2}{Participants}}{154}{Participants}{section*.140}{}}
\@writefile{toc}{\contentsline {subsection}{Design}{155}{section*.141}\protected@file@percent }
\newlabel{morbid-curiosity-methods-design}{{\M@TitleReference {5.2}{Design}}{155}{Design}{section*.141}{}}
\@writefile{toc}{\contentsline {subsection}{Materials}{155}{section*.142}\protected@file@percent }
\newlabel{morbid-curiosity-methods-materials}{{\M@TitleReference {5.2}{Materials}}{155}{Materials}{section*.142}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experimental task}{155}{section*.143}\protected@file@percent }
\newlabel{morbid-curiosity-methods-materials-experimental-task}{{\M@TitleReference {5.2}{Experimental task}}{155}{Experimental task}{section*.143}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cues}{157}{section*.144}\protected@file@percent }
\newlabel{morbid-curiosity-methods-materials-cues}{{\M@TitleReference {5.2}{Cues}}{157}{Cues}{section*.144}{}}
\@writefile{toc}{\contentsline {subsubsection}{Images}{158}{section*.145}\protected@file@percent }
\newlabel{morbid-curiosity-methods-materials-images}{{\M@TitleReference {5.2}{Images}}{158}{Images}{section*.145}{}}
\@writefile{toc}{\contentsline {subsubsection}{Questionnaires}{158}{section*.146}\protected@file@percent }
\newlabel{morbid-curiosity-methods-materials-questionnaires}{{\M@TitleReference {5.2}{Questionnaires}}{158}{Questionnaires}{section*.146}{}}
\@writefile{toc}{\contentsline {subsection}{Procedure}{159}{section*.147}\protected@file@percent }
\newlabel{morbid-curiosity-methods-procedure}{{\M@TitleReference {5.2}{Procedure}}{159}{Procedure}{section*.147}{}}
\@writefile{toc}{\contentsline {subsection}{Behavioral analysis}{159}{section*.148}\protected@file@percent }
\newlabel{morbid-curiosity-methods-behavioral-analysis}{{\M@TitleReference {5.2}{Behavioral analysis}}{159}{Behavioral analysis}{section*.148}{}}
\@writefile{toc}{\contentsline {subsection}{Imaging details}{160}{section*.149}\protected@file@percent }
\newlabel{morbid-curiosity-methods-imaging-details}{{\M@TitleReference {5.2}{Imaging details}}{160}{Imaging details}{section*.149}{}}
\@writefile{toc}{\contentsline {subsubsection}{Image acquisition}{160}{section*.150}\protected@file@percent }
\newlabel{morbid-curiosity-methods-imaging-details-image-acquisition}{{\M@TitleReference {5.2}{Image acquisition}}{160}{Image acquisition}{section*.150}{}}
\@writefile{toc}{\contentsline {subsubsection}{Preprocessing}{160}{section*.151}\protected@file@percent }
\newlabel{morbid-curiosity-methods-imaging-details-preprocessing}{{\M@TitleReference {5.2}{Preprocessing}}{160}{Preprocessing}{section*.151}{}}
\@writefile{toc}{\contentsline {subsubsection}{First-level analysis}{161}{section*.152}\protected@file@percent }
\newlabel{morbid-curiosity-methods-imaging-first-level-analysis}{{\M@TitleReference {5.2}{First-level analysis}}{161}{First-level analysis}{section*.152}{}}
\@writefile{toc}{\contentsline {subsubsection}{ROI-based group analysis}{162}{section*.153}\protected@file@percent }
\newlabel{morbid-curiosity-methods-imaging-roi-analysis}{{\M@TitleReference {5.2}{ROI-based group analysis}}{162}{ROI-based group analysis}{section*.153}{}}
\@writefile{toc}{\contentsline {subsubsection}{Further exploratory analyses}{164}{section*.154}\protected@file@percent }
\newlabel{morbid-curiosity-methods-imaging-further-exploratory-analyses}{{\M@TitleReference {5.2}{Further exploratory analyses}}{164}{Further exploratory analyses}{section*.154}{}}
\@writefile{toc}{\contentsline {subsection}{Data availability}{164}{section*.155}\protected@file@percent }
\newlabel{morbid-curiosity-data-availability}{{\M@TitleReference {5.2}{Data availability}}{164}{Data availability}{section*.155}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{164}{section.5.3}\protected@file@percent }
\newlabel{morbid-curiosity-results}{{\M@TitleReference {5.3}{Results}}{164}{Results}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{Participants}{164}{section*.156}\protected@file@percent }
\newlabel{morbid-curiosity-results-participants}{{\M@TitleReference {5.3}{Participants}}{164}{Participants}{section*.156}{}}
\@writefile{toc}{\contentsline {subsection}{Behavior and subjective report}{165}{section*.157}\protected@file@percent }
\newlabel{behavior-and-subjective-report}{{\M@TitleReference {5.3}{Behavior and subjective report}}{165}{Behavior and subjective report}{section*.157}{}}
\@writefile{toc}{\contentsline {subsection}{ROI analyses}{165}{section*.158}\protected@file@percent }
\newlabel{roi-analyses}{{\M@TitleReference {5.3}{ROI analyses}}{165}{ROI analyses}{section*.158}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Results of confirmatory ROI analyses for the induction phase (\textbf  {A}) the contrast negative\textsubscript  {active\textgreater passive} (\textbf  {B}) the contrast negative\textsubscript  {active\textgreater passive} \textgreater {} positive\textsubscript  {active\textgreater passive}. Voxels in red/yellow represent significant \emph  {t}-values (\emph  {p} \textless {} 0.05, corrected for multiple comparisons using the maximum statistic approach). The colored outlines represent the different brain regions within the probabilistic ROIs for the striatum (left) and inferior frontal gyrus (IFG; right). The outlines represent the border of the ROIs thresholded at 0. When voxels within one ROI had a nonzero probability in more than one brain region (e.g., the caudate and nucleus accumbens), the voxel was assigned to the brain region with the largest probability.\relax }}{166}{figure.caption.159}\protected@file@percent }
\newlabel{fig:fig-morbid-curiosity-2}{{\M@TitleReference {5.2}{Results of confirmatory ROI analyses for the induction phase (\textbf  {A}) the contrast negative\textsubscript  {active\textgreater passive} (\textbf  {B}) the contrast negative\textsubscript  {active\textgreater passive} \textgreater {} positive\textsubscript  {active\textgreater passive}. Voxels in red/yellow represent significant \emph  {t}-values (\emph  {p} \textless {} 0.05, corrected for multiple comparisons using the maximum statistic approach). The colored outlines represent the different brain regions within the probabilistic ROIs for the striatum (left) and inferior frontal gyrus (IFG; right). The outlines represent the border of the ROIs thresholded at 0. When voxels within one ROI had a nonzero probability in more than one brain region (e.g., the caudate and nucleus accumbens), the voxel was assigned to the brain region with the largest probability.\relax }}{166}{Results of confirmatory ROI analyses for the induction phase (\textbf {A}) the contrast negative\textsubscript {active\textgreater passive} (\textbf {B}) the contrast negative\textsubscript {active\textgreater passive} \textgreater {} positive\textsubscript {active\textgreater passive}. Voxels in red/yellow represent significant \emph {t}-values (\emph {p} \textless {} 0.05, corrected for multiple comparisons using the maximum statistic approach). The colored outlines represent the different brain regions within the probabilistic ROIs for the striatum (left) and inferior frontal gyrus (IFG; right). The outlines represent the border of the ROIs thresholded at 0. When voxels within one ROI had a nonzero probability in more than one brain region (e.g., the caudate and nucleus accumbens), the voxel was assigned to the brain region with the largest probability.\relax }{figure.caption.159}{}}
\gdef \LT@viii {\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{131.15137pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}}
\@writefile{toc}{\contentsline {subsection}{Whole-brain analyses}{167}{section*.160}\protected@file@percent }
\newlabel{whole-brain-analyses}{{\M@TitleReference {5.3}{Whole-brain analyses}}{167}{Whole-brain analyses}{section*.160}{}}
\newlabel{tab:tab-morbid-curiosity-1}{{\M@TitleReference {5.1}{Whole-brain analyses}}{167}{Whole-brain analyses}{table.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Cluster statistics and associated brain regions from the exploratory whole-brain analysis.\relax }}{167}{table.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Results of the exploratory whole-brain analyses for the induction phase: (\textbf  {A}) the contrast negative\textsubscript  {active\textgreater passive} (red), positive\textsubscript  {active\textgreater passive} (blue), and their conjunction (yellow); (\textbf  {B}) the contrast negative\textsubscript  {active\textgreater passive} \textgreater {} positive\textsubscript  {active\textgreater passive}; (\textbf  {C}) the contrast positive\textsubscript  {active\textgreater passive} \textgreater {} negative\textsubscript  {active\textgreater passive} (empty).\relax }}{170}{figure.caption.161}\protected@file@percent }
\newlabel{fig:fig-morbid-curiosity-3}{{\M@TitleReference {5.3}{Results of the exploratory whole-brain analyses for the induction phase: (\textbf  {A}) the contrast negative\textsubscript  {active\textgreater passive} (red), positive\textsubscript  {active\textgreater passive} (blue), and their conjunction (yellow); (\textbf  {B}) the contrast negative\textsubscript  {active\textgreater passive} \textgreater {} positive\textsubscript  {active\textgreater passive}; (\textbf  {C}) the contrast positive\textsubscript  {active\textgreater passive} \textgreater {} negative\textsubscript  {active\textgreater passive} (empty).\relax }}{170}{Results of the exploratory whole-brain analyses for the induction phase: (\textbf {A}) the contrast negative\textsubscript {active\textgreater passive} (red), positive\textsubscript {active\textgreater passive} (blue), and their conjunction (yellow); (\textbf {B}) the contrast negative\textsubscript {active\textgreater passive} \textgreater {} positive\textsubscript {active\textgreater passive}; (\textbf {C}) the contrast positive\textsubscript {active\textgreater passive} \textgreater {} negative\textsubscript {active\textgreater passive} (empty).\relax }{figure.caption.161}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Discussion}{170}{section.5.4}\protected@file@percent }
\newlabel{morbid-curiosity-discussion}{{\M@TitleReference {5.4}{Discussion}}{170}{Discussion}{section.5.4}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Explainable models of facial movements predict emotion perception behavior}{177}{chapter.6}\protected@file@percent }
\newlabel{hypothesis-kernel-analysis}{{\M@TitleReference {6}{Explainable models of facial movements predict emotion perception behavior}}{177}{Explainable models of facial movements predict emotion perception behavior}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{179}{section.6.1}\protected@file@percent }
\newlabel{hka-introduction}{{\M@TitleReference {6.1}{Introduction}}{179}{Introduction}{section.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{The prediction-explanation-exploration framework}{179}{section*.162}\protected@file@percent }
\newlabel{the-prediction-explanation-exploration-framework}{{\M@TitleReference {6.1}{The prediction-explanation-exploration framework}}{179}{The prediction-explanation-exploration framework}{section*.162}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The modelling framework used in the current study, which uses models for prediction, explanation, and exploration.\relax }}{181}{figure.caption.163}\protected@file@percent }
\newlabel{fig:fig-hka-1}{{\M@TitleReference {6.1}{The modelling framework used in the current study, which uses models for prediction, explanation, and exploration.\relax }}{181}{The modelling framework used in the current study, which uses models for prediction, explanation, and exploration.\relax }{figure.caption.163}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Methods}{181}{section.6.2}\protected@file@percent }
\newlabel{hka-methods}{{\M@TitleReference {6.2}{Methods}}{181}{Methods}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{Hypothesis kernel analysis}{182}{section*.164}\protected@file@percent }
\newlabel{hypothesis-kernel-analysis-1}{{\M@TitleReference {6.2}{Hypothesis kernel analysis}}{182}{Hypothesis kernel analysis}{section*.164}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Schematic visualization of the proposed method using a set of hypothetical AU-emotion mappings (\(\mathbf  {M}\)) and stimuli (\(\mathbf  {S}\)) based on a small set of AUs (five in total). The variable \(P\) represents the number of variables (here: AUs), \(Q\) represents the number of classes (here: emotions), and \(N\) represents the number of trials (here: facial expression stimuli). Note that the AU space technically may contain any number of (\(P\)) dimensions, but is shown here in two dimensions for convenience.\relax }}{183}{figure.caption.165}\protected@file@percent }
\newlabel{fig:fig-hka-2}{{\M@TitleReference {6.2}{Schematic visualization of the proposed method using a set of hypothetical AU-emotion mappings (\(\mathbf  {M}\)) and stimuli (\(\mathbf  {S}\)) based on a small set of AUs (five in total). The variable \(P\) represents the number of variables (here: AUs), \(Q\) represents the number of classes (here: emotions), and \(N\) represents the number of trials (here: facial expression stimuli). Note that the AU space technically may contain any number of (\(P\)) dimensions, but is shown here in two dimensions for convenience.\relax }}{183}{Schematic visualization of the proposed method using a set of hypothetical AU-emotion mappings (\(\mathbf {M}\)) and stimuli (\(\mathbf {S}\)) based on a small set of AUs (five in total). The variable \(P\) represents the number of variables (here: AUs), \(Q\) represents the number of classes (here: emotions), and \(N\) represents the number of trials (here: facial expression stimuli). Note that the AU space technically may contain any number of (\(P\)) dimensions, but is shown here in two dimensions for convenience.\relax }{figure.caption.165}{}}
\@writefile{toc}{\contentsline {subsection}{Ablation and follow-up exploration analyses}{185}{section*.166}\protected@file@percent }
\newlabel{ablation-and-follow-up-exploration-analyses}{{\M@TitleReference {6.2}{Ablation and follow-up exploration analyses}}{185}{Ablation and follow-up exploration analyses}{section*.166}{}}
\@writefile{toc}{\contentsline {subsection}{Noise ceiling estimation}{185}{section*.167}\protected@file@percent }
\newlabel{hka-noise-ceiling}{{\M@TitleReference {6.2}{Noise ceiling estimation}}{185}{Noise ceiling estimation}{section*.167}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The noise ceiling partitions the variance into \emph  {explained variance}, \emph  {unexplained variance}, and \emph  {``irreducible'' noise} for any given model (\(\mathbf  {M}\)). Here, AUROC is used as the metric of model performance, but the noise ceiling can be estimated using any metric.\relax }}{186}{figure.caption.168}\protected@file@percent }
\newlabel{fig:fig-hka-3}{{\M@TitleReference {6.3}{The noise ceiling partitions the variance into \emph  {explained variance}, \emph  {unexplained variance}, and \emph  {``irreducible'' noise} for any given model (\(\mathbf  {M}\)). Here, AUROC is used as the metric of model performance, but the noise ceiling can be estimated using any metric.\relax }}{186}{The noise ceiling partitions the variance into \emph {explained variance}, \emph {unexplained variance}, and \emph {``irreducible'' noise} for any given model (\(\mathbf {M}\)). Here, AUROC is used as the metric of model performance, but the noise ceiling can be estimated using any metric.\relax }{figure.caption.168}{}}
\@writefile{toc}{\contentsline {subsection}{Evaluated mappings}{187}{section*.169}\protected@file@percent }
\newlabel{evaluated-mappings}{{\M@TitleReference {6.2}{Evaluated mappings}}{187}{Evaluated mappings}{section*.169}{}}
\gdef \LT@ix {\LT@entry 
    {1}{39.55225pt}\LT@entry 
    {1}{62.19456pt}\LT@entry 
    {1}{80.88062pt}\LT@entry 
    {1}{62.19456pt}\LT@entry 
    {1}{62.19456pt}\LT@entry 
    {1}{62.19456pt}\LT@entry 
    {1}{62.19456pt}\LT@entry 
    {1}{62.19456pt}}
\newlabel{tab:tab-hka-1}{{\M@TitleReference {6.1}{Evaluated mappings}}{188}{Evaluated mappings}{table.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Evaluated AU-emotion mappings in our study\relax }}{188}{table.6.1}\protected@file@percent }
\newlabel{tab:tab-hka-1}{{\M@TitleReference {6.1}{Evaluated mappings}}{189}{Evaluated mappings}{table.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{Dataset used to evaluate mappings}{191}{section*.170}\protected@file@percent }
\newlabel{hka-dataset}{{\M@TitleReference {6.2}{Dataset used to evaluate mappings}}{191}{Dataset used to evaluate mappings}{section*.170}{}}
\@writefile{toc}{\contentsline {subsection}{Code availability}{192}{section*.171}\protected@file@percent }
\newlabel{hka-code}{{\M@TitleReference {6.2}{Code availability}}{192}{Code availability}{section*.171}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{192}{section.6.3}\protected@file@percent }
\newlabel{hka-results}{{\M@TitleReference {6.3}{Results}}{192}{Results}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{Prediction}{192}{section*.172}\protected@file@percent }
\newlabel{prediction}{{\M@TitleReference {6.3}{Prediction}}{192}{Prediction}{section*.172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \emph  {Prediction}. AUROC scores for each mapping (different bars), shown separately for each emotion (x-axis). Dots indicate individual participants. Dashed line and value directly above represent the noise ceiling (gray area represents ± 1 SD based on bootstrapping the repeated observations). The slightly different noise ceiling for Jack \& Schyns results from using half of the participants for evaluation.\relax }}{194}{figure.caption.173}\protected@file@percent }
\newlabel{fig:fig-hka-4}{{\M@TitleReference {6.4}{\emph  {Prediction}. AUROC scores for each mapping (different bars), shown separately for each emotion (x-axis). Dots indicate individual participants. Dashed line and value directly above represent the noise ceiling (gray area represents ± 1 SD based on bootstrapping the repeated observations). The slightly different noise ceiling for Jack \& Schyns results from using half of the participants for evaluation.\relax }}{194}{\emph {Prediction}. AUROC scores for each mapping (different bars), shown separately for each emotion (x-axis). Dots indicate individual participants. Dashed line and value directly above represent the noise ceiling (gray area represents ± 1 SD based on bootstrapping the repeated observations). The slightly different noise ceiling for Jack \& Schyns results from using half of the participants for evaluation.\relax }{figure.caption.173}{}}
\@writefile{toc}{\contentsline {subsection}{Explanation}{194}{section*.174}\protected@file@percent }
\newlabel{explanation}{{\M@TitleReference {6.3}{Explanation}}{194}{Explanation}{section*.174}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces \emph  {Explanation}. Schematic visualization of the explanation process through ablation of single AUs. The heatmap shows the average decrease (red) or increase (blue) across mappings after ablation of a single AU (x-axis) from a particular emotion configuration (y-axis).\relax }}{195}{figure.caption.175}\protected@file@percent }
\newlabel{fig:fig-hka-5}{{\M@TitleReference {6.5}{\emph  {Explanation}. Schematic visualization of the explanation process through ablation of single AUs. The heatmap shows the average decrease (red) or increase (blue) across mappings after ablation of a single AU (x-axis) from a particular emotion configuration (y-axis).\relax }}{195}{\emph {Explanation}. Schematic visualization of the explanation process through ablation of single AUs. The heatmap shows the average decrease (red) or increase (blue) across mappings after ablation of a single AU (x-axis) from a particular emotion configuration (y-axis).\relax }{figure.caption.175}{}}
\@writefile{toc}{\contentsline {subsection}{Exploration}{195}{section*.176}\protected@file@percent }
\newlabel{exploration}{{\M@TitleReference {6.3}{Exploration}}{195}{Exploration}{section*.176}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces \emph  {Exploration}. Schematic visualization of the exploration process through enriching existing models with additional AUs. Bar graph shows the change in model performance (\$Delta\$ AUROC) of the optimal model relative to the original model (cf.~Figure \ref  {fig:fig-hka-4}). The dashed line represents the original noise ceiling. Bottom: confusion matrices (normalized by the sum across rows, indicating sensitivity) of the original and optimized model and reduction in confusion rate.\relax }}{196}{figure.caption.177}\protected@file@percent }
\newlabel{fig:fig-hka-6}{{\M@TitleReference {6.6}{\emph  {Exploration}. Schematic visualization of the exploration process through enriching existing models with additional AUs. Bar graph shows the change in model performance (\$Delta\$ AUROC) of the optimal model relative to the original model (cf.~Figure \ref  {fig:fig-hka-4}). The dashed line represents the original noise ceiling. Bottom: confusion matrices (normalized by the sum across rows, indicating sensitivity) of the original and optimized model and reduction in confusion rate.\relax }}{196}{\emph {Exploration}. Schematic visualization of the exploration process through enriching existing models with additional AUs. Bar graph shows the change in model performance (\$Delta\$ AUROC) of the optimal model relative to the original model (cf.~Figure \ref {fig:fig-hka-4}). The dashed line represents the original noise ceiling. Bottom: confusion matrices (normalized by the sum across rows, indicating sensitivity) of the original and optimized model and reduction in confusion rate.\relax }{figure.caption.177}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discussion}{197}{section.6.4}\protected@file@percent }
\newlabel{hka-discussion}{{\M@TitleReference {6.4}{Discussion}}{197}{Discussion}{section.6.4}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Affective face perception integrates both static and dynamic information}{200}{chapter.7}\protected@file@percent }
\newlabel{static-vs-dynamic}{{\M@TitleReference {7}{Affective face perception integrates both static and dynamic information}}{200}{Affective face perception integrates both static and dynamic information}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{202}{section.7.1}\protected@file@percent }
\newlabel{svsd-introduction}{{\M@TitleReference {7.1}{Introduction}}{202}{Introduction}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Decomposition of facial expressions in static information (facial morphology) and dynamic information (facial movement), where static information is operationalized as shape deviation relative to the average face while dynamic information is operationalized as shape deviation relative to the static face. The current study's aim is to quantify the importance of static information, relative to dynamic information, in affective perception.\relax }}{205}{figure.caption.178}\protected@file@percent }
\newlabel{fig:fig-svsd-1}{{\M@TitleReference {7.1}{Decomposition of facial expressions in static information (facial morphology) and dynamic information (facial movement), where static information is operationalized as shape deviation relative to the average face while dynamic information is operationalized as shape deviation relative to the static face. The current study's aim is to quantify the importance of static information, relative to dynamic information, in affective perception.\relax }}{205}{Decomposition of facial expressions in static information (facial morphology) and dynamic information (facial movement), where static information is operationalized as shape deviation relative to the average face while dynamic information is operationalized as shape deviation relative to the static face. The current study's aim is to quantify the importance of static information, relative to dynamic information, in affective perception.\relax }{figure.caption.178}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Methods}{206}{section.7.2}\protected@file@percent }
\newlabel{svsd-methods}{{\M@TitleReference {7.2}{Methods}}{206}{Methods}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{Participants}{206}{section*.179}\protected@file@percent }
\newlabel{svsd-participants}{{\M@TitleReference {7.2}{Participants}}{206}{Participants}{section*.179}{}}
\@writefile{toc}{\contentsline {subsection}{Experimental design}{207}{section*.180}\protected@file@percent }
\newlabel{svsd-experimental-design}{{\M@TitleReference {7.2}{Experimental design}}{207}{Experimental design}{section*.180}{}}
\@writefile{toc}{\contentsline {subsection}{Procedure}{207}{section*.181}\protected@file@percent }
\newlabel{svsd-procedure}{{\M@TitleReference {7.2}{Procedure}}{207}{Procedure}{section*.181}{}}
\@writefile{toc}{\contentsline {subsubsection}{Stimulus presentation}{209}{section*.182}\protected@file@percent }
\newlabel{svsd-stimulus-presentation}{{\M@TitleReference {7.2}{Stimulus presentation}}{209}{Stimulus presentation}{section*.182}{}}
\@writefile{toc}{\contentsline {subsubsection}{Categorical emotion ratings}{209}{section*.183}\protected@file@percent }
\newlabel{categorical-emotion-ratings}{{\M@TitleReference {7.2}{Categorical emotion ratings}}{209}{Categorical emotion ratings}{section*.183}{}}
\@writefile{toc}{\contentsline {subsubsection}{Valence/arousal ratings}{210}{section*.184}\protected@file@percent }
\newlabel{valencearousal-ratings}{{\M@TitleReference {7.2}{Valence/arousal ratings}}{210}{Valence/arousal ratings}{section*.184}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Prompts for categorical emotion ratings (left) and valence/arousal ratings (right).\relax }}{211}{figure.caption.185}\protected@file@percent }
\newlabel{fig:fig-svsd-2}{{\M@TitleReference {7.2}{Prompts for categorical emotion ratings (left) and valence/arousal ratings (right).\relax }}{211}{Prompts for categorical emotion ratings (left) and valence/arousal ratings (right).\relax }{figure.caption.185}{}}
\@writefile{toc}{\contentsline {subsection}{Data preprocessing}{211}{section*.186}\protected@file@percent }
\newlabel{svsd-data-preproc}{{\M@TitleReference {7.2}{Data preprocessing}}{211}{Data preprocessing}{section*.186}{}}
\@writefile{toc}{\contentsline {subsubsection}{Rating preprocessing}{211}{section*.187}\protected@file@percent }
\newlabel{rating-preprocessing}{{\M@TitleReference {7.2}{Rating preprocessing}}{211}{Rating preprocessing}{section*.187}{}}
\@writefile{toc}{\contentsline {subsubsection}{Static and dynamic feature operationalization}{212}{section*.188}\protected@file@percent }
\newlabel{static-and-dynamic-feature-operationalization}{{\M@TitleReference {7.2}{Static and dynamic feature operationalization}}{212}{Static and dynamic feature operationalization}{section*.188}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Visualization of the extracted PCA components. The first four PCA components of both the dynamic features (left) and static features (right) are visualized by plotting the inverse transform of a single low-dimensional feature set to 3 standard deviations above the average (\(X_{j} := 3\hat  {{"σ}}_{X_{j}}\)). Colors represent the signed deviation from the mean in standard deviations.\relax }}{214}{figure.caption.189}\protected@file@percent }
\newlabel{fig:fig-svsd-3}{{\M@TitleReference {7.3}{Visualization of the extracted PCA components. The first four PCA components of both the dynamic features (left) and static features (right) are visualized by plotting the inverse transform of a single low-dimensional feature set to 3 standard deviations above the average (\(X_{j} := 3\hat  {{"σ}}_{X_{j}}\)). Colors represent the signed deviation from the mean in standard deviations.\relax }}{214}{Visualization of the extracted PCA components. The first four PCA components of both the dynamic features (left) and static features (right) are visualized by plotting the inverse transform of a single low-dimensional feature set to 3 standard deviations above the average (\(X_{j} := 3\hat {\sigma }_{X_{j}}\)). Colors represent the signed deviation from the mean in standard deviations.\relax }{figure.caption.189}{}}
\@writefile{toc}{\contentsline {subsection}{Predictive analysis}{215}{section*.190}\protected@file@percent }
\newlabel{svsd-pred-analysis}{{\M@TitleReference {7.2}{Predictive analysis}}{215}{Predictive analysis}{section*.190}{}}
\@writefile{toc}{\contentsline {subsubsection}{Categorical emotion model}{215}{section*.191}\protected@file@percent }
\newlabel{svsd-cat-emo}{{\M@TitleReference {7.2}{Categorical emotion model}}{215}{Categorical emotion model}{section*.191}{}}
\@writefile{toc}{\contentsline {subsubsection}{Valence and arousal models}{216}{section*.192}\protected@file@percent }
\newlabel{svsd-valaro}{{\M@TitleReference {7.2}{Valence and arousal models}}{216}{Valence and arousal models}{section*.192}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation procedure}{217}{section*.193}\protected@file@percent }
\newlabel{svsd-cv}{{\M@TitleReference {7.2}{Cross-validation procedure}}{217}{Cross-validation procedure}{section*.193}{}}
\@writefile{toc}{\contentsline {subsubsection}{Performance metrics}{218}{section*.194}\protected@file@percent }
\newlabel{svsd-perf}{{\M@TitleReference {7.2}{Performance metrics}}{218}{Performance metrics}{section*.194}{}}
\@writefile{toc}{\contentsline {subsubsection}{Population prevalence}{218}{section*.195}\protected@file@percent }
\newlabel{population-prevalence}{{\M@TitleReference {7.2}{Population prevalence}}{218}{Population prevalence}{section*.195}{}}
\@writefile{toc}{\contentsline {subsection}{Noise ceiling estimation}{220}{section*.196}\protected@file@percent }
\newlabel{noise-ceiling-estimation}{{\M@TitleReference {7.2}{Noise ceiling estimation}}{220}{Noise ceiling estimation}{section*.196}{}}
\@writefile{toc}{\contentsline {subsection}{Bayesian reconstructions}{221}{section*.197}\protected@file@percent }
\newlabel{svsd-bayes}{{\M@TitleReference {7.2}{Bayesian reconstructions}}{221}{Bayesian reconstructions}{section*.197}{}}
\@writefile{toc}{\contentsline {subsection}{Code availability}{225}{section*.199}\protected@file@percent }
\newlabel{svsd-code}{{\M@TitleReference {7.2}{Code availability}}{225}{Code availability}{section*.199}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Visualization of the encoding (``forward'') model (top) and the reconstruction (``backward'') model (bottom). Note that during the encoding step, a set of \(N\) observations is used to estimate the parameters of the encoding model, while in the reconstruction step only a single observation is reconstructed (although reconstruction can be done for multiple observations at once, if desired).\relax }}{226}{figure.caption.198}\protected@file@percent }
\newlabel{fig:fig-svsd-4}{{\M@TitleReference {7.4}{Visualization of the encoding (``forward'') model (top) and the reconstruction (``backward'') model (bottom). Note that during the encoding step, a set of \(N\) observations is used to estimate the parameters of the encoding model, while in the reconstruction step only a single observation is reconstructed (although reconstruction can be done for multiple observations at once, if desired).\relax }}{226}{Visualization of the encoding (``forward'') model (top) and the reconstruction (``backward'') model (bottom). Note that during the encoding step, a set of \(N\) observations is used to estimate the parameters of the encoding model, while in the reconstruction step only a single observation is reconstructed (although reconstruction can be done for multiple observations at once, if desired).\relax }{figure.caption.198}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{226}{section.7.3}\protected@file@percent }
\newlabel{svsd-results}{{\M@TitleReference {7.3}{Results}}{226}{Results}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{Encoding model performance}{226}{section*.200}\protected@file@percent }
\newlabel{encoding-model-performance}{{\M@TitleReference {7.3}{Encoding model performance}}{226}{Encoding model performance}{section*.200}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Cross-validated model performance, shown separately for each target feature (top: categorical emotion, bottom left: valence, bottom right: arousal) and feature set (static, dynamic, and combined). The bar height represents the mean model performance across participants and the error bars represent ±1 SD. The average within-participant noise ceiling of the \emph  {combined} feature set is plotted for reference as a dashed black line, which is surrounded by a grey area indicating ±1 SD.\relax }}{227}{figure.caption.201}\protected@file@percent }
\newlabel{fig:fig-svsd-5}{{\M@TitleReference {7.5}{Cross-validated model performance, shown separately for each target feature (top: categorical emotion, bottom left: valence, bottom right: arousal) and feature set (static, dynamic, and combined). The bar height represents the mean model performance across participants and the error bars represent ±1 SD. The average within-participant noise ceiling of the \emph  {combined} feature set is plotted for reference as a dashed black line, which is surrounded by a grey area indicating ±1 SD.\relax }}{227}{Cross-validated model performance, shown separately for each target feature (top: categorical emotion, bottom left: valence, bottom right: arousal) and feature set (static, dynamic, and combined). The bar height represents the mean model performance across participants and the error bars represent ±1 SD. The average within-participant noise ceiling of the \emph {combined} feature set is plotted for reference as a dashed black line, which is surrounded by a grey area indicating ±1 SD.\relax }{figure.caption.201}{}}
\@writefile{toc}{\contentsline {subsubsection}{Categorical emotion model}{227}{section*.202}\protected@file@percent }
\newlabel{categorical-emotion-model}{{\M@TitleReference {7.3}{Categorical emotion model}}{227}{Categorical emotion model}{section*.202}{}}
\@writefile{toc}{\contentsline {subsubsection}{Valence/arousal models}{228}{section*.203}\protected@file@percent }
\newlabel{valencearousal-models}{{\M@TitleReference {7.3}{Valence/arousal models}}{228}{Valence/arousal models}{section*.203}{}}
\@writefile{toc}{\contentsline {subsection}{Reconstruction model visualizations}{228}{section*.205}\protected@file@percent }
\newlabel{reconstruction-model-visualizations}{{\M@TitleReference {7.3}{Reconstruction model visualizations}}{228}{Reconstruction model visualizations}{section*.205}{}}
\@writefile{toc}{\contentsline {subsubsection}{Categorical emotion reconstructions}{228}{section*.206}\protected@file@percent }
\newlabel{categorical-emotion-reconstructions}{{\M@TitleReference {7.3}{Categorical emotion reconstructions}}{228}{Categorical emotion reconstructions}{section*.206}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces The posterior distributions for the population prevalence proportion of the results from the categorical emotion (top), valence (middle), and arousal models (bottom). The posteriors for the different emotions with respect to the dynamic and combined categorical emotion model performances completely overlap, so only a single posterior is shown. The filled area represents the probability density higher than the lower bound of the 96\% highest density interval (McElreath, \hyperlink {ref-McElreath2020-pz}{2020}).\relax }}{229}{figure.caption.204}\protected@file@percent }
\newlabel{fig:fig-svsd-6}{{\M@TitleReference {7.6}{The posterior distributions for the population prevalence proportion of the results from the categorical emotion (top), valence (middle), and arousal models (bottom). The posteriors for the different emotions with respect to the dynamic and combined categorical emotion model performances completely overlap, so only a single posterior is shown. The filled area represents the probability density higher than the lower bound of the 96\% highest density interval (McElreath, \hyperlink {ref-McElreath2020-pz}{2020}).\relax }}{229}{The posterior distributions for the population prevalence proportion of the results from the categorical emotion (top), valence (middle), and arousal models (bottom). The posteriors for the different emotions with respect to the dynamic and combined categorical emotion model performances completely overlap, so only a single posterior is shown. The filled area represents the probability density higher than the lower bound of the 96\% highest density interval (McElreath, \protect \hyperlink {ref-McElreath2020-pz}{2020}).\relax }{figure.caption.204}{}}
\@writefile{toc}{\contentsline {subsubsection}{Valence/arousal reconstructions}{230}{section*.208}\protected@file@percent }
\newlabel{valencearousal-reconstructions}{{\M@TitleReference {7.3}{Valence/arousal reconstructions}}{230}{Valence/arousal reconstructions}{section*.208}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces The results from the Bayesian reconstruction approach for each emotion from the categorical emotion model and for seven levels, -0.6, -0.4, -0.2, 0, +0.2, +0.4, and +0.6, of the valence and arousal model, shown separately for the static and dynamic feature sets. Color saturation is proportional to the movement or deviation in vertex space (in \emph  {SD}).\relax }}{231}{figure.caption.207}\protected@file@percent }
\newlabel{fig:fig-svsd-7}{{\M@TitleReference {7.7}{The results from the Bayesian reconstruction approach for each emotion from the categorical emotion model and for seven levels, -0.6, -0.4, -0.2, 0, +0.2, +0.4, and +0.6, of the valence and arousal model, shown separately for the static and dynamic feature sets. Color saturation is proportional to the movement or deviation in vertex space (in \emph  {SD}).\relax }}{231}{The results from the Bayesian reconstruction approach for each emotion from the categorical emotion model and for seven levels, -0.6, -0.4, -0.2, 0, +0.2, +0.4, and +0.6, of the valence and arousal model, shown separately for the static and dynamic feature sets. Color saturation is proportional to the movement or deviation in vertex space (in \emph {SD}).\relax }{figure.caption.207}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Correlations between reconstructions in vertex space for the categorical emotion (left), valence (middle), and arousal (right) models. The top row shows the correlations across all dynamic reconstructions. The middle row shows the correlations across all static reconstructions. The bottom row shows the correlations across each combination of a single dynamic and static reconstruction (e.g., in the bottom left correlation matrix, the top right cell represents the correlation between the static anger and the dynamic surprise reconstruction).\relax }}{233}{figure.caption.209}\protected@file@percent }
\newlabel{fig:fig-svsd-8}{{\M@TitleReference {7.8}{Correlations between reconstructions in vertex space for the categorical emotion (left), valence (middle), and arousal (right) models. The top row shows the correlations across all dynamic reconstructions. The middle row shows the correlations across all static reconstructions. The bottom row shows the correlations across each combination of a single dynamic and static reconstruction (e.g., in the bottom left correlation matrix, the top right cell represents the correlation between the static anger and the dynamic surprise reconstruction).\relax }}{233}{Correlations between reconstructions in vertex space for the categorical emotion (left), valence (middle), and arousal (right) models. The top row shows the correlations across all dynamic reconstructions. The middle row shows the correlations across all static reconstructions. The bottom row shows the correlations across each combination of a single dynamic and static reconstruction (e.g., in the bottom left correlation matrix, the top right cell represents the correlation between the static anger and the dynamic surprise reconstruction).\relax }{figure.caption.209}{}}
\@writefile{toc}{\contentsline {subsubsection}{Correlations between affective properties}{234}{section*.210}\protected@file@percent }
\newlabel{correlations-between-affective-properties}{{\M@TitleReference {7.3}{Correlations between affective properties}}{234}{Correlations between affective properties}{section*.210}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Discussion}{234}{section.7.4}\protected@file@percent }
\newlabel{svsd-discussion}{{\M@TitleReference {7.4}{Discussion}}{234}{Discussion}{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{Facial morphology independently contributes to affective face perception}{235}{section*.211}\protected@file@percent }
\newlabel{facial-morphology-independently-contributes-to-affective-face-perception}{{\M@TitleReference {7.4}{Facial morphology independently contributes to affective face perception}}{235}{Facial morphology independently contributes to affective face perception}{section*.211}{}}
\@writefile{toc}{\contentsline {subsection}{The influence of facial morphology does not result from visual similarity to facial movements}{236}{section*.212}\protected@file@percent }
\newlabel{the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements}{{\M@TitleReference {7.4}{The influence of facial morphology does not result from visual similarity to facial movements}}{236}{The influence of facial morphology does not result from visual similarity to facial movements}{section*.212}{}}
\@writefile{toc}{\contentsline {subsection}{Categorical representations of experienced valence and arousal correlate with representations of perceived emotions}{237}{section*.213}\protected@file@percent }
\newlabel{categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions}{{\M@TitleReference {7.4}{Categorical representations of experienced valence and arousal correlate with representations of perceived emotions}}{237}{Categorical representations of experienced valence and arousal correlate with representations of perceived emotions}{section*.213}{}}
\@writefile{toc}{\contentsline {subsection}{Predictive models quantify what is (not yet) known}{238}{section*.214}\protected@file@percent }
\newlabel{predictive-models-quantify-what-is-not-yet-known}{{\M@TitleReference {7.4}{Predictive models quantify what is (not yet) known}}{238}{Predictive models quantify what is (not yet) known}{section*.214}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and further directions}{238}{section*.215}\protected@file@percent }
\newlabel{limitations-and-further-directions}{{\M@TitleReference {7.4}{Limitations and further directions}}{238}{Limitations and further directions}{section*.215}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Conclusion}{239}{section.7.5}\protected@file@percent }
\newlabel{svsd-conclusion}{{\M@TitleReference {7.5}{Conclusion}}{239}{Conclusion}{section.7.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Discussion}{241}{chapter.8}\protected@file@percent }
\newlabel{general-discussion}{{\M@TitleReference {8}{Discussion}}{241}{Discussion}{chapter.8}{}}
\@writefile{toc}{\contentsline {part}{Appendices}{247}{section*.216}\protected@file@percent }
\gdef \LT@x {\LT@entry 
    {1}{61.20044pt}\LT@entry 
    {1}{118.39978pt}\LT@entry 
    {1}{118.39978pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Supplement to Chapter \ref  {shared-states}}{248}{appendix.A}\protected@file@percent }
\newlabel{shared-states-supplement}{{\M@TitleReference {A}{Supplement to Chapter \ref  {shared-states}}}{248}{Supplement to Chapter \ref {shared-states}}{appendix.A}{}}
\newlabel{tab:tab-shared-states-S1}{{\M@TitleReference {A.1}{Supplement to Chapter \ref  {shared-states}}}{248}{Supplement to Chapter \ref {shared-states}}{table.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Stimuli used for SF-task\relax }}{248}{table.A.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph  {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph  {F}(2, 17) = 17.74, \emph  {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph  {M} = 74.00, \emph  {SE} = 2.10) were significantly less successful (\emph  {p} \textless {} 0.001) than both action-trials (\emph  {M} = 85.50, \emph  {SE} = 1.85) and situation trials (\emph  {M} = 90.00, \emph  {SE} = 1.92).\relax }}{254}{figure.caption.217}\protected@file@percent }
\newlabel{fig:fig-shared-states-S1}{{\M@TitleReference {A.1}{Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph  {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph  {F}(2, 17) = 17.74, \emph  {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph  {M} = 74.00, \emph  {SE} = 2.10) were significantly less successful (\emph  {p} \textless {} 0.001) than both action-trials (\emph  {M} = 85.50, \emph  {SE} = 1.85) and situation trials (\emph  {M} = 90.00, \emph  {SE} = 1.92).\relax }}{254}{Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph {F}(2, 17) = 17.74, \emph {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph {M} = 74.00, \emph {SE} = 2.10) were significantly less successful (\emph {p} \textless {} 0.001) than both action-trials (\emph {M} = 85.50, \emph {SE} = 1.85) and situation trials (\emph {M} = 90.00, \emph {SE} = 1.92).\relax }{figure.caption.217}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf  {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf  {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf  {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf  {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }}{255}{figure.caption.218}\protected@file@percent }
\newlabel{fig:fig-shared-states-S2}{{\M@TitleReference {A.2}{Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf  {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf  {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf  {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf  {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }}{255}{Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }{figure.caption.218}{}}
\gdef \LT@xi {\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}}
\gdef \LT@xii {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\newlabel{tab:tab-shared-states-S2}{{\M@TitleReference {A.2}{Supplement to Chapter \ref  {shared-states}}}{256}{Supplement to Chapter \ref {shared-states}}{table.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Parameters assessed in the optimization set\relax }}{256}{table.A.2}\protected@file@percent }
\newlabel{tab:tab-shared-states-S3}{{\M@TitleReference {A.3}{Supplement to Chapter \ref  {shared-states}}}{256}{Supplement to Chapter \ref {shared-states}}{table.A.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Mean general classification scores per subject for the self- and cross-analysis on the validation-set only.\relax }}{256}{table.A.3}\protected@file@percent }
\gdef \LT@xiii {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\newlabel{tab:tab-shared-states-S4}{{\M@TitleReference {A.4}{Supplement to Chapter \ref  {shared-states}}}{258}{Supplement to Chapter \ref {shared-states}}{table.A.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Most important voxels in terms of their average weight across iterations and subjects.\relax }}{258}{table.A.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph  {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph  {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph  {p} = 0.014 and \emph  {p} = 0.0007 respectively. Interoception was classified at chance level, \emph  {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }}{261}{figure.caption.219}\protected@file@percent }
\newlabel{fig:fig-shared-states-S3}{{\M@TitleReference {A.3}{Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph  {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph  {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph  {p} = 0.014 and \emph  {p} = 0.0007 respectively. Interoception was classified at chance level, \emph  {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }}{261}{Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph {p} = 0.014 and \emph {p} = 0.0007 respectively. Interoception was classified at chance level, \emph {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }{figure.caption.219}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }}{262}{figure.caption.220}\protected@file@percent }
\newlabel{fig:fig-shared-states-S4}{{\M@TitleReference {A.4}{Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }}{262}{Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }{figure.caption.220}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph  {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph  {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph  {p} = 0.0013 and \emph  {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph  {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }}{263}{figure.caption.221}\protected@file@percent }
\newlabel{fig:fig-shared-states-S5}{{\M@TitleReference {A.5}{A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph  {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph  {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph  {p} = 0.0013 and \emph  {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph  {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }}{263}{A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph {p} = 0.0013 and \emph {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }{figure.caption.221}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf  {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph  {r} = -0.04, \emph  {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf  {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf  {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }}{264}{figure.caption.222}\protected@file@percent }
\newlabel{fig:fig-shared-states-S6}{{\M@TitleReference {A.6}{Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf  {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph  {r} = -0.04, \emph  {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf  {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf  {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }}{264}{Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph {r} = -0.04, \emph {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }{figure.caption.222}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph  {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph  {p}(action) \textless {} 0.001, \emph  {p}(interoception) = 0.008, \emph  {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph  {p} \textless {} 0.001), but not significant for situation (\emph  {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }}{265}{figure.caption.223}\protected@file@percent }
\newlabel{fig:fig-shared-states-S7}{{\M@TitleReference {A.7}{Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph  {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph  {p}(action) \textless {} 0.001, \emph  {p}(interoception) = 0.008, \emph  {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph  {p} \textless {} 0.001), but not significant for situation (\emph  {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }}{265}{Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph {p}(action) \textless {} 0.001, \emph {p}(interoception) = 0.008, \emph {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph {p} \textless {} 0.001), but not significant for situation (\emph {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }{figure.caption.223}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }}{266}{figure.caption.224}\protected@file@percent }
\newlabel{fig:fig-shared-states-S8}{{\M@TitleReference {A.8}{Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }}{266}{Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }{figure.caption.224}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {B}Supplement to Chapter \ref  {confounds-decoding}}{267}{appendix.B}\protected@file@percent }
\newlabel{confounds-decoding-supplement}{{\M@TitleReference {B}{Supplement to Chapter \ref  {confounds-decoding}}}{267}{Supplement to Chapter \ref {confounds-decoding}}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Supplementary methods}{267}{section.B.1}\protected@file@percent }
\newlabel{supplementary-methods}{{\M@TitleReference {B.1}{Supplementary methods}}{267}{Supplementary methods}{section.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{Functional MRI simulation}{267}{section*.225}\protected@file@percent }
\newlabel{functional-mri-simulation}{{\M@TitleReference {B.1}{Functional MRI simulation}}{267}{Functional MRI simulation}{section*.225}{}}
\@writefile{toc}{\contentsline {subsubsection}{Rationale}{267}{section*.226}\protected@file@percent }
\newlabel{rationale}{{\M@TitleReference {B.1}{Rationale}}{267}{Rationale}{section*.226}{}}
\@writefile{toc}{\contentsline {subsubsection}{Generation of the data (\(X\), \(y\), \(C\))}{268}{section*.227}\protected@file@percent }
\newlabel{generation-of-the-data-x-y-c}{{\M@TitleReference {B.1}{Generation of the data (\(X\), \(y\), \(C\))}}{268}{\texorpdfstring {Generation of the data (\(X\), \(y\), \(C\))}{Generation of the data (X, y, C)}}{section*.227}{}}
\@writefile{toc}{\contentsline {subsubsection}{Estimating activity patterns from the data}{270}{section*.228}\protected@file@percent }
\newlabel{estimating-activity-patterns-from-the-data}{{\M@TitleReference {B.1}{Estimating activity patterns from the data}}{270}{Estimating activity patterns from the data}{section*.228}{}}
\@writefile{toc}{\contentsline {subsection}{Testing confound regression on simulated fMRI data}{271}{section*.229}\protected@file@percent }
\newlabel{testing-confound-regression-on-simulated-fmri-data}{{\M@TitleReference {B.1}{Testing confound regression on simulated fMRI data}}{271}{Testing confound regression on simulated fMRI data}{section*.229}{}}
\@writefile{toc}{\contentsline {subsection}{Controlling for confounds during pattern estimation}{272}{section*.230}\protected@file@percent }
\newlabel{controlling-for-confounds-during-pattern-estimation}{{\M@TitleReference {B.1}{Controlling for confounds during pattern estimation}}{272}{Controlling for confounds during pattern estimation}{section*.230}{}}
\@writefile{toc}{\contentsline {subsection}{Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size}{273}{section*.231}\protected@file@percent }
\newlabel{linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size}{{\M@TitleReference {B.1}{Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size}}{273}{Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size}{section*.231}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Supplementary results}{274}{section.B.2}\protected@file@percent }
\newlabel{supplementary-results}{{\M@TitleReference {B.2}{Supplementary results}}{274}{Supplementary results}{section.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{Testing confound regression on simulated fMRI data}{274}{section*.232}\protected@file@percent }
\newlabel{testing-confound-regression-on-simulated-fmri-data-1}{{\M@TitleReference {B.2}{Testing confound regression on simulated fMRI data}}{274}{Testing confound regression on simulated fMRI data}{section*.232}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95\% CI across iterations.\relax }}{275}{figure.caption.233}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S1}{{\M@TitleReference {B.1}{Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95\% CI across iterations.\relax }}{275}{Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95\% CI across iterations.\relax }{figure.caption.233}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \({"σ}_{\mathrm  {filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).\relax }}{275}{figure.caption.234}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S2}{{\M@TitleReference {B.2}{Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \({"σ}_{\mathrm  {filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).\relax }}{275}{Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \(\sigma _{\mathrm {filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).\relax }{figure.caption.234}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Model performance when controlling for confounds during pattern estimation using the ``default'' (upper panels) and ``aggressive'' (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.\relax }}{276}{figure.caption.236}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S3}{{\M@TitleReference {B.3}{Model performance when controlling for confounds during pattern estimation using the ``default'' (upper panels) and ``aggressive'' (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.\relax }}{276}{Model performance when controlling for confounds during pattern estimation using the ``default'' (upper panels) and ``aggressive'' (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.\relax }{figure.caption.236}{}}
\@writefile{toc}{\contentsline {subsection}{Controlling for confounds during pattern estimation}{276}{section*.235}\protected@file@percent }
\newlabel{controlling-for-confounds-during-pattern-estimation-1}{{\M@TitleReference {B.2}{Controlling for confounds during pattern estimation}}{276}{Controlling for confounds during pattern estimation}{section*.235}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Distribution of first-level parameter estimates, \(\hat  {{"β}}_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition.\relax }}{277}{figure.caption.238}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S4}{{\M@TitleReference {B.4}{Distribution of first-level parameter estimates, \(\hat  {{"β}}_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition.\relax }}{277}{Distribution of first-level parameter estimates, \(\hat {\beta }_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition.\relax }{figure.caption.238}{}}
\@writefile{toc}{\contentsline {subsubsection}{Explanation for bias in trial-wise decoding analyses}{277}{section*.237}\protected@file@percent }
\newlabel{explanation-for-bias-in-trial-wise-decoding-analyses}{{\M@TitleReference {B.2}{Explanation for bias in trial-wise decoding analyses}}{277}{Explanation for bias in trial-wise decoding analyses}{section*.237}{}}
\@writefile{toc}{\contentsline {subsubsection}{Explanation for bias in run-wise decoding analyses}{278}{section*.240}\protected@file@percent }
\newlabel{explanation-for-bias-in-run-wise-decoding-analyses}{{\M@TitleReference {B.2}{Explanation for bias in run-wise decoding analyses}}{278}{Explanation for bias in run-wise decoding analyses}{section*.240}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (``true generative model'') shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\({"β}_{X|y=0} = {"β}_{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (``signal'') shows the signal resulting from the generative model (including noise, \({"ε}\)). The lower panel (``estimated parameters'') shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.\relax }}{279}{figure.caption.239}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S5}{{\M@TitleReference {B.5}{Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (``true generative model'') shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\({"β}_{X|y=0} = {"β}_{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (``signal'') shows the signal resulting from the generative model (including noise, \({"ε}\)). The lower panel (``estimated parameters'') shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.\relax }}{279}{Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (``true generative model'') shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\(\beta _{X|y=0} = \beta _{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (``signal'') shows the signal resulting from the generative model (including noise, \(\epsilon \)). The lower panel (``estimated parameters'') shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.\relax }{figure.caption.239}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Visualization of model performance and feature distributions based on patterns of ``raw'' parameter estimates (\(\hat  {{"β}}_{X}\)), variance of parameter estimates (\(\mathrm  {var}(\hat  {{"β}}_{X})\)), or \emph  {t}-values (\(t(\hat  {{"β}}_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that ``variance decoding'' only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\).\relax }}{280}{figure.caption.241}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S6}{{\M@TitleReference {B.6}{Visualization of model performance and feature distributions based on patterns of ``raw'' parameter estimates (\(\hat  {{"β}}_{X}\)), variance of parameter estimates (\(\mathrm  {var}(\hat  {{"β}}_{X})\)), or \emph  {t}-values (\(t(\hat  {{"β}}_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that ``variance decoding'' only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\).\relax }}{280}{Visualization of model performance and feature distributions based on patterns of ``raw'' parameter estimates (\(\hat {\beta }_{X}\)), variance of parameter estimates (\(\mathrm {var}(\hat {\beta }_{X})\)), or \emph {t}-values (\(t(\hat {\beta }_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that ``variance decoding'' only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\).\relax }{figure.caption.241}{}}
\@writefile{toc}{\contentsline {subsection}{Linear vs.~nonlinear confound models: predicting VBM and TBSS intensity using brain size}{280}{section*.242}\protected@file@percent }
\newlabel{linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size}{{\M@TitleReference {B.2}{Linear vs.~nonlinear confound models: predicting VBM and TBSS intensity using brain size}}{280}{Linear vs.~nonlinear confound models: predicting VBM and TBSS intensity using brain size}{section*.242}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions reasonably well.\relax }}{281}{figure.caption.243}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S7}{{\M@TitleReference {B.7}{Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions reasonably well.\relax }}{281}{Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions reasonably well.\relax }{figure.caption.243}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) in voxel set B.\relax }}{283}{figure.caption.244}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S8}{{\M@TitleReference {B.8}{Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) in voxel set B.\relax }}{283}{Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \(\Delta R^{2}_{\mathrm {linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \(\Delta R^{2}_{\mathrm {linear-cubic}}\) in voxel set B.\relax }{figure.caption.244}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.9}{\ignorespaces Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions well.\relax }}{284}{figure.caption.245}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S9}{{\M@TitleReference {B.9}{Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions well.\relax }}{284}{Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions well.\relax }{figure.caption.245}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.10}{\ignorespaces Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (``train only'') on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm  {test}} = X_{\mathrm  {test}} - C_{\mathrm  {test}}\hat  {{"β}}_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).\relax }}{285}{figure.caption.246}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S10}{{\M@TitleReference {B.10}{Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (``train only'') on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm  {test}} = X_{\mathrm  {test}} - C_{\mathrm  {test}}\hat  {{"β}}_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).\relax }}{285}{Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (``train only'') on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf.~Figure \ref {fig:fig-confounds-decoding-8} in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm {test}} = X_{\mathrm {test}} - C_{\mathrm {test}}\hat {\beta }_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).\relax }{figure.caption.246}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.11}{\ignorespaces Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, \hyperlink {ref-powers2020evaluation}{2011}). The results are highly similar to results obtained when using the \(F_{1}\) score (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text).\relax }}{285}{figure.caption.247}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S11}{{\M@TitleReference {B.11}{Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, \hyperlink {ref-powers2020evaluation}{2011}). The results are highly similar to results obtained when using the \(F_{1}\) score (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text).\relax }}{285}{Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, \protect \hyperlink {ref-powers2020evaluation}{2011}). The results are highly similar to results obtained when using the \(F_{1}\) score (cf.~Figure \ref {fig:fig-confounds-decoding-8} in the main text).\relax }{figure.caption.247}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.12}{\ignorespaces Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the ``Include confound in model'' method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.\relax }}{286}{figure.caption.248}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S12}{{\M@TitleReference {B.12}{Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the ``Include confound in model'' method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.\relax }}{286}{Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the ``Include confound in model'' method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.\relax }{figure.caption.248}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.13}{\ignorespaces Reproduction of Figure \ref  {fig:fig-confounds-decoding-8} from the main text (``generic simulation'' results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the ``targeted subsampling'' method (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text), albeit much slower.\relax }}{286}{figure.caption.249}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S13}{{\M@TitleReference {B.13}{Reproduction of Figure \ref  {fig:fig-confounds-decoding-8} from the main text (``generic simulation'' results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the ``targeted subsampling'' method (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text), albeit much slower.\relax }}{286}{Reproduction of Figure \ref {fig:fig-confounds-decoding-8} from the main text (``generic simulation'' results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the ``targeted subsampling'' method (cf.~Figure \ref {fig:fig-confounds-decoding-8} in the main text), albeit much slower.\relax }{figure.caption.249}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.14}{\ignorespaces Reproduction of Figure \ref  {fig:fig-confounds-decoding-10} from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90\% smaller sample).\relax }}{287}{figure.caption.250}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S14}{{\M@TitleReference {B.14}{Reproduction of Figure \ref  {fig:fig-confounds-decoding-10} from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90\% smaller sample).\relax }}{287}{Reproduction of Figure \ref {fig:fig-confounds-decoding-10} from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90\% smaller sample).\relax }{figure.caption.250}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.15}{\ignorespaces These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{288}{figure.caption.251}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S15}{{\M@TitleReference {B.15}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{288}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph {sd}(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }{figure.caption.251}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.16}{\ignorespaces These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., \hyperlink {ref-Jamalabadi2016-gr}{2016}). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{288}{figure.caption.252}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S16}{{\M@TitleReference {B.16}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., \hyperlink {ref-Jamalabadi2016-gr}{2016}). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{288}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., \protect \hyperlink {ref-Jamalabadi2016-gr}{2016}). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }{figure.caption.252}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.17}{\ignorespaces These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on \emph  {sd}(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{288}{figure.caption.253}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S17}{{\M@TitleReference {B.17}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on \emph  {sd}(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{288}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph {sd}(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on \emph {sd}(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }{figure.caption.253}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.18}{\ignorespaces The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\).\relax }}{289}{figure.caption.254}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S18}{{\M@TitleReference {B.18}{The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\).\relax }}{289}{The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\).\relax }{figure.caption.254}{}}
\gdef \LT@xiv {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {C}Supplement to Chapter \ref  {aomic}}{290}{appendix.C}\protected@file@percent }
\newlabel{aomic-supplement}{{\M@TitleReference {C}{Supplement to Chapter \ref  {aomic}}}{290}{Supplement to Chapter \ref {aomic}}{appendix.C}{}}
\newlabel{tab:tab-aomic-S1}{{\M@TitleReference {C.1}{Supplement to Chapter \ref  {aomic}}}{290}{Supplement to Chapter \ref {aomic}}{table.C.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Acquisition parameters for the T1-weighted scans acquired across all three datasets.\relax }}{290}{table.C.1}\protected@file@percent }
\gdef \LT@xv {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\gdef \LT@xvi {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\newlabel{tab:tab-aomic-S2}{{\M@TitleReference {C.2}{Supplement to Chapter \ref  {aomic}}}{291}{Supplement to Chapter \ref {aomic}}{table.C.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.2}{\ignorespaces Acquisition parameters for the phase-difference fieldmap scans acquired across all three datasets.\relax }}{291}{table.C.2}\protected@file@percent }
\newlabel{tab:tab-aomic-S3}{{\M@TitleReference {C.3}{Supplement to Chapter \ref  {aomic}}}{291}{Supplement to Chapter \ref {aomic}}{table.C.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.3}{\ignorespaces Acquisition parameters for the fMRI scans acquired across all three datasets.\relax }}{291}{table.C.3}\protected@file@percent }
\gdef \LT@xvii {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\newlabel{tab:tab-aomic-S4}{{\M@TitleReference {C.4}{Supplement to Chapter \ref  {aomic}}}{292}{Supplement to Chapter \ref {aomic}}{table.C.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.4}{\ignorespaces Acquisition parameters for the DWI scans acquired across all three datasets.\relax }}{292}{table.C.4}\protected@file@percent }
\gdef \LT@xviii {\LT@entry 
    {1}{74.97656pt}\LT@entry 
    {1}{55.75586pt}\LT@entry 
    {1}{55.75586pt}\LT@entry 
    {1}{55.75586pt}\LT@entry 
    {1}{55.75586pt}}
\newlabel{tab:tab-aomic-S5}{{\M@TitleReference {C.5}{Supplement to Chapter \ref  {aomic}}}{294}{Supplement to Chapter \ref {aomic}}{table.C.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.5}{\ignorespaces Description of the subject variables and psychometric variables.\relax }}{294}{table.C.5}\protected@file@percent }
\gdef \LT@xix {\LT@entry 
    {1}{27.74414pt}\LT@entry 
    {1}{90.08528pt}\LT@entry 
    {1}{90.08528pt}\LT@entry 
    {1}{90.08528pt}}
\newlabel{tab:tab-aomic-S6}{{\M@TitleReference {C.6}{Supplement to Chapter \ref  {aomic}}}{296}{Supplement to Chapter \ref {aomic}}{table.C.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.6}{\ignorespaces All data types with associated identifiers, descriptions, and modalities.\relax }}{296}{table.C.6}\protected@file@percent }
\gdef \LT@xx {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {D}Supplement to Chapter \ref  {morbid-curiosity}}{299}{appendix.D}\protected@file@percent }
\newlabel{morbid-curiosity-supplement}{{\M@TitleReference {D}{Supplement to Chapter \ref  {morbid-curiosity}}}{299}{Supplement to Chapter \ref {morbid-curiosity}}{appendix.D}{}}
\newlabel{tab:tab-morbid-curiosity-S1}{{\M@TitleReference {D.1}{Supplement to Chapter \ref  {morbid-curiosity}}}{299}{Supplement to Chapter \ref {morbid-curiosity}}{table.D.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.1}{\ignorespaces  List of Neurosynth terms associated with the results of the exploratory whole-brain analyses.\relax }}{299}{table.D.1}\protected@file@percent }
\gdef \LT@xxi {\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{131.15137pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}}
\newlabel{tab:tab-morbid-curiosity-S2}{{\M@TitleReference {D.2}{Supplement to Chapter \ref  {morbid-curiosity}}}{300}{Supplement to Chapter \ref {morbid-curiosity}}{table.D.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.2}{\ignorespaces Cluster statistics and associated brain regions from the exploratory whole-brain analysis.\relax }}{300}{table.D.2}\protected@file@percent }
\gdef \LT@xxii {\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{131.15137pt}\LT@entry 
    {1}{20.85608pt}\LT@entry 
    {1}{20.85608pt}}
\newlabel{tab:tab-morbid-curiosity-S3}{{\M@TitleReference {D.3}{Supplement to Chapter \ref  {morbid-curiosity}}}{303}{Supplement to Chapter \ref {morbid-curiosity}}{table.D.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.3}{\ignorespaces Cluster statistics and associated brain regions from the exploratory whole-brain analysis.\relax }}{303}{table.D.3}\protected@file@percent }
\gdef \LT@xxiii {\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}}
\newlabel{tab:tab-morbid-curiosity-S4}{{\M@TitleReference {D.4}{Supplement to Chapter \ref  {morbid-curiosity}}}{305}{Supplement to Chapter \ref {morbid-curiosity}}{table.D.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.4}{\ignorespaces Stimulus codes for the images used in the choice task.\relax }}{305}{table.D.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {D.1}{\ignorespaces Subplots of individual regressors from the significant voxels in the confirmatory contrast negative\textasciitilde active - passive\textasciitilde {} - positive\textasciitilde active - passive\textasciitilde {} in the induction-phase. These plots show the direction of the effects. Plots are averaged over all significant voxels within each ROI (striatum in upper plots, IFG in lower plots), separately for the negative trials (left plots) and positive trials (right plots) with subplots for the active choice and passive viewing condition. Dots represent the participant-specific ROI-average parameter estimate from the first-level analysis. The horizontal line in the boxplots represents the median and the whiskers represent the interquartile range. Note that this figure is only meant to show the directionality of the effects, not their statistical significance (as the ROIs itself only contain voxels that were significant in the group-analysis).\relax }}{308}{figure.caption.255}\protected@file@percent }
\newlabel{fig:fig-morbid-curiosity-S1}{{\M@TitleReference {D.1}{Subplots of individual regressors from the significant voxels in the confirmatory contrast negative\textasciitilde active - passive\textasciitilde {} - positive\textasciitilde active - passive\textasciitilde {} in the induction-phase. These plots show the direction of the effects. Plots are averaged over all significant voxels within each ROI (striatum in upper plots, IFG in lower plots), separately for the negative trials (left plots) and positive trials (right plots) with subplots for the active choice and passive viewing condition. Dots represent the participant-specific ROI-average parameter estimate from the first-level analysis. The horizontal line in the boxplots represents the median and the whiskers represent the interquartile range. Note that this figure is only meant to show the directionality of the effects, not their statistical significance (as the ROIs itself only contain voxels that were significant in the group-analysis).\relax }}{308}{Subplots of individual regressors from the significant voxels in the confirmatory contrast negative\textasciitilde active - passive\textasciitilde {} - positive\textasciitilde active - passive\textasciitilde {} in the induction-phase. These plots show the direction of the effects. Plots are averaged over all significant voxels within each ROI (striatum in upper plots, IFG in lower plots), separately for the negative trials (left plots) and positive trials (right plots) with subplots for the active choice and passive viewing condition. Dots represent the participant-specific ROI-average parameter estimate from the first-level analysis. The horizontal line in the boxplots represents the median and the whiskers represent the interquartile range. Note that this figure is only meant to show the directionality of the effects, not their statistical significance (as the ROIs itself only contain voxels that were significant in the group-analysis).\relax }{figure.caption.255}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {E}Supplement to Chapter \ref  {hypothesis-kernel-analysis}}{309}{appendix.E}\protected@file@percent }
\newlabel{hypothesis-kernel-analysis-supplement}{{\M@TitleReference {E}{Supplement to Chapter \ref  {hypothesis-kernel-analysis}}}{309}{Supplement to Chapter \ref {hypothesis-kernel-analysis}}{appendix.E}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E.1}Supplementary methods}{309}{section.E.1}\protected@file@percent }
\newlabel{hka-supplementary-methods}{{\M@TitleReference {E.1}{Supplementary methods}}{309}{Supplementary methods}{section.E.1}{}}
\@writefile{toc}{\contentsline {subsection}{Hypothesis kernel analysis (in detail)}{309}{section*.256}\protected@file@percent }
\newlabel{hypothesis-kernel-analysis-in-detail}{{\M@TitleReference {E.1}{Hypothesis kernel analysis (in detail)}}{309}{Hypothesis kernel analysis (in detail)}{section*.256}{}}
\@writefile{toc}{\contentsline {subsubsection}{Step 1: encoding mappings}{309}{section*.257}\protected@file@percent }
\newlabel{step-1-encoding-mappings}{{\M@TitleReference {E.1}{Step 1: encoding mappings}}{309}{Step 1: encoding mappings}{section*.257}{}}
\@writefile{toc}{\contentsline {subsubsection}{Step 2: encoding stimuli}{311}{section*.258}\protected@file@percent }
\newlabel{step-2-encoding-stimuli}{{\M@TitleReference {E.1}{Step 2: encoding stimuli}}{311}{Step 2: encoding stimuli}{section*.258}{}}
\@writefile{toc}{\contentsline {subsubsection}{Step 3: kernel functions}{312}{section*.259}\protected@file@percent }
\newlabel{step-3-kernel-functions}{{\M@TitleReference {E.1}{Step 3: kernel functions}}{312}{Step 3: kernel functions}{section*.259}{}}
\@writefile{toc}{\contentsline {subsubsection}{Step 4: computing predictions}{313}{section*.260}\protected@file@percent }
\newlabel{step-4-computing-predictions}{{\M@TitleReference {E.1}{Step 4: computing predictions}}{313}{Step 4: computing predictions}{section*.260}{}}
\@writefile{toc}{\contentsline {subsubsection}{Step 5: quantifying model performance}{314}{section*.261}\protected@file@percent }
\newlabel{step-5-quantifying-model-performance}{{\M@TitleReference {E.1}{Step 5: quantifying model performance}}{314}{Step 5: quantifying model performance}{section*.261}{}}
\@writefile{toc}{\contentsline {subsection}{Noise ceiling estimation (in detail)}{315}{section*.262}\protected@file@percent }
\newlabel{hka-noise-ceiling-detail}{{\M@TitleReference {E.1}{Noise ceiling estimation (in detail)}}{315}{Noise ceiling estimation (in detail)}{section*.262}{}}
\gdef \LT@xxiv {\LT@entry 
    {1}{35.61621pt}\LT@entry 
    {1}{65.59595pt}\LT@entry 
    {1}{65.59595pt}\LT@entry 
    {1}{65.59595pt}\LT@entry 
    {1}{65.59595pt}}
\newlabel{tab:tab-hka-S1}{{\M@TitleReference {E.1}{Noise ceiling estimation (in detail)}}{317}{Noise ceiling estimation (in detail)}{table.E.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {E.1}{\ignorespaces Hypothetical emotion ratings from three subjects in response to two stimuli\relax }}{317}{table.E.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E.2}Supplementary figures}{318}{section.E.2}\protected@file@percent }
\newlabel{hka-supp-fig}{{\M@TitleReference {E.2}{Supplementary figures}}{318}{Supplementary figures}{section.E.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.1}{\ignorespaces Difference in class-average model performance (AUROC) between discrete and probabilistic predictions.\relax }}{319}{figure.caption.263}\protected@file@percent }
\newlabel{fig:fig-hka-S1}{{\M@TitleReference {E.1}{Difference in class-average model performance (AUROC) between discrete and probabilistic predictions.\relax }}{319}{Difference in class-average model performance (AUROC) between discrete and probabilistic predictions.\relax }{figure.caption.263}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.2}{\ignorespaces Performance of different models for different values of the ``inverse temperature'' (\({"β}\)) parameter. A cosine kernel was used.\relax }}{320}{figure.caption.264}\protected@file@percent }
\newlabel{fig:fig-hka-S2}{{\M@TitleReference {E.2}{Performance of different models for different values of the ``inverse temperature'' (\({"β}\)) parameter. A cosine kernel was used.\relax }}{320}{Performance of different models for different values of the ``inverse temperature'' (\(\beta \)) parameter. A cosine kernel was used.\relax }{figure.caption.264}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.3}{\ignorespaces Performance of different models for different kernel functions. The cosine, sigmoid, and linear kernels are measures of similarity, but the Euclidean, L1, and L2 kernels measure distance. For these distance functions, the distances were converted to similarities by inverting them. A fixed ``inverse temperature'' (\({"β}\)) parameter of 1 was used.\relax }}{321}{figure.caption.265}\protected@file@percent }
\newlabel{fig:fig-hka-S3}{{\M@TitleReference {E.3}{Performance of different models for different kernel functions. The cosine, sigmoid, and linear kernels are measures of similarity, but the Euclidean, L1, and L2 kernels measure distance. For these distance functions, the distances were converted to similarities by inverting them. A fixed ``inverse temperature'' (\({"β}\)) parameter of 1 was used.\relax }}{321}{Performance of different models for different kernel functions. The cosine, sigmoid, and linear kernels are measures of similarity, but the Euclidean, L1, and L2 kernels measure distance. For these distance functions, the distances were converted to similarities by inverting them. A fixed ``inverse temperature'' (\(\beta \)) parameter of 1 was used.\relax }{figure.caption.265}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.4}{\ignorespaces Results from the simulation analysis with random mapping matrices in which the number of AUs per configuration (A) and the number of configurations per output class (B) were systematically varied. Bars represents the average AUROC score across 1000 simulations (error bars represent ±1 \emph  {SD}).\relax }}{322}{figure.caption.266}\protected@file@percent }
\newlabel{fig:fig-hka-S4}{{\M@TitleReference {E.4}{Results from the simulation analysis with random mapping matrices in which the number of AUs per configuration (A) and the number of configurations per output class (B) were systematically varied. Bars represents the average AUROC score across 1000 simulations (error bars represent ±1 \emph  {SD}).\relax }}{322}{Results from the simulation analysis with random mapping matrices in which the number of AUs per configuration (A) and the number of configurations per output class (B) were systematically varied. Bars represents the average AUROC score across 1000 simulations (error bars represent ±1 \emph {SD}).\relax }{figure.caption.266}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.5}{\ignorespaces Changes in model performance (AUROC) for each emotion and mapping after ablating an AU. Error bars indicate a 95\% confidence interval obtained with 1000 bootstraps of the data.\relax }}{323}{figure.caption.267}\protected@file@percent }
\newlabel{fig:fig-hka-S5}{{\M@TitleReference {E.5}{Changes in model performance (AUROC) for each emotion and mapping after ablating an AU. Error bars indicate a 95\% confidence interval obtained with 1000 bootstraps of the data.\relax }}{323}{Changes in model performance (AUROC) for each emotion and mapping after ablating an AU. Error bars indicate a 95\% confidence interval obtained with 1000 bootstraps of the data.\relax }{figure.caption.267}{}}
\gdef \LT@xxv {\LT@entry 
    {1}{35.61621pt}\LT@entry 
    {1}{35.61621pt}\LT@entry 
    {1}{35.61621pt}\LT@entry 
    {1}{191.15137pt}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.6}{\ignorespaces . The proportion of explained AUROC (from 0.5 to top of bar), unexplained AUROC (from top of bar to noise ceiling), and irreducible noise/variance due to individual differences (from noise ceiling to 1.0) expressed as a percentage of the total AUROC.\relax }}{324}{figure.caption.268}\protected@file@percent }
\newlabel{fig:fig-hka-S6}{{\M@TitleReference {E.6}{. The proportion of explained AUROC (from 0.5 to top of bar), unexplained AUROC (from top of bar to noise ceiling), and irreducible noise/variance due to individual differences (from noise ceiling to 1.0) expressed as a percentage of the total AUROC.\relax }}{324}{. The proportion of explained AUROC (from 0.5 to top of bar), unexplained AUROC (from top of bar to noise ceiling), and irreducible noise/variance due to individual differences (from noise ceiling to 1.0) expressed as a percentage of the total AUROC.\relax }{figure.caption.268}{}}
\newlabel{tab:tab-hka-S2}{{\M@TitleReference {E.2}{Supplementary figures}}{324}{Supplementary figures}{table.E.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {E.2}{\ignorespaces AUs with the largest ablation affects per emotion across mappings\relax }}{324}{table.E.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {F}Supplement to Chapter \ref  {static-vs-dynamic}}{327}{appendix.F}\protected@file@percent }
\newlabel{static-vs-dynamic-supplement}{{\M@TitleReference {F}{Supplement to Chapter \ref  {static-vs-dynamic}}}{327}{Supplement to Chapter \ref {static-vs-dynamic}}{appendix.F}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.1}{\ignorespaces The explained variance ratio of each PCA component (bars; left y-axis) and the cumulative explained variance ratio (dashed line; right y-axis) for the PCA done on the dynamic features (top) and static features (bottom).\relax }}{328}{figure.caption.269}\protected@file@percent }
\newlabel{fig:fig-svsd-S1}{{\M@TitleReference {F.1}{The explained variance ratio of each PCA component (bars; left y-axis) and the cumulative explained variance ratio (dashed line; right y-axis) for the PCA done on the dynamic features (top) and static features (bottom).\relax }}{328}{The explained variance ratio of each PCA component (bars; left y-axis) and the cumulative explained variance ratio (dashed line; right y-axis) for the PCA done on the dynamic features (top) and static features (bottom).\relax }{figure.caption.269}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.2}{\ignorespaces Distribution of the categorical emotion labels (bars height represents the average proportion; dots represent individual participants; top) and the distribution of the valence ratings (middle) and arousal rating (bottom), shown separately for the optimization set (left) and test set (right). In the valence and arousal subplots, the filled area represents the distribution of the ratings pooled over participants and the lines represents the distribution of the ratings of individual participants. The valence and arousal distributions were created using kernel density estimation as implemented in the Python package \emph  {seaborn} (which uses the \emph  {scipy} function \texttt  {gaussian\_kde} with Scott's rule for bandwidth selection).\relax }}{329}{figure.caption.270}\protected@file@percent }
\newlabel{fig:fig-svsd-S2}{{\M@TitleReference {F.2}{Distribution of the categorical emotion labels (bars height represents the average proportion; dots represent individual participants; top) and the distribution of the valence ratings (middle) and arousal rating (bottom), shown separately for the optimization set (left) and test set (right). In the valence and arousal subplots, the filled area represents the distribution of the ratings pooled over participants and the lines represents the distribution of the ratings of individual participants. The valence and arousal distributions were created using kernel density estimation as implemented in the Python package \emph  {seaborn} (which uses the \emph  {scipy} function \texttt  {gaussian\_kde} with Scott's rule for bandwidth selection).\relax }}{329}{Distribution of the categorical emotion labels (bars height represents the average proportion; dots represent individual participants; top) and the distribution of the valence ratings (middle) and arousal rating (bottom), shown separately for the optimization set (left) and test set (right). In the valence and arousal subplots, the filled area represents the distribution of the ratings pooled over participants and the lines represents the distribution of the ratings of individual participants. The valence and arousal distributions were created using kernel density estimation as implemented in the Python package \emph {seaborn} (which uses the \emph {scipy} function \texttt {gaussian\_kde} with Scott's rule for bandwidth selection).\relax }{figure.caption.270}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.3}{\ignorespaces Cross-validated model performance on optimization set, obtained by repeated 10-fold cross-validation. Because the optimization set does not contain repeated observations, the noise ceiling from the test set is visualized.\relax }}{330}{figure.caption.271}\protected@file@percent }
\newlabel{fig:fig-svsd-S3}{{\M@TitleReference {F.3}{Cross-validated model performance on optimization set, obtained by repeated 10-fold cross-validation. Because the optimization set does not contain repeated observations, the noise ceiling from the test set is visualized.\relax }}{330}{Cross-validated model performance on optimization set, obtained by repeated 10-fold cross-validation. Because the optimization set does not contain repeated observations, the noise ceiling from the test set is visualized.\relax }{figure.caption.271}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.4}{\ignorespaces Visualization of the comparison between the sum of static and dynamic model performance (orange) and the combined model performance (blue) for the categorical emotion model (left) and valence/arousal model (right).\relax }}{330}{figure.caption.272}\protected@file@percent }
\newlabel{fig:fig-svsd-S4}{{\M@TitleReference {F.4}{Visualization of the comparison between the sum of static and dynamic model performance (orange) and the combined model performance (blue) for the categorical emotion model (left) and valence/arousal model (right).\relax }}{330}{Visualization of the comparison between the sum of static and dynamic model performance (orange) and the combined model performance (blue) for the categorical emotion model (left) and valence/arousal model (right).\relax }{figure.caption.272}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.5}{\ignorespaces Confusion matrices for the categorical emotions models (left) and regression plots for the valence model (middle) and arousal model (right), separately for the models based on dynamic information (top) and static information (bottom). The confusion matrices are normalized by the sum across rows, thus representing the recall (or sensitivity) on the diagonal. The categorical emotion classifier based on dynamic information relatively frequently misclassifies ``anger'' and ``disgust'' (and vice versa) as well as ``surprise'' and ``fear'' (and vice versa), replicating earlier findings from human ratings (Jack et al., \hyperlink {ref-Jack2014-ku}{2014}, \hyperlink {ref-Jack2009-yy}{2009}). In contrast, the confusion rates for the static emotion classifier are relatively uniform across categorical emotion pairs. Different colors in the regression plot represent ratings from different participants. The regression plots for the valence and arousal models (middle and right) show that both the models based on dynamic and static features tend to underestimate the magnitude of the predictions (i.e., few predictions are made above 0.5 or below -0.5). These attenuated predictions is a direct consequence of the strong regularization applied to the regression models (i.e., \({"λ}= 500\)), which causes the parameters (i.e., \(\hat  {{"β}}\)) to be shrunk towards zero, thus shrinking predictions (\(X\hat  {beta}\)) towards the mean as well.\relax }}{331}{figure.caption.273}\protected@file@percent }
\newlabel{fig:fig-svsd-S5}{{\M@TitleReference {F.5}{Confusion matrices for the categorical emotions models (left) and regression plots for the valence model (middle) and arousal model (right), separately for the models based on dynamic information (top) and static information (bottom). The confusion matrices are normalized by the sum across rows, thus representing the recall (or sensitivity) on the diagonal. The categorical emotion classifier based on dynamic information relatively frequently misclassifies ``anger'' and ``disgust'' (and vice versa) as well as ``surprise'' and ``fear'' (and vice versa), replicating earlier findings from human ratings (Jack et al., \hyperlink {ref-Jack2014-ku}{2014}, \hyperlink {ref-Jack2009-yy}{2009}). In contrast, the confusion rates for the static emotion classifier are relatively uniform across categorical emotion pairs. Different colors in the regression plot represent ratings from different participants. The regression plots for the valence and arousal models (middle and right) show that both the models based on dynamic and static features tend to underestimate the magnitude of the predictions (i.e., few predictions are made above 0.5 or below -0.5). These attenuated predictions is a direct consequence of the strong regularization applied to the regression models (i.e., \({"λ}= 500\)), which causes the parameters (i.e., \(\hat  {{"β}}\)) to be shrunk towards zero, thus shrinking predictions (\(X\hat  {beta}\)) towards the mean as well.\relax }}{331}{Confusion matrices for the categorical emotions models (left) and regression plots for the valence model (middle) and arousal model (right), separately for the models based on dynamic information (top) and static information (bottom). The confusion matrices are normalized by the sum across rows, thus representing the recall (or sensitivity) on the diagonal. The categorical emotion classifier based on dynamic information relatively frequently misclassifies ``anger'' and ``disgust'' (and vice versa) as well as ``surprise'' and ``fear'' (and vice versa), replicating earlier findings from human ratings (Jack et al., \protect \hyperlink {ref-Jack2014-ku}{2014}, \protect \hyperlink {ref-Jack2009-yy}{2009}). In contrast, the confusion rates for the static emotion classifier are relatively uniform across categorical emotion pairs. Different colors in the regression plot represent ratings from different participants. The regression plots for the valence and arousal models (middle and right) show that both the models based on dynamic and static features tend to underestimate the magnitude of the predictions (i.e., few predictions are made above 0.5 or below -0.5). These attenuated predictions is a direct consequence of the strong regularization applied to the regression models (i.e., \(\lambda = 500\)), which causes the parameters (i.e., \(\hat {\beta }\)) to be shrunk towards zero, thus shrinking predictions (\(X\hat {beta}\)) towards the mean as well.\relax }{figure.caption.273}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.6}{\ignorespaces Posterior distributions for the first ten \emph  {dynamic} features (\(X_{1}-X_{10}\); in rows) for the inverted categorical emotion model (left; different emotions as separate lines), the inverted valence model (middle; different levels as separate lines), and the inverted arousal model (right; different levels as separate lines). The dashed vertical lines represent the value chosen for the reconstructions (i.e., the midpoint of the 5\% HDI interval). The drop at the edges of the posterior is an artifact induced by the kernel density estimation.\relax }}{332}{figure.caption.274}\protected@file@percent }
\newlabel{fig:fig-svsd-S6}{{\M@TitleReference {F.6}{Posterior distributions for the first ten \emph  {dynamic} features (\(X_{1}-X_{10}\); in rows) for the inverted categorical emotion model (left; different emotions as separate lines), the inverted valence model (middle; different levels as separate lines), and the inverted arousal model (right; different levels as separate lines). The dashed vertical lines represent the value chosen for the reconstructions (i.e., the midpoint of the 5\% HDI interval). The drop at the edges of the posterior is an artifact induced by the kernel density estimation.\relax }}{332}{Posterior distributions for the first ten \emph {dynamic} features (\(X_{1}-X_{10}\); in rows) for the inverted categorical emotion model (left; different emotions as separate lines), the inverted valence model (middle; different levels as separate lines), and the inverted arousal model (right; different levels as separate lines). The dashed vertical lines represent the value chosen for the reconstructions (i.e., the midpoint of the 5\% HDI interval). The drop at the edges of the posterior is an artifact induced by the kernel density estimation.\relax }{figure.caption.274}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.7}{\ignorespaces Posterior distributions for the first ten \emph  {static} features (\(X_{1}-X_{10}\); in rows) for the inverted categorical emotion model (left; different emotions as separate lines), the inverted valence model (middle; different levels as separate lines), and the inverted arousal model (right; different levels as separate lines). The dashed vertical lines represent the value chosen for the reconstructions (i.e., the midpoint of the 5\% HDI interval). The drop at the edges of the posterior is an artifact induced by the kernel density estimation.\relax }}{333}{figure.caption.275}\protected@file@percent }
\newlabel{fig:fig-svsd-S7}{{\M@TitleReference {F.7}{Posterior distributions for the first ten \emph  {static} features (\(X_{1}-X_{10}\); in rows) for the inverted categorical emotion model (left; different emotions as separate lines), the inverted valence model (middle; different levels as separate lines), and the inverted arousal model (right; different levels as separate lines). The dashed vertical lines represent the value chosen for the reconstructions (i.e., the midpoint of the 5\% HDI interval). The drop at the edges of the posterior is an artifact induced by the kernel density estimation.\relax }}{333}{Posterior distributions for the first ten \emph {static} features (\(X_{1}-X_{10}\); in rows) for the inverted categorical emotion model (left; different emotions as separate lines), the inverted valence model (middle; different levels as separate lines), and the inverted arousal model (right; different levels as separate lines). The dashed vertical lines represent the value chosen for the reconstructions (i.e., the midpoint of the 5\% HDI interval). The drop at the edges of the posterior is an artifact induced by the kernel density estimation.\relax }{figure.caption.275}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.8}{\ignorespaces Correlations between dynamic categorical emotion, valence, and arousal reconstructions. The different affective properties are delineated by the magenta borders. The green boxes highlight noteworthy correlations between high and low arousal and negative emotions (anger, disgust, and fear) and happiness/sadness, respectively. The yellow borders highlight the noteworthy correlations between positive and negative valence and happiness and negative emotions (anger, disgust, and fear), respectively.\relax }}{334}{figure.caption.276}\protected@file@percent }
\newlabel{fig:fig-svsd-S8}{{\M@TitleReference {F.8}{Correlations between dynamic categorical emotion, valence, and arousal reconstructions. The different affective properties are delineated by the magenta borders. The green boxes highlight noteworthy correlations between high and low arousal and negative emotions (anger, disgust, and fear) and happiness/sadness, respectively. The yellow borders highlight the noteworthy correlations between positive and negative valence and happiness and negative emotions (anger, disgust, and fear), respectively.\relax }}{334}{Correlations between dynamic categorical emotion, valence, and arousal reconstructions. The different affective properties are delineated by the magenta borders. The green boxes highlight noteworthy correlations between high and low arousal and negative emotions (anger, disgust, and fear) and happiness/sadness, respectively. The yellow borders highlight the noteworthy correlations between positive and negative valence and happiness and negative emotions (anger, disgust, and fear), respectively.\relax }{figure.caption.276}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.9}{\ignorespaces Correlations between static categorical emotion, valence, and arousal reconstructions. The different affective properties are delineated by the magenta borders. The green boxes highlight noteworthy correlations between high and low arousal and negative emotions (anger, disgust, and fear) and happiness/sadness, respectively. The yellow borders highlight the noteworthy correlations between positive and negative valence and happiness and negative emotions (anger, disgust, and fear), respectively.\relax }}{335}{figure.caption.277}\protected@file@percent }
\newlabel{fig:fig-svsd-S9}{{\M@TitleReference {F.9}{Correlations between static categorical emotion, valence, and arousal reconstructions. The different affective properties are delineated by the magenta borders. The green boxes highlight noteworthy correlations between high and low arousal and negative emotions (anger, disgust, and fear) and happiness/sadness, respectively. The yellow borders highlight the noteworthy correlations between positive and negative valence and happiness and negative emotions (anger, disgust, and fear), respectively.\relax }}{335}{Correlations between static categorical emotion, valence, and arousal reconstructions. The different affective properties are delineated by the magenta borders. The green boxes highlight noteworthy correlations between high and low arousal and negative emotions (anger, disgust, and fear) and happiness/sadness, respectively. The yellow borders highlight the noteworthy correlations between positive and negative valence and happiness and negative emotions (anger, disgust, and fear), respectively.\relax }{figure.caption.277}{}}
\gdef \LT@xxvi {\LT@entry 
    {1}{31.68018pt}\LT@entry 
    {1}{51.36035pt}\LT@entry 
    {1}{51.36035pt}\LT@entry 
    {1}{163.59912pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {G}Data, code, and educational materials}{336}{appendix.G}\protected@file@percent }
\newlabel{resources-supplement}{{\M@TitleReference {G}{Data, code, and educational materials}}{336}{Data, code, and educational materials}{appendix.G}{}}
\newlabel{tab:tab-resources-chapters}{{\M@TitleReference {G.1}{Data, code, and educational materials}}{336}{Data, code, and educational materials}{table.G.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.1}{\ignorespaces Chapter resources\relax }}{336}{table.G.1}\protected@file@percent }
\gdef \LT@xxvii {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\newlabel{tab:tab-resources-teaching}{{\M@TitleReference {G.2}{Data, code, and educational materials}}{337}{Data, code, and educational materials}{table.G.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.2}{\ignorespaces Teaching resources\relax }}{337}{table.G.2}\protected@file@percent }
\newlabel{bibliography}{{\M@TitleReference {G}{Bibliography}}{338}{Bibliography}{appendix*.278}{}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{338}{appendix*.278}\protected@file@percent }
\newlabel{contributions-to-the-chapters}{{\M@TitleReference {G}{Contributions to the chapters}}{381}{Contributions to the chapters}{appendix*.279}{}}
\@writefile{toc}{\contentsline {chapter}{Contributions to the chapters}{381}{appendix*.279}\protected@file@percent }
\newlabel{list-of-other-publications}{{\M@TitleReference {G}{List of other publications}}{384}{List of other publications}{appendix*.280}{}}
\@writefile{toc}{\contentsline {chapter}{List of other publications}{384}{appendix*.280}\protected@file@percent }
\newlabel{nederlandse-samenvatting-summary-in-dutch}{{\M@TitleReference {G}{Nederlandse samenvatting (Summary in Dutch)}}{385}{Nederlandse samenvatting (Summary in Dutch)}{appendix*.281}{}}
\@writefile{toc}{\contentsline {chapter}{Nederlandse samenvatting (Summary in Dutch)}{385}{appendix*.281}\protected@file@percent }
\bgroup 
\@writefile{toc}{\bgroup }
\@writefile{lof}{\bgroup }
\@writefile{lot}{\bgroup }
\selectlanguage *{dutch}
\@writefile{toc}{\selectlanguage *{dutch}}
\@writefile{lof}{\selectlanguage *{dutch}}
\@writefile{lot}{\selectlanguage *{dutch}}
\egroup 
\@writefile{toc}{\egroup }
\@writefile{lof}{\egroup }
\@writefile{lot}{\egroup }
\selectlanguage *[variant=american]{english}
\@writefile{toc}{\selectlanguage *[variant=american]{english}}
\@writefile{lof}{\selectlanguage *[variant=american]{english}}
\@writefile{lot}{\selectlanguage *[variant=american]{english}}
\newlabel{acknowledgments}{{\M@TitleReference {G}{Acknowledgments}}{386}{Acknowledgments}{appendix*.282}{}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{386}{appendix*.282}\protected@file@percent }
\memsetcounter{lastsheet}{395}
\memsetcounter{lastpage}{386}
\gdef \@abspage@last{395}
