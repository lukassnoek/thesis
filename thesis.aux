\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\catcode `"\active 
\HyPL@Entry{0<</P(\376\377\000c\000o\000v\000e\000r)>>}
\selectlanguage *[variant=american]{english}
\@writefile{toc}{\selectlanguage *[variant=american]{english}}
\@writefile{lof}{\selectlanguage *[variant=american]{english}}
\@writefile{lot}{\selectlanguage *[variant=american]{english}}
\HyPL@Entry{1<</S/r>>}
\@writefile{toc}{\contentsline {chapter}{Contents}{v}{section*.1}\protected@file@percent }
\HyPL@Entry{9<</S/D>>}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\newlabel{introduction}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The brain is not a dictionary}{1}{section.1.1}\protected@file@percent }
\newlabel{the-brain-is-not-a-dictionary}{{\M@TitleReference {1.1}{The brain is not a dictionary}}{1}{The brain is not a dictionary}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The brain (probably) does not care about your hypothesis}{1}{section.1.2}\protected@file@percent }
\newlabel{the-brain-probably-does-not-care-about-your-hypothesis}{{\M@TitleReference {1.2}{The brain (probably) does not care about your hypothesis}}{1}{The brain (probably) does not care about your hypothesis}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Interpretability and prediction are a trade-off (for now)}{2}{section.1.3}\protected@file@percent }
\newlabel{interpretability-and-prediction-are-a-trade-off-for-now}{{\M@TitleReference {1.3}{Interpretability and prediction are a trade-off (for now)}}{2}{Interpretability and prediction are a trade-off (for now)}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Exploration should be embraced more}{2}{section.1.4}\protected@file@percent }
\newlabel{exploration-should-be-embraced-more}{{\M@TitleReference {1.4}{Exploration should be embraced more}}{2}{Exploration should be embraced more}{section.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Proper generalization is hard}{2}{section.1.5}\protected@file@percent }
\newlabel{proper-generalization-is-hard}{{\M@TitleReference {1.5}{Proper generalization is hard}}{2}{Proper generalization is hard}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Psychology is complex, so it needs complex models}{2}{section.1.6}\protected@file@percent }
\newlabel{psychology-is-complex-so-it-needs-complex-models}{{\M@TitleReference {1.6}{Psychology is complex, so it needs complex models}}{2}{Psychology is complex, so it needs complex models}{section.1.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}{3}{chapter.2}\protected@file@percent }
\newlabel{shared-states}{{\M@TitleReference {2}{Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}}{3}{Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{4}{section.2.1}\protected@file@percent }
\newlabel{shared-states-introduction}{{\M@TitleReference {2.1}{Introduction}}{4}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methods}{8}{section.2.2}\protected@file@percent }
\newlabel{shared-states-methods}{{\M@TitleReference {2.2}{Methods}}{8}{Methods}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{Subjects}{8}{section*.2}\protected@file@percent }
\newlabel{shared-states-methods-subjects}{{\M@TitleReference {2.2}{Subjects}}{8}{Subjects}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Experimental design}{9}{section*.3}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design}{{\M@TitleReference {2.2}{Experimental design}}{9}{Experimental design}{section*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Self-focused emotion imagery task}{9}{section*.4}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design-sf-task}{{\M@TitleReference {2.2}{Self-focused emotion imagery task}}{9}{Self-focused emotion imagery task}{section*.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of the self-focused and other-focused task.\relax }}{10}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig-shared-states-1}{{\M@TitleReference {2.1}{Overview of the self-focused and other-focused task.\relax }}{10}{Overview of the self-focused and other-focused task.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Other-focused emotion understanding task}{10}{section*.6}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design-of-task}{{\M@TitleReference {2.2}{Other-focused emotion understanding task}}{10}{Other-focused emotion understanding task}{section*.6}{}}
\@writefile{toc}{\contentsline {subsection}{Procedure}{11}{section*.7}\protected@file@percent }
\newlabel{shared-states-methods-procedure}{{\M@TitleReference {2.2}{Procedure}}{11}{Procedure}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{Image acquisition}{12}{section*.8}\protected@file@percent }
\newlabel{shared-states-methods-image-acquisition}{{\M@TitleReference {2.2}{Image acquisition}}{12}{Image acquisition}{section*.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Model optimization procedure}{12}{section.2.3}\protected@file@percent }
\newlabel{shared-states-methods-model-optimization-procedure}{{\M@TitleReference {2.3}{Model optimization procedure}}{12}{Model optimization procedure}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{Preprocessing and single-trial modeling}{13}{section*.10}\protected@file@percent }
\newlabel{shared-states-methods-preprocessing}{{\M@TitleReference {2.3}{Preprocessing and single-trial modeling}}{13}{Preprocessing and single-trial modeling}{section*.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematic overview of the cross-validation procedures. \textbf  {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf  {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:fig-shared-states-2}{{\M@TitleReference {2.2}{Schematic overview of the cross-validation procedures. \textbf  {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf  {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }}{14}{Schematic overview of the cross-validation procedures. \textbf {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{Multi-voxel pattern analysis}{15}{section*.11}\protected@file@percent }
\newlabel{shared-states-methods-mvpa}{{\M@TitleReference {2.3}{Multi-voxel pattern analysis}}{15}{Multi-voxel pattern analysis}{section*.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{MVPA pipeline}{15}{section*.12}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-pipeline}{{\M@TitleReference {2.3}{MVPA pipeline}}{15}{MVPA pipeline}{section*.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation scheme and bagging procedure}{16}{section*.13}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-cv-and-bagging}{{\M@TitleReference {2.3}{Cross-validation scheme and bagging procedure}}{16}{Cross-validation scheme and bagging procedure}{section*.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Statistical evaluation}{18}{section*.14}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-statistical-evaluation}{{\M@TitleReference {2.3}{Statistical evaluation}}{18}{Statistical evaluation}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial representation}{18}{section*.15}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-spatial-representation}{{\M@TitleReference {2.3}{Spatial representation}}{18}{Spatial representation}{section*.15}{}}
\@writefile{toc}{\contentsline {subsection}{Additional analyses}{19}{section*.16}\protected@file@percent }
\newlabel{shared-states-methods-additional-analyses}{{\M@TitleReference {2.3}{Additional analyses}}{19}{Additional analyses}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{Univariate analysis}{19}{section*.17}\protected@file@percent }
\newlabel{shared-states-methods-univariate-analysis}{{\M@TitleReference {2.3}{Univariate analysis}}{19}{Univariate analysis}{section*.17}{}}
\@writefile{toc}{\contentsline {subsection}{Code availability}{20}{section*.18}\protected@file@percent }
\newlabel{shared-states-methods-code-availability}{{\M@TitleReference {2.3}{Code availability}}{20}{Code availability}{section*.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Results}{20}{section.2.4}\protected@file@percent }
\newlabel{shared-states-results}{{\M@TitleReference {2.4}{Results}}{20}{Results}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{Multi-voxel pattern analysis}{20}{section*.19}\protected@file@percent }
\newlabel{shared-states-results-mvpa}{{\M@TitleReference {2.4}{Multi-voxel pattern analysis}}{20}{Multi-voxel pattern analysis}{section*.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }}{21}{figure.caption.20}\protected@file@percent }
\newlabel{fig:fig-shared-states-3}{{\M@TitleReference {2.3}{Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }}{21}{Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Uncorrected \emph  {t}-value map of average feature weights across subjects; \emph  {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }}{22}{figure.caption.21}\protected@file@percent }
\newlabel{fig:fig-shared-states-4}{{\M@TitleReference {2.4}{Uncorrected \emph  {t}-value map of average feature weights across subjects; \emph  {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }}{22}{Uncorrected \emph {t}-value map of average feature weights across subjects; \emph {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{Univariate analyses}{22}{section*.22}\protected@file@percent }
\newlabel{shared-states-results-univariate}{{\M@TitleReference {2.4}{Univariate analyses}}{22}{Univariate analyses}{section*.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Univariate contrasts for the self-focused and other-focused task.\relax }}{23}{figure.caption.23}\protected@file@percent }
\newlabel{fig:fig-shared-states-5}{{\M@TitleReference {2.5}{Univariate contrasts for the self-focused and other-focused task.\relax }}{23}{Univariate contrasts for the self-focused and other-focused task.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Discussion}{24}{section.2.5}\protected@file@percent }
\newlabel{shared-states-discussion}{{\M@TitleReference {2.5}{Discussion}}{24}{Discussion}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Acknowledgements}{29}{section.2.6}\protected@file@percent }
\newlabel{shared-states-acknowledgements}{{\M@TitleReference {2.6}{Acknowledgements}}{29}{Acknowledgements}{section.2.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}How to control for confounds in decoding analyses of neuroimaging data}{30}{chapter.3}\protected@file@percent }
\newlabel{confounds-decoding}{{\M@TitleReference {3}{How to control for confounds in decoding analyses of neuroimaging data}}{30}{How to control for confounds in decoding analyses of neuroimaging data}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{32}{section.3.1}\protected@file@percent }
\newlabel{confounds-decoding-introduction}{{\M@TitleReference {3.1}{Introduction}}{32}{Introduction}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{Partitioning effects into \emph  {true} signal and \emph  {confounded} signal}{34}{section*.24}\protected@file@percent }
\newlabel{confounds-decoding-introduction-true-vs-confounded}{{\M@TitleReference {3.1}{Partitioning effects into \emph  {true} signal and \emph  {confounded} signal}}{34}{\texorpdfstring {Partitioning effects into \emph {true} signal and \emph {confounded} signal}{Partitioning effects into true signal and confounded signal}}{section*.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }}{37}{figure.caption.25}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-1}{{\M@TitleReference {3.1}{Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }}{37}{Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }{figure.caption.25}{}}
\gdef \LT@i {\LT@entry 
    {1}{41.52026pt}\LT@entry 
    {1}{41.52026pt}\LT@entry 
    {1}{214.95947pt}}
\@writefile{toc}{\contentsline {subsection}{Methods for confound control}{38}{section*.26}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods}{{\M@TitleReference {3.1}{Methods for confound control}}{38}{Methods for confound control}{section*.26}{}}
\newlabel{tab:tab-confounds-decoding-1}{{\M@TitleReference {3.1}{Methods for confound control}}{38}{Methods for confound control}{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Notation.\relax }}{38}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{A priori counterbalancing}{39}{section*.27}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-apriori-counterbalancing}{{\M@TitleReference {3.1}{A priori counterbalancing}}{39}{A priori counterbalancing}{section*.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Include confounds in the data}{41}{section*.28}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-include-in-data}{{\M@TitleReference {3.1}{Include confounds in the data}}{41}{Include confounds in the data}{section*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{Control for confounds during pattern estimation}{43}{section*.29}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-pattern-estimation}{{\M@TitleReference {3.1}{Control for confounds during pattern estimation}}{43}{Control for confounds during pattern estimation}{section*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{Post hoc counterbalancing of confounds}{44}{section*.30}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-posthoc-counterbalancing}{{\M@TitleReference {3.1}{Post hoc counterbalancing of confounds}}{44}{Post hoc counterbalancing of confounds}{section*.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{44}{section*.32}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-confound-regression}{{\M@TitleReference {3.1}{Confound regression}}{44}{Confound regression}{section*.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }}{45}{figure.caption.31}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-2}{{\M@TitleReference {3.2}{A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }}{45}{A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{Current study}{47}{section*.33}\protected@file@percent }
\newlabel{confounds-decoding-introduction-current-study}{{\M@TitleReference {3.1}{Current study}}{47}{Current study}{section*.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Methods}{49}{section.3.2}\protected@file@percent }
\newlabel{confounds-decoding-methods}{{\M@TitleReference {3.2}{Methods}}{49}{Methods}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{Data}{49}{section*.34}\protected@file@percent }
\newlabel{confounds-decoding-methods-data}{{\M@TitleReference {3.2}{Data}}{49}{Data}{section*.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{VBM acquisition \& analysis}{49}{section*.35}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-vbm}{{\M@TitleReference {3.2}{VBM acquisition \& analysis}}{49}{VBM acquisition \& analysis}{section*.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{TBSS acquisition \& analysis}{50}{section*.36}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-tbss}{{\M@TitleReference {3.2}{TBSS acquisition \& analysis}}{50}{TBSS acquisition \& analysis}{section*.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Brain size estimation}{50}{section*.37}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-brainsize}{{\M@TitleReference {3.2}{Brain size estimation}}{50}{Brain size estimation}{section*.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data and code availability}{51}{section*.38}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-data-and-code}{{\M@TitleReference {3.2}{Data and code availability}}{51}{Data and code availability}{section*.38}{}}
\@writefile{toc}{\contentsline {subsection}{Decoding pipeline}{51}{section*.39}\protected@file@percent }
\newlabel{confounds-decoding-methods-pipeline}{{\M@TitleReference {3.2}{Decoding pipeline}}{51}{Decoding pipeline}{section*.39}{}}
\@writefile{toc}{\contentsline {subsection}{Evaluated methods for confound control}{52}{section*.40}\protected@file@percent }
\newlabel{confounds-decoding-methods-evaluated-methods}{{\M@TitleReference {3.2}{Evaluated methods for confound control}}{52}{Evaluated methods for confound control}{section*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Post hoc counterbalancing}{52}{section*.41}\protected@file@percent }
\newlabel{confounds-decoding-methods-evaluated-methods-counterbalancing}{{\M@TitleReference {3.2}{Post hoc counterbalancing}}{52}{Post hoc counterbalancing}{section*.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{54}{section*.42}\protected@file@percent }
\newlabel{confound-regression}{{\M@TitleReference {3.2}{Confound regression}}{54}{Confound regression}{section*.42}{}}
\newlabel{eq:cvcr-train}{{3.8}{55}{Confound regression}{equation.3.2.8}{}}
\newlabel{eq:cvcr-test}{{3.9}{56}{Confound regression}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Control for confounds during pattern estimation}{56}{section*.43}\protected@file@percent }
\newlabel{control-for-confounds-during-pattern-estimation}{{\M@TitleReference {3.2}{Control for confounds during pattern estimation}}{56}{Control for confounds during pattern estimation}{section*.43}{}}
\@writefile{toc}{\contentsline {subsection}{Analyses of simulated data}{56}{section*.44}\protected@file@percent }
\newlabel{analyses-of-simulated-data}{{\M@TitleReference {3.2}{Analyses of simulated data}}{56}{Analyses of simulated data}{section*.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analyses}{57}{section*.45}\protected@file@percent }
\newlabel{efficacy-analyses}{{\M@TitleReference {3.2}{Efficacy analyses}}{57}{Efficacy analyses}{section*.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of positive bias after post hoc counterbalancing}{59}{section*.46}\protected@file@percent }
\newlabel{confounds-decoding-methods-counterbalancing-bias}{{\M@TitleReference {3.2}{Analysis of positive bias after post hoc counterbalancing}}{59}{Analysis of positive bias after post hoc counterbalancing}{section*.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of negative bias after WDCR}{61}{section*.48}\protected@file@percent }
\newlabel{analysis-of-negative-bias-after-wdcr}{{\M@TitleReference {3.2}{Analysis of negative bias after WDCR}}{61}{Analysis of negative bias after WDCR}{section*.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }}{62}{figure.caption.47}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-3}{{\M@TitleReference {3.3}{Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }}{62}{Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Results}{63}{section.3.3}\protected@file@percent }
\newlabel{results}{{\M@TitleReference {3.3}{Results}}{63}{Results}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{Influence of brain size}{63}{section*.49}\protected@file@percent }
\newlabel{influence-of-brain-size}{{\M@TitleReference {3.3}{Influence of brain size}}{63}{Influence of brain size}{section*.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \({"ρ}= 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{65}{figure.caption.50}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-4}{{\M@TitleReference {3.4}{A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \({"ρ}= 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{65}{A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \(\rho = 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{Baseline model: no confound control}{65}{section*.51}\protected@file@percent }
\newlabel{baseline-model-no-confound-control}{{\M@TitleReference {3.3}{Baseline model: no confound control}}{65}{Baseline model: no confound control}{section*.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{66}{figure.caption.52}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-5}{{\M@TitleReference {3.5}{Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{66}{Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{Post hoc counterbalancing}{66}{section*.53}\protected@file@percent }
\newlabel{post-hoc-counterbalancing}{{\M@TitleReference {3.3}{Post hoc counterbalancing}}{66}{Post hoc counterbalancing}{section*.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{66}{section*.54}\protected@file@percent }
\newlabel{empirical-results}{{\M@TitleReference {3.3}{Empirical results}}{66}{Empirical results}{section*.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{67}{figure.caption.55}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-6}{{\M@TitleReference {3.6}{Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{67}{Model performance after counterbalancing (green) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data (upper row) and the difference in performance between the methods (lower row). Performance reflects the average (difference) \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{68}{figure.caption.56}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-7}{{\M@TitleReference {3.7}{Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{68}{Density plots of the correlations between the target and voxels across all voxels before (blue) and after (green) subsampling for both the VBM and TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.\relax }}{69}{figure.caption.58}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-8}{{\M@TitleReference {3.8}{Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.\relax }}{69}{Results from the different confound control methods on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\). The orange line represents the average performance (±1 SD) when confound \(R^2 = 0\), which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The results from the WDCR and CVCR methods are explained later.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{69}{section*.57}\protected@file@percent }
\newlabel{confounds-decoding-results-cb-efficacy}{{\M@TitleReference {3.3}{Efficacy analysis}}{69}{Efficacy analysis}{section*.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm  {ACC}_{\mathrm  {CB}} - \mathrm  {ACC}_{\mathrm  {baseline}}\)).\relax }}{71}{figure.caption.59}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-9}{{\M@TitleReference {3.9}{The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm  {ACC}_{\mathrm  {CB}} - \mathrm  {ACC}_{\mathrm  {baseline}}\)).\relax }}{71}{The relationship between the increase in correlations between target and data (\(r_{yX}\)) after subsampling, confound \(R^2\), difference in model performance (here: accuracy) between the counterbalance model and baseline model (\(\mathrm {ACC}_{\mathrm {CB}} - \mathrm {ACC}_{\mathrm {baseline}}\)).\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of positive bias after post hoc counterbalancing}{71}{section*.60}\protected@file@percent }
\newlabel{analysis-of-positive-bias-after-post-hoc-counterbalancing}{{\M@TitleReference {3.3}{Analysis of positive bias after post hoc counterbalancing}}{71}{Analysis of positive bias after post hoc counterbalancing}{section*.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{73}{figure.caption.61}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-10}{{\M@TitleReference {3.10}{Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{73}{Both scatterplots visualize the relationship between the data (\(X\) with \(K=1\), on the x-axis), the confound (\(C\), on the y-axis) and the target (\(y\)). Dots with a white border in the upper scatterplot indicate samples that are rejected in the subsampling process; the lower scatterplot visualizes the data without these rejected samples. The dashed black lines in the scatterplot represent the decision boundary of the SVM classifier; the color of the background shows how samples in that area are classified (a blue background means a prediction of \(y = 0\) and a green background means a prediction of \(y = 1\)). The density plots parallel to the y-axis depict the distribution of the confound (\(C\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). The density plots parallel to x-axis depict the distribution of the data (\(X\)) for the samples in which \(y = 0\) (blue) and in which \(y = 1\) (green). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces \textbf  {A}) The proportion of samples classified correctly, separately for the ``retained'' samples (blue line) and ``rejected'' samples (green line); the dashed line represents chance level (0.5). \textbf  {B}) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the ``wrong'' side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{74}{figure.caption.62}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-11}{{\M@TitleReference {3.11}{\textbf  {A}) The proportion of samples classified correctly, separately for the ``retained'' samples (blue line) and ``rejected'' samples (green line); the dashed line represents chance level (0.5). \textbf  {B}) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the ``wrong'' side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{74}{\textbf {A}) The proportion of samples classified correctly, separately for the ``retained'' samples (blue line) and ``rejected'' samples (green line); the dashed line represents chance level (0.5). \textbf {B}) The average distance to the classification boundary for the retained and rejected samples; the dashed line represents the decision boundary, with values below the line representing samples on the ``wrong'' side of the boundary (and vice versa). Asterisks indicates a significant difference between the retained and rejected samples: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{Whole-dataset confound regression (WDCR)}{75}{section*.63}\protected@file@percent }
\newlabel{whole-dataset-confound-regression-wdcr}{{\M@TitleReference {3.3}{Whole-dataset confound regression (WDCR)}}{75}{Whole-dataset confound regression (WDCR)}{section*.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{75}{section*.64}\protected@file@percent }
\newlabel{empirical-results-1}{{\M@TitleReference {3.3}{Empirical results}}{75}{Empirical results}{section*.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{76}{figure.caption.65}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-12}{{\M@TitleReference {3.12}{Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{76}{Model performance after WDCR (orange) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the WDCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{76}{section*.66}\protected@file@percent }
\newlabel{efficacy-analysis}{{\M@TitleReference {3.3}{Efficacy analysis}}{76}{Efficacy analysis}{section*.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of negative bias after WDCR}{76}{section*.67}\protected@file@percent }
\newlabel{confounds-decoding-results-wdcr-bias}{{\M@TitleReference {3.3}{Analysis of negative bias after WDCR}}{76}{Analysis of negative bias after WDCR}{section*.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces \textbf  {A}) The relationship between the standard deviation of the distribution of feature-target correlations, \emph  {sd}(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \({"ρ}= 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure \ref  {fig:fig-confounds-decoding-S15} for the same plot with different values for \(N\)). \textbf  {B}) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) ``flips'' between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50\% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. \textbf  {C}) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dataset-shift}\unskip \@@italiccorr )}}) and accuracy.\relax }}{78}{figure.caption.68}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-13}{{\M@TitleReference {3.13}{\textbf  {A}) The relationship between the standard deviation of the distribution of feature-target correlations, \emph  {sd}(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \({"ρ}= 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure \ref  {fig:fig-confounds-decoding-S15} for the same plot with different values for \(N\)). \textbf  {B}) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) ``flips'' between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50\% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. \textbf  {C}) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dataset-shift}\unskip \@@italiccorr )}}) and accuracy.\relax }}{78}{\textbf {A}) The relationship between the standard deviation of the distribution of feature-target correlations, \emph {sd}(\(r_{yX}\)), and accuracy across iterations of cross-validated classification analyses of null data. The vertical dashed line represents the standard deviation from the sampling distribution parameterized with \(\rho = 0\) and \(N = 100\) (i.e., the same parameters used to generate the null data); the horizontal dashed line represents the expected accuracy for data with this standard deviation based on the regression line estimated from the data across simulations (see Supplementary Figure \ref {fig:fig-confounds-decoding-S15} for the same plot with different values for \(N\)). \textbf {B}) The relationship between the proportion of features of which the sign of their correlation with the target (\(r_{Xy}\)) ``flips'' between the train-set and the test-set and accuracy. The vertical dashed line represents a proportion of 0.5., i.e., 50\% of the features flip their correlation sign, which corresponds approximately with an accuracy of 0.5. \textbf {C}) The relationship between the weighted difference between feature-target correlations in the train and test set (see equation \eqref {eq:dataset-shift}) and accuracy.\relax }{figure.caption.68}{}}
\newlabel{eq:dataset-shift}{{3.14}{79}{Analysis of negative bias after WDCR}{equation.3.3.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces \textbf  {A}) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). \textbf  {B}) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. \textbf  {C}) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. \textbf  {D}) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure \ref  {fig:fig-confounds-decoding-S18} for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data.\relax }}{80}{figure.caption.69}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-14}{{\M@TitleReference {3.14}{\textbf  {A}) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). \textbf  {B}) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. \textbf  {C}) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. \textbf  {D}) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure \ref  {fig:fig-confounds-decoding-S18} for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data.\relax }}{80}{\textbf {A}) The effect of WDCR on data varying in the correlation of the confound with the target (\(r_{Cy}\); x-axis) and the number of features (\(K\); different lines). \textbf {B}) The effect of CVCR on data varying in the correlation of the confound with the target and the number of features. The dashed black line represents chance model performance in subplots A and B. \textbf {C}) The relation between the correlation of the confound with the target (\(r_{Cy}\)) and the standard deviation of the feature-target correlation distribution, \emph {sd}(\(r_{yX}\)) for the WDCR data. The dashed black line represents the standard deviation of the correlation distribution predicted by the sampling distribution. \textbf {D}) The relation of the standard deviation of the correlation distribution and accuracy for the WDCR data (only shown for the data when \(K = 100\); see Supplementary Figure \ref {fig:fig-confounds-decoding-S18} for visualizations of this effect for different values of \(K\)). The data depicted in all panels are null data.\relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{Cross-validated confound regression (CVCR)}{81}{section*.70}\protected@file@percent }
\newlabel{cross-validated-confound-regression-cvcr}{{\M@TitleReference {3.3}{Cross-validated confound regression (CVCR)}}{81}{Cross-validated confound regression (CVCR)}{section*.70}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{81}{section*.71}\protected@file@percent }
\newlabel{empirical-results-2}{{\M@TitleReference {3.3}{Empirical results}}{81}{Empirical results}{section*.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{81}{section*.73}\protected@file@percent }
\newlabel{efficacy-analysis-1}{{\M@TitleReference {3.3}{Efficacy analysis}}{81}{Efficacy analysis}{section*.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{82}{figure.caption.72}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-15}{{\M@TitleReference {3.15}{Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{82}{Model performance after CVCR (pink) versus the baseline performance (blue) for both the VBM (left) and TBSS (right) data. Performance reflects the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals across 1000 bootstrap replications. The dashed black line reflect theoretical chance level performance (0.5) and the dashed orange line reflects the average model performance when only brain size is used as a predictor. Asterisks indicates performance of the CVCR model that is significantly above or below chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{Summary methods for confound control}{82}{section*.74}\protected@file@percent }
\newlabel{summary-methods-for-confound-control}{{\M@TitleReference {3.3}{Summary methods for confound control}}{82}{Summary methods for confound control}{section*.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.\relax }}{83}{figure.caption.75}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-16}{{\M@TitleReference {3.16}{An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.\relax }}{83}{An overview of the empirical results on the four different confound methods: None, post hoc counterbalancing, WDCR, and CVCR.\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Discussion}{83}{section.3.4}\protected@file@percent }
\newlabel{confounds-decoding-discussion}{{\M@TitleReference {3.4}{Discussion}}{83}{Discussion}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{Relevance and consequences for previous and future research}{85}{section*.76}\protected@file@percent }
\newlabel{relevance-and-consequences-for-previous-and-future-research}{{\M@TitleReference {3.4}{Relevance and consequences for previous and future research}}{85}{Relevance and consequences for previous and future research}{section*.76}{}}
\@writefile{toc}{\contentsline {subsubsection}{A priori and post hoc counterbalancing}{85}{section*.77}\protected@file@percent }
\newlabel{a-priori-and-post-hoc-counterbalancing}{{\M@TitleReference {3.4}{A priori and post hoc counterbalancing}}{85}{A priori and post hoc counterbalancing}{section*.77}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{86}{section*.78}\protected@file@percent }
\newlabel{confound-regression-1}{{\M@TitleReference {3.4}{Confound regression}}{86}{Confound regression}{section*.78}{}}
\@writefile{toc}{\contentsline {subsubsection}{Relevance to other analysis methods}{87}{section*.79}\protected@file@percent }
\newlabel{relevance-to-other-analysis-methods}{{\M@TitleReference {3.4}{Relevance to other analysis methods}}{87}{Relevance to other analysis methods}{section*.79}{}}
\@writefile{toc}{\contentsline {subsubsection}{Importance for gender decoding studies}{89}{section*.80}\protected@file@percent }
\newlabel{importance-for-gender-decoding-studies}{{\M@TitleReference {3.4}{Importance for gender decoding studies}}{89}{Importance for gender decoding studies}{section*.80}{}}
\@writefile{toc}{\contentsline {subsection}{Choosing a confound model: linear vs.~nonlinear models}{89}{section*.81}\protected@file@percent }
\newlabel{choosing-a-confound-model-linear-vs.-nonlinear-models}{{\M@TitleReference {3.4}{Choosing a confound model: linear vs.~nonlinear models}}{89}{Choosing a confound model: linear vs.~nonlinear models}{section*.81}{}}
\@writefile{toc}{\contentsline {subsection}{Practical recommendations}{90}{section*.82}\protected@file@percent }
\newlabel{practical-recommendations}{{\M@TitleReference {3.4}{Practical recommendations}}{90}{Practical recommendations}{section*.82}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusions}{92}{section.3.5}\protected@file@percent }
\newlabel{conclusions}{{\M@TitleReference {3.5}{Conclusions}}{92}{Conclusions}{section.3.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}{94}{chapter.4}\protected@file@percent }
\newlabel{aomic}{{\M@TitleReference {4}{The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}}{94}{The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Background \& summary}{95}{section.4.1}\protected@file@percent }
\newlabel{background-summary}{{\M@TitleReference {4.1}{Background \& summary}}{95}{Background \& summary}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces General overview of AOMIC's contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of ``derivatives'', i.e., data derived from the original ``raw'' data through state-of-the-art preprocessing pipelines.\relax }}{97}{figure.caption.83}\protected@file@percent }
\newlabel{fig:fig-aomic-1}{{\M@TitleReference {4.1}{General overview of AOMIC's contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of ``derivatives'', i.e., data derived from the original ``raw'' data through state-of-the-art preprocessing pipelines.\relax }}{97}{General overview of AOMIC's contents. Each dataset (ID1000, PIOP1, PIOP2) contains multimodal MRI data, physiology (concurrent with fMRI acquisition), demographic and psychometric data, as well as a large set of ``derivatives'', i.e., data derived from the original ``raw'' data through state-of-the-art preprocessing pipelines.\relax }{figure.caption.83}{}}
\gdef \LT@ii {\LT@entry 
    {1}{61.20044pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}\LT@entry 
    {1}{39.68178pt}}
\newlabel{tab:tab-aomic-1}{{\M@TitleReference {4.1}{Background \& summary}}{98}{Background \& summary}{table.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Overview of the number of subjects per dataset and tasks.\relax }}{98}{table.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Methods}{99}{section.4.2}\protected@file@percent }
\newlabel{methods}{{\M@TitleReference {4.2}{Methods}}{99}{Methods}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{Scanner details and general scanning protocol (all datasets)}{99}{section*.84}\protected@file@percent }
\newlabel{scanner-details-and-general-scanning-protocol-all-datasets}{{\M@TitleReference {4.2}{Scanner details and general scanning protocol (all datasets)}}{99}{Scanner details and general scanning protocol (all datasets)}{section*.84}{}}
\@writefile{toc}{\contentsline {subsection}{ID1000 specifics}{101}{section*.85}\protected@file@percent }
\newlabel{id1000-specifics}{{\M@TitleReference {4.2}{ID1000 specifics}}{101}{ID1000 specifics}{section*.85}{}}
\@writefile{toc}{\contentsline {subsubsection}{Subjects}{101}{section*.86}\protected@file@percent }
\newlabel{subjects}{{\M@TitleReference {4.2}{Subjects}}{101}{Subjects}{section*.86}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data collection protocol}{102}{section*.87}\protected@file@percent }
\newlabel{data-collection-protocol}{{\M@TitleReference {4.2}{Data collection protocol}}{102}{Data collection protocol}{section*.87}{}}
\@writefile{toc}{\contentsline {subsubsection}{Functional MRI paradigm}{103}{section*.88}\protected@file@percent }
\newlabel{functional-mri-paradigm}{{\M@TitleReference {4.2}{Functional MRI paradigm}}{103}{Functional MRI paradigm}{section*.88}{}}
\@writefile{toc}{\contentsline {subsubsection}{Previous analyses}{104}{section*.89}\protected@file@percent }
\newlabel{previous-analyses}{{\M@TitleReference {4.2}{Previous analyses}}{104}{Previous analyses}{section*.89}{}}
\@writefile{toc}{\contentsline {subsection}{PIOP1 and PIOP2 specifics}{104}{section*.90}\protected@file@percent }
\newlabel{piop1-and-piop2-specifics}{{\M@TitleReference {4.2}{PIOP1 and PIOP2 specifics}}{104}{PIOP1 and PIOP2 specifics}{section*.90}{}}
\@writefile{toc}{\contentsline {subsubsection}{Subjects}{104}{section*.91}\protected@file@percent }
\newlabel{subjects-1}{{\M@TitleReference {4.2}{Subjects}}{104}{Subjects}{section*.91}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data collection protocol}{105}{section*.92}\protected@file@percent }
\newlabel{data-collection-protocol-1}{{\M@TitleReference {4.2}{Data collection protocol}}{105}{Data collection protocol}{section*.92}{}}
\@writefile{toc}{\contentsline {subsubsection}{Functional MRI paradigms}{106}{section*.93}\protected@file@percent }
\newlabel{functional-mri-paradigms}{{\M@TitleReference {4.2}{Functional MRI paradigms}}{106}{Functional MRI paradigms}{section*.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.\relax }}{107}{figure.caption.94}\protected@file@percent }
\newlabel{fig:fig-aomic-2}{{\M@TitleReference {4.2}{A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.\relax }}{107}{A visual representation of all experimental paradigms during task-based fMRI. ISI: inter-stimulus interval.\relax }{figure.caption.94}{}}
\@writefile{toc}{\contentsline {subsubsection}{Previous analyses}{115}{section*.95}\protected@file@percent }
\newlabel{previous-analyses-1}{{\M@TitleReference {4.2}{Previous analyses}}{115}{Previous analyses}{section*.95}{}}
\@writefile{toc}{\contentsline {subsubsection}{Differences between PIOP1 and PIOP2}{115}{section*.96}\protected@file@percent }
\newlabel{differences-between-piop1-and-piop2}{{\M@TitleReference {4.2}{Differences between PIOP1 and PIOP2}}{115}{Differences between PIOP1 and PIOP2}{section*.96}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Descriptive statistics for biological sex, age, and education level for all three datasets.\relax }}{116}{table.caption.99}\protected@file@percent }
\newlabel{tab:tab-aomic-2}{{\M@TitleReference {4.2}{Descriptive statistics for biological sex, age, and education level for all three datasets.\relax }}{116}{Descriptive statistics for biological sex, age, and education level for all three datasets.\relax }{table.caption.99}{}}
\@writefile{toc}{\contentsline {subsection}{Subject variables (all datasets)}{116}{section*.97}\protected@file@percent }
\newlabel{subject-variables-all-datasets}{{\M@TitleReference {4.2}{Subject variables (all datasets)}}{116}{Subject variables (all datasets)}{section*.97}{}}
\@writefile{toc}{\contentsline {subsubsection}{Age}{116}{section*.98}\protected@file@percent }
\newlabel{age}{{\M@TitleReference {4.2}{Age}}{116}{Age}{section*.98}{}}
\@writefile{toc}{\contentsline {subsubsection}{Biological sex and gender identity}{117}{section*.100}\protected@file@percent }
\newlabel{biological-sex-and-gender-identity}{{\M@TitleReference {4.2}{Biological sex and gender identity}}{117}{Biological sex and gender identity}{section*.100}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sexual orientation}{117}{section*.101}\protected@file@percent }
\newlabel{sexual-orientation}{{\M@TitleReference {4.2}{Sexual orientation}}{117}{Sexual orientation}{section*.101}{}}
\@writefile{toc}{\contentsline {subsubsection}{BMI}{117}{section*.102}\protected@file@percent }
\newlabel{bmi}{{\M@TitleReference {4.2}{BMI}}{117}{BMI}{section*.102}{}}
\gdef \LT@iii {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {subsubsection}{Handedness}{118}{section*.103}\protected@file@percent }
\newlabel{handedness}{{\M@TitleReference {4.2}{Handedness}}{118}{Handedness}{section*.103}{}}
\@writefile{toc}{\contentsline {subsubsection}{Educational level / category}{118}{section*.104}\protected@file@percent }
\newlabel{educational-level-category}{{\M@TitleReference {4.2}{Educational level / category}}{118}{Educational level / category}{section*.104}{}}
\newlabel{tab:tab-aomic-3}{{\M@TitleReference {4.3}{Educational level / category}}{118}{Educational level / category}{table.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Distribution of educational level in the Dutch population (in 2010) and in ID1000.\relax }}{118}{table.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Religion (PIOP1 and ID1000 only)}{119}{section*.106}\protected@file@percent }
\newlabel{religion-piop1-and-id1000-only}{{\M@TitleReference {4.2}{Religion (PIOP1 and ID1000 only)}}{119}{Religion (PIOP1 and ID1000 only)}{section*.106}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Distribution of background SES.\relax }}{120}{table.caption.105}\protected@file@percent }
\newlabel{tab:tab-aomic-4}{{\M@TitleReference {4.4}{Distribution of background SES.\relax }}{120}{Distribution of background SES.\relax }{table.caption.105}{}}
\@writefile{toc}{\contentsline {subsection}{Psychometric variables (all datasets)}{120}{section*.107}\protected@file@percent }
\newlabel{psychometric-variables-all-datasets}{{\M@TitleReference {4.2}{Psychometric variables (all datasets)}}{120}{Psychometric variables (all datasets)}{section*.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Overview of the types of data and ``derivatives'' included in AOMIC and the software packages used to preprocess and analyze them.\relax }}{122}{figure.caption.109}\protected@file@percent }
\newlabel{fig:fig-aomic-3}{{\M@TitleReference {4.3}{Overview of the types of data and ``derivatives'' included in AOMIC and the software packages used to preprocess and analyze them.\relax }}{122}{Overview of the types of data and ``derivatives'' included in AOMIC and the software packages used to preprocess and analyze them.\relax }{figure.caption.109}{}}
\@writefile{toc}{\contentsline {subsection}{Data standardization, preprocessing, and derivatives}{122}{section*.108}\protected@file@percent }
\newlabel{aomic-derivatives}{{\M@TitleReference {4.2}{Data standardization, preprocessing, and derivatives}}{122}{Data standardization, preprocessing, and derivatives}{section*.108}{}}
\@writefile{toc}{\contentsline {subsubsection}{Raw data standardization}{123}{section*.110}\protected@file@percent }
\newlabel{raw-data-standardization}{{\M@TitleReference {4.2}{Raw data standardization}}{123}{Raw data standardization}{section*.110}{}}
\@writefile{toc}{\contentsline {subsubsection}{Anatomical and functional MRI preprocessing}{124}{section*.111}\protected@file@percent }
\newlabel{anatomical-and-functional-mri-preprocessing}{{\M@TitleReference {4.2}{Anatomical and functional MRI preprocessing}}{124}{Anatomical and functional MRI preprocessing}{section*.111}{}}
\@writefile{toc}{\contentsline {subsubsection}{Diffusion MRI (pre)processing}{126}{section*.112}\protected@file@percent }
\newlabel{diffusion-mri-preprocessing}{{\M@TitleReference {4.2}{Diffusion MRI (pre)processing}}{126}{Diffusion MRI (pre)processing}{section*.112}{}}
\@writefile{toc}{\contentsline {subsubsection}{Freesurfer morphological statistics}{127}{section*.113}\protected@file@percent }
\newlabel{freesurfer-morphological-statistics}{{\M@TitleReference {4.2}{Freesurfer morphological statistics}}{127}{Freesurfer morphological statistics}{section*.113}{}}
\@writefile{toc}{\contentsline {subsubsection}{Voxel-based morphology}{128}{section*.114}\protected@file@percent }
\newlabel{voxel-based-morphology}{{\M@TitleReference {4.2}{Voxel-based morphology}}{128}{Voxel-based morphology}{section*.114}{}}
\@writefile{toc}{\contentsline {subsubsection}{Physiological noise processing}{129}{section*.115}\protected@file@percent }
\newlabel{physiological-noise-processing}{{\M@TitleReference {4.2}{Physiological noise processing}}{129}{Physiological noise processing}{section*.115}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Data Records}{130}{section.4.3}\protected@file@percent }
\newlabel{data-records}{{\M@TitleReference {4.3}{Data Records}}{130}{Data Records}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{Data formats and types}{130}{section*.116}\protected@file@percent }
\newlabel{data-formats-and-types}{{\M@TitleReference {4.3}{Data formats and types}}{130}{Data formats and types}{section*.116}{}}
\gdef \LT@iv {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\@writefile{toc}{\contentsline {subsection}{Data repositories used}{131}{section*.117}\protected@file@percent }
\newlabel{data-repositories-used}{{\M@TitleReference {4.3}{Data repositories used}}{131}{Data repositories used}{section*.117}{}}
\newlabel{tab:tab-aomic-5}{{\M@TitleReference {4.5}{Data repositories used}}{131}{Data repositories used}{table.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Data repository identifiers for subject data (OpenNeuro) and group-level data (NeuroVault).\relax }}{131}{table.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data anonymization}{132}{section*.118}\protected@file@percent }
\newlabel{data-anonymization}{{\M@TitleReference {4.3}{Data anonymization}}{132}{Data anonymization}{section*.118}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Technical validation}{132}{section.4.4}\protected@file@percent }
\newlabel{technical-validation}{{\M@TitleReference {4.4}{Technical validation}}{132}{Technical validation}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{T1-weighted scans}{133}{section*.119}\protected@file@percent }
\newlabel{t1-weighted-scans}{{\M@TitleReference {4.4}{T1-weighted scans}}{133}{T1-weighted scans}{section*.119}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., \hyperlink {ref-Magnotta2006-zs}{2006}); CJV: coefficient of joint variation (Ganzetti et al., \hyperlink {ref-Ganzetti2016-yy}{2016}), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., \hyperlink {ref-Atkinson1997-eu}{1997}), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95\% percentile of all signal intensities; low values may lead to problems with tissue segmentation.\relax }}{134}{figure.caption.120}\protected@file@percent }
\newlabel{fig:fig-aomic-4}{{\M@TitleReference {4.4}{Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., \hyperlink {ref-Magnotta2006-zs}{2006}); CJV: coefficient of joint variation (Ganzetti et al., \hyperlink {ref-Ganzetti2016-yy}{2016}), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., \hyperlink {ref-Atkinson1997-eu}{1997}), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95\% percentile of all signal intensities; low values may lead to problems with tissue segmentation.\relax }}{134}{Quality control metrics related to the T1-weighted scans. CNR: contrast-to-noise ratio (Magnotta et al., \protect \hyperlink {ref-Magnotta2006-zs}{2006}); CJV: coefficient of joint variation (Ganzetti et al., \protect \hyperlink {ref-Ganzetti2016-yy}{2016}), an index reflecting head motion and spatial inhomogeneity; EFC: entropy-focused criterion (Atkinson et al., \protect \hyperlink {ref-Atkinson1997-eu}{1997}), an index reflecting head motion and ghosting; INU: intensity non-uniformity, an index of spatial inhomogeneity; WM2MAX: ratio of median white-matter intensity to the 95\% percentile of all signal intensities; low values may lead to problems with tissue segmentation.\relax }{figure.caption.120}{}}
\@writefile{toc}{\contentsline {subsection}{Functional (BOLD) scans}{134}{section*.121}\protected@file@percent }
\newlabel{functional-bold-scans}{{\M@TitleReference {4.4}{Functional (BOLD) scans}}{134}{Functional (BOLD) scans}{section*.121}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., \hyperlink {ref-Power2012-kt}{2012}), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., \hyperlink {ref-Saad2013-zd}{2013}); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.\relax }}{136}{figure.caption.122}\protected@file@percent }
\newlabel{fig:fig-aomic-5}{{\M@TitleReference {4.5}{Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., \hyperlink {ref-Power2012-kt}{2012}), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., \hyperlink {ref-Saad2013-zd}{2013}); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.\relax }}{136}{Quality control metrics related to the functional (BOLD) MRI scans. SNR: signal-to-noise ratio, an index of signal quality; FD: framewise displacement (Power et al., \protect \hyperlink {ref-Power2012-kt}{2012}), an index of overall movement; GCOR: global correlation, an index of the presence of global signals (Saad et al., \protect \hyperlink {ref-Saad2013-zd}{2013}); GSR: ghost-to-signal ratio, an index of ghosting along the phase-encoding axis.\relax }{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.\relax }}{137}{figure.caption.123}\protected@file@percent }
\newlabel{fig:fig-aomic-6}{{\M@TitleReference {4.6}{Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.\relax }}{137}{Average (across subjects and runs) temporal signal-to-noise (tSNR) maps of each type of functional (BOLD) MRI scan in each dataset. Unthresholded whole-brain tSNR maps are available on NeuroVault.\relax }{figure.caption.123}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Results from task-specific group-level analyses. Brain maps show uncorrected effects (\emph  {p} \textless {} 0.00001, two-sided) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{138}{figure.caption.124}\protected@file@percent }
\newlabel{fig:fig-aomic-7}{{\M@TitleReference {4.7}{Results from task-specific group-level analyses. Brain maps show uncorrected effects (\emph  {p} \textless {} 0.00001, two-sided) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{138}{Results from task-specific group-level analyses. Brain maps show uncorrected effects (\emph {p} \textless {} 0.00001, two-sided) and were linearly interpolated for visualization in \emph {FSLeyes}. Unthresholded whole-brain \emph {z}-value maps are available on NeuroVault. Unthresholded whole-brain \emph {z}-value maps are available on NeuroVault.\relax }{figure.caption.124}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded \emph  {z}-value maps are available on NeuroVault.\relax }}{139}{figure.caption.125}\protected@file@percent }
\newlabel{fig:fig-aomic-8}{{\M@TitleReference {4.8}{Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded \emph  {z}-value maps are available on NeuroVault.\relax }}{139}{Group-level dual regression results for the first four components of Smith and colleagues (2009). Unthresholded \emph {z}-value maps are available on NeuroVault.\relax }{figure.caption.125}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.\relax }}{140}{figure.caption.126}\protected@file@percent }
\newlabel{fig:fig-aomic-9}{{\M@TitleReference {4.9}{Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.\relax }}{140}{Results from the voxelwise ISC analysis, arbitrarily thresholded at 0.1. An unthresholded whole-brain ISC map is available on NeuroVault.\relax }{figure.caption.126}{}}
\@writefile{toc}{\contentsline {subsection}{Diffusion-weighted scans}{140}{section*.127}\protected@file@percent }
\newlabel{diffusion-weighted-scans}{{\M@TitleReference {4.4}{Diffusion-weighted scans}}{140}{Diffusion-weighted scans}{section*.127}{}}
\@writefile{toc}{\contentsline {subsection}{Physiological data}{141}{section*.130}\protected@file@percent }
\newlabel{physiological-data}{{\M@TitleReference {4.4}{Physiological data}}{141}{Physiological data}{section*.130}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.\relax }}{142}{figure.caption.128}\protected@file@percent }
\newlabel{fig:fig-aomic-10}{{\M@TitleReference {4.10}{Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.\relax }}{142}{Quality control metrics related to the diffusion-weighted scans. FD: framewise displacement, Std EC: standard deviation of the linear terms of the eddy current distortions in Hz/mm.\relax }{figure.caption.128}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.\relax }}{143}{figure.caption.129}\protected@file@percent }
\newlabel{fig:fig-aomic-11}{{\M@TitleReference {4.11}{Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.\relax }}{143}{Diffusion-encoded color images of the FA-modulated median DTI eigenvectors across subjects. Red colors denote preferential diffusion along the sagittal axis (left-right), green colors denote preferential diffusion along the coronal axis (anterior-posterior), and blue colors denote preferential diffusion along the axial axis (inferior-superior). Brighter colors denote stronger preferential diffusion.\relax }{figure.caption.129}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at \emph  {z} \textgreater {} 6) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{144}{figure.caption.131}\protected@file@percent }
\newlabel{fig:fig-aomic-12}{{\M@TitleReference {4.12}{Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at \emph  {z} \textgreater {} 6) and were linearly interpolated for visualization in \emph  {FSLeyes}. Unthresholded whole-brain \emph  {z}-value maps are available on NeuroVault.\relax }}{144}{Results from group-level physiology analyses. Brain maps show uncorrected effects (thresholded arbitrarily at \emph {z} \textgreater {} 6) and were linearly interpolated for visualization in \emph {FSLeyes}. Unthresholded whole-brain \emph {z}-value maps are available on NeuroVault.\relax }{figure.caption.131}{}}
\gdef \LT@v {\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}\LT@entry 
    {1}{42.57143pt}}
\@writefile{toc}{\contentsline {subsection}{Psychometric data}{145}{section*.132}\protected@file@percent }
\newlabel{psychometric-data}{{\M@TitleReference {4.4}{Psychometric data}}{145}{Psychometric data}{section*.132}{}}
\@writefile{toc}{\contentsline {subsubsection}{Intelligence Structure Test (IST)}{145}{section*.133}\protected@file@percent }
\newlabel{intelligence-structure-test-ist}{{\M@TitleReference {4.4}{Intelligence Structure Test (IST)}}{145}{Intelligence Structure Test (IST)}{section*.133}{}}
\newlabel{tab:tab-aomic-6}{{\M@TitleReference {4.6}{Intelligence Structure Test (IST)}}{145}{Intelligence Structure Test (IST)}{table.4.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Correlations between total score, and subscales of the IST and relevant external variables.\relax }}{145}{table.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Personality: NEO-FFI}{145}{section*.134}\protected@file@percent }
\newlabel{personality-neo-ffi}{{\M@TitleReference {4.4}{Personality: NEO-FFI}}{145}{Personality: NEO-FFI}{section*.134}{}}
\gdef \LT@vi {\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}\LT@entry 
    {1}{49.66667pt}}
\newlabel{tab:tab-aomic-7}{{\M@TitleReference {4.7}{Personality: NEO-FFI}}{146}{Personality: NEO-FFI}{table.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Cross-correlations for the subscales of the NEO-FFI for the ID1000, PIOP1 and PIOP2 samples.\relax }}{146}{table.4.7}\protected@file@percent }
\gdef \LT@vii {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {subsubsection}{BIS/BAS}{147}{section*.135}\protected@file@percent }
\newlabel{bisbas}{{\M@TitleReference {4.4}{BIS/BAS}}{147}{BIS/BAS}{section*.135}{}}
\newlabel{tab:tab-aomic-8}{{\M@TitleReference {4.8}{BIS/BAS}}{147}{BIS/BAS}{table.4.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Cross-correlations for the subscales of the BIS/BAS for the ID1000 sample.\relax }}{147}{table.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{STAI-T}{148}{section*.136}\protected@file@percent }
\newlabel{stai-t}{{\M@TitleReference {4.4}{STAI-T}}{148}{STAI-T}{section*.136}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Acknowledgements}{148}{section.4.5}\protected@file@percent }
\newlabel{acknowledgements}{{\M@TitleReference {4.5}{Acknowledgements}}{148}{Acknowledgements}{section.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Code availability}{148}{section.4.6}\protected@file@percent }
\newlabel{aomic-code-availability}{{\M@TitleReference {4.6}{Code availability}}{148}{Code availability}{section.4.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Choosing to view morbid information involves reward circuitry}{151}{chapter.5}\protected@file@percent }
\newlabel{morbid-curiosity}{{\M@TitleReference {5}{Choosing to view morbid information involves reward circuitry}}{151}{Choosing to view morbid information involves reward circuitry}{chapter.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Using predictive modeling to quantify the importance and limitations of action units in emotion perception}{152}{chapter.6}\protected@file@percent }
\newlabel{au-limitations}{{\M@TitleReference {6}{Using predictive modeling to quantify the importance and limitations of action units in emotion perception}}{152}{Using predictive modeling to quantify the importance and limitations of action units in emotion perception}{chapter.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Comparing models of dynamic facial expression perception}{153}{chapter.7}\protected@file@percent }
\newlabel{facial-expression-models}{{\M@TitleReference {7}{Comparing models of dynamic facial expression perception}}{153}{Comparing models of dynamic facial expression perception}{chapter.7}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Summary and general discussion}{154}{chapter.8}\protected@file@percent }
\newlabel{summary-and-general-discussion}{{\M@TitleReference {8}{Summary and general discussion}}{154}{Summary and general discussion}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Explore!}{154}{section.8.1}\protected@file@percent }
\newlabel{explore}{{\M@TitleReference {8.1}{Explore!}}{154}{Explore!}{section.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Think \emph  {big}}{154}{section.8.2}\protected@file@percent }
\newlabel{think-big}{{\M@TitleReference {8.2}{Think \emph  {big}}}{154}{\texorpdfstring {Think \emph {big}}{Think big}}{section.8.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Rethink psychology education}{154}{section.8.3}\protected@file@percent }
\newlabel{rethink-psychology-education}{{\M@TitleReference {8.3}{Rethink psychology education}}{154}{Rethink psychology education}{section.8.3}{}}
\@writefile{toc}{\contentsline {part}{Appendices}{155}{section*.137}\protected@file@percent }
\gdef \LT@viii {\LT@entry 
    {1}{61.20044pt}\LT@entry 
    {1}{118.39978pt}\LT@entry 
    {1}{118.39978pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Supplement to Chapter \ref  {shared-states}}{156}{appendix.A}\protected@file@percent }
\newlabel{shared-states-supplement}{{\M@TitleReference {A}{Supplement to Chapter \ref  {shared-states}}}{156}{Supplement to Chapter \ref {shared-states}}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Stimuli used for SF-task}{156}{section.A.1}\protected@file@percent }
\newlabel{stimuli-used-for-sf-task}{{\M@TitleReference {A.1}{Stimuli used for SF-task}}{156}{Stimuli used for SF-task}{section.A.1}{}}
\newlabel{tab:tab-shared-states-S1}{{\M@TitleReference {A.1}{Stimuli used for SF-task}}{156}{Stimuli used for SF-task}{table.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Stimuli used for SF-task\relax }}{156}{table.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Instructions}{160}{section.A.2}\protected@file@percent }
\newlabel{instructions}{{\M@TitleReference {A.2}{Instructions}}{160}{Instructions}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{Full instruction for the other-focused emotion understanding task}{160}{section*.138}\protected@file@percent }
\newlabel{full-instruction-for-the-other-focused-emotion-understanding-task}{{\M@TitleReference {A.2}{Full instruction for the other-focused emotion understanding task}}{160}{Full instruction for the other-focused emotion understanding task}{section*.138}{}}
\@writefile{toc}{\contentsline {subsection}{Full instruction for the self-focused emotion imagery task}{161}{section*.139}\protected@file@percent }
\newlabel{full-instruction-for-the-self-focused-emotion-imagery-task}{{\M@TitleReference {A.2}{Full instruction for the self-focused emotion imagery task}}{161}{Full instruction for the self-focused emotion imagery task}{section*.139}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Behavioral results}{163}{section.A.3}\protected@file@percent }
\newlabel{behavioral-results}{{\M@TitleReference {A.3}{Behavioral results}}{163}{Behavioral results}{section.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph  {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph  {F}(2, 17) = 17.74, \emph  {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph  {M} = 74.00, \emph  {SE} = 2.10) were significantly less successful (\emph  {p} \textless {} 0.001) than both action-trials (\emph  {M} = 85.50, \emph  {SE} = 1.85) and situation trials (\emph  {M} = 90.00, \emph  {SE} = 1.92).\relax }}{163}{figure.caption.140}\protected@file@percent }
\newlabel{fig:fig-shared-states-S1}{{\M@TitleReference {A.1}{Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph  {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph  {F}(2, 17) = 17.74, \emph  {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph  {M} = 74.00, \emph  {SE} = 2.10) were significantly less successful (\emph  {p} \textless {} 0.001) than both action-trials (\emph  {M} = 85.50, \emph  {SE} = 1.85) and situation trials (\emph  {M} = 90.00, \emph  {SE} = 1.92).\relax }}{163}{Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph {F}(2, 17) = 17.74, \emph {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph {M} = 74.00, \emph {SE} = 2.10) were significantly less successful (\emph {p} \textless {} 0.001) than both action-trials (\emph {M} = 85.50, \emph {SE} = 1.85) and situation trials (\emph {M} = 90.00, \emph {SE} = 1.92).\relax }{figure.caption.140}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Optimization results}{165}{section.A.4}\protected@file@percent }
\newlabel{optimization-results}{{\M@TitleReference {A.4}{Optimization results}}{165}{Optimization results}{section.A.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf  {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf  {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf  {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf  {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }}{165}{figure.caption.141}\protected@file@percent }
\newlabel{fig:fig-shared-states-S2}{{\M@TitleReference {A.2}{Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf  {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf  {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf  {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf  {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }}{165}{Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }{figure.caption.141}{}}
\gdef \LT@ix {\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}}
\newlabel{tab:tab-shared-states-S2}{{\M@TitleReference {A.2}{Behavioral results}}{166}{Behavioral results}{table.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Parameters assessed in the optimization set\relax }}{166}{table.A.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph  {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph  {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph  {p} = 0.014 and \emph  {p} = 0.0007 respectively. Interoception was classified at chance level, \emph  {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }}{167}{figure.caption.142}\protected@file@percent }
\newlabel{fig:fig-shared-states-S3}{{\M@TitleReference {A.3}{Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph  {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph  {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph  {p} = 0.014 and \emph  {p} = 0.0007 respectively. Interoception was classified at chance level, \emph  {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }}{167}{Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph {p} = 0.014 and \emph {p} = 0.0007 respectively. Interoception was classified at chance level, \emph {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }{figure.caption.142}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Bagging procedure}{168}{section.A.5}\protected@file@percent }
\newlabel{bagging-procedure}{{\M@TitleReference {A.5}{Bagging procedure}}{168}{Bagging procedure}{section.A.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }}{168}{figure.caption.143}\protected@file@percent }
\newlabel{fig:fig-shared-states-S4}{{\M@TitleReference {A.4}{Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }}{168}{Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }{figure.caption.143}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Precision vs.~recall}{170}{section.A.6}\protected@file@percent }
\newlabel{precision-vs.-recall}{{\M@TitleReference {A.6}{Precision vs.~recall}}{170}{Precision vs.~recall}{section.A.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph  {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph  {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph  {p} = 0.0013 and \emph  {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph  {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }}{170}{figure.caption.144}\protected@file@percent }
\newlabel{fig:fig-shared-states-S5}{{\M@TitleReference {A.5}{A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph  {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph  {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph  {p} = 0.0013 and \emph  {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph  {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }}{170}{A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph {p} = 0.0013 and \emph {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }{figure.caption.144}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}Self vs.~other classification}{172}{section.A.7}\protected@file@percent }
\newlabel{self-vs.-other-classification}{{\M@TitleReference {A.7}{Self vs.~other classification}}{172}{Self vs.~other classification}{section.A.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf  {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph  {r} = -0.04, \emph  {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf  {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf  {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }}{172}{figure.caption.145}\protected@file@percent }
\newlabel{fig:fig-shared-states-S6}{{\M@TitleReference {A.6}{Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf  {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph  {r} = -0.04, \emph  {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf  {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf  {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }}{172}{Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph {r} = -0.04, \emph {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }{figure.caption.145}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph  {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph  {p}(action) \textless {} 0.001, \emph  {p}(interoception) = 0.008, \emph  {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph  {p} \textless {} 0.001), but not significant for situation (\emph  {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }}{173}{figure.caption.146}\protected@file@percent }
\newlabel{fig:fig-shared-states-S7}{{\M@TitleReference {A.7}{Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph  {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph  {p}(action) \textless {} 0.001, \emph  {p}(interoception) = 0.008, \emph  {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph  {p} \textless {} 0.001), but not significant for situation (\emph  {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }}{173}{Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph {p}(action) \textless {} 0.001, \emph {p}(interoception) = 0.008, \emph {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph {p} \textless {} 0.001), but not significant for situation (\emph {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }{figure.caption.146}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.8}Condition-average results}{175}{section.A.8}\protected@file@percent }
\newlabel{condition-average-results}{{\M@TitleReference {A.8}{Condition-average results}}{175}{Condition-average results}{section.A.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }}{175}{figure.caption.147}\protected@file@percent }
\newlabel{fig:fig-shared-states-S8}{{\M@TitleReference {A.8}{Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }}{175}{Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }{figure.caption.147}{}}
\gdef \LT@x {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {section}{\numberline {A.9}Individual subject scores}{176}{section.A.9}\protected@file@percent }
\newlabel{individual-subject-scores}{{\M@TitleReference {A.9}{Individual subject scores}}{176}{Individual subject scores}{section.A.9}{}}
\newlabel{tab:tab-shared-states-S3}{{\M@TitleReference {A.3}{Individual subject scores}}{176}{Individual subject scores}{table.A.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Mean general classification scores per subject for the self- and cross-analysis on the validation-set only.\relax }}{176}{table.A.3}\protected@file@percent }
\gdef \LT@xi {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {section}{\numberline {A.10}Brain region importance}{178}{section.A.10}\protected@file@percent }
\newlabel{brain-region-importance}{{\M@TitleReference {A.10}{Brain region importance}}{178}{Brain region importance}{section.A.10}{}}
\newlabel{tab:tab-shared-states-S4}{{\M@TitleReference {A.4}{Brain region importance}}{178}{Brain region importance}{table.A.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Most important voxels in terms of their average weight across iterations and subjects.\relax }}{178}{table.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.11}General note about tables with voxel-coordinates}{180}{section.A.11}\protected@file@percent }
\newlabel{general-note-about-tables-with-voxel-coordinates}{{\M@TitleReference {A.11}{General note about tables with voxel-coordinates}}{180}{General note about tables with voxel-coordinates}{section.A.11}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {B}Supplement to Chapter \ref  {confounds-decoding}}{182}{appendix.B}\protected@file@percent }
\newlabel{confounds-decoding-supplement}{{\M@TitleReference {B}{Supplement to Chapter \ref  {confounds-decoding}}}{182}{Supplement to Chapter \ref {confounds-decoding}}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Supplementary methods}{182}{section.B.1}\protected@file@percent }
\newlabel{supplementary-methods}{{\M@TitleReference {B.1}{Supplementary methods}}{182}{Supplementary methods}{section.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{Functional MRI simulation}{182}{section*.148}\protected@file@percent }
\newlabel{functional-mri-simulation}{{\M@TitleReference {B.1}{Functional MRI simulation}}{182}{Functional MRI simulation}{section*.148}{}}
\@writefile{toc}{\contentsline {subsubsection}{Rationale}{182}{section*.149}\protected@file@percent }
\newlabel{rationale}{{\M@TitleReference {B.1}{Rationale}}{182}{Rationale}{section*.149}{}}
\@writefile{toc}{\contentsline {subsubsection}{Generation of the data (\(X\), \(y\), \(C\))}{183}{section*.150}\protected@file@percent }
\newlabel{generation-of-the-data-x-y-c}{{\M@TitleReference {B.1}{Generation of the data (\(X\), \(y\), \(C\))}}{183}{\texorpdfstring {Generation of the data (\(X\), \(y\), \(C\))}{Generation of the data (X, y, C)}}{section*.150}{}}
\@writefile{toc}{\contentsline {subsubsection}{Estimating activity patterns from the data}{186}{section*.151}\protected@file@percent }
\newlabel{estimating-activity-patterns-from-the-data}{{\M@TitleReference {B.1}{Estimating activity patterns from the data}}{186}{Estimating activity patterns from the data}{section*.151}{}}
\@writefile{toc}{\contentsline {subsection}{Testing confound regression on simulated fMRI data}{187}{section*.152}\protected@file@percent }
\newlabel{testing-confound-regression-on-simulated-fmri-data}{{\M@TitleReference {B.1}{Testing confound regression on simulated fMRI data}}{187}{Testing confound regression on simulated fMRI data}{section*.152}{}}
\@writefile{toc}{\contentsline {subsection}{Controlling for confounds during pattern estimation}{188}{section*.153}\protected@file@percent }
\newlabel{controlling-for-confounds-during-pattern-estimation}{{\M@TitleReference {B.1}{Controlling for confounds during pattern estimation}}{188}{Controlling for confounds during pattern estimation}{section*.153}{}}
\@writefile{toc}{\contentsline {subsection}{Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size}{189}{section*.154}\protected@file@percent }
\newlabel{linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size}{{\M@TitleReference {B.1}{Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size}}{189}{Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size}{section*.154}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Supplementary results}{190}{section.B.2}\protected@file@percent }
\newlabel{supplementary-results}{{\M@TitleReference {B.2}{Supplementary results}}{190}{Supplementary results}{section.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{Testing confound regression on simulated fMRI data}{190}{section*.155}\protected@file@percent }
\newlabel{testing-confound-regression-on-simulated-fmri-data-1}{{\M@TitleReference {B.2}{Testing confound regression on simulated fMRI data}}{190}{Testing confound regression on simulated fMRI data}{section*.155}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95\% CI across iterations.\relax }}{191}{figure.caption.156}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S1}{{\M@TitleReference {B.1}{Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95\% CI across iterations.\relax }}{191}{Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95\% CI across iterations.\relax }{figure.caption.156}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \({"σ}_{\mathrm  {filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).\relax }}{192}{figure.caption.157}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S2}{{\M@TitleReference {B.2}{Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \({"σ}_{\mathrm  {filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).\relax }}{192}{Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \(\sigma _{\mathrm {filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).\relax }{figure.caption.157}{}}
\@writefile{toc}{\contentsline {subsection}{Controlling for confounds during pattern estimation}{192}{section*.158}\protected@file@percent }
\newlabel{controlling-for-confounds-during-pattern-estimation-1}{{\M@TitleReference {B.2}{Controlling for confounds during pattern estimation}}{192}{Controlling for confounds during pattern estimation}{section*.158}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Model performance when controlling for confounds during pattern estimation using the ``default'' (upper panels) and ``aggressive'' (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.\relax }}{193}{figure.caption.159}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S3}{{\M@TitleReference {B.3}{Model performance when controlling for confounds during pattern estimation using the ``default'' (upper panels) and ``aggressive'' (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.\relax }}{193}{Model performance when controlling for confounds during pattern estimation using the ``default'' (upper panels) and ``aggressive'' (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.\relax }{figure.caption.159}{}}
\@writefile{toc}{\contentsline {subsubsection}{Explanation for bias in trial-wise decoding analyses}{193}{section*.160}\protected@file@percent }
\newlabel{explanation-for-bias-in-trial-wise-decoding-analyses}{{\M@TitleReference {B.2}{Explanation for bias in trial-wise decoding analyses}}{193}{Explanation for bias in trial-wise decoding analyses}{section*.160}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Distribution of first-level parameter estimates, \(\hat  {{"β}}_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition.\relax }}{194}{figure.caption.161}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S4}{{\M@TitleReference {B.4}{Distribution of first-level parameter estimates, \(\hat  {{"β}}_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition.\relax }}{194}{Distribution of first-level parameter estimates, \(\hat {\beta }_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition.\relax }{figure.caption.161}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (``true generative model'') shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\({"β}_{X|y=0} = {"β}_{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (``signal'') shows the signal resulting from the generative model (including noise, \({"ε}\)). The lower panel (``estimated parameters'') shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.\relax }}{196}{figure.caption.162}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S5}{{\M@TitleReference {B.5}{Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (``true generative model'') shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\({"β}_{X|y=0} = {"β}_{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (``signal'') shows the signal resulting from the generative model (including noise, \({"ε}\)). The lower panel (``estimated parameters'') shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.\relax }}{196}{Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (``true generative model'') shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\(\beta _{X|y=0} = \beta _{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (``signal'') shows the signal resulting from the generative model (including noise, \(\epsilon \)). The lower panel (``estimated parameters'') shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.\relax }{figure.caption.162}{}}
\@writefile{toc}{\contentsline {subsubsection}{Explanation for bias in run-wise decoding analyses}{196}{section*.163}\protected@file@percent }
\newlabel{explanation-for-bias-in-run-wise-decoding-analyses}{{\M@TitleReference {B.2}{Explanation for bias in run-wise decoding analyses}}{196}{Explanation for bias in run-wise decoding analyses}{section*.163}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Visualization of model performance and feature distributions based on patterns of ``raw'' parameter estimates (\(\hat  {{"β}}_{X}\)), variance of parameter estimates (\(\mathrm  {var}(\hat  {{"β}}_{X})\)), or \emph  {t}-values (\(t(\hat  {{"β}}_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that ``variance decoding'' only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\).\relax }}{198}{figure.caption.164}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S6}{{\M@TitleReference {B.6}{Visualization of model performance and feature distributions based on patterns of ``raw'' parameter estimates (\(\hat  {{"β}}_{X}\)), variance of parameter estimates (\(\mathrm  {var}(\hat  {{"β}}_{X})\)), or \emph  {t}-values (\(t(\hat  {{"β}}_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that ``variance decoding'' only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\).\relax }}{198}{Visualization of model performance and feature distributions based on patterns of ``raw'' parameter estimates (\(\hat {\beta }_{X}\)), variance of parameter estimates (\(\mathrm {var}(\hat {\beta }_{X})\)), or \emph {t}-values (\(t(\hat {\beta }_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that ``variance decoding'' only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\).\relax }{figure.caption.164}{}}
\@writefile{toc}{\contentsline {subsection}{Linear vs.~nonlinear confound models: predicting VBM and TBSS intensity using brain size}{199}{section*.165}\protected@file@percent }
\newlabel{linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size}{{\M@TitleReference {B.2}{Linear vs.~nonlinear confound models: predicting VBM and TBSS intensity using brain size}}{199}{Linear vs.~nonlinear confound models: predicting VBM and TBSS intensity using brain size}{section*.165}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions reasonably well.\relax }}{200}{figure.caption.166}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S7}{{\M@TitleReference {B.7}{Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions reasonably well.\relax }}{200}{Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions reasonably well.\relax }{figure.caption.166}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) in voxel set B.\relax }}{202}{figure.caption.167}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S8}{{\M@TitleReference {B.8}{Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \({"Δ}R^{2}_{\mathrm  {linear-cubic}}\) in voxel set B.\relax }}{202}{Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \(\Delta R^{2}_{\mathrm {linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \(\Delta R^{2}_{\mathrm {linear-cubic}}\) in voxel set B.\relax }{figure.caption.167}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.9}{\ignorespaces Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions well.\relax }}{203}{figure.caption.168}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S9}{{\M@TitleReference {B.9}{Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions well.\relax }}{203}{Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case --- all models seem to fit the distributions well.\relax }{figure.caption.168}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.10}{\ignorespaces Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (``train only'') on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm  {test}} = X_{\mathrm  {test}} - C_{\mathrm  {test}}\hat  {{"β}}_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).\relax }}{204}{figure.caption.169}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S10}{{\M@TitleReference {B.10}{Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (``train only'') on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm  {test}} = X_{\mathrm  {test}} - C_{\mathrm  {test}}\hat  {{"β}}_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).\relax }}{204}{Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (``train only'') on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf.~Figure \ref {fig:fig-confounds-decoding-8} in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a ``reference performance'' for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm {test}} = X_{\mathrm {test}} - C_{\mathrm {test}}\hat {\beta }_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).\relax }{figure.caption.169}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.11}{\ignorespaces Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, \hyperlink {ref-powers2020evaluation}{2011}). The results are highly similar to results obtained when using the \(F_{1}\) score (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text).\relax }}{204}{figure.caption.170}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S11}{{\M@TitleReference {B.11}{Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, \hyperlink {ref-powers2020evaluation}{2011}). The results are highly similar to results obtained when using the \(F_{1}\) score (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text).\relax }}{204}{Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, \protect \hyperlink {ref-powers2020evaluation}{2011}). The results are highly similar to results obtained when using the \(F_{1}\) score (cf.~Figure \ref {fig:fig-confounds-decoding-8} in the main text).\relax }{figure.caption.170}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.12}{\ignorespaces Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the ``Include confound in model'' method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.\relax }}{205}{figure.caption.171}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S12}{{\M@TitleReference {B.12}{Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the ``Include confound in model'' method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.\relax }}{205}{Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the ``Include confound in model'' method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.\relax }{figure.caption.171}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.13}{\ignorespaces Reproduction of Figure \ref  {fig:fig-confounds-decoding-8} from the main text (``generic simulation'' results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the ``targeted subsampling'' method (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text), albeit much slower.\relax }}{205}{figure.caption.172}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S13}{{\M@TitleReference {B.13}{Reproduction of Figure \ref  {fig:fig-confounds-decoding-8} from the main text (``generic simulation'' results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the ``targeted subsampling'' method (cf.~Figure \ref  {fig:fig-confounds-decoding-8} in the main text), albeit much slower.\relax }}{205}{Reproduction of Figure \ref {fig:fig-confounds-decoding-8} from the main text (``generic simulation'' results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the ``targeted subsampling'' method (cf.~Figure \ref {fig:fig-confounds-decoding-8} in the main text), albeit much slower.\relax }{figure.caption.172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.14}{\ignorespaces Reproduction of Figure \ref  {fig:fig-confounds-decoding-10} from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90\% smaller sample).\relax }}{206}{figure.caption.173}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S14}{{\M@TitleReference {B.14}{Reproduction of Figure \ref  {fig:fig-confounds-decoding-10} from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90\% smaller sample).\relax }}{206}{Reproduction of Figure \ref {fig:fig-confounds-decoding-10} from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90\% smaller sample).\relax }{figure.caption.173}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.15}{\ignorespaces These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{207}{figure.caption.174}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S15}{{\M@TitleReference {B.15}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{207}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph {sd}(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }{figure.caption.174}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.16}{\ignorespaces These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., \hyperlink {ref-Jamalabadi2016-gr}{2016}). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{207}{figure.caption.175}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S16}{{\M@TitleReference {B.16}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., \hyperlink {ref-Jamalabadi2016-gr}{2016}). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{207}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., \protect \hyperlink {ref-Jamalabadi2016-gr}{2016}). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }{figure.caption.175}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.17}{\ignorespaces These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on \emph  {sd}(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{208}{figure.caption.176}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S17}{{\M@TitleReference {B.17}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph  {sd}(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on \emph  {sd}(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }}{208}{These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, \emph {sd}(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on \emph {sd}(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.\relax }{figure.caption.176}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.18}{\ignorespaces The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\).\relax }}{208}{figure.caption.177}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-S18}{{\M@TitleReference {B.18}{The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\).\relax }}{208}{The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\).\relax }{figure.caption.177}{}}
\gdef \LT@xii {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {C}Supplement to Chapter \ref  {aomic}}{209}{appendix.C}\protected@file@percent }
\newlabel{aomic-supplement}{{\M@TitleReference {C}{Supplement to Chapter \ref  {aomic}}}{209}{Supplement to Chapter \ref {aomic}}{appendix.C}{}}
\newlabel{tab:tab-aomic-S1}{{\M@TitleReference {C.1}{Supplement to Chapter \ref  {aomic}}}{209}{Supplement to Chapter \ref {aomic}}{table.C.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Acquisition parameters for the T1-weighted scans acquired across all three datasets.\relax }}{209}{table.C.1}\protected@file@percent }
\gdef \LT@xiii {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\gdef \LT@xiv {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\newlabel{tab:tab-aomic-S2}{{\M@TitleReference {C.2}{Supplement to Chapter \ref  {aomic}}}{210}{Supplement to Chapter \ref {aomic}}{table.C.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.2}{\ignorespaces Acquisition parameters for the phase-difference fieldmap scans acquired across all three datasets.\relax }}{210}{table.C.2}\protected@file@percent }
\newlabel{tab:tab-aomic-S3}{{\M@TitleReference {C.3}{Supplement to Chapter \ref  {aomic}}}{210}{Supplement to Chapter \ref {aomic}}{table.C.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.3}{\ignorespaces Acquisition parameters for the fMRI scans acquired across all three datasets.\relax }}{210}{table.C.3}\protected@file@percent }
\gdef \LT@xv {\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}\LT@entry 
    {1}{74.5pt}}
\newlabel{tab:tab-aomic-S4}{{\M@TitleReference {C.4}{Supplement to Chapter \ref  {aomic}}}{211}{Supplement to Chapter \ref {aomic}}{table.C.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.4}{\ignorespaces Acquisition parameters for the DWI scans acquired across all three datasets.\relax }}{211}{table.C.4}\protected@file@percent }
\gdef \LT@xvi {\LT@entry 
    {1}{74.97656pt}\LT@entry 
    {1}{55.75586pt}\LT@entry 
    {1}{55.75586pt}\LT@entry 
    {1}{55.75586pt}\LT@entry 
    {1}{55.75586pt}}
\newlabel{tab:tab-aomic-S5}{{\M@TitleReference {C.5}{Supplement to Chapter \ref  {aomic}}}{213}{Supplement to Chapter \ref {aomic}}{table.C.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.5}{\ignorespaces Description of the subject variables and psychometric variables.\relax }}{213}{table.C.5}\protected@file@percent }
\gdef \LT@xvii {\LT@entry 
    {1}{27.74414pt}\LT@entry 
    {1}{90.08528pt}\LT@entry 
    {1}{90.08528pt}\LT@entry 
    {1}{90.08528pt}}
\newlabel{tab:tab-aomic-S6}{{\M@TitleReference {C.6}{Supplement to Chapter \ref  {aomic}}}{215}{Supplement to Chapter \ref {aomic}}{table.C.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.6}{\ignorespaces All data types with associated identifiers, descriptions, and modalities.\relax }}{215}{table.C.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {D}Supplement to Chapter \ref  {morbid-curiosity}}{218}{appendix.D}\protected@file@percent }
\newlabel{morbid-curiosity-supplement}{{\M@TitleReference {D}{Supplement to Chapter \ref  {morbid-curiosity}}}{218}{Supplement to Chapter \ref {morbid-curiosity}}{appendix.D}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {E}Supplement to Chapter \ref  {au-limitations}}{219}{appendix.E}\protected@file@percent }
\newlabel{au-limitations-supplement}{{\M@TitleReference {E}{Supplement to Chapter \ref  {au-limitations}}}{219}{Supplement to Chapter \ref {au-limitations}}{appendix.E}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {F}Supplement to Chapter \ref  {facial-expression-models}}{220}{appendix.F}\protected@file@percent }
\newlabel{facial-expression-models-supplement}{{\M@TitleReference {F}{Supplement to Chapter \ref  {facial-expression-models}}}{220}{Supplement to Chapter \ref {facial-expression-models}}{appendix.F}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {G}Data, code and materials}{221}{appendix.G}\protected@file@percent }
\newlabel{resources-supplement}{{\M@TitleReference {G}{Data, code and materials}}{221}{Data, code and materials}{appendix.G}{}}
\newlabel{bibliography}{{\M@TitleReference {G}{Bibliography}}{222}{Bibliography}{appendix*.178}{}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{222}{appendix*.178}\protected@file@percent }
\newlabel{contributions-to-the-chapters}{{\M@TitleReference {G}{Contributions to the chapters}}{249}{Contributions to the chapters}{appendix*.179}{}}
\@writefile{toc}{\contentsline {chapter}{Contributions to the chapters}{249}{appendix*.179}\protected@file@percent }
\newlabel{list-of-other-publications}{{\M@TitleReference {G}{List of other publications}}{250}{List of other publications}{appendix*.180}{}}
\@writefile{toc}{\contentsline {chapter}{List of other publications}{250}{appendix*.180}\protected@file@percent }
\newlabel{nederlandse-samenvatting-summary-in-dutch}{{\M@TitleReference {G}{Nederlandse samenvatting (Summary in Dutch)}}{251}{Nederlandse samenvatting (Summary in Dutch)}{appendix*.181}{}}
\@writefile{toc}{\contentsline {chapter}{Nederlandse samenvatting (Summary in Dutch)}{251}{appendix*.181}\protected@file@percent }
\bgroup 
\@writefile{toc}{\bgroup }
\@writefile{lof}{\bgroup }
\@writefile{lot}{\bgroup }
\selectlanguage *{dutch}
\@writefile{toc}{\selectlanguage *{dutch}}
\@writefile{lof}{\selectlanguage *{dutch}}
\@writefile{lot}{\selectlanguage *{dutch}}
\egroup 
\@writefile{toc}{\egroup }
\@writefile{lof}{\egroup }
\@writefile{lot}{\egroup }
\selectlanguage *[variant=american]{english}
\@writefile{toc}{\selectlanguage *[variant=american]{english}}
\@writefile{lof}{\selectlanguage *[variant=american]{english}}
\@writefile{lot}{\selectlanguage *[variant=american]{english}}
\newlabel{acknowledgments}{{\M@TitleReference {G}{Acknowledgments}}{252}{Acknowledgments}{appendix*.182}{}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{252}{appendix*.182}\protected@file@percent }
\memsetcounter{lastsheet}{261}
\memsetcounter{lastpage}{252}
\gdef \@abspage@last{261}
