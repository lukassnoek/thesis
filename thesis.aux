\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\catcode `"\active 
\HyPL@Entry{0<</S/r>>}
\selectlanguage *[variant=american]{english}
\@writefile{toc}{\selectlanguage *[variant=american]{english}}
\@writefile{lof}{\selectlanguage *[variant=american]{english}}
\@writefile{lot}{\selectlanguage *[variant=american]{english}}
\HyPL@Entry{3<</P(\376\377\000c\000o\000v\000e\000r)>>}
\@writefile{toc}{\contentsline {chapter}{Contents}{cover}{section*.1}\protected@file@percent }
\HyPL@Entry{8<</S/D>>}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\newlabel{introduction}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The brain is not a dictionary}{1}{section.1.1}\protected@file@percent }
\newlabel{the-brain-is-not-a-dictionary}{{\M@TitleReference {1.1}{The brain is not a dictionary}}{1}{The brain is not a dictionary}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The brain (probably) does not care about your hypothesis}{1}{section.1.2}\protected@file@percent }
\newlabel{the-brain-probably-does-not-care-about-your-hypothesis}{{\M@TitleReference {1.2}{The brain (probably) does not care about your hypothesis}}{1}{The brain (probably) does not care about your hypothesis}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Interpretability and prediction are a trade-off (for now)}{2}{section.1.3}\protected@file@percent }
\newlabel{interpretability-and-prediction-are-a-trade-off-for-now}{{\M@TitleReference {1.3}{Interpretability and prediction are a trade-off (for now)}}{2}{Interpretability and prediction are a trade-off (for now)}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Exploration should be embraced more}{2}{section.1.4}\protected@file@percent }
\newlabel{exploration-should-be-embraced-more}{{\M@TitleReference {1.4}{Exploration should be embraced more}}{2}{Exploration should be embraced more}{section.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Proper generalization is hard}{2}{section.1.5}\protected@file@percent }
\newlabel{proper-generalization-is-hard}{{\M@TitleReference {1.5}{Proper generalization is hard}}{2}{Proper generalization is hard}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Psychology is complex, so it needs complex models}{2}{section.1.6}\protected@file@percent }
\newlabel{psychology-is-complex-so-it-needs-complex-models}{{\M@TitleReference {1.6}{Psychology is complex, so it needs complex models}}{2}{Psychology is complex, so it needs complex models}{section.1.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}{3}{chapter.2}\protected@file@percent }
\newlabel{shared-states}{{\M@TitleReference {2}{Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}}{3}{Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{4}{section.2.1}\protected@file@percent }
\newlabel{shared-states-introduction}{{\M@TitleReference {2.1}{Introduction}}{4}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methods}{8}{section.2.2}\protected@file@percent }
\newlabel{shared-states-methods}{{\M@TitleReference {2.2}{Methods}}{8}{Methods}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{Subjects}{8}{section*.2}\protected@file@percent }
\newlabel{shared-states-methods-subjects}{{\M@TitleReference {2.2}{Subjects}}{8}{Subjects}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Experimental design}{9}{section*.3}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design}{{\M@TitleReference {2.2}{Experimental design}}{9}{Experimental design}{section*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Self-focused emotion imagery task}{9}{section*.4}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design-sf-task}{{\M@TitleReference {2.2}{Self-focused emotion imagery task}}{9}{Self-focused emotion imagery task}{section*.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of the self-focused and other-focused task.\relax }}{10}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig-shared-states-1}{{\M@TitleReference {2.1}{Overview of the self-focused and other-focused task.\relax }}{10}{Overview of the self-focused and other-focused task.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Other-focused emotion understanding task}{10}{section*.6}\protected@file@percent }
\newlabel{shared-states-methods-experimental-design-of-task}{{\M@TitleReference {2.2}{Other-focused emotion understanding task}}{10}{Other-focused emotion understanding task}{section*.6}{}}
\@writefile{toc}{\contentsline {subsection}{Procedure}{11}{section*.7}\protected@file@percent }
\newlabel{shared-states-methods-procedure}{{\M@TitleReference {2.2}{Procedure}}{11}{Procedure}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{Image acquisition}{12}{section*.8}\protected@file@percent }
\newlabel{shared-states-methods-image-acquisition}{{\M@TitleReference {2.2}{Image acquisition}}{12}{Image acquisition}{section*.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Model optimization procedure}{12}{section.2.3}\protected@file@percent }
\newlabel{shared-states-methods-model-optimization-procedure}{{\M@TitleReference {2.3}{Model optimization procedure}}{12}{Model optimization procedure}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{Preprocessing and single-trial modeling}{13}{section*.10}\protected@file@percent }
\newlabel{shared-states-methods-preprocessing}{{\M@TitleReference {2.3}{Preprocessing and single-trial modeling}}{13}{Preprocessing and single-trial modeling}{section*.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematic overview of the cross-validation procedures. \textbf  {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf  {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:fig-shared-states-2}{{\M@TitleReference {2.2}{Schematic overview of the cross-validation procedures. \textbf  {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf  {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }}{14}{Schematic overview of the cross-validation procedures. \textbf {A}) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. \textbf {B}) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90\% of the self-data trials (i.e.~train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e.~scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{Multi-voxel pattern analysis}{15}{section*.11}\protected@file@percent }
\newlabel{shared-states-methods-mvpa}{{\M@TitleReference {2.3}{Multi-voxel pattern analysis}}{15}{Multi-voxel pattern analysis}{section*.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{MVPA pipeline}{15}{section*.12}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-pipeline}{{\M@TitleReference {2.3}{MVPA pipeline}}{15}{MVPA pipeline}{section*.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation scheme and bagging procedure}{16}{section*.13}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-cv-and-bagging}{{\M@TitleReference {2.3}{Cross-validation scheme and bagging procedure}}{16}{Cross-validation scheme and bagging procedure}{section*.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Statistical evaluation}{18}{section*.14}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-statistical-evaluation}{{\M@TitleReference {2.3}{Statistical evaluation}}{18}{Statistical evaluation}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial representation}{18}{section*.15}\protected@file@percent }
\newlabel{shared-states-methods-mvpa-spatial-representation}{{\M@TitleReference {2.3}{Spatial representation}}{18}{Spatial representation}{section*.15}{}}
\@writefile{toc}{\contentsline {subsection}{Additional analyses}{19}{section*.16}\protected@file@percent }
\newlabel{shared-states-methods-additional-analyses}{{\M@TitleReference {2.3}{Additional analyses}}{19}{Additional analyses}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{Univariate analysis}{19}{section*.17}\protected@file@percent }
\newlabel{shared-states-methods-univariate-analysis}{{\M@TitleReference {2.3}{Univariate analysis}}{19}{Univariate analysis}{section*.17}{}}
\@writefile{toc}{\contentsline {subsection}{Code availability}{20}{section*.18}\protected@file@percent }
\newlabel{shared-states-methods-code-availability}{{\M@TitleReference {2.3}{Code availability}}{20}{Code availability}{section*.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Results}{20}{section.2.4}\protected@file@percent }
\newlabel{shared-states-results}{{\M@TitleReference {2.4}{Results}}{20}{Results}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{Multi-voxel pattern analysis}{20}{section*.19}\protected@file@percent }
\newlabel{shared-states-results-mvpa}{{\M@TitleReference {2.4}{Multi-voxel pattern analysis}}{20}{Multi-voxel pattern analysis}{section*.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }}{21}{figure.caption.20}\protected@file@percent }
\newlabel{fig:fig-shared-states-3}{{\M@TitleReference {2.3}{Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }}{21}{Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Uncorrected \emph  {t}-value map of average feature weights across subjects; \emph  {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }}{22}{figure.caption.21}\protected@file@percent }
\newlabel{fig:fig-shared-states-4}{{\M@TitleReference {2.4}{Uncorrected \emph  {t}-value map of average feature weights across subjects; \emph  {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }}{22}{Uncorrected \emph {t}-value map of average feature weights across subjects; \emph {t}-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{Univariate analyses}{22}{section*.22}\protected@file@percent }
\newlabel{shared-states-results-univariate}{{\M@TitleReference {2.4}{Univariate analyses}}{22}{Univariate analyses}{section*.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Univariate contrasts for the self-focused and other-focused task.\relax }}{23}{figure.caption.23}\protected@file@percent }
\newlabel{fig:fig-shared-states-5}{{\M@TitleReference {2.5}{Univariate contrasts for the self-focused and other-focused task.\relax }}{23}{Univariate contrasts for the self-focused and other-focused task.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Discussion}{24}{section.2.5}\protected@file@percent }
\newlabel{shared-states-discussion}{{\M@TitleReference {2.5}{Discussion}}{24}{Discussion}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Acknowledgements}{29}{section.2.6}\protected@file@percent }
\newlabel{shared-states-acknowledgements}{{\M@TitleReference {2.6}{Acknowledgements}}{29}{Acknowledgements}{section.2.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}How to control for confounds in decoding analyses of neuroimaging data}{30}{chapter.3}\protected@file@percent }
\newlabel{confounds-decoding}{{\M@TitleReference {3}{How to control for confounds in decoding analyses of neuroimaging data}}{30}{How to control for confounds in decoding analyses of neuroimaging data}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{32}{section.3.1}\protected@file@percent }
\newlabel{confounds-decoding-introduction}{{\M@TitleReference {3.1}{Introduction}}{32}{Introduction}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{Partitioning effects into \emph  {true} signal and \emph  {confounded} signal}{34}{section*.24}\protected@file@percent }
\newlabel{confounds-decoding-introduction-true-vs-confounded}{{\M@TitleReference {3.1}{Partitioning effects into \emph  {true} signal and \emph  {confounded} signal}}{34}{\texorpdfstring {Partitioning effects into \emph {true} signal and \emph {confounded} signal}{Partitioning effects into true signal and confounded signal}}{section*.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }}{37}{figure.caption.25}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-1}{{\M@TitleReference {3.1}{Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }}{37}{Visualization of how variance in brain data (\(X\)) can partitioned into ``True signal'' and ``Confounded signal'', depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables.\relax }{figure.caption.25}{}}
\gdef \LT@i {\LT@entry 
    {1}{41.52026pt}\LT@entry 
    {1}{41.52026pt}\LT@entry 
    {1}{214.95947pt}}
\@writefile{toc}{\contentsline {subsection}{Methods for confound control}{38}{section*.26}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods}{{\M@TitleReference {3.1}{Methods for confound control}}{38}{Methods for confound control}{section*.26}{}}
\newlabel{tab:tab-confounds-decoding-1}{{\M@TitleReference {3.1}{Methods for confound control}}{38}{Methods for confound control}{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Notation.\relax }}{38}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{A priori counterbalancing}{39}{section*.27}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-apriori-counterbalancing}{{\M@TitleReference {3.1}{A priori counterbalancing}}{39}{A priori counterbalancing}{section*.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Include confounds in the data}{41}{section*.28}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-include-in-data}{{\M@TitleReference {3.1}{Include confounds in the data}}{41}{Include confounds in the data}{section*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{Control for confounds during pattern estimation}{43}{section*.29}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-pattern-estimation}{{\M@TitleReference {3.1}{Control for confounds during pattern estimation}}{43}{Control for confounds during pattern estimation}{section*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{Post hoc counterbalancing of confounds}{44}{section*.30}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-posthoc-counterbalancing}{{\M@TitleReference {3.1}{Post hoc counterbalancing of confounds}}{44}{Post hoc counterbalancing of confounds}{section*.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{44}{section*.32}\protected@file@percent }
\newlabel{confounds-decoding-introduction-methods-confound-regression}{{\M@TitleReference {3.1}{Confound regression}}{44}{Confound regression}{section*.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }}{45}{figure.caption.31}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-2}{{\M@TitleReference {3.2}{A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }}{45}{A schematic visualization how the main two confound control methods evaluated in this article deal with the ``confounded signal'', making sure decoding models only capitalize on the ``true signal''.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{Current study}{47}{section*.33}\protected@file@percent }
\newlabel{confounds-decoding-introduction-current-study}{{\M@TitleReference {3.1}{Current study}}{47}{Current study}{section*.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Methods}{49}{section.3.2}\protected@file@percent }
\newlabel{confounds-decoding-methods}{{\M@TitleReference {3.2}{Methods}}{49}{Methods}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{Data}{49}{section*.34}\protected@file@percent }
\newlabel{confounds-decoding-methods-data}{{\M@TitleReference {3.2}{Data}}{49}{Data}{section*.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{VBM acquisition \& analysis}{49}{section*.35}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-vbm}{{\M@TitleReference {3.2}{VBM acquisition \& analysis}}{49}{VBM acquisition \& analysis}{section*.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{TBSS acquisition \& analysis}{50}{section*.36}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-tbss}{{\M@TitleReference {3.2}{TBSS acquisition \& analysis}}{50}{TBSS acquisition \& analysis}{section*.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Brain size estimation}{50}{section*.37}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-brainsize}{{\M@TitleReference {3.2}{Brain size estimation}}{50}{Brain size estimation}{section*.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data and code availability}{51}{section*.38}\protected@file@percent }
\newlabel{confounds-decoding-methods-data-data-and-code}{{\M@TitleReference {3.2}{Data and code availability}}{51}{Data and code availability}{section*.38}{}}
\@writefile{toc}{\contentsline {subsection}{Decoding pipeline}{51}{section*.39}\protected@file@percent }
\newlabel{confounds-decoding-methods-pipeline}{{\M@TitleReference {3.2}{Decoding pipeline}}{51}{Decoding pipeline}{section*.39}{}}
\@writefile{toc}{\contentsline {subsection}{Evaluated methods for confound control}{52}{section*.40}\protected@file@percent }
\newlabel{confounds-decoding-methods-evaluated-methods}{{\M@TitleReference {3.2}{Evaluated methods for confound control}}{52}{Evaluated methods for confound control}{section*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Post hoc counterbalancing}{52}{section*.41}\protected@file@percent }
\newlabel{confounds-decoding-methods-evaluated-methods-counterbalancing}{{\M@TitleReference {3.2}{Post hoc counterbalancing}}{52}{Post hoc counterbalancing}{section*.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{54}{section*.42}\protected@file@percent }
\newlabel{confound-regression}{{\M@TitleReference {3.2}{Confound regression}}{54}{Confound regression}{section*.42}{}}
\newlabel{eq:cvcr-train}{{3.8}{55}{Confound regression}{equation.3.2.8}{}}
\newlabel{eq:cvcr-test}{{3.9}{56}{Confound regression}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Control for confounds during pattern estimation}{56}{section*.43}\protected@file@percent }
\newlabel{control-for-confounds-during-pattern-estimation}{{\M@TitleReference {3.2}{Control for confounds during pattern estimation}}{56}{Control for confounds during pattern estimation}{section*.43}{}}
\@writefile{toc}{\contentsline {subsection}{Analyses of simulated data}{56}{section*.44}\protected@file@percent }
\newlabel{analyses-of-simulated-data}{{\M@TitleReference {3.2}{Analyses of simulated data}}{56}{Analyses of simulated data}{section*.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analyses}{57}{section*.45}\protected@file@percent }
\newlabel{efficacy-analyses}{{\M@TitleReference {3.2}{Efficacy analyses}}{57}{Efficacy analyses}{section*.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of positive bias after post hoc counterbalancing}{59}{section*.46}\protected@file@percent }
\newlabel{analysis-of-positive-bias-after-post-hoc-counterbalancing}{{\M@TitleReference {3.2}{Analysis of positive bias after post hoc counterbalancing}}{59}{Analysis of positive bias after post hoc counterbalancing}{section*.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of negative bias after WDCR}{61}{section*.48}\protected@file@percent }
\newlabel{analysis-of-negative-bias-after-wdcr}{{\M@TitleReference {3.2}{Analysis of negative bias after WDCR}}{61}{Analysis of negative bias after WDCR}{section*.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }}{62}{figure.caption.47}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-3}{{\M@TitleReference {3.3}{Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }}{62}{Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Results}{63}{section.3.3}\protected@file@percent }
\newlabel{results}{{\M@TitleReference {3.3}{Results}}{63}{Results}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{Influence of brain size}{63}{section*.49}\protected@file@percent }
\newlabel{influence-of-brain-size}{{\M@TitleReference {3.3}{Influence of brain size}}{63}{Influence of brain size}{section*.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \({"ρ}= 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{65}{figure.caption.50}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-4}{{\M@TitleReference {3.4}{A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \({"ρ}= 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }}{65}{A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \(\rho = 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule (Scott, \protect \hyperlink {ref-scott1979optimal}{1979}) for bandwidth selection.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{Baseline model: no confound control}{65}{section*.51}\protected@file@percent }
\newlabel{baseline-model-no-confound-control}{{\M@TitleReference {3.3}{Baseline model: no confound control}}{65}{Baseline model: no confound control}{section*.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{66}{figure.caption.52}\protected@file@percent }
\newlabel{fig:fig-confounds-decoding-5}{{\M@TitleReference {3.5}{Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }}{66}{Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95\% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p < 0.001\), ** = \(p < 0.01\), * = \(p < 0.05\).\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{Post hoc counterbalancing}{66}{section*.53}\protected@file@percent }
\newlabel{post-hoc-counterbalancing}{{\M@TitleReference {3.3}{Post hoc counterbalancing}}{66}{Post hoc counterbalancing}{section*.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{66}{section*.54}\protected@file@percent }
\newlabel{empirical-results}{{\M@TitleReference {3.3}{Empirical results}}{66}{Empirical results}{section*.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{66}{section*.55}\protected@file@percent }
\newlabel{efficacy-analysis}{{\M@TitleReference {3.3}{Efficacy analysis}}{66}{Efficacy analysis}{section*.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of positive bias after post hoc counterbalancing}{66}{section*.56}\protected@file@percent }
\newlabel{analysis-of-positive-bias-after-post-hoc-counterbalancing-1}{{\M@TitleReference {3.3}{Analysis of positive bias after post hoc counterbalancing}}{66}{Analysis of positive bias after post hoc counterbalancing}{section*.56}{}}
\@writefile{toc}{\contentsline {subsection}{Whole-dataset confound regression (WDCR)}{66}{section*.57}\protected@file@percent }
\newlabel{whole-dataset-confound-regression-wdcr}{{\M@TitleReference {3.3}{Whole-dataset confound regression (WDCR)}}{66}{Whole-dataset confound regression (WDCR)}{section*.57}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{66}{section*.58}\protected@file@percent }
\newlabel{empirical-results-1}{{\M@TitleReference {3.3}{Empirical results}}{66}{Empirical results}{section*.58}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{66}{section*.59}\protected@file@percent }
\newlabel{efficacy-analysis-1}{{\M@TitleReference {3.3}{Efficacy analysis}}{66}{Efficacy analysis}{section*.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of negative bias after WDCR}{66}{section*.60}\protected@file@percent }
\newlabel{confounds-decoding-results-wdcr-bias}{{\M@TitleReference {3.3}{Analysis of negative bias after WDCR}}{66}{Analysis of negative bias after WDCR}{section*.60}{}}
\@writefile{toc}{\contentsline {subsection}{Cross-validated confound regression (CVCR)}{66}{section*.61}\protected@file@percent }
\newlabel{cross-validated-confound-regression-cvcr}{{\M@TitleReference {3.3}{Cross-validated confound regression (CVCR)}}{66}{Cross-validated confound regression (CVCR)}{section*.61}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical results}{67}{section*.62}\protected@file@percent }
\newlabel{empirical-results-2}{{\M@TitleReference {3.3}{Empirical results}}{67}{Empirical results}{section*.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficacy analysis}{67}{section*.63}\protected@file@percent }
\newlabel{efficacy-analysis-2}{{\M@TitleReference {3.3}{Efficacy analysis}}{67}{Efficacy analysis}{section*.63}{}}
\@writefile{toc}{\contentsline {subsection}{Summary methods for confound control}{67}{section*.64}\protected@file@percent }
\newlabel{summary-methods-for-confound-control}{{\M@TitleReference {3.3}{Summary methods for confound control}}{67}{Summary methods for confound control}{section*.64}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Discussion}{67}{section.3.4}\protected@file@percent }
\newlabel{confounds-decoding-discussion}{{\M@TitleReference {3.4}{Discussion}}{67}{Discussion}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{Relevance and consequences for previous and future research}{67}{section*.65}\protected@file@percent }
\newlabel{relevance-and-consequences-for-previous-and-future-research}{{\M@TitleReference {3.4}{Relevance and consequences for previous and future research}}{67}{Relevance and consequences for previous and future research}{section*.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{A priori and post hoc counterbalancing}{67}{section*.66}\protected@file@percent }
\newlabel{a-priori-and-post-hoc-counterbalancing}{{\M@TitleReference {3.4}{A priori and post hoc counterbalancing}}{67}{A priori and post hoc counterbalancing}{section*.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{Confound regression}{67}{section*.67}\protected@file@percent }
\newlabel{confound-regression-1}{{\M@TitleReference {3.4}{Confound regression}}{67}{Confound regression}{section*.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{Relevance to other analysis methods}{67}{section*.68}\protected@file@percent }
\newlabel{relevance-to-other-analysis-methods}{{\M@TitleReference {3.4}{Relevance to other analysis methods}}{67}{Relevance to other analysis methods}{section*.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{Importance for gender decoding studies}{67}{section*.69}\protected@file@percent }
\newlabel{importance-for-gender-decoding-studies}{{\M@TitleReference {3.4}{Importance for gender decoding studies}}{67}{Importance for gender decoding studies}{section*.69}{}}
\@writefile{toc}{\contentsline {subsection}{Choosing a confound model: linear vs.~nonlinear models}{67}{section*.70}\protected@file@percent }
\newlabel{choosing-a-confound-model-linear-vs.-nonlinear-models}{{\M@TitleReference {3.4}{Choosing a confound model: linear vs.~nonlinear models}}{67}{Choosing a confound model: linear vs.~nonlinear models}{section*.70}{}}
\@writefile{toc}{\contentsline {subsection}{Practical recommendations}{67}{section*.71}\protected@file@percent }
\newlabel{practical-recommendations}{{\M@TitleReference {3.4}{Practical recommendations}}{67}{Practical recommendations}{section*.71}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusions}{67}{section.3.5}\protected@file@percent }
\newlabel{conclusions}{{\M@TitleReference {3.5}{Conclusions}}{67}{Conclusions}{section.3.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}{68}{chapter.4}\protected@file@percent }
\newlabel{AOMIC}{{\M@TitleReference {4}{The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}}{68}{The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses}{chapter.4}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Choosing to view morbid information involves reward circuitry}{69}{chapter.5}\protected@file@percent }
\newlabel{morbid-curiosity}{{\M@TitleReference {5}{Choosing to view morbid information involves reward circuitry}}{69}{Choosing to view morbid information involves reward circuitry}{chapter.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Using predictive modeling to quantify the importance and limitations of action units in emotion perception}{70}{chapter.6}\protected@file@percent }
\newlabel{au-limitations}{{\M@TitleReference {6}{Using predictive modeling to quantify the importance and limitations of action units in emotion perception}}{70}{Using predictive modeling to quantify the importance and limitations of action units in emotion perception}{chapter.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Comparing models of dynamic facial expression perception}{71}{chapter.7}\protected@file@percent }
\newlabel{facial-expression-models}{{\M@TitleReference {7}{Comparing models of dynamic facial expression perception}}{71}{Comparing models of dynamic facial expression perception}{chapter.7}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Summary and general discussion}{72}{chapter.8}\protected@file@percent }
\newlabel{summary-and-general-discussion}{{\M@TitleReference {8}{Summary and general discussion}}{72}{Summary and general discussion}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Explore!}{72}{section.8.1}\protected@file@percent }
\newlabel{explore}{{\M@TitleReference {8.1}{Explore!}}{72}{Explore!}{section.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Think \emph  {big}}{72}{section.8.2}\protected@file@percent }
\newlabel{think-big}{{\M@TitleReference {8.2}{Think \emph  {big}}}{72}{\texorpdfstring {Think \emph {big}}{Think big}}{section.8.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Rethink psychology education}{72}{section.8.3}\protected@file@percent }
\newlabel{rethink-psychology-education}{{\M@TitleReference {8.3}{Rethink psychology education}}{72}{Rethink psychology education}{section.8.3}{}}
\@writefile{toc}{\contentsline {part}{Appendices}{73}{section*.72}\protected@file@percent }
\gdef \LT@ii {\LT@entry 
    {1}{61.20044pt}\LT@entry 
    {1}{118.39978pt}\LT@entry 
    {1}{118.39978pt}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Supplement to Chapter \ref  {shared-states}}{74}{appendix.A}\protected@file@percent }
\newlabel{shared-states-supplement}{{\M@TitleReference {A}{Supplement to Chapter \ref  {shared-states}}}{74}{Supplement to Chapter \ref {shared-states}}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Stimuli used for SF-task}{74}{section.A.1}\protected@file@percent }
\newlabel{stimuli-used-for-sf-task}{{\M@TitleReference {A.1}{Stimuli used for SF-task}}{74}{Stimuli used for SF-task}{section.A.1}{}}
\newlabel{tab:tab-shared-states-S1}{{\M@TitleReference {A.1}{Stimuli used for SF-task}}{74}{Stimuli used for SF-task}{table.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Stimuli used for SF-task\relax }}{74}{table.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Instructions}{78}{section.A.2}\protected@file@percent }
\newlabel{instructions}{{\M@TitleReference {A.2}{Instructions}}{78}{Instructions}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{Full instruction for the other-focused emotion understanding task}{78}{section*.73}\protected@file@percent }
\newlabel{full-instruction-for-the-other-focused-emotion-understanding-task}{{\M@TitleReference {A.2}{Full instruction for the other-focused emotion understanding task}}{78}{Full instruction for the other-focused emotion understanding task}{section*.73}{}}
\@writefile{toc}{\contentsline {subsection}{Full instruction for the self-focused emotion imagery task}{79}{section*.74}\protected@file@percent }
\newlabel{full-instruction-for-the-self-focused-emotion-imagery-task}{{\M@TitleReference {A.2}{Full instruction for the self-focused emotion imagery task}}{79}{Full instruction for the self-focused emotion imagery task}{section*.74}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Behavioral results}{81}{section.A.3}\protected@file@percent }
\newlabel{behavioral-results}{{\M@TitleReference {A.3}{Behavioral results}}{81}{Behavioral results}{section.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph  {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph  {F}(2, 17) = 17.74, \emph  {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph  {M} = 74.00, \emph  {SE} = 2.10) were significantly less successful (\emph  {p} \textless {} 0.001) than both action-trials (\emph  {M} = 85.50, \emph  {SE} = 1.85) and situation trials (\emph  {M} = 90.00, \emph  {SE} = 1.92).\relax }}{81}{figure.caption.75}\protected@file@percent }
\newlabel{fig:fig-shared-states-S1}{{\M@TitleReference {A.1}{Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph  {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph  {F}(2, 17) = 17.74, \emph  {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph  {M} = 74.00, \emph  {SE} = 2.10) were significantly less successful (\emph  {p} \textless {} 0.001) than both action-trials (\emph  {M} = 85.50, \emph  {SE} = 1.85) and situation trials (\emph  {M} = 90.00, \emph  {SE} = 1.92).\relax }}{81}{Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95\% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, \emph {F}(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, \emph {F}(2, 17) = 17.74, \emph {p} \textless {} 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (\emph {M} = 74.00, \emph {SE} = 2.10) were significantly less successful (\emph {p} \textless {} 0.001) than both action-trials (\emph {M} = 85.50, \emph {SE} = 1.85) and situation trials (\emph {M} = 90.00, \emph {SE} = 1.92).\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Optimization results}{83}{section.A.4}\protected@file@percent }
\newlabel{optimization-results}{{\M@TitleReference {A.4}{Optimization results}}{83}{Optimization results}{section.A.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf  {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf  {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf  {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf  {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }}{83}{figure.caption.76}\protected@file@percent }
\newlabel{fig:fig-shared-states-S2}{{\M@TitleReference {A.2}{Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf  {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf  {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf  {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf  {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }}{83}{Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e.~the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. \textbf {A}) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. \textbf {B}) Classification results of using a low-pass filter (2 seconds) or not. \textbf {C}) Classification results for different numbers of test-trials per class (1 to 5). \textbf {D}) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not.\relax }{figure.caption.76}{}}
\gdef \LT@iii {\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}\LT@entry 
    {1}{99.33333pt}}
\newlabel{tab:tab-shared-states-S2}{{\M@TitleReference {A.2}{Behavioral results}}{84}{Behavioral results}{table.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Parameters assessed in the optimization set\relax }}{84}{table.A.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph  {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph  {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph  {p} = 0.014 and \emph  {p} = 0.0007 respectively. Interoception was classified at chance level, \emph  {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }}{85}{figure.caption.77}\protected@file@percent }
\newlabel{fig:fig-shared-states-S3}{{\M@TitleReference {A.3}{Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph  {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph  {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph  {p} = 0.014 and \emph  {p} = 0.0007 respectively. Interoception was classified at chance level, \emph  {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }}{85}{Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent \emph {t}-test against chance-level classification (i.e.~0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e.~13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all \emph {p} \textless {} 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43\% correct) and situation (44\% correct) classes scored significantly above chance, \emph {p} = 0.014 and \emph {p} = 0.0007 respectively. Interoception was classified at chance level, \emph {p} = 0.99, which stands in contrast with the results in the validation-set.\relax }{figure.caption.77}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Bagging procedure}{86}{section.A.5}\protected@file@percent }
\newlabel{bagging-procedure}{{\M@TitleReference {A.5}{Bagging procedure}}{86}{Bagging procedure}{section.A.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }}{86}{figure.caption.78}\protected@file@percent }
\newlabel{fig:fig-shared-states-S4}{{\M@TitleReference {A.4}{Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }}{86}{Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial's final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated.\relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Precision vs.~recall}{88}{section.A.6}\protected@file@percent }
\newlabel{precision-vs.-recall}{{\M@TitleReference {A.6}{Precision vs.~recall}}{88}{Precision vs.~recall}{section.A.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph  {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph  {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph  {p} = 0.0013 and \emph  {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph  {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }}{88}{figure.caption.79}\protected@file@percent }
\newlabel{fig:fig-shared-states-S5}{{\M@TitleReference {A.5}{A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph  {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph  {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph  {p} = 0.0013 and \emph  {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph  {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }}{88}{A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all \emph {p} \textless {} 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all \emph {p} \textless {} 0.001); for recall both action and situation were decoded significantly above chance (\emph {p} = 0.0013 and \emph {p} \textless {} 0.001, respectively), while interoception was decoded below chance. All \emph {p}-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44\%) compared to its recall score (14\%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 \%) compared to its precision score (44\%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59\% of the interoception-trials are misclassified as situation-trials.\relax }{figure.caption.79}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}Self vs.~other classification}{90}{section.A.7}\protected@file@percent }
\newlabel{self-vs.-other-classification}{{\M@TitleReference {A.7}{Self vs.~other classification}}{90}{Self vs.~other classification}{section.A.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf  {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph  {r} = -0.04, \emph  {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf  {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf  {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }}{90}{figure.caption.80}\protected@file@percent }
\newlabel{fig:fig-shared-states-S6}{{\M@TitleReference {A.6}{Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf  {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph  {r} = -0.04, \emph  {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf  {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf  {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }}{90}{Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. \textbf {A}) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, \emph {r} = -0.04, \emph {p} = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. \textbf {B}) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. \textbf {C}) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4).\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph  {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph  {p}(action) \textless {} 0.001, \emph  {p}(interoception) = 0.008, \emph  {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph  {p} \textless {} 0.001), but not significant for situation (\emph  {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }}{91}{figure.caption.81}\protected@file@percent }
\newlabel{fig:fig-shared-states-S7}{{\M@TitleReference {A.7}{Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph  {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph  {p}(action) \textless {} 0.001, \emph  {p}(interoception) = 0.008, \emph  {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph  {p} \textless {} 0.001), but not significant for situation (\emph  {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }}{91}{Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. \emph {P}-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, \emph {p}(action) \textless {} 0.001, \emph {p}(interoception) = 0.008, \emph {p}(situation) \textless {} 0.001. For recall, classification scores for action and interoception were significant (both \emph {p} \textless {} 0.001), but not significant for situation (\emph {p} = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e.~90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis.\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.8}Condition-average results}{93}{section.A.8}\protected@file@percent }
\newlabel{condition-average-results}{{\M@TitleReference {A.8}{Condition-average results}}{93}{Condition-average results}{section.A.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }}{93}{figure.caption.82}\protected@file@percent }
\newlabel{fig:fig-shared-states-S8}{{\M@TitleReference {A.8}{Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }}{93}{Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95\% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p \textless {} 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals.\relax }{figure.caption.82}{}}
\gdef \LT@iv {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {section}{\numberline {A.9}Individual subject scores}{94}{section.A.9}\protected@file@percent }
\newlabel{individual-subject-scores}{{\M@TitleReference {A.9}{Individual subject scores}}{94}{Individual subject scores}{section.A.9}{}}
\newlabel{tab:tab-shared-states-S3}{{\M@TitleReference {A.3}{Individual subject scores}}{94}{Individual subject scores}{table.A.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Mean general classification scores per subject for the self- and cross-analysis on the validation-set only.\relax }}{94}{table.A.3}\protected@file@percent }
\gdef \LT@v {\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}\LT@entry 
    {1}{59.6pt}}
\@writefile{toc}{\contentsline {section}{\numberline {A.10}Brain region importance}{96}{section.A.10}\protected@file@percent }
\newlabel{brain-region-importance}{{\M@TitleReference {A.10}{Brain region importance}}{96}{Brain region importance}{section.A.10}{}}
\newlabel{tab:tab-shared-states-S4}{{\M@TitleReference {A.4}{Brain region importance}}{96}{Brain region importance}{table.A.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Most important voxels in terms of their average weight across iterations and subjects.\relax }}{96}{table.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.11}General note about tables with voxel-coordinates}{98}{section.A.11}\protected@file@percent }
\newlabel{general-note-about-tables-with-voxel-coordinates}{{\M@TitleReference {A.11}{General note about tables with voxel-coordinates}}{98}{General note about tables with voxel-coordinates}{section.A.11}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {B}Supplement to Chapter \ref  {confounds-decoding}}{100}{appendix.B}\protected@file@percent }
\newlabel{confounds-decoding-supplement}{{\M@TitleReference {B}{Supplement to Chapter \ref  {confounds-decoding}}}{100}{Supplement to Chapter \ref {confounds-decoding}}{appendix.B}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {C}Supplement to Chapter \ref  {morbid-curiosity}}{101}{appendix.C}\protected@file@percent }
\newlabel{morbid-curiosity-supplement}{{\M@TitleReference {C}{Supplement to Chapter \ref  {morbid-curiosity}}}{101}{Supplement to Chapter \ref {morbid-curiosity}}{appendix.C}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {D}Supplement to Chapter \ref  {au-limitations}}{102}{appendix.D}\protected@file@percent }
\newlabel{au-limitations-supplement}{{\M@TitleReference {D}{Supplement to Chapter \ref  {au-limitations}}}{102}{Supplement to Chapter \ref {au-limitations}}{appendix.D}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {E}Supplement to Chapter \ref  {facial-expression-models}}{103}{appendix.E}\protected@file@percent }
\newlabel{facial-expression-models-supplement}{{\M@TitleReference {E}{Supplement to Chapter \ref  {facial-expression-models}}}{103}{Supplement to Chapter \ref {facial-expression-models}}{appendix.E}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {F}Data, code and materials}{104}{appendix.F}\protected@file@percent }
\newlabel{resources-supplement}{{\M@TitleReference {F}{Data, code and materials}}{104}{Data, code and materials}{appendix.F}{}}
\newlabel{bibliography}{{\M@TitleReference {F}{Bibliography}}{105}{Bibliography}{appendix*.83}{}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{105}{appendix*.83}\protected@file@percent }
\newlabel{contributions-to-the-chapters}{{\M@TitleReference {F}{Contributions to the chapters}}{119}{Contributions to the chapters}{appendix*.84}{}}
\@writefile{toc}{\contentsline {chapter}{Contributions to the chapters}{119}{appendix*.84}\protected@file@percent }
\newlabel{list-of-other-publications}{{\M@TitleReference {F}{List of other publications}}{120}{List of other publications}{appendix*.85}{}}
\@writefile{toc}{\contentsline {chapter}{List of other publications}{120}{appendix*.85}\protected@file@percent }
\newlabel{nederlandse-samenvatting-summary-in-dutch}{{\M@TitleReference {F}{Nederlandse samenvatting (Summary in Dutch)}}{121}{Nederlandse samenvatting (Summary in Dutch)}{appendix*.86}{}}
\@writefile{toc}{\contentsline {chapter}{Nederlandse samenvatting (Summary in Dutch)}{121}{appendix*.86}\protected@file@percent }
\bgroup 
\@writefile{toc}{\bgroup }
\@writefile{lof}{\bgroup }
\@writefile{lot}{\bgroup }
\selectlanguage *{dutch}
\@writefile{toc}{\selectlanguage *{dutch}}
\@writefile{lof}{\selectlanguage *{dutch}}
\@writefile{lot}{\selectlanguage *{dutch}}
\egroup 
\@writefile{toc}{\egroup }
\@writefile{lof}{\egroup }
\@writefile{lot}{\egroup }
\selectlanguage *[variant=american]{english}
\@writefile{toc}{\selectlanguage *[variant=american]{english}}
\@writefile{lof}{\selectlanguage *[variant=american]{english}}
\@writefile{lot}{\selectlanguage *[variant=american]{english}}
\newlabel{acknowledgments}{{\M@TitleReference {F}{Acknowledgments}}{122}{Acknowledgments}{appendix*.87}{}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{122}{appendix*.87}\protected@file@percent }
\memsetcounter{lastsheet}{130}
\memsetcounter{lastpage}{122}
\gdef \@abspage@last{130}
