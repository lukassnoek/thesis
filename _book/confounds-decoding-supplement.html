<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>B Supplement to Chapter 3 | Towards prediction</title>
  <meta name="description" content="B Supplement to Chapter 3 | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="B Supplement to Chapter 3 | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B Supplement to Chapter 3 | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shared-states-supplement.html"/>
<link rel="next" href="aomic-supplement.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-brain-is-not-a-dictionary"><i class="fa fa-check"></i><b>1.1</b> The brain is not a dictionary</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-brain-probably-does-not-care-about-your-hypothesis"><i class="fa fa-check"></i><b>1.2</b> The brain (probably) does not care about your hypothesis</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretability-and-prediction-are-a-trade-off-for-now"><i class="fa fa-check"></i><b>1.3</b> Interpretability and prediction are a trade-off (for now)</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#exploration-should-be-embraced-more"><i class="fa fa-check"></i><b>1.4</b> Exploration should be embraced more</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#proper-generalization-is-hard"><i class="fa fa-check"></i><b>1.5</b> Proper generalization is hard</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#psychology-is-complex-so-it-needs-complex-models"><i class="fa fa-check"></i><b>1.6</b> Psychology is complex, so it needs complex models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.3</b> Model optimization procedure</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.3.1</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.3.2</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.3.3</b> Additional analyses</a></li>
<li class="chapter" data-level="2.3.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.3.4</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.3.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.3.5</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.4</b> Results</a><ul>
<li class="chapter" data-level="2.4.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.4.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.4.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.4.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-acknowledgements"><i class="fa fa-check"></i><b>2.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusions"><i class="fa fa-check"></i><b>3.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="4.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>4.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="4.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="4.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>4.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="4.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>4.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="4.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>4.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="4.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>4.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>4.3</b> Data Records</a><ul>
<li class="chapter" data-level="4.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>4.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="4.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>4.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>4.4</b> Technical validation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>4.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="4.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>4.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="4.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>4.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="4.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>4.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="4.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>4.4.5</b> Psychometric data</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="aomic.html"><a href="aomic.html#acknowledgements"><i class="fa fa-check"></i><b>4.5</b> Acknowledgements</a></li>
<li class="chapter" data-level="4.6" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>4.6</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a></li>
<li class="chapter" data-level="6" data-path="au-limitations.html"><a href="au-limitations.html"><i class="fa fa-check"></i><b>6</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a></li>
<li class="chapter" data-level="7" data-path="facial-expression-models.html"><a href="facial-expression-models.html"><i class="fa fa-check"></i><b>7</b> Comparing models of dynamic facial expression perception</a></li>
<li class="chapter" data-level="8" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html"><i class="fa fa-check"></i><b>8</b> Summary and general discussion</a><ul>
<li class="chapter" data-level="8.1" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#explore"><i class="fa fa-check"></i><b>8.1</b> Explore!</a></li>
<li class="chapter" data-level="8.2" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#think-big"><i class="fa fa-check"></i><b>8.2</b> Think <em>big</em></a></li>
<li class="chapter" data-level="8.3" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#rethink-psychology-education"><i class="fa fa-check"></i><b>8.3</b> Rethink psychology education</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a><ul>
<li class="chapter" data-level="A.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#stimuli-used-for-sf-task"><i class="fa fa-check"></i><b>A.1</b> Stimuli used for SF-task</a></li>
<li class="chapter" data-level="A.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#instructions"><i class="fa fa-check"></i><b>A.2</b> Instructions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-other-focused-emotion-understanding-task"><i class="fa fa-check"></i><b>A.2.1</b> Full instruction for the other-focused emotion understanding task</a></li>
<li class="chapter" data-level="A.2.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-self-focused-emotion-imagery-task"><i class="fa fa-check"></i><b>A.2.2</b> Full instruction for the self-focused emotion imagery task</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#behavioral-results"><i class="fa fa-check"></i><b>A.3</b> Behavioral results</a></li>
<li class="chapter" data-level="A.4" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#optimization-results"><i class="fa fa-check"></i><b>A.4</b> Optimization results</a></li>
<li class="chapter" data-level="A.5" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#bagging-procedure"><i class="fa fa-check"></i><b>A.5</b> Bagging procedure</a></li>
<li class="chapter" data-level="A.6" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#precision-vs.-recall"><i class="fa fa-check"></i><b>A.6</b> Precision vs. recall</a></li>
<li class="chapter" data-level="A.7" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#self-vs.-other-classification"><i class="fa fa-check"></i><b>A.7</b> Self vs. other classification</a></li>
<li class="chapter" data-level="A.8" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#condition-average-results"><i class="fa fa-check"></i><b>A.8</b> Condition-average results</a></li>
<li class="chapter" data-level="A.9" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#individual-subject-scores"><i class="fa fa-check"></i><b>A.9</b> Individual subject scores</a></li>
<li class="chapter" data-level="A.10" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#brain-region-importance"><i class="fa fa-check"></i><b>A.10</b> Brain region importance</a></li>
<li class="chapter" data-level="A.11" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#general-note-about-tables-with-voxel-coordinates"><i class="fa fa-check"></i><b>A.11</b> General note about tables with voxel-coordinates</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="au-limitations-supplement.html"><a href="au-limitations-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="F" data-path="facial-expression-models-supplement.html"><a href="facial-expression-models-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code and materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confounds-decoding-supplement" class="section level1">
<h1><span class="header-section-number">B</span> Supplement to Chapter 3</h1>
<p>The following supplementary methods describe the methods for the additional analyses done related to controlling for confounds in decoding analyses of (simulated) fMRI data and the validation of using linear confound models for brain size. All code for these simulations, analyses, and results for the fMRI-related sections can be found in <code>functional_MRI_simulation.ipynb</code> notebook. The code for the validation of linear confound models can be found in the notebook <code>empirical_analysis_gender_classification.ipynb</code>. Both notebooks are stored in the project’s Github repository (<a href="https://github.com/lukassnoek/MVCA">https://github.com/lukassnoek/MVCA</a>).</p>
<div id="supplementary-methods" class="section level2">
<h2><span class="header-section-number">B.1</span> Supplementary methods</h2>
<div id="functional-mri-simulation" class="section level3">
<h3><span class="header-section-number">B.1.1</span> Functional MRI simulation</h3>
<div id="rationale" class="section level4">
<h4><span class="header-section-number">B.1.1.1</span> Rationale</h4>
<p>The simulations of fMRI data as described here are meant to test the efficacy of our proposed methods for confound control (CVCR) and those proposed by others [“Control for confounds during pattern estimation”; <span class="citation">Woolgar et al. (<a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>) when applied to fMRI data instead of structural MRI data, as we did in the main text. One reason to suspect differences in how these methods behave between structural and functional MRI data is that we need to estimate the feature patterns (<span class="math inline">\(X\)</span>) from time series for fMRI data while the feature patterns in structural MRI data are readily available. Moreover, samples in pattern analyses of fMRI data are often correlated (due to temporal autocorrelation in the fMRI signal) while it is reasonable to believe that structural MRI yields independent samples (often individual subjects).</p>
</div>
<div id="generation-of-the-data-x-y-c" class="section level4">
<h4><span class="header-section-number">B.1.1.2</span> Generation of the data (<span class="math inline">\(X\)</span>, <span class="math inline">\(y\)</span>, <span class="math inline">\(C\)</span>)</h4>
<p>In these simulations, we generated fMRI time series data across a grid of “voxels” (<span class="math inline">\(K\)</span>) which may or may not activate in trials from different conditions. Additionally, we allow for the additive influence of a confounding variable with a prespecified correlation to the target variable <span class="citation">(corresponding to the “additive” model from Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>. In short, we generate voxel signal (<span class="math inline">\(s\)</span>) as a function of both true effects of the trials from different conditions (<span class="math inline">\(\beta_{X}\)</span>), the effect of the confound (<span class="math inline">\(\beta_{C}\)</span>), and autocorrelated noise (<span class="math inline">\(\epsilon\)</span>):</p>
<p><span class="math display">\[\begin{equation}
s = X\beta_{X} + C\beta_{C} + \epsilon, \epsilon \sim \mathcal{N}(0, V)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(V\)</span> specifies the covariance matrix of a signal autocorrelated as described by to an AR(1) process (we use <span class="math inline">\(\phi_{1} = 0.5\)</span>). Note that, here, <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> refer to time-varying (HRF-convolved) predictors instead of, as in in the main text, arrays of features (voxels) across different samples. In this simulation, we evaluate two types of MVPA approaches. In the first approach, which we call “trial-wise decoding”, an activity pattern is estimated for each trial separately using the least-squares all technique <span class="citation">(LSA; Abdulrahman &amp; Henson, <a href="bibliography.html#ref-abdulrahman2016effect" role="doc-biblioref">2016</a>)</span>. In LSA, each trial gets its own regressor in a first-level GLM. This approach is often used when there is only a single fMRI run available. In the second approach, which we call “run-wise decoding”, an activity pattern is estimated for each condition separately. Now, suppose one acquires a single run with two conditions and twenty trials per condition. In the trial-wise decoding approach, one would estimate the patterns of 40 trials (twenty for condition 1 and twenty for condition 2). Alternatively, suppose that one acquires ten runs with two conditions and again twenty trials per run. In the run-wise decoding approach, one would estimate in total twenty patterns (ten for condition 1 and ten for condition 2).</p>
<p>Formally, for <span class="math inline">\(I\)</span> trials across <span class="math inline">\(P\)</span> conditions, <span class="math inline">\(X\)</span> is ultimately of shape <span class="math inline">\(T\)</span> (timepoints) <span class="math inline">\(\times (I \times P + 1)\)</span> in the trial-wise decoding approach and <span class="math inline">\(T \times (P+1)\)</span> in the run-wise decoding approach (the <span class="math inline">\(+ 1\)</span> refers to the addition of an intercept). In our trial-wise simulations, we simulate 40 trials (<span class="math inline">\(I\)</span>) across 2 conditions (<span class="math inline">\(P\)</span>). In our run-wise simulations, we simulate 40 trials across 2 conditions in 10 runs. Note that the length of the fMRI run is automatically adjusted to the number of trials (<span class="math inline">\(I\)</span>), conditions (<span class="math inline">\(P\)</span>), trial duration, and interstimulus interval (ISI); increasing any of these parameters will increase the length of the run. We use a trial duration of 1 second and a jittered ISI between 4 and 5 seconds (mean = 4.5 seconds).</p>
<p>The initial (non-HRF-convolved) confound (<span class="math inline">\(C\)</span>, with shape <span class="math inline">\(N \times 1\)</span>) is generated with a prespecified correlation to the target (<span class="math inline">\(y\)</span>) by adding noise (<span class="math inline">\(\epsilon\)</span>) drawn from a standard normal distribution multiplied by a scaling factor (<span class="math inline">\(\gamma\)</span>):</p>
<p><span class="math display">\[\begin{equation}
C = y + \gamma\epsilon
\end{equation}\]</span></p>
<p>This process starts with a very small scaling factor (<span class="math inline">\(\gamma\)</span>). If the correlation between the target and the confound is too high, the scaling factor is increased and the process is repeated, which is iterated until the desired correlation has been found. The confound is then scaled from 0 to 1 using min-max scaling. After scaling, similar to the single-trial regressors (<span class="math inline">\(X_{j}\)</span>), the confound (<span class="math inline">\(C\)</span>) is also convolved with an HRF (the SPM default), representing a regressor which is parametrically modulated by the value of the confound for each trial (which could represent, for example, reaction time). The confound, <span class="math inline">\(C\)</span>, now represents a time-varying array of shape <span class="math inline">\(T \times 1\)</span>. This process is identical for the trial-wise and run-wise decoding approaches. However, when evaluating the efficacy of confound regression (as explained in the next section) in the context of run-wise decoding, we used the means per condition of the confound instead of the trial-wise confound values.</p>
<p>The simulation draws the true activation parameters for the trials from different conditions (<span class="math inline">\(y \in \{0,1\}\)</span> for <span class="math inline">\(P=2\)</span>) from a normal distribution with a specified mean and standard deviation. To generate null data (i.e., without any true difference across, e.g., two conditions), we generate the true parameters as follows:</p>
<p><span class="math display">\[\begin{equation}
\beta_{X(y = p)} \sim \mathcal{N}(\mu, \sigma)
\end{equation}\]</span></p>
<p>where, in the case of null data, <span class="math inline">\(\mu\)</span> represents the same mean for all conditions <span class="math inline">\(p = 0, \dots , P-1\)</span>. The weight of the confound (<span class="math inline">\(\beta_{C}\)</span>) is also drawn from a normal distribution with a prespecified mean (<span class="math inline">\(\mu_{C}\)</span>) and standard deviation (<span class="math inline">\(\sigma_{C}\)</span>):</p>
<p><span class="math display">\[\begin{equation}
\beta_{C} \sim \mathcal{N}(\mu_{C}, \sigma_{C})
\end{equation}\]</span></p>
<p>The weights for both <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> are drawn independently for the <span class="math inline">\(K\)</span> voxels (we use <span class="math inline">\(10 \times 10 = 100\)</span> voxels for all our simulations). However, to simulate spatial autocorrelation in our grid of artificial voxels, we smooth all <span class="math inline">\(T\)</span> 2D “volumes” (of shape <span class="math inline">\(\sqrt{K} \times \sqrt{K}\)</span>) separately with a 2D Gaussian smoothing kernel with a prespecified standard deviation. For our simulations, we use a standard deviation of 1 for the kernel.</p>
</div>
<div id="estimating-activity-patterns-from-the-data" class="section level4">
<h4><span class="header-section-number">B.1.1.3</span> Estimating activity patterns from the data</h4>
<p>After generating the signals (<span class="math inline">\(s\)</span>) of shape <span class="math inline">\(T \times K\)</span> (in which the 2D voxel dimension has been flattened to a single dimension), the “activity” parameters (<span class="math inline">\(\hat{\beta}_{X}\)</span>) for the trials (for trial-wise decoding) or conditions (for run-wise decoding) are estimated across all K voxels. We estimate these parameters using a generalized least squares (GLS) model on <span class="math inline">\(y\)</span> using the design matrix <span class="math inline">\(X\)</span> and covariance matrix <span class="math inline">\(V\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{X} = (X^{T}V^{-1}X)^{-1}X^{T}V^{-1}y
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}_{X}\)</span> is of shape <span class="math inline">\(N \times K\)</span> (where <span class="math inline">\(N\)</span> refers to the amount of trials in trial-wise decoding and the amount of conditions in run-wise decoding). Note that <span class="math inline">\(C\)</span> is not part of the design matrix <span class="math inline">\(X\)</span>, here. This would amount to the “control for confounds during pattern estimation” discussed in Supplementary Methods section “Controlling for confounds during pattern estimation”. Before entering these activity estimates (<span class="math inline">\(\hat{\beta}_{X}\)</span>) in our decoding pipeline, we divide these them by their standard error to generate <span class="math inline">\(t\)</span>-values <span class="citation">(as advised in Misaki et al., <a href="bibliography.html#ref-misaki2010comparison" role="doc-biblioref">2010</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
t_{\hat{\beta}_{X}} = \frac{\hat{\beta}_{X}}{\sqrt{\sigma^{2}\mathrm{diag}(X^{T}V^{-1}X)^{-1}}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma^{2}\)</span> is the sum-of-squares error divided by the degrees of freedom (<span class="math inline">\(T-N-1\)</span>).</p>
<p>To summarize, in all of our simulations, we keep the following “experimental” parameters constant: we simulate data from two conditions (<span class="math inline">\(y \in \{0,1\}\)</span>, which we refer to as “condition 0” and “condition 1”), each condition has 40 trials, trial duration is 1 second, ISIs are jittered between 4 and 5 seconds (mean = 4.5), noise is autocorrelated according to an AR(1) process (with <span class="math inline">\(\phi_{1} = 0.5)\)</span>, and the data is spatially smoothed using a Gaussian filter with a standard deviation of 2 across a <span class="math inline">\(10 \times 10\)</span> grid of voxels. For run-wise decoding, we simulate 10 runs.</p>
</div>
</div>
<div id="testing-confound-regression-on-simulated-fmri-data" class="section level3">
<h3><span class="header-section-number">B.1.2</span> Testing confound regression on simulated fMRI data</h3>
<p>In this simulation, we aim to test whether confound regression is able to remove the influence of confounds in fMRI data for both trial-wise and run-wise decoding analyses. Similar to the analyses reported in the main article, we contrast WDCR and CVCR, expecting that WDCR leads to similar negative bias while CVCR leads to similar (nearly) unbiased results. We use the same pipeline as in the other simulations, which consists of a normalization step to ensure that each feature has a mean of zero and a unit standard deviation, and a support vector classifier with a linear kernel (regularization parameter <span class="math inline">\(C=1\)</span>). The decoding pipeline uses a 10-fold stratified cross-validation scheme. For the run-wise decoding analyses, this is equivalent to a leave-one-run-out cross-validation scheme. Model performance in this simulation is reported as accuracy (as there is no class imbalance). The simulation was repeated 10 times for robustness.</p>
<p>Specifically, we evaluate and report model performance after confound regression both in the trial-wise decoding and run-wise decoding context and for different strengths of the correlation between the target and the confound (<span class="math inline">\(r_{Cy} \in \{0,0.1, \dots ,0.9,1\}\)</span>). Because arguably the temporal autocorrelation of fMRI data is the most prominent difference between fMRI and structural MRI data, and thus might impact decoding analyses differently <span class="citation">(Gilron et al., <a href="bibliography.html#ref-gilron2016addressing" role="doc-biblioref">2016</a>)</span>, we additionally test CVCR on data that differs in the degree of autocorrelation. We manipulate autocorrelation by temporally smoothing the signals (<span class="math inline">\(y\)</span>) before fitting the first-level GLM using a Gaussian filter with increasing widths (<span class="math inline">\(\sigma_{\mathrm{filter}} = \{0, 1 , \dots , 5\}\)</span>), yielding data with increasing autocorrelation. We used a grid of <span class="math inline">\(4 \times 4\)</span> voxels for this simulation to reduce computation time.</p>
</div>
<div id="controlling-for-confounds-during-pattern-estimation" class="section level3">
<h3><span class="header-section-number">B.1.3</span> Controlling for confounds during pattern estimation</h3>
<p>As discussed in the main text, one way to potentially control for confounds in fMRI data is to remove their influence when estimating the activity patterns in an initial first-level model <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>. In this section of the Supplementary methods, we tested the efficacy of this method on simulated fMRI data in both trial-wise decoding and run-wise decoding contexts. Note that the original article on this method <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span> performed run-wise decoding.</p>
<p>Specifically, we tested whether confounds can be controlled by adding the (HRF-convolved) confound to the design matrix <span class="math inline">\(X\)</span> in the first-level pattern estimation procedure. According to @[Woolgar2014-jb], when assuming that the confound has a truly additive effect, adding the confound to the design matrix will yield (single-trial) pattern estimates (<span class="math inline">\(\hat{\beta}_{X}\)</span>X) that only capture unique variance (i.e., no variance related to the confound). In our simulations, we tested two versions of this method. In one version, which we call the “default” version, the confound is added to the design matrix and the (GLS) model is estimated on using the design-matrix including both the single-trial (for trial-wise decoding) or condition regressors (for run-wise decoding) and the confound regressor. In the other version, which we call the “aggressive” version <span class="citation">(reflecting the same terminology as the fMRI-denoising method “ICA-AROMA”; Pruim et al., <a href="bibliography.html#ref-pruim2015ica" role="doc-biblioref">2015</a>)</span>, the confound regressor is first regressed out of the signal (<span class="math inline">\(s_{\mathrm{corr}} = s- C\hat{\beta}_{C}\)</span>) before fitting the regular first-level model using the design matrix (<span class="math inline">\(X\)</span>) without the confound regressor. The reason for testing these two methods is because it is unclear from the original Woolgar et al. articles <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-woolgar2011multi" role="doc-biblioref">2011</a>, <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span> which version was used and whether the two versions yield different results.</p>
<p>In our simulation, we varied the correlation between the (non-HRF-convolved) confound and the target (here, <span class="math inline">\(y \in \{0,1\}\)</span>). Importantly, we generated data without any effect, i.e., the parameters of the trials from the two different conditions (<span class="math inline">\(\beta_{X | y = 0}\)</span> and <span class="math inline">\(\beta_{X | y = 1}\)</span>) were (independently) drawn from the same normal distribution with a mean of 1 and standard deviation of 0.5. Similar to the other simulations, we use patterns of t-values in our decoding pipeline (unless explicitly stated otherwise). For both trial-wise and run-wise decoding, we report results from the default and aggressive procedure. All analyses are documented in the aforementioned notebook containing the fMRI simulations.</p>
</div>
<div id="linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size" class="section level3">
<h3><span class="header-section-number">B.1.4</span> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</h3>
<p>In the main text, we used linear models to regress out variance associated with brain size from VBM and TBSS data. Here, we test whether a linear model of the relation between brain size and VBM and TBSS data is suitable, and whether possibly a nonlinear model should have been preferred. We do this by performing a model comparison between linear, quadratic, and cubic regression models. All analyses and results can be found in the <code>brain_data_vs_brainsize.ipynb</code> notebook from the Github repository associated with this article.</p>
<p>For all analyses in this section, brain size with and without second and third degree polynomials were used as independent variables. As target variables, we created four voxel sets: first, we selected 500 random voxels from the VBM and TBSS data (voxel set A). Second, to inspect how large the <em>misfit</em> of a linear model could be, we selected as target variables the 500 voxels which have the highest quadratic (voxel set B) and cubic (voxel set C) correlation with brain size. Finally we select the 500 voxels with highest linear correlation with brain size (voxel set D), to inspect how large the misfit of a polynomial model is in these voxels.</p>
<p>We applied a 10-fold cross-validation pipeline which consisted of scaling features to mean 0 and standard deviation 1, and fitting an ordinary least squares regression model. Explained variance (<span class="math inline">\(R^2\)</span>) was used as a metric of model performance. The pipeline was repeated 50 times with random shuffling of samples.</p>
<p>For each voxel, we calculated the difference between model performance of linear and polynomial models. Negative differences (<span class="math inline">\(\Delta R^2_{\mathrm{linear - polynomial}} &lt; 0\)</span>) indicate that the polynomial model has higher cross-validated <span class="math inline">\(R^2\)</span> than a linear model, and thus, that a linear confound regression model would leave variance arguably associated with the confound in the target voxel. We plot the distributions of <span class="math inline">\(\Delta R^2_{\mathrm{linear - polynomial}}\)</span> to inspect for how many voxels this is the case, and for how many voxels linear models perform better.</p>
</div>
</div>
<div id="supplementary-results" class="section level2">
<h2><span class="header-section-number">B.2</span> Supplementary results</h2>
<p>The following supplementary results describe the results from the supplementary analyses related to controlling for confounds in decoding analyses of (simulated) fMRI data.</p>
<div id="testing-confound-regression-on-simulated-fmri-data-1" class="section level3">
<h3><span class="header-section-number">B.2.1</span> Testing confound regression on simulated fMRI data</h3>
<p>Here, we evaluated the efficacy of confound regression (both WDCR and CVCR) on simulated fMRI data in both trial-wise and run-wise decoding analyses across different strengths of the correlation between the target and the confound. Similar to the results reported in the main article, we find that WDCR yields consistent below chance accuracy in both the trial-wise and run-wise decoding analyses (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S1">B.1</a>, upper panels) and that CVCR yields (nearly) unbiased results for both trial-wise and run-wise decoding (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S1">B.1</a>, lower panels).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S1"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S1.png" alt="Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (\(r_{Cy}\)). Error-bars reflect the 95% CI across iterations."  />
<p class="caption">
Figure B.1: Model performance when using WDCR (upper panels) and CVCR (lower panels) to remove the influence of confounds in simulated fMRI data across different correlations between the confound and the target (<span class="math inline">\(r_{Cy}\)</span>). Error-bars reflect the 95% CI across iterations.
</p>
</div>

<p>Moreover, CVCR effectively controls for confounds on fMRI data with varying amounts of autocorrelation for both trial-wise and run-wise decoding analyses, as is shown in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S2">B.2</a>.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S2"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S2.png" alt="Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, \(\sigma_{\mathrm{filter}}\)) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation)."  />
<p class="caption">
Figure B.2: Model performance using CVCR versus no control and baseline (data with no confound) for different levels of autocorrelation (after smoothing with a Gaussian filter with an increasing standard deviation, <span class="math inline">\(\sigma_{\mathrm{filter}}\)</span>) for trial-wise and run-wise decoding. Note that for trial-wise decoding, high autocorrelation leads to below chance-accuracy for CVCR, but this is also present in the baseline data, which suggests that high autocorrelation in general leads to negative bias (at least in our simulation).
</p>
</div>

</div>
<div id="controlling-for-confounds-during-pattern-estimation-1" class="section level3">
<h3><span class="header-section-number">B.2.2</span> Controlling for confounds during pattern estimation</h3>
<p>Here, we tested the efficacy of controlling for confound during pattern estimation <span class="citation">(as proposed by Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>. Similar to the previous Supplementary analyses, we evaluated this method’s efficacy in both trial-wise and run-wise decoding analyses. We furthermore evaluated both the “default” (add the confound to the first-level design matrix) and “aggressive” (regress the confound from the signal before fitting the first-level model) approaches.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S3"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S3.png" alt="Model performance when controlling for confounds during pattern estimation using the “default” (upper panels) and “aggressive” (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features."  />
<p class="caption">
Figure B.3: Model performance when controlling for confounds during pattern estimation using the “default” (upper panels) and “aggressive” (lower panels) versions for both trial-wise (left panels) and run-wise decoding (right panels). Note that, in these analyses, patterns of t-values from the first-level model are used as features.
</p>
</div>

<p>As can be seen in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S3">B.3</a>, this method fails to control for confounds for all variants that are tested (trial-wise vs. run-wise decoding, “default” vs. "aggressive|). Below, we provide a potential explanation of this bias. We argue that the mechanism underlying this bias is different in trial-wise decoding than in run-wise decoding. We will first focus on the trial-wise decoding analyses.</p>
<div id="explanation-for-bias-in-trial-wise-decoding-analyses" class="section level4">
<h4><span class="header-section-number">B.2.2.1</span> Explanation for bias in trial-wise decoding analyses</h4>
<p>To supplement this explanation, in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S4">B.4</a> we visualized the distribution of parameter estimates from the first-level model, <span class="math inline">\(\hat{\beta}_{X}\)</span> across the two conditions after controlling for the confound during pattern estimation using the “aggressive” version in trial-wise decoding (but the graphs are similar when plotting the data from the “default” version; graphs for run-wise decoding analyses are, however, different, which will be discussed later).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S4"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S4.png" alt="Distribution of first-level parameter estimates, \(\hat{\beta}_{X}\), for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (\(r_{Cy}\)), with the colored dashed lines indicating the mean feature value for each condition."  />
<p class="caption">
Figure B.4: Distribution of first-level parameter estimates, <span class="math inline">\(\hat{\beta}_{X}\)</span>, for the two conditions (condition 0 in blue, condition 1 in orange) across different correlations between the target and the confound (<span class="math inline">\(r_{Cy}\)</span>), with the colored dashed lines indicating the mean feature value for each condition.
</p>
</div>

<p>When inspecting Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S4">B.4</a>, recall that the data were generated with a confound that was positively correlated with the target. Given that the target variable represents the two different conditions (<span class="math inline">\(y \in \{0, 1\}\)</span>), the existence of a positive correlation between the target and the confound implies that the confound increases the activation of the voxel in trials from condition 0. The confound’s effect on the voxel increases with higher correlations between the target and the confound. For example, suppose trials from condition 0 and condition 1 both truly activate the voxel with 1 unit (<span class="math inline">\(\beta_{X | y=0} = 1\)</span> and <span class="math inline">\(\beta_{X | y = 1} = 1\)</span>; Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S5">B.5</a>, upper panel), and that the confound is perfectly correlated with the target (<span class="math inline">\(r_{Cy} = 1\)</span>) and thus activates the voxel additionally with 1 unit (<span class="math inline">\(\beta_{C} = 1\)</span>). In this case, without confound control, one would expect the voxel to be activated in response to trials from condition 1 with a magnitude of 2 (<span class="math inline">\(\hat{\beta}_{X | y = 1} \approx \beta_{X | y = 1} + \beta_{C}\)</span>; Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S5">B.5</a>, middle panel). If one in fact would control for the confound by regressing out the confound from the signal (i.e., the “aggressive” approach), one would completely remove both the true effect (<span class="math inline">\(\beta_{X|y=1}\)</span>) and the confound effect (<span class="math inline">\(\beta_{C}\)</span>), driving the estimated activation parameter for condition 1 towards 0 (<span class="math inline">\(\hat{\beta}_{X|y=1} \approx 0\)</span>). The activation parameter for condition 0 is unaffected by removing the confound, as they are uncorrelated, and will thus be estimated correctly (<span class="math inline">\(\hat{\beta}_{X|y=0} = 1\)</span>; see Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S5">B.5</a>, lower panel). In this way, controlling for the confound created an artificial “effect”: trials from condition 0 seem to activate the voxel more than trials from condition 1 (<span class="math inline">\(\hat{\beta}_{X|y=0} &gt; \hat{\beta}_{X|y=1}\)</span>). We believe that this phenomenon underlies the positive bias when controlling for confounds during pattern estimation in trial-wise decoding analyses.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<div class="figure"><span id="fig:fig-confounds-decoding-S5"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S5.png" alt="Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (“true generative model”) shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,\(\beta_{X|y=0} = \beta_{X|y=1} = 1\)) and the confound (here, \(r_{Cy} = 0.9\)). The middle panel (“signal”) shows the signal resulting from the generative model (including noise, \(\epsilon\)). The lower panel (“estimated parameters”) shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero."  />
<p class="caption">
Figure B.5: Visualization of the issue underlying positive bias arising when controlling for confounds during pattern estimation. The upper panel (“true generative model”) shows the individual single-trial regressors for the different conditions, scaled by their true weight (here,<span class="math inline">\(\beta_{X|y=0} = \beta_{X|y=1} = 1\)</span>) and the confound (here, <span class="math inline">\(r_{Cy} = 0.9\)</span>). The middle panel (“signal”) shows the signal resulting from the generative model (including noise, <span class="math inline">\(\epsilon\)</span>). The lower panel (“estimated parameters”) shows the estimated model parameters for the different single-trial regressors. The dashed lines represent the average estimated parameter per condition, which shows that the estimated parameters of the condition that is correlated with the confound are driven towards zero.
</p>
</div>

</div>
<div id="explanation-for-bias-in-run-wise-decoding-analyses" class="section level4">
<h4><span class="header-section-number">B.2.2.2</span> Explanation for bias in run-wise decoding analyses</h4>
<p>We believe that the cause of bias in run-wise decoding analyses after controlling for confounds during pattern estimation is different than the cause of bias in trial-wise decoding analyses. Upon further inspection of the results of this simulation, we found that in the specific case of run-wise decoding with the “default” approach (i.e., including the confound in the first-level model instead of regressing the confound out of the signal before fitting the first-level model), there is no bias when using patterns of parameter estimates (<span class="math inline">\(X\)</span>) instead of patterns of <em>t</em>-values (<em>t</em>(<span class="math inline">\(X\)</span>); Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S6">B.6</a>, upper row, left panel). Indeed, when visualizing the distributions of the feature values (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S6">B.6</a>, lower row), using the “raw” parameter estimates (<span class="math inline">\(\hat{\beta}_{X}\)</span>, left column) or <em>t</em>-values (right column), it is clear that the bias only arises when using <em>t</em>-values. In fact, this bias in <em>t</em>-values is caused by unequal variance (Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S6">B.6</a>, middle panel) of the parameter estimates. The cause of the increased variance for condition 1, here, is due to the fact that a positive correlation between the confound and target (<span class="math inline">\(r_{Cy}\)</span>) results in a relatively higher correlation between the regressor of condition 1 and the confound regressor compared to the correlation between the regressor of condition 0 and the confound regressor. (Note that if the correlation would be negative, e.g., <span class="math inline">\(r_{Cy} = -0.9\)</span>, then the reverse would be true.) This issue of classifiers picking up differences in parameter variance in the process of estimating patterns for MVPA has been termed “variance decoding”, which is described in detail in
<span class="citation">Görgen et al. (<a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span> and <span class="citation">Hebart &amp; Baker (<a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S6"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S6.png" alt="Visualization of model performance and feature distributions based on patterns of “raw” parameter estimates (\(\hat{\beta}_{X}\)), variance of parameter estimates (\(\mathrm{var}(\hat{\beta}_{X})\)), or t-values (\(t(\hat{\beta}_{X})\))) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (\(r_{Cy}\)) for the different types of features. Note that the middle panel shows that “variance decoding” only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when \(r_{Cy} = 0.9\)."  />
<p class="caption">
Figure B.6: Visualization of model performance and feature distributions based on patterns of “raw” parameter estimates (<span class="math inline">\(\hat{\beta}_{X}\)</span>), variance of parameter estimates (<span class="math inline">\(\mathrm{var}(\hat{\beta}_{X})\)</span>), or <em>t</em>-values (<span class="math inline">\(t(\hat{\beta}_{X})\)</span>)) after controlling for confounds. The upper row shows the average accuracy across folds across different values of the correlation between the confound and the target (<span class="math inline">\(r_{Cy}\)</span>) for the different types of features. Note that the middle panel shows that “variance decoding” only occurs when controlling for confounds, as model performance is at chance when using patterns of variance estimates (the blue line in the middle panel). The lower row represents the distributions of feature values for the three different statistics when <span class="math inline">\(r_{Cy} = 0.9\)</span>.
</p>
</div>

<p>To summarize, we found that controlling for confound during pattern estimation leads to positive bias in all cases except in run-wise decoding using the “default” approach. We believe that cross-validated confound regression (CVCR) is nevertheless preferable to this method because it both controls for confounds effectively and allows the use of <em>t</em>-values (or other statistics based on parameter estimates, like multivariate noise-normalized parameter estimates), which has been shown to be more sensitive than using “raw” parameter estimates in MVPA <span class="citation">(Guggenmos et al., <a href="bibliography.html#ref-Guggenmos2018-rr" role="doc-biblioref">2018</a>; Misaki et al., <a href="bibliography.html#ref-misaki2010comparison" role="doc-biblioref">2010</a>; Walther et al., <a href="bibliography.html#ref-Walther2016-je" role="doc-biblioref">2016</a>)</span>.</p>
</div>
</div>
<div id="linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size" class="section level3">
<h3><span class="header-section-number">B.2.3</span> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</h3>
<p>Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S7">B.7</a> (top left panel) shows the distributions of difference in cross-validated <span class="math inline">\(R^2\)</span> (Linear - Polynomial) for the VBM data with 500 randomly selected voxels (voxel set A). A linear model performs (slightly) better (positive <span class="math inline">\(R^2\)</span>) than a quadratic model for 86.6% of these voxels (mean <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-quadratic}} = 0.009\)</span>, <em>SD</em> = 0.006), and better than a cubic model for 90.8% of the voxels (mean <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}} = 0.019\)</span>, <em>SD</em> = 0.027). Note that it can be expected that polynomial and cubic models perform better in a minority of the voxels simply due to random noise in the data (since we compare 500 <span class="math inline">\(R^2\)</span>-values), even if the “true” underlying relation is linear. To visualize the quality of fit of these models fit, brain size is plotted against VBM voxel intensity for a randomly selected voxel from this set in the bottom left panel. Lines are regression lines for the linear, quadratic, and cubic models. Supporting the use of a linear model, there is no clear deviation from bivariate normality.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S7"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S7.png" alt="Top row: \(R^2\) distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case — all models seem to fit the distributions reasonably well."  />
<p class="caption">
Figure B.7: Top row: <span class="math inline">\(R^2\)</span> distributions for the four voxel sets of the VBM data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case — all models seem to fit the distributions reasonably well.
</p>
</div>

<p>To explore further how well linear models perform in voxels where we expect polynomial models to perform best, we plotted the <span class="math inline">\(R^2\)</span> distributions for the 500 voxels with the highest overall quadratic correlation with brain size (voxel set B; second column). These are the voxels where a quadratic model would remove most variance (if used in a confound regression procedure). Also for these voxels, a linear model performs equally well as or better than a quadratic model (<span class="math inline">\(R^2_{\mathrm{linear-quadratic}} &gt; 0\)</span> for 89.4% of the voxels, mean <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-quadratic}} = 0.006\)</span>, <em>SD</em> = 0.007) and a cubic model (<span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}} &gt; 0\)</span> for 57.8% of these voxels, mean <span class="math inline">\(\Delta R^2_{\mathrm{linear-cubic}} = 0.003\)</span>, <em>SD</em> = 0.015). For a randomly selected voxel from this set, a scatter plot is included in the bottom panel to visualize how well the regression models fit. The plot indicates no obvious misfit of any of the models.</p>
<p>The same analysis was also performed for the 500 voxels that have the highest overall cubic correlation with brain size (voxel set C; third column). The histograms look similar to the histograms in the second column because voxel sets B and C consisted of largely the same voxels. It should not thus not be surprising that linear models again perform well compared to quadratic models (<span class="math inline">\(\Delta R^{2}_{\mathrm{linear-quadratic}} &gt; 0\)</span> for 90.4% of the voxels, mean <span class="math inline">\(\Delta R^2_{\mathrm{linear-quadratic}} = 0.006\)</span>, <em>SD</em> = 0.006), and compared to cubic models (<span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}} &gt; 0\)</span> for 57.6% of these voxels, mean <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}} = 0.003\)</span>, <em>SD</em> = 0.014). The bottom panel shows a randomly selected voxel from this voxel set, which again shows no obvious misfit.</p>
<p>Finally, we inspect the 500 voxels with the highest overall linear correlation with brain size (voxel set D, fourth column). Again, these turned out the be partly the same voxels as in set B and C. Therefore, linear models perform again equally well as or better than quadratic models (<span class="math inline">\(\Delta R^2_{\mathrm{linear-quadratic}} &gt; 0\)</span> for 94.2% of the voxels, mean <span class="math inline">\(\Delta R^2_{\mathrm{linear-quadratic}} = 0.007\)</span>, <em>SD</em> = 0.004) and cubic models (<span class="math inline">\(\Delta R^{2}_{\mathrm{linear-quadratic}} &gt; 0\)</span> for 59.2% of the voxels, mean <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-quadratic}} = 0.004\)</span>, <em>SD</em> = 0.014). The bottom panel show a randomly selected voxel from this voxel set, and indicates that all models capture the structure in the data.</p>
<p>Together, these results seem to imply that for all voxel sets, linear models perform mostly equally well as or better than polynomial models. Yet, it is interesting to inspect the “worst case” voxels; that is, to inspect how large the maximal misfit of a linear model is. Therefore, in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S8">B.8</a>, we plot the relation between brain size and VBM intensity for the voxel with most negative <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}}\)</span> from voxel set A (left panel) and voxel set B. For comparison, we also plot the relation between brain size and the voxel where a linear model performs better than a cubic model (selected from voxel set B). For the selected voxels, <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}}\)</span> values are -0.036 (left panel), -0.039(middle panel) and 0.032. Especially in the middle and right panel, the difference in fit between the linear and cubic model is mostly apparent the tails of the brain size distribution, where the model fit is based on least observations. For most brain sizes, both models make similar predictions about voxel intensity.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S8"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S8.png" alt="Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative \(\Delta R^{2}_{\mathrm{linear-cubic}}\) (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive \(\Delta R^{2}_{\mathrm{linear-cubic}}\) in voxel set B."  />
<p class="caption">
Figure B.8: Visualisation of the relation between brain size and VBM intensity for three voxels. The left two voxels have most negative <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}}\)</span> (i.e., the cubic model performs maximally better than the linear model) in voxel sets A and B, respectively. The voxel plotted in the right panel has the most positive <span class="math inline">\(\Delta R^{2}_{\mathrm{linear-cubic}}\)</span> in voxel set B.
</p>
</div>

<p>We repeated the same analyses for the TBSS data, and summarize the results in Supplementary Figure <a href="confounds-decoding-supplement.html#fig:fig-confounds-decoding-S9">B.9</a>. Since the results are qualitatively the same as for the VBM data, and lead to the same conclusions, we do not discuss them in detail. Those interested can find additional details in the notebook of this simulation.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-S9"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S9.png" alt="Top row: \(R^2\) distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case — all models seem to fit the distributions well."  />
<p class="caption">
Figure B.9: Top row: <span class="math inline">\(R^2\)</span> distributions for the four voxel sets of the TBSS data. Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection. Bottom row: scatter plots of the relation between brain size (scaled to mean 0 and SD 1) and voxel intensity from randomly selected voxels from each voxel sets. These panels are included to visualize the quality of model fits and to inspect whether there are no obvious misfits, i.e., whether the models miss patterns in the data. This is not the case — all models seem to fit the distributions well.
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S10"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S10.png" alt="Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (“train only”) on simulated data without any experimental effect (signal \(R^2 = 0.004\); left graph) and with some experimental effect (signal \(R^2 = 0.1\); right graph) for different values of confound \(R^2\) (cf. Figure 3.8 in the main text). The orange line represents the average performance (± 1 SD) when confound \(R^2\) = 0, which serves as a “reference performance” for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, \(r_{yC}\), is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute \(X_{\mathrm{test}} = X_{\mathrm{test}} - C_{\mathrm{test}}\hat{\beta}_{C}\)). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph)."  />
<p class="caption">
Figure B.10: Model performance of fully cross-validated confound regression (CVCR) versus confound regression on the train-set only (“train only”) on simulated data without any experimental effect (signal <span class="math inline">\(R^2 = 0.004\)</span>; left graph) and with some experimental effect (signal <span class="math inline">\(R^2 = 0.1\)</span>; right graph) for different values of confound <span class="math inline">\(R^2\)</span> (cf. Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a> in the main text). The orange line represents the average performance (± 1 SD) when confound <span class="math inline">\(R^2\)</span> = 0, which serves as a “reference performance” for when there is no confounded signal in the data. For both graphs, the correlation between the target and the confound, <span class="math inline">\(r_{yC}\)</span>, is fixed at 0.65. The reason for testing this version of confound regression (i.e., on the train-set only) is because it reduces the computation time substantially compared to fully cross-validated confound regression (as it does not have to compute <span class="math inline">\(X_{\mathrm{test}} = X_{\mathrm{test}} - C_{\mathrm{test}}\hat{\beta}_{C}\)</span>). However, this method seems to yield substantial bias when there is (almost) no signal (left graph), but intriguingly not when there is true signal (right graph).
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S11"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S11.png" alt="Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of \(F_{1}\) score, as this latter metric has been criticized because it neglects false negatives (Powers, 2011). The results are highly similar to results obtained when using the \(F_{1}\) score (cf. Figure 3.8 in the main text)."  />
<p class="caption">
Figure B.11: Model performance of the different evaluated methods for confound control but using the AUC-ROC metric to measure model performance instead of <span class="math inline">\(F_{1}\)</span> score, as this latter metric has been criticized because it neglects false negatives <span class="citation">(Powers, <a href="bibliography.html#ref-powers2020evaluation" role="doc-biblioref">2011</a>)</span>. The results are highly similar to results obtained when using the <span class="math inline">\(F_{1}\)</span> score (cf. Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a> in the main text).
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S12"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S12.png" alt="Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the “Include confound in model” method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds."  />
<p class="caption">
Figure B.12: Model performance of the different evaluated methods for confound control, including a method proposed by a reviewer. This method entails training the decoding model on data including the confound as a predictor (i.e., an implementation of the “Include confound in model” method), but setting the confound values to their mean in the test set. The rationale is that the decoding model cannot profit from the confound in the test set. However, contrary to expectations, this method performs similarly to not controlling for confounds.
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S13"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S13.png" alt="Reproduction of Figure 3.8 from the main text (“generic simulation” results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the “targeted subsampling” method (cf. Figure 3.8 in the main text), albeit much slower."  />
<p class="caption">
Figure B.13: Reproduction of Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a> from the main text (“generic simulation” results), but with the random subsampling procedure instead of the targeted subsampling procedure (from only a single iteration due to time constraints). This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The results from counterbalancing, here, are qualitatively similar to the results when using the “targeted subsampling” method (cf. Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-8">3.8</a> in the main text), albeit much slower.
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S14"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S14.png" alt="Reproduction of Figure 3.10 from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90% smaller sample)."  />
<p class="caption">
Figure B.14: Reproduction of Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-10">3.10</a> from the main text, but with the random subsampling procedure instead of the targeted subsampling procedure. This procedure attempts to find a subsample of the data of a given size without a correlation between target and confound for 10.000 tries. If such a subsample cannot be found, the subsample size is decreased by 1, after which again 10.000 attempts are made to find a good subsample with the new size. The plot shows that also random subsampling can induce a positive bias, even with extreme power loss (90% smaller sample).
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S15"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S15.png" alt="These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, sd(\(r_{yX}\)), and accuracy holds for different samples sizes (i.e., values for \(N\)). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section."  />
<p class="caption">
Figure B.15: These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, <em>sd</em>(<span class="math inline">\(r_{yX}\)</span>), and accuracy holds for different samples sizes (i.e., values for <span class="math inline">\(N\)</span>). Note that the predicted accuracy based on the standard deviation expected from the sampling distribution is at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S16"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S16.png" alt="These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set (replicating results from H. Jamalabadi et al., 2016). Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section."  />
<p class="caption">
Figure B.16: These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution and accuracy also holds for sizes of the test-set <span class="citation">(replicating results from H. Jamalabadi et al., <a href="bibliography.html#ref-Jamalabadi2016-gr" role="doc-biblioref">2016</a>)</span>. Note that the predicted accuracy is again at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S17"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S17.png" alt="These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, sd(\(r_{yX}\)), and accuracy also holds for different numbers of features (\(K\)). Note that the predicted accuracy based on sd(\(r_{yX}\)) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section."  />
<p class="caption">
Figure B.17: These plots show that the relationship between the standard deviation of the empirical feature-target correlation distribution, <em>sd</em>(<span class="math inline">\(r_{yX}\)</span>), and accuracy also holds for different numbers of features (<span class="math inline">\(K\)</span>). Note that the predicted accuracy based on <em>sd</em>(<span class="math inline">\(r_{yX}\)</span>) is approximately at 0.5 for every plot. The data were generated in the same manner as reported in the WDCR follow-up section.
</p>
</div>

<div class="figure"><span id="fig:fig-confounds-decoding-S18"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_S18.png" alt="The relation of the standard deviation of the correlation distribution and accuracy for different values of \(K\)."  />
<p class="caption">
Figure B.18: The relation of the standard deviation of the correlation distribution and accuracy for different values of <span class="math inline">\(K\)</span>.
</p>
</div>


</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="14">
<li id="fn14"><p>One could argue that this issue only poses a problem when the true parameters are non-zero (<span class="math inline">\(\beta_{X|y=0} = \beta_{X|y=1} \neq 0\)</span>) and when the true parameters are in fact all zero (<span class="math inline">\(\beta_{X|y=0} = \beta_{X|y=1} = 0\)</span>), there would be no positive bias. This is indeed the case, but we note that the true parameters are never known in empirical anlayses, so we nonetheless advise against using this method.<a href="confounds-decoding-supplement.html#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shared-states-supplement.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="aomic-supplement.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
