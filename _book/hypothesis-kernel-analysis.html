<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Using predictive modeling to quantify the importance and limitations of action units in emotion perception | Towards prediction</title>
  <meta name="description" content="7 Using predictive modeling to quantify the importance and limitations of action units in emotion perception | Towards prediction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Using predictive modeling to quantify the importance and limitations of action units in emotion perception | Towards prediction" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Using predictive modeling to quantify the importance and limitations of action units in emotion perception | Towards prediction" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="morbid-curiosity.html"/>
<link rel="next" href="static-vs-dynamic.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> This paragraph begins with “This thesis was typeset using (R) Markdown,  and the  R-package”</a><ul>
<li class="chapter" data-level="1.0.1" data-path="index.html"><a href="index.html#title-page"><i class="fa fa-check"></i><b>1.0.1</b> Title page</a></li>
<li class="chapter" data-level="1.0.2" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.0.2</b> Colophon</a></li>
<li class="chapter" data-level="1.0.3" data-path="index.html"><a href="index.html#committee"><i class="fa fa-check"></i><b>1.0.3</b> Committee</a></li>
<li class="chapter" data-level="1.0.4" data-path="index.html"><a href="index.html#book-settings"><i class="fa fa-check"></i><b>1.0.4</b> Book settings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="general-introduction.html"><a href="general-introduction.html#inference-done-differently"><i class="fa fa-check"></i><b>2.1</b> Inference done differently</a></li>
<li class="chapter" data-level="2.2" data-path="general-introduction.html"><a href="general-introduction.html#towards-prediction"><i class="fa fa-check"></i><b>2.2</b> Towards prediction</a></li>
<li class="chapter" data-level="2.3" data-path="general-introduction.html"><a href="general-introduction.html#outline-of-this-thesis"><i class="fa fa-check"></i><b>2.3</b> Outline of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>3</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>3.2.1</b> Subjects</a></li>
<li class="chapter" data-level="3.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>3.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="3.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>3.2.3</b> Procedure</a></li>
<li class="chapter" data-level="3.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>3.2.4</b> Image acquisition</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>3.3</b> Model optimization procedure</a><ul>
<li class="chapter" data-level="3.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>3.3.1</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="3.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>3.3.2</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="3.3.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>3.3.3</b> Additional analyses</a></li>
<li class="chapter" data-level="3.3.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>3.3.4</b> Univariate analysis</a></li>
<li class="chapter" data-level="3.3.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>3.3.5</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>3.4</b> Results</a><ul>
<li class="chapter" data-level="3.4.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>3.4.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="3.4.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>3.4.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>4</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>4.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="4.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>4.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="4.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>4.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>4.2.1</b> Data</a></li>
<li class="chapter" data-level="4.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>4.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="4.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>4.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="4.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>4.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>4.3</b> Results</a><ul>
<li class="chapter" data-level="4.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>4.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="4.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>4.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="4.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>4.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="4.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>4.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="4.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>4.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="4.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>4.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>4.4</b> Discussion</a><ul>
<li class="chapter" data-level="4.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>4.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="4.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>4.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="4.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>4.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusions"><i class="fa fa-check"></i><b>4.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="aomic.html"><a href="aomic.html"><i class="fa fa-check"></i><b>5</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a><ul>
<li class="chapter" data-level="5.1" data-path="aomic.html"><a href="aomic.html#background-summary"><i class="fa fa-check"></i><b>5.1</b> Background &amp; summary</a></li>
<li class="chapter" data-level="5.2" data-path="aomic.html"><a href="aomic.html#methods"><i class="fa fa-check"></i><b>5.2</b> Methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="aomic.html"><a href="aomic.html#scanner-details-and-general-scanning-protocol-all-datasets"><i class="fa fa-check"></i><b>5.2.1</b> Scanner details and general scanning protocol (all datasets)</a></li>
<li class="chapter" data-level="5.2.2" data-path="aomic.html"><a href="aomic.html#id1000-specifics"><i class="fa fa-check"></i><b>5.2.2</b> ID1000 specifics</a></li>
<li class="chapter" data-level="5.2.3" data-path="aomic.html"><a href="aomic.html#piop1-and-piop2-specifics"><i class="fa fa-check"></i><b>5.2.3</b> PIOP1 and PIOP2 specifics</a></li>
<li class="chapter" data-level="5.2.4" data-path="aomic.html"><a href="aomic.html#subject-variables-all-datasets"><i class="fa fa-check"></i><b>5.2.4</b> Subject variables (all datasets)</a></li>
<li class="chapter" data-level="5.2.5" data-path="aomic.html"><a href="aomic.html#psychometric-variables-all-datasets"><i class="fa fa-check"></i><b>5.2.5</b> Psychometric variables (all datasets)</a></li>
<li class="chapter" data-level="5.2.6" data-path="aomic.html"><a href="aomic.html#aomic-derivatives"><i class="fa fa-check"></i><b>5.2.6</b> Data standardization, preprocessing, and derivatives</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="aomic.html"><a href="aomic.html#data-records"><i class="fa fa-check"></i><b>5.3</b> Data Records</a><ul>
<li class="chapter" data-level="5.3.1" data-path="aomic.html"><a href="aomic.html#data-formats-and-types"><i class="fa fa-check"></i><b>5.3.1</b> Data formats and types</a></li>
<li class="chapter" data-level="5.3.2" data-path="aomic.html"><a href="aomic.html#data-repositories-used"><i class="fa fa-check"></i><b>5.3.2</b> Data repositories used</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="aomic.html"><a href="aomic.html#technical-validation"><i class="fa fa-check"></i><b>5.4</b> Technical validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="aomic.html"><a href="aomic.html#t1-weighted-scans"><i class="fa fa-check"></i><b>5.4.1</b> T1-weighted scans</a></li>
<li class="chapter" data-level="5.4.2" data-path="aomic.html"><a href="aomic.html#functional-bold-scans"><i class="fa fa-check"></i><b>5.4.2</b> Functional (BOLD) scans</a></li>
<li class="chapter" data-level="5.4.3" data-path="aomic.html"><a href="aomic.html#diffusion-weighted-scans"><i class="fa fa-check"></i><b>5.4.3</b> Diffusion-weighted scans</a></li>
<li class="chapter" data-level="5.4.4" data-path="aomic.html"><a href="aomic.html#physiological-data"><i class="fa fa-check"></i><b>5.4.4</b> Physiological data</a></li>
<li class="chapter" data-level="5.4.5" data-path="aomic.html"><a href="aomic.html#psychometric-data"><i class="fa fa-check"></i><b>5.4.5</b> Psychometric data</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="aomic.html"><a href="aomic.html#aomic-code-availability"><i class="fa fa-check"></i><b>5.5</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>6</b> Choosing to view morbid information involves reward circuitry</a><ul>
<li class="chapter" data-level="6.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-participants"><i class="fa fa-check"></i><b>6.2.1</b> Participants</a></li>
<li class="chapter" data-level="6.2.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-design"><i class="fa fa-check"></i><b>6.2.2</b> Design</a></li>
<li class="chapter" data-level="6.2.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-materials"><i class="fa fa-check"></i><b>6.2.3</b> Materials</a></li>
<li class="chapter" data-level="6.2.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-procedure"><i class="fa fa-check"></i><b>6.2.4</b> Procedure</a></li>
<li class="chapter" data-level="6.2.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-behavioral-analysis"><i class="fa fa-check"></i><b>6.2.5</b> Behavioral analysis</a></li>
<li class="chapter" data-level="6.2.6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-methods-imaging-details"><i class="fa fa-check"></i><b>6.2.6</b> Imaging details</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-data-availability"><i class="fa fa-check"></i><b>6.3</b> Data availability</a></li>
<li class="chapter" data-level="6.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results"><i class="fa fa-check"></i><b>6.4</b> Results</a><ul>
<li class="chapter" data-level="6.4.1" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-results-participants"><i class="fa fa-check"></i><b>6.4.1</b> Participants</a></li>
<li class="chapter" data-level="6.4.2" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#behavior-and-subjective-report"><i class="fa fa-check"></i><b>6.4.2</b> Behavior and subjective report</a></li>
<li class="chapter" data-level="6.4.3" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#roi-analyses"><i class="fa fa-check"></i><b>6.4.3</b> ROI analyses</a></li>
<li class="chapter" data-level="6.4.4" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#whole-brain-analyses"><i class="fa fa-check"></i><b>6.4.4</b> Whole-brain analyses</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html#morbid-curiosity-discussion"><i class="fa fa-check"></i><b>6.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html"><i class="fa fa-check"></i><b>7</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-methods"><i class="fa fa-check"></i><b>7.1</b> Methods</a><ul>
<li class="chapter" data-level="7.1.1" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hypothesis-kernel-analysis-1"><i class="fa fa-check"></i><b>7.1.1</b> Hypothesis kernel analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#ablation-and-follow-up-exploration-analyses"><i class="fa fa-check"></i><b>7.1.2</b> Ablation and follow-up exploration analyses</a></li>
<li class="chapter" data-level="7.1.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-noise-ceiling"><i class="fa fa-check"></i><b>7.1.3</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="7.1.4" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#evaluated-mappings"><i class="fa fa-check"></i><b>7.1.4</b> Evaluated mappings</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-results"><i class="fa fa-check"></i><b>7.2</b> Results</a></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-kernel-analysis.html"><a href="hypothesis-kernel-analysis.html#hka-discussion"><i class="fa fa-check"></i><b>7.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html"><i class="fa fa-check"></i><b>8</b> Affective face perception integrates both static and dynamic information</a><ul>
<li class="chapter" data-level="8.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-methods"><i class="fa fa-check"></i><b>8.2</b> Methods</a><ul>
<li class="chapter" data-level="8.2.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-participants"><i class="fa fa-check"></i><b>8.2.1</b> Participants</a></li>
<li class="chapter" data-level="8.2.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-experimental-design"><i class="fa fa-check"></i><b>8.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="8.2.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-procedure"><i class="fa fa-check"></i><b>8.2.3</b> Procedure</a></li>
<li class="chapter" data-level="8.2.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-data-preproc"><i class="fa fa-check"></i><b>8.2.4</b> Data preprocessing</a></li>
<li class="chapter" data-level="8.2.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-pred-analysis"><i class="fa fa-check"></i><b>8.2.5</b> Predictive analysis</a></li>
<li class="chapter" data-level="8.2.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#noise-ceiling-estimation"><i class="fa fa-check"></i><b>8.2.6</b> Noise ceiling estimation</a></li>
<li class="chapter" data-level="8.2.7" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-bayes"><i class="fa fa-check"></i><b>8.2.7</b> Bayesian reconstructions</a></li>
<li class="chapter" data-level="8.2.8" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-code"><i class="fa fa-check"></i><b>8.2.8</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-results"><i class="fa fa-check"></i><b>8.3</b> Results</a><ul>
<li class="chapter" data-level="8.3.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#encoding-model-performance"><i class="fa fa-check"></i><b>8.3.1</b> Encoding model performance</a></li>
<li class="chapter" data-level="8.3.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#reconstruction-model-visualizations"><i class="fa fa-check"></i><b>8.3.2</b> Reconstruction model visualizations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-discussion"><i class="fa fa-check"></i><b>8.4</b> Discussion</a><ul>
<li class="chapter" data-level="8.4.1" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#facial-morphology-independently-contributes-to-affective-face-perception"><i class="fa fa-check"></i><b>8.4.1</b> Facial morphology independently contributes to affective face perception</a></li>
<li class="chapter" data-level="8.4.2" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#the-influence-of-facial-morphology-does-not-result-from-visual-similarity-to-facial-movements"><i class="fa fa-check"></i><b>8.4.2</b> The influence of facial morphology does not result from visual similarity to facial movements</a></li>
<li class="chapter" data-level="8.4.3" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#categorical-representations-of-experienced-valence-and-arousal-correlate-with-representations-of-perceived-emotions"><i class="fa fa-check"></i><b>8.4.3</b> Categorical representations of experienced valence and arousal correlate with representations of perceived emotions</a></li>
<li class="chapter" data-level="8.4.4" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#predictive-models-quantify-what-is-not-yet-known"><i class="fa fa-check"></i><b>8.4.4</b> Predictive models quantify what is (not yet) known</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#svsd-conclusion"><i class="fa fa-check"></i><b>8.5</b> Conclusion</a></li>
<li class="chapter" data-level="8.6" data-path="static-vs-dynamic.html"><a href="static-vs-dynamic.html#acknowledgements"><i class="fa fa-check"></i><b>8.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i><b>9</b> Discussion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a><ul>
<li class="chapter" data-level="A.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#instructions"><i class="fa fa-check"></i><b>A.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 3</a><ul>
<li class="chapter" data-level="B.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-methods"><i class="fa fa-check"></i><b>B.1</b> Supplementary methods</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#functional-mri-simulation"><i class="fa fa-check"></i><b>B.1.1</b> Functional MRI simulation</a></li>
<li class="chapter" data-level="B.1.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data"><i class="fa fa-check"></i><b>B.1.2</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.1.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation"><i class="fa fa-check"></i><b>B.1.3</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.1.4" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs-nonlinear-confound-models-predicting-vbm-and-tbss-data-based-on-brain-size"><i class="fa fa-check"></i><b>B.1.4</b> Linear vs nonlinear confound models: predicting VBM and TBSS data based on brain size</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#supplementary-results"><i class="fa fa-check"></i><b>B.2</b> Supplementary results</a><ul>
<li class="chapter" data-level="B.2.1" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#testing-confound-regression-on-simulated-fmri-data-1"><i class="fa fa-check"></i><b>B.2.1</b> Testing confound regression on simulated fMRI data</a></li>
<li class="chapter" data-level="B.2.2" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#controlling-for-confounds-during-pattern-estimation-1"><i class="fa fa-check"></i><b>B.2.2</b> Controlling for confounds during pattern estimation</a></li>
<li class="chapter" data-level="B.2.3" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html#linear-vs.-nonlinear-confound-models-predicting-vbm-and-tbss-intensity-using-brain-size"><i class="fa fa-check"></i><b>B.2.3</b> Linear vs. nonlinear confound models: predicting VBM and TBSS intensity using brain size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="aomic-supplement.html"><a href="aomic-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="D" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="E" data-path="hypothesis-kernel-analysis-supplement.html"><a href="hypothesis-kernel-analysis-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="F" data-path="static-vs-dynamic-supplement.html"><a href="static-vs-dynamic-supplement.html"><i class="fa fa-check"></i><b>F</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="G" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>G</b> Data, code, software and educational materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Towards prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-kernel-analysis" class="section level1">
<h1><span class="header-section-number">7</span> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</h1>


<p><p><strong>Abstract</strong></p>

Bla bla
</p>
<div id="hka-methods" class="section level2">
<h2><span class="header-section-number">7.1</span> Methods</h2>
<div id="hypothesis-kernel-analysis-1" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Hypothesis kernel analysis</h3>
<p>To formalize AU-emotion mappings as predictive models, we propose a novel method which we call “hypothesis kernel analysis”. In the context of the current study, we use this method to reframe AU-emotion mappings as classification models that predict the probability of an emotion given a set of AUs <span class="citation">(analogous to how people attempt to infer the emotion from others’ facial emotion expressions; Jack &amp; Schyns, <a href="bibliography.html#ref-Jack2015-sh" role="doc-biblioref">2015</a>)</span>. In what follows, we conceptually explain how the method works. For a detailed and more mathematical description of the method, we refer the reader to the <a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-supplement">Supplementary Methods</a>.</p>
<p>The underlying idea of the hypothesis kernel analysis is to predict a categorical dependent variable (e.g., the perceived emotion) based on the similarity between an <em>observation</em> with a particular set of features (e.g., a face with a particular set of AUs; the independent variables) and statements of a <em>hypothesis</em> (e.g., “happiness is expressed by AUs 6 and 12”). This prediction can then be compared to real observations to evaluate the accuracy of the hypothesis. The three methodological challenges of this approach are how to measure the similarity between an observation and a hypothesis statement, how to derive a prediction based on this similarity, and how to compare the predictions to real data. Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a> outlines how we have solved these challenges in five steps, which we will describe in turn.</p>
<div class="figure"><span id="fig:fig-hka-2"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_2.png" alt="Schematic visualization of the proposed method using a set of hypothetical AU-emotion mappings (\(\mathbf{M}\)) and stimuli (\(\mathbf{S}\)) based on a small set of AUs (five in total). The variable \(P\) represents the number of variables (here: AUs), \(Q\) represents the number of classes (here: emotions), and \(N\) represents the number of trials (here: facial expression stimuli). Note that the AU space technically may contain any number of (\(P\)) dimensions, but is shown here in two dimensions for convenience."  />
<p class="caption">
Figure 7.1: Schematic visualization of the proposed method using a set of hypothetical AU-emotion mappings (<span class="math inline">\(\mathbf{M}\)</span>) and stimuli (<span class="math inline">\(\mathbf{S}\)</span>) based on a small set of AUs (five in total). The variable <span class="math inline">\(P\)</span> represents the number of variables (here: AUs), <span class="math inline">\(Q\)</span> represents the number of classes (here: emotions), and <span class="math inline">\(N\)</span> represents the number of trials (here: facial expression stimuli). Note that the AU space technically may contain any number of (<span class="math inline">\(P\)</span>) dimensions, but is shown here in two dimensions for convenience.
</p>
</div>

<p>To quantify the similarity between an observation and a hypothesis statement, we embed both in a multidimensional space that is spanned by a particular set of variables (e.g., different AUs). In this space, we start by representing each class of the dependent variable (corresponding to the statements of the hypothesis) as a separate point. In the current study, this amounts to embedding the different hypothesized AU configurations (e.g., “happiness = AU12 + 6”; <span class="math inline">\(\mathbf{M}\)</span> in Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a>) as points in “AU space”, separately for each categorical emotion (see step 1 in Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a>). The coordinates of each point are determined by the hypothesized “importance” of each independent variable for that given class of the target variable. For example, the coordinates of each point in AU space represents the hypothesized relative intensity of each AU for a given emotion. As the AU-emotion mappings evaluated in the current study only specify whether an AU is included or excluded within a particular emotional configuration, we specify the coordinates of their embedding to be binary (0: excluded, 1: included). A different interpretation of the class embeddings described here is that they represent the location of a typical facial expression for this emotion in “AU space” according to a particular hypothesis.</p>
<p>As a second step, we embed each data point in the same space as the hypotheses. This means, the data used for this purpose should contain the same variables as were used to embed the hypotheses. For example, in this study, we use emotion ratings (i.e., the target variable) in response to dynamic facial expression stimuli with random configurations of AUs (i.e., the independent variables; <span class="math inline">\(\mathbf{S}\)</span> in Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a>) to test the hypothesized AU-emotion mappings (see <a href="#hka-dataset">Dataset used to evaluate mappings</a>).</p>
<p>With the hypotheses and the data in the same space, the next step in our method is to compute, for each observation separately, the “similarity” between the data and each class of the target. For this purpose, we use <em>kernel functions</em> (step 3 in Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a>), a technique that quantifies the similarity of two vectors. Any kernel function that computes a measure of similarity can be used, but in our analyses we use the cosine similarity as it normalizes the similarity by the magnitude (specifically, the L2 norm) of the data and hypothesis embeddings (but see Supplementary Figure <a href="#fig:fig-hka-S3"><strong>??</strong></a> for a comparison of model performance across different similarity and distance metrics).</p>
<p>As a fourth step, we interpret the similarity between the data and a given class embedding as being proportional to the evidence for a given class. In other words, the more similar a data point is to the statement of a hypothesis the stronger the prediction for the associated class. To produce a probabilistic prediction of the classes given a particular observation and hypothesis, we normalize the similarity values to the 0-1 range using the <em>softmax</em> function (step 4 in Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a>).<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Finally, the accuracy of the model can be summarized by comparing its predictions to the actual values of the target variable in the dataset (see step 5 in Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-2">7.1</a>). In this study, this means that the predictions are compared to the actual emotion ratings from participants. Although any model performance metric can be used, we use the “Area under the Receiver operating curve” (AUROC) as our model performance metric, because it is insensitive to class imbalance, allows for class-specific scores, and can handle probabilistic predictions <span class="citation">(Dinga et al., <a href="bibliography.html#ref-Dinga2019-mh" role="doc-biblioref">2019</a>)</span>. We report class-specific scores, which means that each class of the categorical dependent variable (i.e., different emotions) gets a separate score with a chance level of 0.5 and a theoretical maximum score of 1.</p>
</div>
<div id="ablation-and-follow-up-exploration-analyses" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Ablation and follow-up exploration analyses</h3>
<p>To gain a better understanding of why some mappings perform better than others, we performed an “ablation analysis”, which entails removing (or “ablating”) AUs one by one from each configuration for each evaluated mapping and subsequently rerunning the kernel analysis to observe how this impacts model performance. If ablating a particular AU <em>decreases</em> model performance for a given emotion, it means that this AU is important for perceiving this emotion. If on the other hand ablating an AU <em>increases</em> performance for a given emotion, it could mean that the inclusion of this AU in a given mapping is incorrect.</p>
<p>Using the results from the ablation analyses, we explored strategies to enhance existing mappings. Specifically, we computed for each emotion which AUs, on average across mappings, led to a decrease in model performance after being ablated. We then ran follow-up analyses in which we enhanced existing mappings with the AUs that were found to decrease model performance in the ablation analyses and reran the predictive analysis. This was done for each AU and emotion separately. For example, if ablation of AU4 was found to decrease model performance for “fear” on average across mappings that included AU4 (i.e., all but Cordaro et al., 2018; ICP), we would rerun the predictive analysis for the mapping(s) that did not contain AU4 (i.e., Cordaro et al., 2018; ICP) using an “enhanced” mapping with AU4 appended.</p>
<p>Finally, we constructed “optimized” models by, for each mapping separately, adding all AUs that led to a <em>decrease</em> in model performance after ablation and removing all AUs that led to an <em>increase</em> in model performance after ablation. Then, the predictive analysis was rerun and the “optimized” model performance was compared to the original model performance.</p>
</div>
<div id="hka-noise-ceiling" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Noise ceiling estimation</h3>
<p>Instead of interpreting model performance relative to the theoretical optimum performance, we propose to interpret model performance relative to the <em>noise ceiling</em>, an estimate of the in principle explainable portion of the target variable. The noise ceiling is a concept often used in systems neuroscience to correct model performance for noise in the measured brain data <span class="citation">(Hsu et al., <a href="bibliography.html#ref-Hsu2004-hs" role="doc-biblioref">2004</a>; Huth et al., <a href="bibliography.html#ref-Huth2012-yc" role="doc-biblioref">2012</a>; Kay et al., <a href="bibliography.html#ref-Kay2013-ch" role="doc-biblioref">2013</a>)</span>. Traditionally, noise ceilings in neuroscience are applied in the context of within-subject regression models <span class="citation">(Lage-Castellanos et al., <a href="bibliography.html#ref-Lage-Castellanos2019-dm" role="doc-biblioref">2019</a>)</span>. Here, we develop a method to derive noise ceilings for classification models, i.e., models with a categorical target variable (such as categorical emotion ratings) that are applicable to both within-subject and between-subject models <span class="citation">(see also Hebart et al., <a href="bibliography.html#ref-Hebart2020-wp" role="doc-biblioref">2020</a>)</span>. In this section, we explain our derivation of noise ceilings for classification models conceptually; the <a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-supplement">Supplementary Methods</a> outline a more detailed and formal description.</p>
<p>Noise ceiling estimation is a method that adjusts the theoretical maximum performance of a predictive model for the presence of irreducible noise in the data. As such, like the theoretical maximum, the noise ceiling imposes an upper bound on model performance. Another way to think about noise ceilings is that they split the variance of the data into three portions: the explained variance, the unexplained variance, and the “irreducible” noise (see Figure <a href="hypothesis-kernel-analysis.html#fig:fig-hka-3">7.2</a>). “Irreducible” is put in quotes because this proportion of noise can, in fact, be explained in principle as will be discussed in the <a href="#kha-discussion">Discussion</a> (see also the <a href="hypothesis-kernel-analysis-supplement.html#hypothesis-kernel-analysis-supplement">Supplementary Methods</a>. Importantly, the noise ceiling thus indicates how much improvement in terms of model performance can be gained for a given dataset (i.e., unexplained variance) and how much cannot be explained by the model (i.e., the “irreducible” noise).</p>
<div class="figure"><span id="fig:fig-hka-3"></span>
<img src="_bookdown_files/hypothesis-kernel-analysis-files/figures/figure_3.png" alt="The noise ceiling partitions the variance into explained variance, unexplained variance, and “irreducible” noise for any given model (\(\mathbf{M}\)). Here, AUROC is used as the metric of model performance, but the noise ceiling can be estimated using any metric."  />
<p class="caption">
Figure 7.2: The noise ceiling partitions the variance into <em>explained variance</em>, <em>unexplained variance</em>, and <em>“irreducible” noise</em> for any given model (<span class="math inline">\(\mathbf{M}\)</span>). Here, AUROC is used as the metric of model performance, but the noise ceiling can be estimated using any metric.
</p>
</div>

<p>In the context of the current study, we use the variance (or “inconsistency”) in emotion ratings across participants in response to the same set of facial expression stimuli to estimate a noise ceiling for the different AU-based models. The noise ceiling gives us insight into whether the evaluated set of AU-based models are sufficiently accurate to explain variance that can in principle be explained by AUs or whether we may need differently parameterized AU-based models. This way, the importance and limitations of AUs can be estimated empirically.</p>
</div>
<div id="evaluated-mappings" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Evaluated mappings</h3>
<p>Many different AU-emotion mappings have been put forward, but in this study we assess those summarized in <span class="citation">Barrett et al. (<a href="bibliography.html#ref-Barrett2019-bc" role="doc-biblioref">2019</a>)</span> (Table 1). Additionally, we included the AU-emotion mappings from the “emotional FACS” (EMFACS) manual <span class="citation">(Friesen &amp; Ekman, <a href="bibliography.html#ref-Friesen1983-ft" role="doc-biblioref">1983</a>)</span>. So, in total, we evaluated six hypothesized AU-emotion mappings, which are summarized in Table <a href="hypothesis-kernel-analysis.html#tab:tab-hka-1">7.1</a> (and an additional data-driven AU-emotion mapping, see below).</p>

<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:tab-hka-1">Table 7.1: </span>Evaluated AU-emotion mappings in our study
</caption>
<thead>
<tr>
<th style="text-align:left;">
Emotion category
</th>
<th style="text-align:left;">
Darwin (1872)
</th>
<th style="text-align:left;">
EMFACS
</th>
<th style="text-align:left;">
Matsumoto et al. (2008)
</th>
<th style="text-align:left;">
Cordaro et al. (2018) - ref.
</th>
<th style="text-align:left;">
Cordaro et al. (2018) - ICP
</th>
<th style="text-align:left;">
Keltner et al. (2019)
</th>
<th style="text-align:left;">
Jack &amp;amp; Schyns
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Anger
</td>
<td style="text-align:left;">
4 + 5 + 24 + 38
</td>
<td style="text-align:left;">
<span class="math inline">\(\\cdot\)</span> Bla
</td>
<td style="text-align:left;">
4 + (5 <span class="math inline">\(\\lor\)</span> 7) + 22 + 23 + 34
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
<span class="math inline">\(\\cdot\)</span> Bla
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>

</div>
</div>
<div id="hka-results" class="section level2">
<h2><span class="header-section-number">7.2</span> Results</h2>
</div>
<div id="hka-discussion" class="section level2">
<h2><span class="header-section-number">7.3</span> Discussion</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Readers familiar with machine learning algorithms may recognize this as a specific implementation of a K-nearest neighbor classification model with <em>K</em> = 1, which is fit on the embedded hypotheses (<span class="math inline">\(\mathbf{M}\)</span>) and cross-validated on the data (<span class="math inline">\(\mathbf{S}\)</span>).<a href="hypothesis-kernel-analysis.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="morbid-curiosity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="static-vs-dynamic.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
