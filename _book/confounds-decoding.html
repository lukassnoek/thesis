<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain</title>
  <meta name="description" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shared-states.html"/>
<link rel="next" href="AOMIC.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-brain-is-not-a-dictionary"><i class="fa fa-check"></i><b>1.1</b> The brain is not a dictionary</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-brain-probably-does-not-care-about-your-hypothesis"><i class="fa fa-check"></i><b>1.2</b> The brain (probably) does not care about your hypothesis</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretability-and-prediction-are-a-trade-off-for-now"><i class="fa fa-check"></i><b>1.3</b> Interpretability and prediction are a trade-off (for now)</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#exploration-should-be-embraced-more"><i class="fa fa-check"></i><b>1.4</b> Exploration should be embraced more</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#proper-generalization-is-hard"><i class="fa fa-check"></i><b>1.5</b> Proper generalization is hard</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#psychology-is-complex-so-it-needs-complex-models"><i class="fa fa-check"></i><b>1.6</b> Psychology is complex, so it needs complex models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.3</b> Model optimization procedure</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.3.1</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.3.2</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.3.3</b> Additional analyses</a></li>
<li class="chapter" data-level="2.3.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.3.4</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.3.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.3.5</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.4</b> Results</a><ul>
<li class="chapter" data-level="2.4.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.4.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.4.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.4.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-acknowledgements"><i class="fa fa-check"></i><b>2.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusions"><i class="fa fa-check"></i><b>3.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="AOMIC.html"><a href="AOMIC.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a></li>
<li class="chapter" data-level="6" data-path="au-limitations.html"><a href="au-limitations.html"><i class="fa fa-check"></i><b>6</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a></li>
<li class="chapter" data-level="7" data-path="facial-expression-models.html"><a href="facial-expression-models.html"><i class="fa fa-check"></i><b>7</b> Comparing models of dynamic facial expression perception</a></li>
<li class="chapter" data-level="8" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html"><i class="fa fa-check"></i><b>8</b> Summary and general discussion</a><ul>
<li class="chapter" data-level="8.1" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#explore"><i class="fa fa-check"></i><b>8.1</b> Explore!</a></li>
<li class="chapter" data-level="8.2" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#think-big"><i class="fa fa-check"></i><b>8.2</b> Think <em>big</em></a></li>
<li class="chapter" data-level="8.3" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#rethink-psychology-education"><i class="fa fa-check"></i><b>8.3</b> Rethink psychology education</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a><ul>
<li class="chapter" data-level="A.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#stimuli-used-for-sf-task"><i class="fa fa-check"></i><b>A.1</b> Stimuli used for SF-task</a></li>
<li class="chapter" data-level="A.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#instructions"><i class="fa fa-check"></i><b>A.2</b> Instructions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-other-focused-emotion-understanding-task"><i class="fa fa-check"></i><b>A.2.1</b> Full instruction for the other-focused emotion understanding task</a></li>
<li class="chapter" data-level="A.2.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-self-focused-emotion-imagery-task"><i class="fa fa-check"></i><b>A.2.2</b> Full instruction for the self-focused emotion imagery task</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#behavioral-results"><i class="fa fa-check"></i><b>A.3</b> Behavioral results</a></li>
<li class="chapter" data-level="A.4" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#optimization-results"><i class="fa fa-check"></i><b>A.4</b> Optimization results</a></li>
<li class="chapter" data-level="A.5" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#bagging-procedure"><i class="fa fa-check"></i><b>A.5</b> Bagging procedure</a></li>
<li class="chapter" data-level="A.6" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#precision-vs.-recall"><i class="fa fa-check"></i><b>A.6</b> Precision vs. recall</a></li>
<li class="chapter" data-level="A.7" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#self-vs.-other-classification"><i class="fa fa-check"></i><b>A.7</b> Self vs. other classification</a></li>
<li class="chapter" data-level="A.8" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#condition-average-results"><i class="fa fa-check"></i><b>A.8</b> Condition-average results</a></li>
<li class="chapter" data-level="A.9" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#individual-subject-scores"><i class="fa fa-check"></i><b>A.9</b> Individual subject scores</a></li>
<li class="chapter" data-level="A.10" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#brain-region-importance"><i class="fa fa-check"></i><b>A.10</b> Brain region importance</a></li>
<li class="chapter" data-level="A.11" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#general-note-about-tables-with-voxel-coordinates"><i class="fa fa-check"></i><b>A.11</b> General note about tables with voxel-coordinates</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="C" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="D" data-path="au-limitations-supplement.html"><a href="au-limitations-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="E" data-path="facial-expression-models-supplement.html"><a href="facial-expression-models-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="F" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>F</b> Data, code and materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning from the brain</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confounds-decoding" class="section level1">
<h1><span class="header-section-number">3</span> How to control for confounds in decoding analyses of neuroimaging data</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Snoek, L.*, Miletić, S.*, &amp; Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. <em>NeuroImage</em>, 184, 741-760.</p>
<p>* Shared first authorship</p>

<p><strong>Abstract</strong></p>
<p>Over the past decade, multivariate “decoding analyses” have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound “brain size”. We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one’s decoding analysis.</p>
<div id="confounds-decoding-introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data <span class="citation">(Haxby, <a href="bibliography.html#ref-Haxby2012-sd" role="doc-biblioref">2012</a>; Norman et al., <a href="bibliography.html#ref-Norman2006-bt" role="doc-biblioref">2006</a><a href="bibliography.html#ref-Norman2006-bt" role="doc-biblioref">b</a>)</span>. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is “decoding”, in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns <span class="citation">(Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span> and working memory representations from EEG data <span class="citation">(LaRocque et al., <a href="bibliography.html#ref-LaRocque2013-sh" role="doc-biblioref">2013</a>)</span>, as well between-subject factors such as Alzheimer’s disease (vs. healthy controls) from structural MRI data <span class="citation">(Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span> and major depressive disorder (vs. healthy controls) from resting-state functional connectivity <span class="citation">(Craddock et al., <a href="bibliography.html#ref-Craddock2009-kz" role="doc-biblioref">2009</a>)</span>. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do <span class="citation">(Jimura &amp; Poldrack, <a href="bibliography.html#ref-Jimura2012-lv" role="doc-biblioref">2012</a>)</span>. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>.</p>
<p>In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical <span class="citation">(Allefeld et al., <a href="bibliography.html#ref-Allefeld2016-xp" role="doc-biblioref">2016</a>; Davis et al., <a href="bibliography.html#ref-Davis2014-lw" role="doc-biblioref">2014</a>; Gilron et al., <a href="bibliography.html#ref-Gilron2017-tl" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>)</span> and more conceptual <span class="citation">(Naselaris &amp; Kay, <a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span> in nature. For the purposes of the current study, we focus on the specific criticism put forward by <span class="citation">Naselaris &amp; Kay (<a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>)</span> , who argue that decoding analyses are inherently ambiguous in terms of what information they use <span class="citation">(see Popov et al., <a href="bibliography.html#ref-popov2018practices" role="doc-biblioref">2018</a> for a similar argument in the context of encoding analyses)</span>. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information <span class="citation">(see also Carlson &amp; Wardle, <a href="bibliography.html#ref-Carlson2015-bz" role="doc-biblioref">2015</a>; Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span>. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of “source ambiguity”.</p>
<p>To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> (male/female) from structural MRI with the aim of contributing to the understanding of gender differences — an endeavor that generated considerable interest and controversy <span class="citation">(Chekroud et al., <a href="bibliography.html#ref-Chekroud2016-tc" role="doc-biblioref">2016</a>; Del Giudice et al., <a href="bibliography.html#ref-Del_Giudice2016-ns" role="doc-biblioref">2016</a>; Glezerman, <a href="bibliography.html#ref-Glezerman2016-xl" role="doc-biblioref">2016</a>; Joel &amp; Fausto-Sterling, <a href="bibliography.html#ref-Joel2016-uo" role="doc-biblioref">2016</a>; Rosenblatt, <a href="bibliography.html#ref-Rosenblatt2016-oy" role="doc-biblioref">2016</a>)</span>. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant’s gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways <span class="citation">(Good et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a>; O’Brien et al., <a href="bibliography.html#ref-OBrien2011-lj" role="doc-biblioref">2011</a>)</span>. First, there is a <em>global</em> difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight <span class="citation">(18.7%; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Lüders et al., <a href="bibliography.html#ref-Luders2002-ms" role="doc-biblioref">2002</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Second, brains of men and women are known to differ <em>locally</em>: some specific brain areas are on average larger in women than in men <span class="citation">(e.g., in superior and middle temporal cortex; Good et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a>)</span> and vice versa <span class="citation">(e.g., in frontomedial cortex; Goldstein et al., <a href="bibliography.html#ref-Goldstein2001-dy" role="doc-biblioref">2001</a>)</span>. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male <em>bodies</em> are on average larger than female bodies <span class="citation">(Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. As such, our hypothetical researcher is likely primarily interested in the <em>local</em> sources of variation in the neuroanatomy of male and female brains.</p>
<p>Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for “uninteresting” sources of information.</p>
<div id="confounds-decoding-introduction-true-vs-confounded" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</h3>
<p>Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate <em>prediction</em>, but not in <em>interpretability</em> of the model <span class="citation">(Bzdok, <a href="bibliography.html#ref-Bzdok2017-li" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is <em>only</em> using information in the neuroimaging data that is related to the variable that is being decoded <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. In this scenario, source ambiguity (i.e., the presence of <em>multiple</em> sources of information) <em>is</em> problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is <em>confounding</em> the decoding analysis of gender. Here, we define a confound as <em>a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.</em></p>
<p>To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable “schizophrenia-or-not” is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender <span class="citation">(i.e., men are more often diagnosed with schizophrenia than women; McGrath et al., <a href="bibliography.html#ref-McGrath2008-oj" role="doc-biblioref">2008</a>)</span> and substance abuse <span class="citation">(Dixon, <a href="bibliography.html#ref-Dixon1999-kl" role="doc-biblioref">1999</a>)</span>, which are also known to affect gray matter <span class="citation">(Bangalore et al., <a href="bibliography.html#ref-Bangalore2008-kc" role="doc-biblioref">2008</a>; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Van Haren et al., <a href="bibliography.html#ref-Van_Haren2013-iv" role="doc-biblioref">2013</a>)</span>. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate <em>prediction</em> of schizophrenia, then this ambiguity due to confounding sources of information is problematic.</p>
<p>Importantly, as our definition suggests, what <em>is</em> or <em>is not</em> regarded as a confound is relative — it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable “schizophrenia-or-no”” becomes the confounding variable. In other words, one researcher’s signal is another researcher’s confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the “true signal” (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the “confounded signal” (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-1">3.1</a>). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-1"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_1.png" alt="Visualization of how variance in brain data (\(X\)) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables."  />
<p class="caption">
Figure 3.1: Visualization of how variance in brain data (<span class="math inline">\(X\)</span>) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (<span class="math inline">\(X\)</span>), the confound (<span class="math inline">\(C\)</span>), and the target (<span class="math inline">\(y\)</span>). Overlapping circles indicate a non-zero (squared) correlation between the two variables.
</p>
</div>

<p>In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.</p>
</div>
<div id="confounds-decoding-introduction-methods" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Methods for confound control</h3>
<p>In decoding analyses, one aims to predict a certain target variable from patterns of neuroimaging data. Various methods discussed in this section are supplemented with a mathematical formalization; for consistency and readability, we define the notation we will use in Table <a href="confounds-decoding.html#tab:tab-confounds-decoding-1">3.1</a>.</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-confounds-decoding-1">Table 3.1: </span>Notation.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Symbol
</th>
<th style="text-align:left;">
Dims.
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of samples (usually subjects or trials)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of neuroimaging features (e.g., voxels or sensors)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(P\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of confound variables (e.g., age, reaction time, or brain size)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(X_{ij}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times K\)</span>
</td>
<td style="text-align:left;">
The neuroimaging patterns (often called the “data” in the current article), where the subescript <span class="math inline">\(i \in {1 \dots N}\)</span> refers to the individual samples (rows), and the subscript <span class="math inline">\(j \in {1 \dots K}\)</span> to individual features (columns)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(y\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times 1\)</span>
</td>
<td style="text-align:left;">
The target variable (i.e., what is to be decoded)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(C\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times P\)</span>
</td>
<td style="text-align:left;">
The confound variable(s)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(\hat{\beta}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K + 1\)</span>
</td>
<td style="text-align:left;">
The parameters estimated in a general linear model (GLM)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(w\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K + 1\)</span>
</td>
<td style="text-align:left;">
The parameters estimated in a decoding model
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(r_{Cy}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Sample Pearson correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(r_{y(X.C)}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Sample semipartial Pearson correlation coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>, controlled for <span class="math inline">\(C\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(p(r_{Cy})\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
<span class="math inline">\(p\)</span>-value of sample Pearson correlation between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span>
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Format based on Diedrichsen and Kriegeskorte (2017). For the correlations (<span class="math inline">\(r\)</span>), we assume that <span class="math inline">\(P = 1\)</span> and thus that the correlations in the table reduce to a scalar.
</td>
</tr>
</tfoot>
</table></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<div id="confounds-decoding-introduction-methods-apriori-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.1.2.1</span> A priori counterbalancing</h4>
<p>Ideally, one would prevent confounding variables from influencing the results as much as possible before the acquisition of the neuroimaging data.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> One common way do this (in both traditional “activation-based” and decoding analyses) is to make sure that potential confounding variables are <em>counterbalanced</em> in the experimental design <span class="citation">(Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. In experimental research, this would entail randomly assigning subjects to design cells (e.g., treatment groups) such that there is no structural correlation between characteristics of the subjects and design cells. In observational designs (e.g., in the gender/brain size example described earlier), it means that the sample is chosen such that there is no correlation between the confound (brain size) and <em>observed</em> target variable (gender). That is, given that men on average have larger brains than women, this would entail including only men with relatively small brains and women with relatively large brains.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> The distinction between experimental and observational studies is important because the former allow the researcher to randomly draw samples from the population, while the latter require the researcher to choose a sample that is not representative of the population, which limits the conclusions that can be drawn about the population (we will revisit this issue in the <a href="confounds-decoding.html#confounds-decoding-discussion">Discussion</a> section).</p>
<p>Formally, in decoding analyses, a design is counterbalanced when the confound <span class="math inline">\(C\)</span> and the target <span class="math inline">\(y\)</span> are statistically independent. In practice, this often means that the sample is chosen so that there is no significant correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> (although this does not necessarily imply that <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> are actually independent). To illustrate the process of counterbalancing, let’s consider another hypothetical experiment: suppose one wants to set up an fMRI experiment in which the goal is to decode abstract object category (e.g., faces vs. houses) from the corresponding fMRI patterns <span class="citation">(cf. Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span>, while controlling for the potential confounding influence of low-level or mid-level stimulus features, such as luminance, spatial frequency, or texture <span class="citation">(Long et al., <a href="bibliography.html#ref-Long2017-fb" role="doc-biblioref">2017</a>)</span>. Proper counterbalancing would entail making sure that the images used for this particular experiments have similar values for these low-level and mid-level features across object categories <span class="citation">(see for details Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. Thus, in this example, low-level and mid-level stimulus features should be counterbalanced with respect to object category, such that above chance decoding of object category cannot be attributed to differences in low-level or mid-level stimulus features (i.e., the confounds).</p>
<p>A priori counterbalancing of potential confounds is, however, not always feasible. For one, the exact measurement of a potentially confounding variable may be impossible until data acquisition. For example, the brain size of a participant is only known after data collection. Similarly, <span class="citation">Todd et al. (<a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> found that their decoding analysis of rule representations was confounded by response times of to the to-be-decoded trials. Another example of a “data-driven” confound is participant motion during data acquisition <span class="citation">(important in, for example, decoding analyses applied to data from clinical populations such as ADHD; Yu-Feng et al., <a href="bibliography.html#ref-Yu-Feng2007-sg" role="doc-biblioref">2007</a>)</span>. In addition, a priori counterbalancing of confounds may be challenging because of the limited size of populations of interest. Especially in clinical research settings, researchers may not have the luxury of selecting a counterbalanced sample due to the small number of patient subjects available for testing. Lastly, researchers may simply discover confounds after data acquisition.</p>
<p>Given that a priori counterbalancing is not possible or undesirable in many situations, it is paramount to explore the possibilities of controlling for confounding variables after data acquisition for the sake of model interpretability, which we discuss next.</p>
</div>
<div id="confounds-decoding-introduction-methods-include-in-data" class="section level4">
<h4><span class="header-section-number">3.1.2.2</span> Include confounds in the data</h4>
<p>One perhaps intuitive method to control for confounds in decoding analyses is to include the confound(s) in the data <span class="citation">(i.e., the neuroimaging data, <span class="math inline">\(X\)</span>; see, e.g., Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span> used by decoding model. That is, when applying a decoding analysis to neuroimaging data, the confound is added to the data as if it were another voxel (or sensor, in electrophysiology). This intuition may stem from the analogous situation in univariate (activation-based) analyses of neuroimaging data, in which confounding variables are controlled for by including them in the design matrix together with the stimulus/task regressors. For example, in univariate analyses of functional MRI, movement of the participant is often controlled for by including motion estimates in the design matrix of first-level analyses <span class="citation">(Johnstone et al., <a href="bibliography.html#ref-Johnstone2006-tn" role="doc-biblioref">2006</a>)</span>; in EEG, some control for activity due to eye-movements by including activity measured by concurrent electro-oculography as covariates in the design-matrix <span class="citation">(Parra et al., <a href="bibliography.html#ref-Parra2005-um" role="doc-biblioref">2005</a>)</span>. Usually, the general linear model is then used to estimate each predictor’s influence on the neuroimaging data. Importantly, the parameter estimates (<span class="math inline">\(\hat{\beta}\)</span>) are often interpreted as reflecting the unique contribution<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> of each predictor variable, independent from the influence of the confound.</p>
<p>Contrary to general linear models as employed in univariate (activation-based) analyses, including confound variables in the data as predictors for <em>decoding</em> models is arguably problematic. If a confound is included in the data in the context of decoding models, the parameter estimates of the features (often called “feature weights”, <span class="math inline">\(w\)</span>, in decoding models) may be corrected for the influence of the confound, but the <em>model performance</em> <span class="citation">(usually measured as explained variance, <span class="math inline">\(R^2\)</span>, or classification accuracy; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span> is not. That is, rather than providing an estimate of decoding performance “controlled for” a confound, one obtains a measure of performance when explicitly <em>including</em> the confound as an interesting source of variance that the decoder is allowed to use. This is problematic because research using decoding analyses generally does not focus on parameter estimates but on statistics of model performance. Model performance statistics (e.g., <span class="math inline">\(R^2\)</span>, classification accuracy) alone cannot disentangle the contribution of different sources of information as they only represent a single summary statistic of model fit <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. One might, then, argue that additionally inspecting feature weights of decoding models may help in disambiguating different sources of information <span class="citation">(Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. However, it has been shown that feature weights cannot be reliably mapped to specific sources of information, i.e., as being task-related or confound-related <span class="citation">(e.g., features with large weights may be completely uncorrelated with the target variable; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. As such, it does not make sense to include confounds in the set of predictors when the goal is to disambiguate the different sources of information in decoding analyses.</p>
<p>Recently, another approach similar to including confounds in the data has been proposed, which is based on the idea of a dose-response curve <span class="citation">(Alizadeh et al., <a href="bibliography.html#ref-alizadeh2017decoding" role="doc-biblioref">2017</a>)</span>. In this method, instead of adding the confound(s) to the model directly, the relative contribution of true and confounded signal is systematically controlled. The authors show that this approach is able to directly quantify the unique contribution of each source of information, thus effectively controlling for confounded signal. However, while sophisticated in its approach, this method only seems to work for categorical confounds, as it is difficult (if not impossible) to systematically vary the proportion of confound-related information when dealing with continuous confounds or when dealing with more than one confound.</p>
</div>
<div id="confounds-decoding-introduction-methods-pattern-estimation" class="section level4">
<h4><span class="header-section-number">3.1.2.3</span> Control for confounds during pattern estimation</h4>
<p>Another method that was used in some decoding studies on functional MRI data aims to control for confounds in the initial procedure of estimating activity patterns of the to-be-decoded events, by leveraging the ability of the GLM to yield parameter estimates reflecting unique variance <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>. In this method, an initial “first-level” (univariate) analysis models the fMRI time series (<span class="math inline">\(s\)</span>) as a function of both predictors-of-interest (<span class="math inline">\(X\)</span>) and the confounds (<span class="math inline">\(C\)</span>), often using the GLM<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>:</p>
<p><span class="math display">\[\begin{equation}
s = X\beta_{x} + C\beta_{c} + \epsilon
\end{equation}\]</span></p>
<p>Then, only the estimated parameters (<span class="math inline">\(\hat{\beta}\)</span>, or normalized parameters, such as <em>t</em>-values or <em>z</em>-values) corresponding to the predictors-of-interest (<span class="math inline">\(\hat{\beta}_{x}\)</span>) are used as activity estimates (i.e., the used for predicting the target <span class="math inline">\(y\)</span>) in the subsequent decoding analyses. This method thus takes advantage of the shared variance partitioning in the pattern estimation step to control for potential confounding variables. However, while elegant in principle, this method is not applicable in between-subject decoding studies <span class="citation">(e.g., clinical decoding studies; Waarde et al., <a href="bibliography.html#ref-Van_Waarde2014-sh" role="doc-biblioref">2014</a>; Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span>, in which confounding variables are defined across subjects, or in electrophysiology studies, in which activity patterns do not have to be<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> estimated in a first-level model, thus limiting the applicability of this method.</p>
</div>
<div id="confounds-decoding-introduction-methods-posthoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.1.2.4</span> Post hoc counterbalancing of confounds</h4>
<p>When a priori counterbalancing is not possible, some have argued that post hoc counterbalancing might control for the influence of confounds <span class="citation">(Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>, pp. 24, 38)</span>. In this method, given that there is some sample correlation between the target and confound (<span class="math inline">\(r_{Cy} \neq 0\)</span>) in the entire dataset, one takes a subset of samples in which there is no empirical relation between the confound and the target (e.g., when <span class="math inline">\(r_{Cy} \approx 0\)</span>). In other words, post hoc counterbalancing is a way to <em>decorrelate</em> the confound and the target by subsampling the data. Then, subsequent decoding analysis on the subsampled data can only capitalize on true signal, as there is no confounded signal anymore (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-2">3.2</a>). While intuitive in principle, we are not aware of whether this method has been evaluated before and whether it yields unbiased performance estimates.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-2"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_2.png" alt="A schematic visualization how the main two confound control methods evaluated in this article deal with the “confounded signal”, making sure decoding models only capitalize on the “true signal”."  />
<p class="caption">
Figure 3.2: A schematic visualization how the main two confound control methods evaluated in this article deal with the “confounded signal”, making sure decoding models only capitalize on the “true signal”.
</p>
</div>

</div>
<div id="confounds-decoding-introduction-methods-confound-regression" class="section level4">
<h4><span class="header-section-number">3.1.2.5</span> Confound regression</h4>
</div>
</div>
<div id="confounds-decoding-introduction-current-study" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Current study</h3>
</div>
</div>
<div id="confounds-decoding-methods" class="section level2">
<h2><span class="header-section-number">3.2</span> Methods</h2>
<div id="confounds-decoding-methods-data" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Data</h3>
<div id="confounds-decoding-methods-data-vbm" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> VBM acquisition &amp; analysis</h4>
</div>
<div id="confounds-decoding-methods-data-tbss" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> TBSS acquisition &amp; analysis</h4>
</div>
<div id="confounds-decoding-methods-data-brainsize" class="section level4">
<h4><span class="header-section-number">3.2.1.3</span> Brain size estimation</h4>
</div>
<div id="confounds-decoding-methods-data-data-and-code" class="section level4">
<h4><span class="header-section-number">3.2.1.4</span> Data and code availability</h4>
</div>
</div>
<div id="confounds-decoding-methods-pipeline" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Decoding pipeline</h3>
</div>
<div id="confounds-decoding-methods-evaluated-methods" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Evaluated methods for confound control</h3>
<div id="confounds-decoding-methods-evaluated-methods-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Post hoc counterbalancing</h4>
</div>
<div id="confound-regression" class="section level4">
<h4><span class="header-section-number">3.2.3.2</span> Confound regression</h4>
</div>
<div id="control-for-confounds-during-pattern-estimation" class="section level4">
<h4><span class="header-section-number">3.2.3.3</span> Control for confounds during pattern estimation</h4>
</div>
</div>
<div id="analyses-of-simulated-data" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Analyses of simulated data</h3>
<div id="efficacy-analyses" class="section level4">
<h4><span class="header-section-number">3.2.4.1</span> Efficacy analyses</h4>
</div>
<div id="analysis-of-positive-bias-after-post-hoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.2.4.2</span> Analysis of positive bias after post hoc counterbalancing</h4>
</div>
<div id="analysis-of-negative-bias-after-wdcr" class="section level4">
<h4><span class="header-section-number">3.2.4.3</span> Analysis of negative bias after WDCR</h4>
</div>
</div>
</div>
<div id="results" class="section level2">
<h2><span class="header-section-number">3.3</span> Results</h2>
<div id="influence-of-brain-size" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Influence of brain size</h3>
</div>
<div id="baseline-model-no-confound-control" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Baseline model: no confound control</h3>
</div>
<div id="post-hoc-counterbalancing" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Post hoc counterbalancing</h3>
<div id="empirical-results" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Empirical results</h4>
</div>
<div id="efficacy-analysis" class="section level4">
<h4><span class="header-section-number">3.3.3.2</span> Efficacy analysis</h4>
</div>
<div id="analysis-of-positive-bias-after-post-hoc-counterbalancing-1" class="section level4">
<h4><span class="header-section-number">3.3.3.3</span> Analysis of positive bias after post hoc counterbalancing</h4>
</div>
</div>
<div id="whole-dataset-confound-regression-wdcr" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Whole-dataset confound regression (WDCR)</h3>
<div id="empirical-results-1" class="section level4">
<h4><span class="header-section-number">3.3.4.1</span> Empirical results</h4>
</div>
<div id="efficacy-analysis-1" class="section level4">
<h4><span class="header-section-number">3.3.4.2</span> Efficacy analysis</h4>
</div>
<div id="analysis-of-negative-bias-after-wdcr-1" class="section level4">
<h4><span class="header-section-number">3.3.4.3</span> Analysis of negative bias after WDCR</h4>
</div>
</div>
<div id="cross-validated-confound-regression-cvcr" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Cross-validated confound regression (CVCR)</h3>
<div id="empirical-results-2" class="section level4">
<h4><span class="header-section-number">3.3.5.1</span> Empirical results</h4>
</div>
<div id="efficacy-analysis-2" class="section level4">
<h4><span class="header-section-number">3.3.5.2</span> Efficacy analysis</h4>
</div>
</div>
<div id="summary-methods-for-confound-control" class="section level3">
<h3><span class="header-section-number">3.3.6</span> Summary methods for confound control</h3>
</div>
</div>
<div id="confounds-decoding-discussion" class="section level2">
<h2><span class="header-section-number">3.4</span> Discussion</h2>
<div id="relevance-and-consequences-for-previous-and-future-research" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Relevance and consequences for previous and future research</h3>
<div id="a-priori-and-post-hoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> A priori and post hoc counterbalancing</h4>
</div>
<div id="confound-regression-1" class="section level4">
<h4><span class="header-section-number">3.4.1.2</span> Confound regression</h4>
</div>
<div id="relevance-to-other-analysis-methods" class="section level4">
<h4><span class="header-section-number">3.4.1.3</span> Relevance to other analysis methods</h4>
</div>
<div id="importance-for-gender-decoding-studies" class="section level4">
<h4><span class="header-section-number">3.4.1.4</span> Importance for gender decoding studies</h4>
</div>
</div>
<div id="choosing-a-confound-model-linear-vs.-nonlinear-models" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Choosing a confound model: linear vs. nonlinear models</h3>
</div>
<div id="practical-recommendations" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Practical recommendations</h3>
</div>
</div>
<div id="conclusions" class="section level2">
<h2><span class="header-section-number">3.5</span> Conclusions</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The terms “gender” and “sex” are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.<a href="confounds-decoding.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration “squeezes” relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa <span class="citation">(Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span>. This effect of global brain size similarly affects functional MRI analyses <span class="citation">(Brodtmann et al., <a href="bibliography.html#ref-brodtmann2009regional" role="doc-biblioref">2009</a>)</span>.<a href="confounds-decoding.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant’s gender.<a href="confounds-decoding.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In the context of behavioral data, a priori counterbalancing is often called “matching” or a employing a “case-control design” @[Cook2002-hb].<a href="confounds-decoding.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Note that the counterbalancing process is the same for both traditional univariate (activation-based) studies and decoding studies, but the direction of analysis is reversed in univariate (e.g., gender → brain) and decoding studies (e.g., brain → gender). As such, in univariate studies the confound (e.g., brain size) is counterbalanced with respect to the predictor(s) (e.g., gender) while in decoding studies the confound (e.g., brain size) is counterbalanced with respect to the target (e.g., gender).<a href="confounds-decoding.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>However, parameter estimates only reflect unique variance when ordinary, weighted, or generalized least squares is used to find the model parameters. Other (regularized) linear models, such as ridge regression or LASSO, are not guaranteed to yield parameters that explain unique proportions of variance.<a href="confounds-decoding.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span>, here, refer to (usually HRF-convolved) predictors of the time series signal (<span class="math inline">\(s\)</span>) for a single voxel. In the rest of the article, <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> refer to features that are defined across samples (not time).<a href="confounds-decoding.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Note that, technically, one could use the “Control for confounds during pattern estimation” method in electrophysiology as well, by first fitting a univariate model explaining the neuroimaging data (<span class="math inline">\(X_{j}\)</span> for <span class="math inline">\(j = 1 \dots K\)</span>) as a function of both the target (<span class="math inline">\(y\)</span>) and the confound (<span class="math inline">\(C\)</span>) and subsequently only using the parameter estimates of the target-predictor (<span class="math inline">\(\hat{\beta}_{x}\)</span>) as patterns in the subsequent decoding analysis.<a href="confounds-decoding.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shared-states.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="AOMIC.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
