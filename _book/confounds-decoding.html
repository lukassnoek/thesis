<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain</title>
  <meta name="description" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shared-states.html"/>
<link rel="next" href="AOMIC.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-brain-is-not-a-dictionary"><i class="fa fa-check"></i><b>1.1</b> The brain is not a dictionary</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-brain-probably-does-not-care-about-your-hypothesis"><i class="fa fa-check"></i><b>1.2</b> The brain (probably) does not care about your hypothesis</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretability-and-prediction-are-a-trade-off-for-now"><i class="fa fa-check"></i><b>1.3</b> Interpretability and prediction are a trade-off (for now)</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#exploration-should-be-embraced-more"><i class="fa fa-check"></i><b>1.4</b> Exploration should be embraced more</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#proper-generalization-is-hard"><i class="fa fa-check"></i><b>1.5</b> Proper generalization is hard</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#psychology-is-complex-so-it-needs-complex-models"><i class="fa fa-check"></i><b>1.6</b> Psychology is complex, so it needs complex models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.3</b> Model optimization procedure</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.3.1</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.3.2</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.3.3</b> Additional analyses</a></li>
<li class="chapter" data-level="2.3.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.3.4</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.3.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.3.5</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.4</b> Results</a><ul>
<li class="chapter" data-level="2.4.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.4.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.4.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.4.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-acknowledgements"><i class="fa fa-check"></i><b>2.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
<li class="chapter" data-level="3.1.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-current-study"><i class="fa fa-check"></i><b>3.1.3</b> Current study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-data"><i class="fa fa-check"></i><b>3.2.1</b> Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-pipeline"><i class="fa fa-check"></i><b>3.2.2</b> Decoding pipeline</a></li>
<li class="chapter" data-level="3.2.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-methods-evaluated-methods"><i class="fa fa-check"></i><b>3.2.3</b> Evaluated methods for confound control</a></li>
<li class="chapter" data-level="3.2.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#analyses-of-simulated-data"><i class="fa fa-check"></i><b>3.2.4</b> Analyses of simulated data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a><ul>
<li class="chapter" data-level="3.3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#influence-of-brain-size"><i class="fa fa-check"></i><b>3.3.1</b> Influence of brain size</a></li>
<li class="chapter" data-level="3.3.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#baseline-model-no-confound-control"><i class="fa fa-check"></i><b>3.3.2</b> Baseline model: no confound control</a></li>
<li class="chapter" data-level="3.3.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#post-hoc-counterbalancing"><i class="fa fa-check"></i><b>3.3.3</b> Post hoc counterbalancing</a></li>
<li class="chapter" data-level="3.3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#whole-dataset-confound-regression-wdcr"><i class="fa fa-check"></i><b>3.3.4</b> Whole-dataset confound regression (WDCR)</a></li>
<li class="chapter" data-level="3.3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#cross-validated-confound-regression-cvcr"><i class="fa fa-check"></i><b>3.3.5</b> Cross-validated confound regression (CVCR)</a></li>
<li class="chapter" data-level="3.3.6" data-path="confounds-decoding.html"><a href="confounds-decoding.html#summary-methods-for-confound-control"><i class="fa fa-check"></i><b>3.3.6</b> Summary methods for confound control</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#relevance-and-consequences-for-previous-and-future-research"><i class="fa fa-check"></i><b>3.4.1</b> Relevance and consequences for previous and future research</a></li>
<li class="chapter" data-level="3.4.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#choosing-a-confound-model-linear-vs.-nonlinear-models"><i class="fa fa-check"></i><b>3.4.2</b> Choosing a confound model: linear vs. nonlinear models</a></li>
<li class="chapter" data-level="3.4.3" data-path="confounds-decoding.html"><a href="confounds-decoding.html#practical-recommendations"><i class="fa fa-check"></i><b>3.4.3</b> Practical recommendations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="confounds-decoding.html"><a href="confounds-decoding.html#conclusions"><i class="fa fa-check"></i><b>3.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="AOMIC.html"><a href="AOMIC.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a></li>
<li class="chapter" data-level="6" data-path="au-limitations.html"><a href="au-limitations.html"><i class="fa fa-check"></i><b>6</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a></li>
<li class="chapter" data-level="7" data-path="facial-expression-models.html"><a href="facial-expression-models.html"><i class="fa fa-check"></i><b>7</b> Comparing models of dynamic facial expression perception</a></li>
<li class="chapter" data-level="8" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html"><i class="fa fa-check"></i><b>8</b> Summary and general discussion</a><ul>
<li class="chapter" data-level="8.1" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#explore"><i class="fa fa-check"></i><b>8.1</b> Explore!</a></li>
<li class="chapter" data-level="8.2" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#think-big"><i class="fa fa-check"></i><b>8.2</b> Think <em>big</em></a></li>
<li class="chapter" data-level="8.3" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#rethink-psychology-education"><i class="fa fa-check"></i><b>8.3</b> Rethink psychology education</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a><ul>
<li class="chapter" data-level="A.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#stimuli-used-for-sf-task"><i class="fa fa-check"></i><b>A.1</b> Stimuli used for SF-task</a></li>
<li class="chapter" data-level="A.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#instructions"><i class="fa fa-check"></i><b>A.2</b> Instructions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-other-focused-emotion-understanding-task"><i class="fa fa-check"></i><b>A.2.1</b> Full instruction for the other-focused emotion understanding task</a></li>
<li class="chapter" data-level="A.2.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-self-focused-emotion-imagery-task"><i class="fa fa-check"></i><b>A.2.2</b> Full instruction for the self-focused emotion imagery task</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#behavioral-results"><i class="fa fa-check"></i><b>A.3</b> Behavioral results</a></li>
<li class="chapter" data-level="A.4" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#optimization-results"><i class="fa fa-check"></i><b>A.4</b> Optimization results</a></li>
<li class="chapter" data-level="A.5" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#bagging-procedure"><i class="fa fa-check"></i><b>A.5</b> Bagging procedure</a></li>
<li class="chapter" data-level="A.6" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#precision-vs.-recall"><i class="fa fa-check"></i><b>A.6</b> Precision vs. recall</a></li>
<li class="chapter" data-level="A.7" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#self-vs.-other-classification"><i class="fa fa-check"></i><b>A.7</b> Self vs. other classification</a></li>
<li class="chapter" data-level="A.8" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#condition-average-results"><i class="fa fa-check"></i><b>A.8</b> Condition-average results</a></li>
<li class="chapter" data-level="A.9" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#individual-subject-scores"><i class="fa fa-check"></i><b>A.9</b> Individual subject scores</a></li>
<li class="chapter" data-level="A.10" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#brain-region-importance"><i class="fa fa-check"></i><b>A.10</b> Brain region importance</a></li>
<li class="chapter" data-level="A.11" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#general-note-about-tables-with-voxel-coordinates"><i class="fa fa-check"></i><b>A.11</b> General note about tables with voxel-coordinates</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="C" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="D" data-path="au-limitations-supplement.html"><a href="au-limitations-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="E" data-path="facial-expression-models-supplement.html"><a href="facial-expression-models-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="F" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>F</b> Data, code and materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning from the brain</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confounds-decoding" class="section level1">
<h1><span class="header-section-number">3</span> How to control for confounds in decoding analyses of neuroimaging data</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Snoek, L.*, Miletić, S.*, &amp; Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. <em>NeuroImage</em>, 184, 741-760.</p>
<p>* Shared first authorship</p>

<p><strong>Abstract</strong></p>
<p>Over the past decade, multivariate “decoding analyses” have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound “brain size”. We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one’s decoding analysis.</p>
<div id="confounds-decoding-introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data <span class="citation">(Haxby, <a href="bibliography.html#ref-Haxby2012-sd" role="doc-biblioref">2012</a>; Norman et al., <a href="bibliography.html#ref-Norman2006-bt" role="doc-biblioref">2006</a><a href="bibliography.html#ref-Norman2006-bt" role="doc-biblioref">b</a>)</span>. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is “decoding”, in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns <span class="citation">(Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span> and working memory representations from EEG data <span class="citation">(LaRocque et al., <a href="bibliography.html#ref-LaRocque2013-sh" role="doc-biblioref">2013</a>)</span>, as well between-subject factors such as Alzheimer’s disease (vs. healthy controls) from structural MRI data <span class="citation">(Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span> and major depressive disorder (vs. healthy controls) from resting-state functional connectivity <span class="citation">(Craddock et al., <a href="bibliography.html#ref-Craddock2009-kz" role="doc-biblioref">2009</a>)</span>. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do <span class="citation">(Jimura &amp; Poldrack, <a href="bibliography.html#ref-Jimura2012-lv" role="doc-biblioref">2012</a>)</span>. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>.</p>
<p>In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical <span class="citation">(Allefeld et al., <a href="bibliography.html#ref-Allefeld2016-xp" role="doc-biblioref">2016</a>; Davis et al., <a href="bibliography.html#ref-Davis2014-lw" role="doc-biblioref">2014</a>; Gilron et al., <a href="bibliography.html#ref-Gilron2017-tl" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>)</span> and more conceptual <span class="citation">(Naselaris &amp; Kay, <a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span> in nature. For the purposes of the current study, we focus on the specific criticism put forward by <span class="citation">Naselaris &amp; Kay (<a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>)</span> , who argue that decoding analyses are inherently ambiguous in terms of what information they use <span class="citation">(see Popov et al., <a href="bibliography.html#ref-popov2018practices" role="doc-biblioref">2018</a> for a similar argument in the context of encoding analyses)</span>. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information <span class="citation">(see also Carlson &amp; Wardle, <a href="bibliography.html#ref-Carlson2015-bz" role="doc-biblioref">2015</a>; Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span>. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of “source ambiguity”.</p>
<p>To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> (male/female) from structural MRI with the aim of contributing to the understanding of gender differences — an endeavor that generated considerable interest and controversy <span class="citation">(Chekroud et al., <a href="bibliography.html#ref-Chekroud2016-tc" role="doc-biblioref">2016</a>; Del Giudice et al., <a href="bibliography.html#ref-Del_Giudice2016-ns" role="doc-biblioref">2016</a>; Glezerman, <a href="bibliography.html#ref-Glezerman2016-xl" role="doc-biblioref">2016</a>; Joel &amp; Fausto-Sterling, <a href="bibliography.html#ref-Joel2016-uo" role="doc-biblioref">2016</a>; Rosenblatt, <a href="bibliography.html#ref-Rosenblatt2016-oy" role="doc-biblioref">2016</a>)</span>. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant’s gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways <span class="citation">(Good et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a>; O’Brien et al., <a href="bibliography.html#ref-OBrien2011-lj" role="doc-biblioref">2011</a>)</span>. First, there is a <em>global</em> difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight <span class="citation">(18.7%; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Lüders et al., <a href="bibliography.html#ref-Luders2002-ms" role="doc-biblioref">2002</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Second, brains of men and women are known to differ <em>locally</em>: some specific brain areas are on average larger in women than in men <span class="citation">(e.g., in superior and middle temporal cortex; Good et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a>)</span> and vice versa <span class="citation">(e.g., in frontomedial cortex; Goldstein et al., <a href="bibliography.html#ref-Goldstein2001-dy" role="doc-biblioref">2001</a>)</span>. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male <em>bodies</em> are on average larger than female bodies <span class="citation">(Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. As such, our hypothetical researcher is likely primarily interested in the <em>local</em> sources of variation in the neuroanatomy of male and female brains.</p>
<p>Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for “uninteresting” sources of information.</p>
<div id="confounds-decoding-introduction-true-vs-confounded" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</h3>
<p>Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate <em>prediction</em>, but not in <em>interpretability</em> of the model <span class="citation">(Bzdok, <a href="bibliography.html#ref-Bzdok2017-li" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is <em>only</em> using information in the neuroimaging data that is related to the variable that is being decoded <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. In this scenario, source ambiguity (i.e., the presence of <em>multiple</em> sources of information) <em>is</em> problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is <em>confounding</em> the decoding analysis of gender. Here, we define a confound as <em>a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.</em></p>
<p>To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable “schizophrenia-or-not” is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender <span class="citation">(i.e., men are more often diagnosed with schizophrenia than women; McGrath et al., <a href="bibliography.html#ref-McGrath2008-oj" role="doc-biblioref">2008</a>)</span> and substance abuse <span class="citation">(Dixon, <a href="bibliography.html#ref-Dixon1999-kl" role="doc-biblioref">1999</a>)</span>, which are also known to affect gray matter <span class="citation">(Bangalore et al., <a href="bibliography.html#ref-Bangalore2008-kc" role="doc-biblioref">2008</a>; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Van Haren et al., <a href="bibliography.html#ref-Van_Haren2013-iv" role="doc-biblioref">2013</a>)</span>. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate <em>prediction</em> of schizophrenia, then this ambiguity due to confounding sources of information is problematic.</p>
<p>Importantly, as our definition suggests, what <em>is</em> or <em>is not</em> regarded as a confound is relative — it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable “schizophrenia-or-no”” becomes the confounding variable. In other words, one researcher’s signal is another researcher’s confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the “true signal” (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the “confounded signal” (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-1">3.1</a>). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-1"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_1.png" alt="Visualization of how variance in brain data (\(X\)) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (\(X\)), the confound (\(C\)), and the target (\(y\)). Overlapping circles indicate a non-zero (squared) correlation between the two variables."  />
<p class="caption">
Figure 3.1: Visualization of how variance in brain data (<span class="math inline">\(X\)</span>) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (<span class="math inline">\(X\)</span>), the confound (<span class="math inline">\(C\)</span>), and the target (<span class="math inline">\(y\)</span>). Overlapping circles indicate a non-zero (squared) correlation between the two variables.
</p>
</div>

<p>In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.</p>
</div>
<div id="confounds-decoding-introduction-methods" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Methods for confound control</h3>
<p>In decoding analyses, one aims to predict a certain target variable from patterns of neuroimaging data. Various methods discussed in this section are supplemented with a mathematical formalization; for consistency and readability, we define the notation we will use in Table <a href="confounds-decoding.html#tab:tab-confounds-decoding-1">3.1</a>.</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:tab-confounds-decoding-1">Table 3.1: </span>Notation.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Symbol
</th>
<th style="text-align:left;">
Dims.
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of samples (usually subjects or trials)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of neuroimaging features (e.g., voxels or sensors)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(P\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Number of confound variables (e.g., age, reaction time, or brain size)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(X_{ij}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times K\)</span>
</td>
<td style="text-align:left;">
The neuroimaging patterns (often called the “data” in the current article), where the subescript <span class="math inline">\(i \in {1 \dots N}\)</span> refers to the individual samples (rows), and the subscript <span class="math inline">\(j \in {1 \dots K}\)</span> to individual features (columns)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(y\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times 1\)</span>
</td>
<td style="text-align:left;">
The target variable (i.e., what is to be decoded)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(C\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(N \times P\)</span>
</td>
<td style="text-align:left;">
The confound variable(s)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(\hat{\beta}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K + 1\)</span>
</td>
<td style="text-align:left;">
The parameters estimated in a general linear model (GLM)
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(w\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(K + 1\)</span>
</td>
<td style="text-align:left;">
The parameters estimated in a decoding model
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(r_{Cy}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Sample Pearson correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(r_{y(X.C)}\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
Sample semipartial Pearson correlation coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>, controlled for <span class="math inline">\(C\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 3em; ">
<span class="math inline">\(p(r_{Cy})\)</span>
</td>
<td style="text-align:left;width: 3em; ">
<ul>
<li></td>
<td style="text-align:left;">
<span class="math inline">\(p\)</span>-value of sample Pearson correlation between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span>
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Note: </span> <sup></sup> Format based on Diedrichsen and Kriegeskorte (2017). For the correlations (<span class="math inline">\(r\)</span>), we assume that <span class="math inline">\(P = 1\)</span> and thus that the correlations in the table reduce to a scalar.
</td>
</tr>
</tfoot>
</table></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<div id="confounds-decoding-introduction-methods-apriori-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.1.2.1</span> A priori counterbalancing</h4>
<p>Ideally, one would prevent confounding variables from influencing the results as much as possible before the acquisition of the neuroimaging data.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> One common way do this (in both traditional “activation-based” and decoding analyses) is to make sure that potential confounding variables are <em>counterbalanced</em> in the experimental design <span class="citation">(Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. In experimental research, this would entail randomly assigning subjects to design cells (e.g., treatment groups) such that there is no structural correlation between characteristics of the subjects and design cells. In observational designs (e.g., in the gender/brain size example described earlier), it means that the sample is chosen such that there is no correlation between the confound (brain size) and <em>observed</em> target variable (gender). That is, given that men on average have larger brains than women, this would entail including only men with relatively small brains and women with relatively large brains.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> The distinction between experimental and observational studies is important because the former allow the researcher to randomly draw samples from the population, while the latter require the researcher to choose a sample that is not representative of the population, which limits the conclusions that can be drawn about the population (we will revisit this issue in the <a href="confounds-decoding.html#confounds-decoding-discussion">Discussion</a> section).</p>
<p>Formally, in decoding analyses, a design is counterbalanced when the confound <span class="math inline">\(C\)</span> and the target <span class="math inline">\(y\)</span> are statistically independent. In practice, this often means that the sample is chosen so that there is no significant correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> (although this does not necessarily imply that <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> are actually independent). To illustrate the process of counterbalancing, let’s consider another hypothetical experiment: suppose one wants to set up an fMRI experiment in which the goal is to decode abstract object category (e.g., faces vs. houses) from the corresponding fMRI patterns <span class="citation">(cf. Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span>, while controlling for the potential confounding influence of low-level or mid-level stimulus features, such as luminance, spatial frequency, or texture <span class="citation">(Long et al., <a href="bibliography.html#ref-Long2017-fb" role="doc-biblioref">2017</a>)</span>. Proper counterbalancing would entail making sure that the images used for this particular experiments have similar values for these low-level and mid-level features across object categories <span class="citation">(see for details Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. Thus, in this example, low-level and mid-level stimulus features should be counterbalanced with respect to object category, such that above chance decoding of object category cannot be attributed to differences in low-level or mid-level stimulus features (i.e., the confounds).</p>
<p>A priori counterbalancing of potential confounds is, however, not always feasible. For one, the exact measurement of a potentially confounding variable may be impossible until data acquisition. For example, the brain size of a participant is only known after data collection. Similarly, <span class="citation">Todd et al. (<a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> found that their decoding analysis of rule representations was confounded by response times of to the to-be-decoded trials. Another example of a “data-driven” confound is participant motion during data acquisition <span class="citation">(important in, for example, decoding analyses applied to data from clinical populations such as ADHD; Yu-Feng et al., <a href="bibliography.html#ref-Yu-Feng2007-sg" role="doc-biblioref">2007</a>)</span>. In addition, a priori counterbalancing of confounds may be challenging because of the limited size of populations of interest. Especially in clinical research settings, researchers may not have the luxury of selecting a counterbalanced sample due to the small number of patient subjects available for testing. Lastly, researchers may simply discover confounds after data acquisition.</p>
<p>Given that a priori counterbalancing is not possible or undesirable in many situations, it is paramount to explore the possibilities of controlling for confounding variables after data acquisition for the sake of model interpretability, which we discuss next.</p>
</div>
<div id="confounds-decoding-introduction-methods-include-in-data" class="section level4">
<h4><span class="header-section-number">3.1.2.2</span> Include confounds in the data</h4>
<p>One perhaps intuitive method to control for confounds in decoding analyses is to include the confound(s) in the data <span class="citation">(i.e., the neuroimaging data, <span class="math inline">\(X\)</span>; see, e.g., Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span> used by decoding model. That is, when applying a decoding analysis to neuroimaging data, the confound is added to the data as if it were another voxel (or sensor, in electrophysiology). This intuition may stem from the analogous situation in univariate (activation-based) analyses of neuroimaging data, in which confounding variables are controlled for by including them in the design matrix together with the stimulus/task regressors. For example, in univariate analyses of functional MRI, movement of the participant is often controlled for by including motion estimates in the design matrix of first-level analyses <span class="citation">(Johnstone et al., <a href="bibliography.html#ref-Johnstone2006-tn" role="doc-biblioref">2006</a>)</span>; in EEG, some control for activity due to eye-movements by including activity measured by concurrent electro-oculography as covariates in the design-matrix <span class="citation">(Parra et al., <a href="bibliography.html#ref-Parra2005-um" role="doc-biblioref">2005</a>)</span>. Usually, the general linear model is then used to estimate each predictor’s influence on the neuroimaging data. Importantly, the parameter estimates (<span class="math inline">\(\hat{\beta}\)</span>) are often interpreted as reflecting the unique contribution<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> of each predictor variable, independent from the influence of the confound.</p>
<p>Contrary to general linear models as employed in univariate (activation-based) analyses, including confound variables in the data as predictors for <em>decoding</em> models is arguably problematic. If a confound is included in the data in the context of decoding models, the parameter estimates of the features (often called “feature weights”, <span class="math inline">\(w\)</span>, in decoding models) may be corrected for the influence of the confound, but the <em>model performance</em> <span class="citation">(usually measured as explained variance, <span class="math inline">\(R^2\)</span>, or classification accuracy; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span> is not. That is, rather than providing an estimate of decoding performance “controlled for” a confound, one obtains a measure of performance when explicitly <em>including</em> the confound as an interesting source of variance that the decoder is allowed to use. This is problematic because research using decoding analyses generally does not focus on parameter estimates but on statistics of model performance. Model performance statistics (e.g., <span class="math inline">\(R^2\)</span>, classification accuracy) alone cannot disentangle the contribution of different sources of information as they only represent a single summary statistic of model fit <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. One might, then, argue that additionally inspecting feature weights of decoding models may help in disambiguating different sources of information <span class="citation">(Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. However, it has been shown that feature weights cannot be reliably mapped to specific sources of information, i.e., as being task-related or confound-related <span class="citation">(e.g., features with large weights may be completely uncorrelated with the target variable; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. As such, it does not make sense to include confounds in the set of predictors when the goal is to disambiguate the different sources of information in decoding analyses.</p>
<p>Recently, another approach similar to including confounds in the data has been proposed, which is based on the idea of a dose-response curve <span class="citation">(Alizadeh et al., <a href="bibliography.html#ref-alizadeh2017decoding" role="doc-biblioref">2017</a>)</span>. In this method, instead of adding the confound(s) to the model directly, the relative contribution of true and confounded signal is systematically controlled. The authors show that this approach is able to directly quantify the unique contribution of each source of information, thus effectively controlling for confounded signal. However, while sophisticated in its approach, this method only seems to work for categorical confounds, as it is difficult (if not impossible) to systematically vary the proportion of confound-related information when dealing with continuous confounds or when dealing with more than one confound.</p>
</div>
<div id="confounds-decoding-introduction-methods-pattern-estimation" class="section level4">
<h4><span class="header-section-number">3.1.2.3</span> Control for confounds during pattern estimation</h4>
<p>Another method that was used in some decoding studies on functional MRI data aims to control for confounds in the initial procedure of estimating activity patterns of the to-be-decoded events, by leveraging the ability of the GLM to yield parameter estimates reflecting unique variance <span class="citation">(Woolgar et al., <a href="bibliography.html#ref-Woolgar2014-jb" role="doc-biblioref">2014</a>)</span>. In this method, an initial “first-level” (univariate) analysis models the fMRI time series (<span class="math inline">\(s\)</span>) as a function of both predictors-of-interest (<span class="math inline">\(X\)</span>) and the confounds (<span class="math inline">\(C\)</span>), often using the GLM<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>:</p>
<p><span class="math display">\[\begin{equation}
s = X\beta_{x} + C\beta_{c} + \epsilon
\end{equation}\]</span></p>
<p>Then, only the estimated parameters (<span class="math inline">\(\hat{\beta}\)</span>, or normalized parameters, such as <em>t</em>-values or <em>z</em>-values) corresponding to the predictors-of-interest (<span class="math inline">\(\hat{\beta}_{x}\)</span>) are used as activity estimates (i.e., the used for predicting the target <span class="math inline">\(y\)</span>) in the subsequent decoding analyses. This method thus takes advantage of the shared variance partitioning in the pattern estimation step to control for potential confounding variables. However, while elegant in principle, this method is not applicable in between-subject decoding studies <span class="citation">(e.g., clinical decoding studies; Waarde et al., <a href="bibliography.html#ref-Van_Waarde2014-sh" role="doc-biblioref">2014</a>; Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span>, in which confounding variables are defined across subjects, or in electrophysiology studies, in which activity patterns do not have to be<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> estimated in a first-level model, thus limiting the applicability of this method.</p>
</div>
<div id="confounds-decoding-introduction-methods-posthoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.1.2.4</span> Post hoc counterbalancing of confounds</h4>
<p>When a priori counterbalancing is not possible, some have argued that post hoc counterbalancing might control for the influence of confounds <span class="citation">(Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>, pp. 24, 38)</span>. In this method, given that there is some sample correlation between the target and confound (<span class="math inline">\(r_{Cy} \neq 0\)</span>) in the entire dataset, one takes a subset of samples in which there is no empirical relation between the confound and the target (e.g., when <span class="math inline">\(r_{Cy} \approx 0\)</span>). In other words, post hoc counterbalancing is a way to <em>decorrelate</em> the confound and the target by subsampling the data. Then, subsequent decoding analysis on the subsampled data can only capitalize on true signal, as there is no confounded signal anymore (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-2">3.2</a>). While intuitive in principle, we are not aware of whether this method has been evaluated before and whether it yields unbiased performance estimates.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-2"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_2.png" alt="A schematic visualization how the main two confound control methods evaluated in this article deal with the “confounded signal”, making sure decoding models only capitalize on the “true signal”."  />
<p class="caption">
Figure 3.2: A schematic visualization how the main two confound control methods evaluated in this article deal with the “confounded signal”, making sure decoding models only capitalize on the “true signal”.
</p>
</div>

</div>
<div id="confounds-decoding-introduction-methods-confound-regression" class="section level4">
<h4><span class="header-section-number">3.1.2.5</span> Confound regression</h4>
<p>The last and perhaps most common method to control for confounds is removing the variance that can be explained by the confound (i.e., the confounded signal) from the neuroimaging data directly <span class="citation">(Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>; Dukart et al., <a href="bibliography.html#ref-Dukart2011-aq" role="doc-biblioref">2011</a>; Kostro et al., <a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>; Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>; Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> — a process we refer to as <em>confound regression</em> <span class="citation">(also known as “image correction”; Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>)</span>. In this method, a (usually linear) regression model is fitted on each feature in the neuroimaging data (i.e., a single voxel or sensor) with the confound(s) as predictor(s). Thus, each feature in the neuroimaging data <span class="math inline">\(X\)</span> is modelled as a linear function of the confounding variable(s), <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[\begin{equation}
X_{j} = C\beta + \epsilon
\end{equation}\]</span></p>
<p>We can estimate the parameter(s) for feature using, for example, ordinary least squares as follows <span class="citation">(for an example using a different model, see Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{j} = (C^{T}C)^{-1}C^{T}X_{j}
\end{equation}\]</span></p>
<p>Then, to remove the variance of (or “regress out”) the confound from the neuroimaging data, we can subtract the variance in the data associated with confound (<span class="math inline">\(C\hat{\beta}_{j}\)</span>) from the original data:</p>
<p><span class="math display">\[\begin{equation}
X_{j,\mathrm{corr}} = X_{j} - C\hat{\beta}_{j}
\end{equation}\]</span></p>
<p>In which <span class="math inline">\(X_{j,\mathrm{corr}}\)</span> represents the neuroimaging feature <span class="math inline">\(X_{j}\)</span> from which all variance of the confound is removed (including the variance shared with <span class="math inline">\(y\)</span>, i.e., the confounded signal; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-2">3.2</a>). When subsequently applying a decoding analysis on this corrected data, one can be sure that the decoder is not capitalizing on signal that is correlated with the confound, which thus improves interpretability of the decoding analysis.</p>
<p>Confound regression has been applied in several decoding studies. <span class="citation">Todd et al. (<a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span> were, as far as the current authors are aware, the first to use this method to control for a confound (in their case, reaction time) that was shown to correlate with their target variable (rule A vs. rule B). Notably, they both regressed out reaction time from the first-level time series data (similar to the “Control for confounds during pattern estimation” method) <em>and</em> regressed out reaction time from the trial-by-trial activity estimates (i.e., confound regression as described in this section). They showed that controlling for reaction time in this way completely eliminated the above chance decoding performance. Similarly, <span class="citation">Kostro et al. (<a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>)</span> observe a substantial drop in classification accuracy when controlling for scanner site in the decoding analysis of Huntington’s disease, but only when scanner site and disease status were actually correlated. Lastly, <span class="citation">Rao et al. (<a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>)</span> found that, in contrast to Kostro et al. and Todd et al., confound regression yielded similar (or slightly lower, but still significant) performance compared to the model without confound control, but it should be noted that this study used a regression model (instead of a classification model) and evaluated confound control in the specific situation when the training set is confounded, but the test set is not.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> In sum, while confound regression has been used before, it has yielded variable results, possibly due to slightly different approaches and differences in the correlation between the confounding variable and the target.</p>
</div>
</div>
<div id="confounds-decoding-introduction-current-study" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Current study</h3>
<p>In summary, multiple methods have been proposed to deal with confounds in decoding analyses. Often, these methods have specific assumptions about the nature or format of the data (such as “A priori counterbalancing” and “Confound control during pattern estimation”), differ in their objective (e.g., <em>prediction</em> vs. <em>interpretation</em>, such as in “Include confounds in the data”), or have yielded variable results (such as “Confound regression”). Therefore, given that we are specifically interested in interpreting decoding analyses, the current study evaluates the two methods that are applicable in most contexts: post hoc counterbalancing and confound regression (but see <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a> for a tentative evaluation of this method based on simulated functional MRI data). In addition to these two methods, we propose a third method — a modified version of confound regression —– which we show yields plausible, seemingly unbiased, and interpretable results.</p>
<p>To test whether these methods are able to effectively control for confounds and whether they yield plausible results, we apply them to empirical data, as well as to simulated data in which the ground truth with respect to the signal in the data (i.e., the proportion of true signal and confounded signal) is known. For our empirical data, we enact the previously mentioned hypothetical study in which participant gender is decoded from structural MRI data. We use a large dataset (<span class="math inline">\(N = 217\)</span>) of structural MRI data and try to predict subjects’ gender (male/female) from gray and white matter patterns while controlling for the confound of “brain size” using the aforementioned methods, which we compare to a baseline model in which confounds are not controlled for. Given the previously reported high correlations between brain size and gender <span class="citation">(Barnes et al., <a href="bibliography.html#ref-Barnes2010-pu" role="doc-biblioref">2010</a>; Smith &amp; Nichols, <a href="bibliography.html#ref-Smith2018-th" role="doc-biblioref">2018</a>)</span>, we expect that successfully controlling for brain size yields lower decoding performance than using uncorrected data, but not below chance level. Note that higher decoding performance after controlling for confounds is theoretically possible when the correlation between the confound and variance in the data <em>unrelated</em> to the target (e.g., noise) is sufficiently high to cause suppressor effects <span class="citation">(see Figure 1 in Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. However, because our confound, brain size, is known to correlate strongly with our target gender <span class="citation">(approx. <span class="math inline">\(r = 0.63\)</span>; Smith &amp; Nichols, <a href="bibliography.html#ref-Smith2018-th" role="doc-biblioref">2018</a>)</span>, it is improbable that it also correlates highly with variance in brain data that is unrelated to gender. It follows then that classical suppression effects are unlikely and we thus expect lower model performance after controlling for brain size.</p>
<p>However, shown in detail below, both post hoc counterbalancing and confound regression lead to unexpected results in our empirical analyses: counterbalancing fails to reduce model performance while confound regression consistently yields low model performance up to the point of significant below chance accuracy. In subsequent analyses of simulated data, we show that both methods lead to <em>biased</em> results: post hoc counterbalancing yields inflated model performance (i.e., positive bias) because subsampling selectively selects a subset of samples in which features correlate more strongly with the target variable, suggesting (indirect) circularity in the analysis <span class="citation">(Kriegeskorte, Simmons, Bellgowan, et al., <a href="bibliography.html#ref-Kriegeskorte2009-yz" role="doc-biblioref">2009</a><a href="bibliography.html#ref-Kriegeskorte2009-yz" role="doc-biblioref">b</a>)</span>. Furthermore, our simulations show that negative bias (including significant below chance classification) after confound regression on the entire dataset is due to reducing the signal below what is expected by chance <span class="citation">(Jamalabadi et al., <a href="bibliography.html#ref-jamalabadi2016classification" role="doc-biblioref">2016</a>)</span>, which we show is related to and can be predicted by the standard deviation of the empirical distribution of correlations between the features in the data and the target. We propose a minor but crucial addition to the confound regression procedure, in which we cross-validate the confound regression models (which we call “cross-validated confound regression”, CVCR), which solves the below chance accuracy issue and yields plausible model performance in both our empirical and simulated data.</p>
</div>
</div>
<div id="confounds-decoding-methods" class="section level2">
<h2><span class="header-section-number">3.2</span> Methods</h2>
<div id="confounds-decoding-methods-data" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Data</h3>
<p>For the empirical analyses, we used voxel-based morphometry (VBM) data based on T1-weighted scans and tract-based spatial statistics (TBSS) data based on diffusion tensor images from 217 participants (122 women, 95 men), acquired with a Philips Achieva 3T MRI-scanner and a 32-channel head coil at the Spinoza Centre for Neuroimaging (Amsterdam, The Netherlands).</p>
<div id="confounds-decoding-methods-data-vbm" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> VBM acquisition &amp; analysis</h4>
<p>The T1-weighted scans with a voxel size of 1.0 × 1.0 × 1.0 mm were acquired using 3D fast field echo (TR: 8.1 ms, TE: 3.7 ms, flip angle: 8°, FOV: 240 × 188 mm, 220 slices). We used “FSL-VBM” protocol <span class="citation">(Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span> from the FSL software package <span class="citation">(version 5.0.9; Smith et al., <a href="bibliography.html#ref-Smith2004-sc" role="doc-biblioref">2004</a>)</span>; using default and recommended parameters (including non-linear registration to standard space). The resulting VBM-maps were spatially smoothed using a Gaussian kernel (3 mm FWHM). Subsequently, we organized the data in the standard pattern-analysis format of a 2D (<span class="math inline">\(N \times K\)</span>) array of shape 217 (subjects) × 412473 (non-zero voxels).</p>
</div>
<div id="confounds-decoding-methods-data-tbss" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> TBSS acquisition &amp; analysis</h4>
<p>Diffusion tensor images with a voxel size of 2.0 × 2.0 × 2.0 mm were acquired using a spin-echo echo-planar imaging (SE-EPI) protocol (TR: 7476 ms, TE: 86 ms, flip angle: 90°, FOV: 224 × 224 mm, 60 slices), which acquired a single b = 0 (non-diffusion-weighted) image and 32 (diffusion-weighted) b = 1000 images. All volumes were corrected for eddy-currents and motion (using the FSL command “eddy_correct”) and the non-diffusion-weighted image was skullstripped (using FSL-BET with the fractional intensity threshold set to 0.3) to create a mask that was subsequently used in the fractional anisotropy (FA) estimation. The FA-images resulting from the diffusion tensor fitting procedure were subsequently processed by FSL’s tract-based spatial statistics (TBSS) pipeline <span class="citation">(Smith et al., <a href="bibliography.html#ref-Smith2006-sf" role="doc-biblioref">2006</a>)</span>, using the recommended parameters (i.e., non-linear registration to FSL’s 1 mm FA image, construction of mean FA-image and skeletonized mean FA-image based on the data from all subjects, and a threshold of 0.2 for the skeletonized FA-mask). Subsequently, we organized the resulting skeletonized FA-maps into a 2D (<span class="math inline">\(N \times K\)</span>) array of shape 217 (subjects) × 128340 (non-zero voxels).</p>
</div>
<div id="confounds-decoding-methods-data-brainsize" class="section level4">
<h4><span class="header-section-number">3.2.1.3</span> Brain size estimation</h4>
<p>To estimate the values for our confound, global brain size, we calculated for each subject the total number of non-zero voxels in the gray matter and white matter map resulting from the segmentation step in the FSL-VBM pipeline <span class="citation">(using FSL’s segmentation algorithm “FAST”; Zhang et al., <a href="bibliography.html#ref-Zhang2001-wa" role="doc-biblioref">2001</a>)</span>. The number of non-zero voxels from the gray matter map was used as the confound for the VBM-based analyses and the number of non-zero voxels from the white matter map was used as the confound for the TBSS-based analyses. Note that brain size estimates from total white matter volume and total gray matter volume correlated strongly, <span class="math inline">\(r (216) = 0.93\)</span>, <span class="math inline">\(p &lt; 0.001\)</span>.</p>
</div>
<div id="confounds-decoding-methods-data-data-and-code" class="section level4">
<h4><span class="header-section-number">3.2.1.4</span> Data and code availability</h4>
<p>In the Github repository corresponding to this article (<a href="https://github.com/lukassnoek/MVCA">https://github.com/lukassnoek/MVCA</a>), we included a script (<code>download_data.py</code>) to download the data (the 4D VBM and TBSS nifti-images as well as the non-zero 2D samples × features arrays). The repository also contains detailed Jupyter notebooks with the annotated empirical analyses and simulations reported in this article.</p>
</div>
</div>
<div id="confounds-decoding-methods-pipeline" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Decoding pipeline</h3>
<p>All empirical analyses and simulations used a common decoding pipeline, implemented using functionality from the <em>scikit-learn</em> Python package for machine learning <span class="citation">(Abraham et al., <a href="bibliography.html#ref-Abraham2014-ef" role="doc-biblioref">2014</a>; Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, et al., <a href="bibliography.html#ref-Pedregosa2011-bp" role="doc-biblioref">2011</a>)</span>. This pipeline included univariate feature selection (based on a prespecified amount of voxels with highest univariate difference in terms of the ANOVA <em>F</em>-statistic), feature-scaling (ensuring zero mean and unit standard deviation for each feature), and a support vector classifier (SVC) with a linear kernel, fixed regularization parameter (<em>C</em> = 1), and sample weights set to be inversely proportional to class frequency (to account for class imbalance). In our empirical analyses, we evaluated model performance for different numbers of voxels (as selected by the univariate feature selection). For our empirical analyses, we report model performance as the <span class="math inline">\(F_{1}\)</span> score, which is insensitive to class imbalance (which, in addition to adjusted sample weights, prevents the classifier from learning the relative probabilities of target classes instead of representative information in the features; see also Supplementary Figure <a href="#fig:fig-shared-states-S14"><strong>??</strong></a> for a replication of part of the results using AUROC, another metric that is insensitive to class imbalance). At chance level classification, the <span class="math inline">\(F_{1}\)</span> score is expected to be 0.5. For our simulations, in which there is no class imbalance, we report model performance using accuracy scores. In figures showing error bars around the average model performance scores, the error bars represent 95% confidence intervals estimated using the “bias-corrected and accelerated” (BCA) bootstrap method using 10,000 bootstrap replications <span class="citation">(Efron, <a href="bibliography.html#ref-efron1987better" role="doc-biblioref">1987</a>)</span>. For calculating BCA bootstrap confidence intervals, we used the implementation from the open source “scikits.bootstrap” Python package (<a href="https://github.com/cgevans/scikits-bootstrap">https://github.com/cgevans/scikits-bootstrap</a>). Statistical significance was calculated using non-parametric permutation tests, as implemented in scikit-learn, with 1000 permutations <span class="citation">(Ojala &amp; Garriga, <a href="bibliography.html#ref-Ojala2010-rc" role="doc-biblioref">2010</a>)</span>.</p>
</div>
<div id="confounds-decoding-methods-evaluated-methods" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Evaluated methods for confound control</h3>
<div id="confounds-decoding-methods-evaluated-methods-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Post hoc counterbalancing</h4>
<p>We implemented post hoc counterbalancing in two steps. First, to quantify the strength of relation between the confound and the target in our dataset, we estimated the point-biserial correlation coefficient between the confound, <span class="math inline">\(C\)</span> (brain size), and the target, <span class="math inline">\(y\)</span> (gender) across the entire dataset (including all samples <span class="math inline">\(i = 1 \dots N\)</span>). Because of both sampling noise and measurement noise, sample correlation coefficients vary around the population correlation coefficient and are thus improbable to be 0 <em>exactly</em>.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Therefore, in the next step, we subsampled the data until the correlation coefficient between and becomes non-significant at some significance threshold <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(p(r_{Cy}) &gt; \alpha\)</span>.</p>
<p>In our analyses, we used an <span class="math inline">\(\alpha\)</span> of 0.1. Note that this is more “strict”<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> than the conventionally used threshold (<span class="math inline">\(\alpha = 0.05\)</span>), but given that decoding analyses are often more sensitive to signal in the data (whether it is confounded or true signal), we chose to err on the safe side and counterbalance the data using a relatively strict threshold of <span class="math inline">\(\alpha = 0.1\)</span>.</p>
<p>Subsampling was done by iteratively removing samples that contribute most to the correlation between the confound and the target until the correlation becomes non-significant. In our empirical data in which brain size is positively correlated with gender (coded as male = 1, female = 0) this amounted to iteratively removing the male subject with the largest brain size and the female subject with the smallest brain size. This procedure optimally balances (1) minimizing the correlation between target and confound and (2) maximizing sample size. As an alternative to this “targeted subsampling”, we additionally implemented a procedure which draws random subsamples of a given sample size until it finds a subsample with a non-significant correlation coefficient. If such a subsample cannot be found after 10,000 random draws, sample size is decreased by 1, which is repeated until a subsample is found. This procedure resulted in much smaller subsamples than the targeted subsampling procedure (i.e., a larger power loss) since the optimal subsample is hard to find randomly.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> In the analyses below, therefore, we used the targeted subsampling procedure. Importantly, even with extreme power loss, random subsampling can cause the same biases as will be described for the targeted subsampling method below (cf. Figure <a href="#fig:fig-confounds-decoding-8"><strong>??</strong></a> and Figure <a href="#fig:fig-confounds-decoding-10"><strong>??</strong></a> and Supplementary Figures <a href="#fig:fig-confounds-decoding-S13"><strong>??</strong></a> and <a href="#fig:fig-confounds-decoding-S14"><strong>??</strong></a>).</p>
<p>Then, given that the subsampled dataset is counterbalanced with respect to the confound, a random stratified K-fold cross-validation scheme is repeatedly initialized until a scheme is found in which <em>all</em> splits are counterbalanced as well <span class="citation">(cf. Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>. This particular counterbalanced cross-validation scheme is subsequently used to cross-validate the MVPA pipeline. We implemented this post hoc counterbalancing method as a scikit-learn-style cross-validator class, available from the aforementioned Github repository (in the <code>counterbalance.py</code> module).</p>
</div>
<div id="confound-regression" class="section level4">
<h4><span class="header-section-number">3.2.3.2</span> Confound regression</h4>
<p>In our empirical analyses and simulations, we tested two different versions of confound regression, which we call “whole-dataset confound regression” (WDCR) and “cross-validated confound regression” (CVCR). In WDCR, we regressed out the confounds from the predictors <em>from the entire dataset at once</em>, i.e., before entering the iterative cross-validated MVPA pipeline <span class="citation">(the approach taken by Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>; Dubois et al., <a href="bibliography.html#ref-Dubois2017-fl" role="doc-biblioref">2017</a>; Kostro et al., <a href="bibliography.html#ref-Kostro2014-cm" role="doc-biblioref">2014</a>; Todd et al., <a href="bibliography.html#ref-Todd2013-sd" role="doc-biblioref">2013</a>)</span>. Note that we can do this for all <span class="math inline">\(K\)</span> voxels at once using the closed-form OLS solution, in which we first estimated the parameters <span class="math inline">\(\hat{\beta}_{C}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{C} = (C^{T}C)^{-1}C^{T}X
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C\)</span> is an array in which the first column contained an intercept and the second column contained the confound brain size. Accordingly, <span class="math inline">\(\hat{\beta}_{C}\)</span> is an <span class="math inline">\(2 \times K\)</span> array. We then removed the variance associated with the confound from our neuroimaging data as follows:</p>
<p><span class="math display">\[\begin{equation}
X_{\mathrm{corr}} = X - C\hat{\beta}_{C}
\end{equation}\]</span></p>
<p>Now, <span class="math inline">\(X_{\mathrm{corr}}\)</span> is an array with the same shape as the original <span class="math inline">\(X\)</span> array, but without any variance that can be explained by confound, <span class="math inline">\(C\)</span> (i.e., <span class="math inline">\(X\)</span> is residualized with regard to <span class="math inline">\(C\)</span>).</p>
<p>In our proposed cross-validated version of confound regression <span class="citation">(which was mentioned but not evaluated by Rao et al., <a href="bibliography.html#ref-Rao2017-bw" role="doc-biblioref">2017</a>, p. 25)</span>, “CVCR”, we similarly regressed out the confounds from the neuroimaging data, but instead of estimating <span class="math inline">\(\hat{\beta}_{C}\)</span> on the entire dataset, we estimated this within each fold of training data (<span class="math inline">\(X_{\mathrm{train}}\)</span>):</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_{C,\mathrm{train}} = (C^{T}_{\mathrm{train}}C_{\mathrm{train}})^{-1}C^{T}_{\mathrm{train}}X_{\mathrm{train}}
\end{equation}\]</span></p>
<p>And we subsequently used these parameters (<span class="math inline">\(\hat{\beta}_{C,\mathrm{train}}\)</span>) to remove the variance related to the confound from both the train set (<span class="math inline">\(X_{\mathrm{train}}\)</span> and <span class="math inline">\(C_{\mathrm{train}}\)</span>):</p>
<p><span class="math display" id="eq:cvcr-train">\[\begin{equation}
X_{\mathrm{train, corr}} = X_{\mathrm{train}} - C_{\mathrm{train}}\hat{\beta}_{C,\mathrm{train}}
\tag{3.1}
\end{equation}\]</span></p>
<p>and the test test (<span class="math inline">\(X_{\mathrm{test}}\)</span> and <span class="math inline">\(C_{\mathrm{test}}\)</span>):</p>
<p><span class="math display" id="eq:cvcr-test">\[\begin{equation}
X_{\mathrm{test, corr}} = X_{\mathrm{test}} - C_{\mathrm{test}}\hat{\beta}_{C,\mathrm{test}}
\tag{3.2}
\end{equation}\]</span></p>
<p>Thus, essentially, CVCR is the cross-validated version of WDCR. One might argue that regressing the confound from the train set only, i.e., implementing only equation <a href="confounds-decoding.html#eq:cvcr-train">(3.1)</a>, not equation <a href="confounds-decoding.html#eq:cvcr-test">(3.2)</a>, is sufficient to control for confounds as it prevents the decoding model from relying on signal related to the confound. We evaluated this method and report the corresponding results in Supplementary Figure <a href="#fig:fig-confounds-decoding-S10"><strong>??</strong></a>.</p>
<p>We implemented these confound regression techniques as a <em>scikit-learn</em> compatible transformer class, available in the open-source <em>skbold</em> Python package (<a href="https://github.com/lukassnoek/skbold">https://github.com/lukassnoek/skbold</a>) and in the aforementioned Github repository.</p>
</div>
<div id="control-for-confounds-during-pattern-estimation" class="section level4">
<h4><span class="header-section-number">3.2.3.3</span> Control for confounds during pattern estimation</h4>
<p>In addition to post hoc counterbalancing and confound regression, we also evaluated how well the “control for confounds during pattern estimation” method controls for the influence of confounds in decoding analyses of (simulated) fMRI data. The simulation methods and results can be found in the <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a>.</p>
</div>
</div>
<div id="analyses-of-simulated-data" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Analyses of simulated data</h3>
<p>In addition to the empirical evaluation of counterbalancing and confound regression in the gender decoding example, we ran three additional analyses on simulated data. First, we investigated the efficacy of the three confound control methods on synthetic data with known quantities of “true signal” and “confounded signal”, in order to detect potential biases. Second, we ran additional analyses on simulated data to investigate the positive bias in model performance observed after post hoc counterbalancing. Third, we ran additional analyses on simulated data to investigate the negative bias in model performance observed after WDCR. In the <a href="confounds-decoding-supplement.html#confounds-decoding-supplement">Supplementary Materials</a>, we investigate whether the confound regression results generalize to (simulated) functional MRI data (Supplementary Figure <a href="#fig:fig-confounds-decoding-S1"><strong>??</strong></a> and <a href="#fig:fig-confounds-decoding-S2"><strong>??</strong></a>).</p>
<div id="efficacy-analyses" class="section level4">
<h4><span class="header-section-number">3.2.4.1</span> Efficacy analyses</h4>
<p>In this simulation, we evaluated the efficacy of the three methods for confound control on synthetic data with a prespecified correlation between the confound and the target, <span class="math inline">\(r_{Cy}\)</span>, and varying amounts of “confounded signal” (i.e., the explained variance in <span class="math inline">\(y\)</span> driven by shared variance between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>). These simulations allowed us to have full control over (and knowledge of) the influence of both signal and confound in the data, and thereby help us diagnose biases associated with post hoc counterbalancing and confound regression.</p>
<p>Specifically, in this efficacy analysis, we generated hypothetical data sets holding the correlation coefficient between <span class="math inline">\(C\)</span> and <span class="math inline">\(y\)</span> constant, while varying the amount of true signal and confounded signal. We operationalized true signal as the squared semipartial Pearson correlation between <span class="math inline">\(y\)</span> and each feature in <span class="math inline">\(X\)</span>, controlled for <span class="math inline">\(C\)</span>. As such, we will refer to this term as <span class="math inline">\(\mathrm{signal}\ R^{2}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{signal}\ R^{2} = r_{y(X.C)}^{2}
\end{equation}\]</span></p>
<p>In the simulations reported and shown in the main article, we used <span class="math inline">\(r_{Cy} = 0.65\)</span>, which corresponds to the observed correlation between brain size and gender in our dataset. To generate synthetic data with this prespecified structure, we generated (1) a data matrix <span class="math inline">\(X\)</span> of shape <span class="math inline">\(N\times K\)</span>, (2) a target variable <span class="math inline">\(y\)</span> of shape <span class="math inline">\(N \times 1\)</span>, and (3) a confound variable <span class="math inline">\(C\)</span> of shape <span class="math inline">\(N \times P\)</span>. For all simulations, we used the following parameters: <span class="math inline">\(N = 200\)</span>, <span class="math inline">\(K = 5\)</span>, and <span class="math inline">\(P = 1\)</span> (i.e., a single confound variable). We generated <span class="math inline">\(y\)</span> as a categorical variable with binary values, <span class="math inline">\(y \in \{0, 1\}\)</span>, with equal class probabilities (i.e., 50%), given that most decoding studies focus on binary classification. We generated <span class="math inline">\(C\)</span> as a continuous random variable drawn from a standard normal distribution. We generated each feature <span class="math inline">\(X_{j}\)</span> as a linear combination of <span class="math inline">\(y\)</span> and <span class="math inline">\(C\)</span> plus Gaussian noise. Thus, for each predictor <span class="math inline">\(j = 1 \dots K\)</span> in <span class="math inline">\(X_{j}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
X_{j} = \beta_{y}y + \beta_{C}C + \epsilon, \epsilon \sim \mathcal{N}(0, \gamma)
\end{equation}\]</span></p>
<p>in which <span class="math inline">\(\beta_{y}\)</span> represented the weight given to <span class="math inline">\(y\)</span>, and <span class="math inline">\(\beta_{C}\)</span> represented the weight given to <span class="math inline">\(C\)</span> in the generation of the feature <span class="math inline">\(X_{j}\)</span>, and <span class="math inline">\(\mathcal{N}(0, \gamma)\)</span> is the normal distribution with zero mean and standard deviation <span class="math inline">\(\gamma\)</span>. The parameters <span class="math inline">\(\beta_{y}\)</span> and <span class="math inline">\(\beta_{C}\)</span> were both initialized with a value of 1. First, if the difference between the total variance explained and the sum of the desired signal <span class="math inline">\(R^2\)</span> and confound <span class="math inline">\(R^2\)</span> values was larger than 0.01, the standard deviation of the normal distribution from which the errors were drawn (i.e., <span class="math inline">\(\gamma\)</span>) was adjusted (decreased with 0.01 when the total <span class="math inline">\(R^2\)</span> is too low, increased with 0.01 when the total <span class="math inline">\(R^2\)</span> is too high), after which was generated again. This process was iterated until the target total <span class="math inline">\(R^2\)</span> value is found. Then, the total variance explained was partitioned into confound <span class="math inline">\(R^2\)</span> and signal <span class="math inline">\(R^2\)</span>. If one or both of these values differed from the targeted values by more than 0.01, the generative parameters <span class="math inline">\(\beta_{y}\)</span> and <span class="math inline">\(\beta_{C}\)</span> were adjusted: if signal <span class="math inline">\(R^2\)</span> is too low, was increased with 0.01, and decreased with 0.01 otherwise. If confound <span class="math inline">\(R^2\)</span> is too low, <span class="math inline">\(\beta_{C}\)</span> was increased with 0.01, and decreased with 0.01 otherwise. After adjusting these parameters, <span class="math inline">\(X_{j}\)</span> was generated again. This process was iterated until the data contain the desired “true signal” and “confounded signal”.</p>
<p>We evaluated the different methods for confound control for two values of signal <span class="math inline">\(R^2\)</span> (0.004, representing plausible null data,<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> and 0.1, representing a plausible true effect) and a range of confound <span class="math inline">\(R^2\)</span> values (in steps of 0.05: <span class="math inline">\(0.00, 0.05, 0.10, \dots , 0.35\)</span>). This simulation was iterated 10 times (with different partitions of the folds) to ensure the results were not influenced by random noise. Importantly, the specific scenario in which confound <span class="math inline">\(R^2\)</span> equals 0, which represents data without any confounded signal (<span class="math inline">\(r_{yX}^2\)</span>), served as “reference model performance” to which we can compare the efficacy the confound control methods. This comparison allowed us to detect potential biases.</p>
<p>After the data were generated, a baseline model (no confound control) and the three methods outlined above (post hoc counterbalancing, WDCR, and CVCR) were applied to the simulated data using the standard pipeline described in the <a href="confounds-decoding.html#confounds-decoding-methods-pipeline">Decoding pipeline</a> section (but without univariate feature selection) and compared to the reference performance.</p>
</div>
<div id="analysis-of-positive-bias-after-post-hoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.2.4.2</span> Analysis of positive bias after post hoc counterbalancing</h4>
<p>As detailed below, post hoc counterbalancing did not lead to the expected decrease in model performance; instead, there appeared to be a trend towards an <em>increase</em> in model performance. To further investigate the cause of this unexpected result, we simulated a multivariate normal dataset with three variables, reflecting our data (<span class="math inline">\(X\)</span>), target (<span class="math inline">\(y\)</span>), and confound (<span class="math inline">\(C\)</span>), with 1000 samples (<span class="math inline">\(N\)</span>) and a single feature (<span class="math inline">\(K = 1\)</span>). We iterated this data generation process 1000 times and subsequently selected the dataset which yielded the largest (positive) difference between model performance after post hoc counterbalancing versus no confound control. In other words, we used the dataset in which the counterbalancing issue was most apparent. While not necessarily representative of typical (neuroimaging) datasets, this process allowed us to explain and visualize how it is possible that model performance increases after counterbalancing the data.</p>
<p>To generate data from a multivariate normal distribution, we first generated variance-covariance matrices with unit variance for all variables, so that covariances can be interpreted as correlations. The covariances in the matrix were generated as pairwise correlations (<span class="math inline">\(r_{yX}\)</span>, <span class="math inline">\(r_{Cy}\)</span>, <span class="math inline">\({r_CX}\)</span>), each sampled from a uniform distribution with range <span class="math inline">\([-0.65, 0.65]\)</span>. We generated data using such prespecified correlation structure because the relative increase in model performance after counterbalancing did not appear to occur when generating completely random (normally distributed) data. Moreover, we restricted the range of the uniform distribution from which the pairwise correlations are drawn to <span class="math inline">\([-0.65, 0.65]\)</span> because a larger range can result in covariance matrices that are not positive-semidefinite. After generating the three variables, we binarized the target variable (<span class="math inline">\(y\)</span>) using a mean-split (<span class="math inline">\(y = 0\)</span> if <span class="math inline">\(y &lt; \bar{y}\)</span>, <span class="math inline">\(y = 1\)</span> otherwise) to frame the analysis as a classification problem rather than a regression problem.</p>
<p>We then subsampled the selected dataset using our post hoc counterbalancing algorithm and subsequently ran the decoding pipeline (without univariate feature selection) on the subsampled (“retained”) data in a 10-fold stratified cross-validation scheme. Notably, we cross-validated our fitted pipeline not only to the left-out <em>retained</em> data, but also to the data that did not survive the subsampling procedure (the <em>rejected</em> data; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-3">3.3</a>). Across the 10 folds, we kept track of two statistics from the retained and rejected samples: (1) the classification performance, and (2) the signed distance to the decision boundary. Negative distances in binary classification (in simple binary classification with <span class="math inline">\(y \in \{0, 1\}\)</span>) reflect a prediction of the sample as <span class="math inline">\(y = 0\)</span>, while positive distances reflect a prediction of the sample as <span class="math inline">\(y = 1\)</span>. As such, a correctly classified sample of class 0 has a negative distance from the decision boundary, while a correctly classified sample of class 1 has a positive distance from the decision boundary. Here, however, we wanted to count the distance of samples that are on the “incorrect” side of the decision boundary as <em>negative</em> distances, while counting the distance of samples that are on the “correct” side of the decision boundary as positive distances. To this end, we used a “re-coded” version of the target variable (<span class="math inline">\(y^{*} = -1\)</span> if <span class="math inline">\(y = 0\)</span>, <span class="math inline">\(y^{*} = 1\)</span> otherwise) and multiplied it with the distance. Consequently, negative distances of <em>correct</em> samples of condition 0 become positive and positive distances of <em>incorrect</em> samples of condition 0 become negative (by multiplying them by <span class="math inline">\(-1\)</span>). As such, we calculated the signed distance from the decision boundary (<span class="math inline">\(\delta_{i}\)</span>) for any sample <span class="math inline">\(i\)</span> as:</p>
<p><span class="math display">\[\begin{equation}
\delta_{i} = y^{*}(w^{T}X_{i} + b)
\end{equation}\]</span></p>
<p>in which <span class="math inline">\(w\)</span> refers to the feature weights (coefficients) and <span class="math inline">\(b\)</span> refers to the intercept term. Any differences in these two statistics (proportion correctly classified and signed distance to the classification boundary) between the retained and rejected samples may signify biases in model performance estimates (i.e., better cross-validated model performance on the retained data than on the rejected data would confirm positive bias, as it indicates that subsampling tends to reject hard-to-classify samples). We applied this analysis also to the empirical data (separately for the different values of <span class="math inline">\(K\)</span>) to show that the effect of counterbalancing, as demonstrated using simulated data, also occurs in the empirical data.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-3"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_3.png" alt="Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates."  />
<p class="caption">
Figure 3.3: Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.
</p>
</div>

</div>
<div id="analysis-of-negative-bias-after-wdcr" class="section level4">
<h4><span class="header-section-number">3.2.4.3</span> Analysis of negative bias after WDCR</h4>
<p>As also detailed below, WDCR can lead to significantly below chance accuracy. To investigate the cause of this below chance performance (and to demonstrate that CVCR does not lead to such results), we performed two follow-up simulations. The first follow-up simulation shows that the occurrence of below chance accuracy depends on the distribution of feature-target correlations <span class="citation">(<span class="math inline">\(r_{yX}\)</span>; see for a similar argument Jamalabadi et al., <a href="bibliography.html#ref-jamalabadi2016classification" role="doc-biblioref">2016</a>)</span>, and the second follow-up simulation shows that WDCR artificially narrows this distribution. This artificial narrowing of the distribution is exacerbated both by an increasing number of features (<span class="math inline">\(K\)</span>), as well as higher correlations between the target and confound (<span class="math inline">\(r_{Cy}\)</span>).</p>
<p>In the first simulation, we simulated random null data (drawn from a standard normal distribution) with 100 samples (<span class="math inline">\(N\)</span>) and 200 features (<span class="math inline">\(K\)</span>), as well as a binary target feature (<span class="math inline">\(y \in \{0, 1\}\)</span>). We then calculated the cross-validated prediction accuracy using the standard pipeline (without univariate feature selection) described in the <a href="confounds-decoding.html#confounds-decoding-methods-pipeline">Decoding pipeline</a> section; we iterate this process 500 times. Then, we show that the variance of the cross-validated accuracy is accurately predicted by the standard deviation (i.e., “width”) of the distribution of correlations between the features and the target (<span class="math inline">\(r_{yX_{j}}\)</span> with <span class="math inline">\(j = 1 \dots K\)</span>), which we will denote by <span class="math inline">\(sd(r_{yX})\)</span>. Importantly, we show that below chance accuracy likely occurs when the standard deviation of the feature-target correlation distribution is lower than the standard deviation of the sampling distribution of the Pearson correlation coefficient parameterized with the same number of samples (<span class="math inline">\(N = 200\)</span>) and the same effect size (i.e., <span class="math inline">\(\rho = 0\)</span>, because we simulated random null data). The sampling distribution of the Pearson correlation coefficient is described by <span class="citation">Kendall &amp; Stuart (<a href="bibliography.html#ref-kendall1973functional" role="doc-biblioref">1973</a>)</span>. When <span class="math inline">\(\rho = 0\)</span> (as in our simulations), the equation is as follows:</p>
<p><span class="math display">\[\begin{equation}
f(r; N) = (1 -r^2)^{\frac{N-4}{2}}[\mathcal{B}](\frac{1}{2}, \frac{N-2}{2})^{-1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}(a, b)\)</span> represents the Beta-function.</p>
<p>Then, in a second simulation, we similarly simulated null data as in the previous simulation, but now we also generate a continuous confound (<span class="math inline">\(C\)</span>) with a varying correlation with the target (<span class="math inline">\(r_{Cy} \in \{0.0, 0.1, 0.2, \dots, 1.0\}\)</span>). Before subjecting the data to the decoding pipeline, we regressed out the confound from the data (i.e., WDCR). We did this for different numbers of features (<span class="math inline">\(K \in \{1, 5, 10, 50, 100, 500, 1000\}\)</span>). Then, we applied CVCR on the simulated data as well for comparison.</p>
</div>
</div>
</div>
<div id="results" class="section level2">
<h2><span class="header-section-number">3.3</span> Results</h2>
<div id="influence-of-brain-size" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Influence of brain size</h3>
<p>Before evaluating the different methods for confound control, we determined whether brain size is truly a confound given our proposed definition (“a variable that is not of primary interest, correlates with the target, and is encoded in the neuroimaging data”). We evaluated the relationship between the target and the confound in two ways. First, we calculated the (point-biserial) correlation between gender and brain size, which was significant for both the estimation based on white matter, <span class="math inline">\(r(216) = .645, p &lt; 0.001\)</span>, and the estimation based on grey matter, <span class="math inline">\(r(216) = .588, p &lt; 0.001\)</span>, corroborating the findings by <span class="citation">Smith &amp; Nichols (<a href="bibliography.html#ref-Smith2018-th" role="doc-biblioref">2018</a>)</span>. Second, as recommended by <span class="citation">Görgen et al. (<a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>, who argue that the potential influence of confounds can be discovered by running a classification analysis using the confound as the (single) feature predicting the target, we ran our decoding pipeline (without univariate feature selection) using brain size as a single feature to predict gender. This analysis yielded a mean classification performance (<span class="math inline">\(F_{1}\)</span> score) of 0.78 (<em>SD</em> = .10) when using brain size estimated from white matter and 0.81 (<em>SD</em> = .09) when using brain size estimated from gray matter, which are both significant with <span class="math inline">\(p &lt; 0.001\)</span> (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-4">3.4</a>A).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-4"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_4.png" alt="A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual \(F_{1}\) scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (\(r_{XC}\)), overlayed with the analytic sampling distribution of correlation coefficients when \(\rho = 0\) and \(N = 217\), for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule (Scott, 1979) for bandwidth selection."  />
<p class="caption">
Figure 3.4: A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual <span class="math inline">\(F_{1}\)</span> scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels (<span class="math inline">\(r_{XC}\)</span>), overlayed with the analytic sampling distribution of correlation coefficients when <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 217\)</span>, for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott’s rule <span class="citation">(Scott, <a href="bibliography.html#ref-scott1979optimal" role="doc-biblioref">1979</a>)</span> for bandwidth selection.
</p>
</div>

<p>To estimate whether brain size is encoded in the neuroimaging data, we compared the distribution of bivariate correlation coefficients (of each voxel with brain size) with the sampling distribution of correlation coefficients when <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 217\)</span> (see section <a href="confounds-decoding.html#confounds-decoding-results-wdcr-bias">Analysis of negative bias after WDCR</a> for details). Under the null hypothesis that there are no correlations between brain size and voxel intensities, each individual correlation coefficient between a voxel and the confound can be regarded as an independent sample with <span class="math inline">\(N = 217\)</span> (ignoring correlations between voxels for simplicity). Because <span class="math inline">\(K\)</span> is very large for both the VBM and TBSS data, the empirical distribution of correlation coefficients should, under the null hypothesis, approach the analytic distribution of correlation coefficients parametrized by <span class="math inline">\(\rho = 0\)</span> and <span class="math inline">\(N = 217\)</span>. Contrarily, the density plots in Fig. <a href="confounds-decoding.html#fig:fig-confounds-decoding-4">3.4</a>B clearly show that the observed correlation coefficients distribution does not follow the sampling distribution (with both an increase in variance and a shift of the mode). This indicates that at least some of the correlation coefficients between voxel intensities and brain size are extremely unlikely under the null hypothesis. Note that this interpretation is contingent on the assumption that the relation between brain size and VBM/TBSS data is linear. In the Supplementary Materials and Results (Supplementary Figures <a href="#fig:fig-confounds-decoding-S7"><strong>??</strong></a>-<a href="#fig:fig-confounds-decoding-S9"><strong>??</strong></a>), we provide some evidence for the validity of this assumption.</p>
</div>
<div id="baseline-model-no-confound-control" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Baseline model: no confound control</h3>
<p>In our baseline model on the empirical data, for different numbers of voxels, we predicted gender from structural MRI data (VBM and TBSS) without controlling for brain size (see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-5">3.5</a>). The results show significant above chance performance of the MVPA pipeline based on both the VBM data and the TBSS data. All performance scores averaged across folds were significant (<span class="math inline">\(p &lt; 0.001\)</span>).</p>
<div class="figure"><span id="fig:fig-confounds-decoding-5"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_5.png" alt="Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average \(F_{1}\) score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = \(p &lt; 0.001\), ** = \(p &lt; 0.01\), * = \(p &lt; 0.05\)."  />
<p class="caption">
Figure 3.5: Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average <span class="math inline">\(F_{1}\)</span> score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = <span class="math inline">\(p &lt; 0.001\)</span>, ** = <span class="math inline">\(p &lt; 0.01\)</span>, * = <span class="math inline">\(p &lt; 0.05\)</span>.
</p>
</div>

</div>
<div id="post-hoc-counterbalancing" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Post hoc counterbalancing</h3>
<div id="empirical-results" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Empirical results</h4>
</div>
<div id="efficacy-analysis" class="section level4">
<h4><span class="header-section-number">3.3.3.2</span> Efficacy analysis</h4>
</div>
<div id="analysis-of-positive-bias-after-post-hoc-counterbalancing-1" class="section level4">
<h4><span class="header-section-number">3.3.3.3</span> Analysis of positive bias after post hoc counterbalancing</h4>
</div>
</div>
<div id="whole-dataset-confound-regression-wdcr" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Whole-dataset confound regression (WDCR)</h3>
<div id="empirical-results-1" class="section level4">
<h4><span class="header-section-number">3.3.4.1</span> Empirical results</h4>
</div>
<div id="efficacy-analysis-1" class="section level4">
<h4><span class="header-section-number">3.3.4.2</span> Efficacy analysis</h4>
</div>
<div id="confounds-decoding-results-wdcr-bias" class="section level4">
<h4><span class="header-section-number">3.3.4.3</span> Analysis of negative bias after WDCR</h4>
</div>
</div>
<div id="cross-validated-confound-regression-cvcr" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Cross-validated confound regression (CVCR)</h3>
<div id="empirical-results-2" class="section level4">
<h4><span class="header-section-number">3.3.5.1</span> Empirical results</h4>
</div>
<div id="efficacy-analysis-2" class="section level4">
<h4><span class="header-section-number">3.3.5.2</span> Efficacy analysis</h4>
</div>
</div>
<div id="summary-methods-for-confound-control" class="section level3">
<h3><span class="header-section-number">3.3.6</span> Summary methods for confound control</h3>
</div>
</div>
<div id="confounds-decoding-discussion" class="section level2">
<h2><span class="header-section-number">3.4</span> Discussion</h2>
<div id="relevance-and-consequences-for-previous-and-future-research" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Relevance and consequences for previous and future research</h3>
<div id="a-priori-and-post-hoc-counterbalancing" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> A priori and post hoc counterbalancing</h4>
</div>
<div id="confound-regression-1" class="section level4">
<h4><span class="header-section-number">3.4.1.2</span> Confound regression</h4>
</div>
<div id="relevance-to-other-analysis-methods" class="section level4">
<h4><span class="header-section-number">3.4.1.3</span> Relevance to other analysis methods</h4>
</div>
<div id="importance-for-gender-decoding-studies" class="section level4">
<h4><span class="header-section-number">3.4.1.4</span> Importance for gender decoding studies</h4>
</div>
</div>
<div id="choosing-a-confound-model-linear-vs.-nonlinear-models" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Choosing a confound model: linear vs. nonlinear models</h3>
</div>
<div id="practical-recommendations" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Practical recommendations</h3>
</div>
</div>
<div id="conclusions" class="section level2">
<h2><span class="header-section-number">3.5</span> Conclusions</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The terms “gender” and “sex” are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.<a href="confounds-decoding.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration “squeezes” relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa <span class="citation">(Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span>. This effect of global brain size similarly affects functional MRI analyses <span class="citation">(Brodtmann et al., <a href="bibliography.html#ref-brodtmann2009regional" role="doc-biblioref">2009</a>)</span>.<a href="confounds-decoding.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant’s gender.<a href="confounds-decoding.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In the context of behavioral data, a priori counterbalancing is often called “matching” or a employing a “case-control design” @[Cook2002-hb].<a href="confounds-decoding.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Note that the counterbalancing process is the same for both traditional univariate (activation-based) studies and decoding studies, but the direction of analysis is reversed in univariate (e.g., gender → brain) and decoding studies (e.g., brain → gender). As such, in univariate studies the confound (e.g., brain size) is counterbalanced with respect to the predictor(s) (e.g., gender) while in decoding studies the confound (e.g., brain size) is counterbalanced with respect to the target (e.g., gender).<a href="confounds-decoding.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>However, parameter estimates only reflect unique variance when ordinary, weighted, or generalized least squares is used to find the model parameters. Other (regularized) linear models, such as ridge regression or LASSO, are not guaranteed to yield parameters that explain unique proportions of variance.<a href="confounds-decoding.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span>, here, refer to (usually HRF-convolved) predictors of the time series signal (<span class="math inline">\(s\)</span>) for a single voxel. In the rest of the article, <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> refer to features that are defined across samples (not time).<a href="confounds-decoding.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Note that, technically, one could use the “Control for confounds during pattern estimation” method in electrophysiology as well, by first fitting a univariate model explaining the neuroimaging data (<span class="math inline">\(X_{j}\)</span> for <span class="math inline">\(j = 1 \dots K\)</span>) as a function of both the target (<span class="math inline">\(y\)</span>) and the confound (<span class="math inline">\(C\)</span>) and subsequently only using the parameter estimates of the target-predictor (<span class="math inline">\(\hat{\beta}_{x}\)</span>) as patterns in the subsequent decoding analysis.<a href="confounds-decoding.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Note that we did not discuss studies that implement a different confound regression procedure <span class="citation">(e.g., Abdulkadir et al., <a href="bibliography.html#ref-Abdulkadir2014-bh" role="doc-biblioref">2014</a>; Dukart et al., <a href="bibliography.html#ref-Dukart2011-aq" role="doc-biblioref">2011</a>)</span>, in which confound regression is only estimated on the samples from a single class of the target variable (e.g., in our gender decoding example, this would mean that confound regression models are only estimated on the data from male, or female, subjects). As this form of confound regression does not disambiguate the sources of information driving the decoder, it is not discussed further in this article.<a href="confounds-decoding.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>For continuous confounds, it is practically impossible to achieve a correlation with the target of <em>exactly</em> zero, which is the reason we subsample until it is smaller than a prespecified threshold. For categorical confounds, however, a correlation between the confound and the target of exactly zero is possible <span class="citation">(this amounts to equal proportions of levels of <span class="math inline">\(c\)</span> within each class of <span class="math inline">\(y\)</span>; Görgen et al., <a href="bibliography.html#ref-Gorgen2017-sy" role="doc-biblioref">2017</a>)</span>, even <em>necessary</em>, because it is impossible to find a (<em>K</em>-fold) cross-validation partitioning in which each split is counterbalanced w.r.t. the confound if the correlation <em>in the entire dataset</em> between the target and the confound is not zero.<a href="confounds-decoding.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>We refer to a relatively high α as “strict”, here, because we use it here for the purpose of demonstrating no effect.<a href="confounds-decoding.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>One could run the “random subsampling” procedure with more than 10,000 draws in order to reduce the aforementioned power loss; but in the extreme, this would result in the same optimal subsample that can be found much faster by targeted subsampling.<a href="confounds-decoding.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Note that plausible null data do not reflect a signal <span class="math inline">\(R^2\)</span> of 0, because this statistic is biased towards values larger than 0 (because it represents a squared number) when dealing with noisy data, hence our choice of signal <span class="math inline">\(R^2 = 0.004\)</span>.<a href="confounds-decoding.html#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shared-states.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="AOMIC.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
