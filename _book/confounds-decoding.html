<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain</title>
  <meta name="description" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 How to control for confounds in decoding analyses of neuroimaging data | Learning from the brain" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shared-states.html"/>
<link rel="next" href="AOMIC.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-brain-is-not-a-dictionary"><i class="fa fa-check"></i><b>1.1</b> The brain is not a dictionary</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-brain-probably-does-not-care-about-your-hypothesis"><i class="fa fa-check"></i><b>1.2</b> The brain (probably) does not care about your hypothesis</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretability-and-prediction-are-a-trade-off-for-now"><i class="fa fa-check"></i><b>1.3</b> Interpretability and prediction are a trade-off (for now)</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#exploration-should-be-embraced-more"><i class="fa fa-check"></i><b>1.4</b> Exploration should be embraced more</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#proper-generalization-is-hard"><i class="fa fa-check"></i><b>1.5</b> Proper generalization is hard</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#psychology-is-complex-so-it-needs-complex-models"><i class="fa fa-check"></i><b>1.6</b> Psychology is complex, so it needs complex models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
<li class="chapter" data-level="2.2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.2.4</b> Image acquisition</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.3</b> Model optimization procedure</a><ul>
<li class="chapter" data-level="2.3.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.3.1</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.3.2</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.3.3</b> Additional analyses</a></li>
<li class="chapter" data-level="2.3.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.3.4</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.3.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.3.5</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-results"><i class="fa fa-check"></i><b>2.4</b> Results</a><ul>
<li class="chapter" data-level="2.4.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-mvpa"><i class="fa fa-check"></i><b>2.4.1</b> Multi-voxel pattern analysis</a></li>
<li class="chapter" data-level="2.4.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-results-univariate"><i class="fa fa-check"></i><b>2.4.2</b> Univariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-acknowledgements"><i class="fa fa-check"></i><b>2.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a><ul>
<li class="chapter" data-level="3.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-true-vs-confounded"><i class="fa fa-check"></i><b>3.1.1</b> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</a></li>
<li class="chapter" data-level="3.1.2" data-path="confounds-decoding.html"><a href="confounds-decoding.html#confounds-decoding-introduction-methods-confound-control"><i class="fa fa-check"></i><b>3.1.2</b> Methods for confound control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="AOMIC.html"><a href="AOMIC.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a></li>
<li class="chapter" data-level="5" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a></li>
<li class="chapter" data-level="6" data-path="au-limitations.html"><a href="au-limitations.html"><i class="fa fa-check"></i><b>6</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a></li>
<li class="chapter" data-level="7" data-path="facial-expression-models.html"><a href="facial-expression-models.html"><i class="fa fa-check"></i><b>7</b> Comparing models of dynamic facial expression perception</a></li>
<li class="chapter" data-level="8" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html"><i class="fa fa-check"></i><b>8</b> Summary and general discussion</a><ul>
<li class="chapter" data-level="8.1" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#explore"><i class="fa fa-check"></i><b>8.1</b> Explore!</a></li>
<li class="chapter" data-level="8.2" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#think-big"><i class="fa fa-check"></i><b>8.2</b> Think <em>big</em></a></li>
<li class="chapter" data-level="8.3" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#rethink-psychology-education"><i class="fa fa-check"></i><b>8.3</b> Rethink psychology education</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a><ul>
<li class="chapter" data-level="A.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#stimuli-used-for-sf-task"><i class="fa fa-check"></i><b>A.1</b> Stimuli used for SF-task</a></li>
<li class="chapter" data-level="A.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#instructions"><i class="fa fa-check"></i><b>A.2</b> Instructions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-other-focused-emotion-understanding-task"><i class="fa fa-check"></i><b>A.2.1</b> Full instruction for the other-focused emotion understanding task</a></li>
<li class="chapter" data-level="A.2.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-self-focused-emotion-imagery-task"><i class="fa fa-check"></i><b>A.2.2</b> Full instruction for the self-focused emotion imagery task</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#behavioral-results"><i class="fa fa-check"></i><b>A.3</b> Behavioral results</a></li>
<li class="chapter" data-level="A.4" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#optimization-results"><i class="fa fa-check"></i><b>A.4</b> Optimization results</a></li>
<li class="chapter" data-level="A.5" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#bagging-procedure"><i class="fa fa-check"></i><b>A.5</b> Bagging procedure</a></li>
<li class="chapter" data-level="A.6" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#precision-vs.-recall"><i class="fa fa-check"></i><b>A.6</b> Precision vs. recall</a></li>
<li class="chapter" data-level="A.7" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#self-vs.-other-classification"><i class="fa fa-check"></i><b>A.7</b> Self vs. other classification</a></li>
<li class="chapter" data-level="A.8" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#condition-average-results"><i class="fa fa-check"></i><b>A.8</b> Condition-average results</a></li>
<li class="chapter" data-level="A.9" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#individual-subject-scores"><i class="fa fa-check"></i><b>A.9</b> Individual subject scores</a></li>
<li class="chapter" data-level="A.10" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#brain-region-importance"><i class="fa fa-check"></i><b>A.10</b> Brain region importance</a></li>
<li class="chapter" data-level="A.11" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#general-note-about-tables-with-voxel-coordinates"><i class="fa fa-check"></i><b>A.11</b> General note about tables with voxel-coordinates</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="C" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="D" data-path="au-limitations-supplement.html"><a href="au-limitations-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="E" data-path="facial-expression-models-supplement.html"><a href="facial-expression-models-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="F" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>F</b> Data, code and materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning from the brain</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confounds-decoding" class="section level1">
<h1><span class="header-section-number">3</span> How to control for confounds in decoding analyses of neuroimaging data</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Snoek, L.*, Miletić, S.*, &amp; Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. <em>NeuroImage</em>, 184, 741-760.</p>
<p>* Shared first authorship</p>

<p><strong>Abstract</strong></p>
<p>Over the past decade, multivariate “decoding analyses” have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound “brain size”. We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one’s decoding analysis.</p>
<div id="confounds-decoding-introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data <span class="citation">(Haxby, <a href="bibliography.html#ref-Haxby2012-sd" role="doc-biblioref">2012</a>; Norman et al., <a href="bibliography.html#ref-Norman2006-bt" role="doc-biblioref">2006</a><a href="bibliography.html#ref-Norman2006-bt" role="doc-biblioref">b</a>)</span>. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is “decoding”, in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns <span class="citation">(Haxby et al., <a href="bibliography.html#ref-Haxby2001-os" role="doc-biblioref">2001</a>)</span> and working memory representations from EEG data <span class="citation">(LaRocque et al., <a href="bibliography.html#ref-LaRocque2013-sh" role="doc-biblioref">2013</a>)</span>, as well between-subject factors such as Alzheimer’s disease (vs. healthy controls) from structural MRI data <span class="citation">(Cuingnet et al., <a href="bibliography.html#ref-Cuingnet2011-hv" role="doc-biblioref">2011</a>)</span> and major depressive disorder (vs. healthy controls) from resting-state functional connectivity <span class="citation">(Craddock et al., <a href="bibliography.html#ref-Craddock2009-kz" role="doc-biblioref">2009</a>)</span>. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do <span class="citation">(Jimura &amp; Poldrack, <a href="bibliography.html#ref-Jimura2012-lv" role="doc-biblioref">2012</a>)</span>. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>.</p>
<p>In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical <span class="citation">(Allefeld et al., <a href="bibliography.html#ref-Allefeld2016-xp" role="doc-biblioref">2016</a>; Davis et al., <a href="bibliography.html#ref-Davis2014-lw" role="doc-biblioref">2014</a>; Gilron et al., <a href="bibliography.html#ref-Gilron2017-tl" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>)</span> and more conceptual <span class="citation">(Naselaris &amp; Kay, <a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span> in nature. For the purposes of the current study, we focus on the specific criticism put forward by <span class="citation">Naselaris &amp; Kay (<a href="bibliography.html#ref-Naselaris2015-jn" role="doc-biblioref">2015</a>)</span> , who argue that decoding analyses are inherently ambiguous in terms of what information they use <span class="citation">(see Popov et al., <a href="bibliography.html#ref-popov2018practices" role="doc-biblioref">2018</a> for a similar argument in the context of encoding analyses)</span>. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information <span class="citation">(see also Carlson &amp; Wardle, <a href="bibliography.html#ref-Carlson2015-bz" role="doc-biblioref">2015</a>; Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>; Weichwald et al., <a href="bibliography.html#ref-Weichwald2015-aj" role="doc-biblioref">2015</a>)</span>. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of “source ambiguity”.</p>
<p>To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> (male/female) from structural MRI with the aim of contributing to the understanding of gender differences — an endeavor that generated considerable interest and controversy <span class="citation">(Chekroud et al., <a href="bibliography.html#ref-Chekroud2016-tc" role="doc-biblioref">2016</a>; Del Giudice et al., <a href="bibliography.html#ref-Del_Giudice2016-ns" role="doc-biblioref">2016</a>; Glezerman, <a href="bibliography.html#ref-Glezerman2016-xl" role="doc-biblioref">2016</a>; Joel &amp; Fausto-Sterling, <a href="bibliography.html#ref-Joel2016-uo" role="doc-biblioref">2016</a>; Rosenblatt, <a href="bibliography.html#ref-Rosenblatt2016-oy" role="doc-biblioref">2016</a>)</span>. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant’s gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways <span class="citation">(Good et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a>; O’Brien et al., <a href="bibliography.html#ref-OBrien2011-lj" role="doc-biblioref">2011</a>)</span>. First, there is a <em>global</em> difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight <span class="citation">(18.7%; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Lüders et al., <a href="bibliography.html#ref-Luders2002-ms" role="doc-biblioref">2002</a>)</span><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. Second, brains of men and women are known to differ <em>locally</em>: some specific brain areas are on average larger in women than in men <span class="citation">(e.g., in superior and middle temporal cortex; Good et al., <a href="bibliography.html#ref-Good2001-ak" role="doc-biblioref">2001</a>)</span> and vice versa <span class="citation">(e.g., in frontomedial cortex; Goldstein et al., <a href="bibliography.html#ref-Goldstein2001-dy" role="doc-biblioref">2001</a>)</span>. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male <em>bodies</em> are on average larger than female bodies <span class="citation">(Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Sepehrband et al., <a href="bibliography.html#ref-Sepehrband2018-dy" role="doc-biblioref">2018</a>)</span>. As such, our hypothetical researcher is likely primarily interested in the <em>local</em> sources of variation in the neuroanatomy of male and female brains.</p>
<p>Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for “uninteresting” sources of information.</p>
<div id="confounds-decoding-introduction-true-vs-confounded" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Partitioning effects into <em>true</em> signal and <em>confounded</em> signal</h3>
<p>Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis <span class="citation">(Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate <em>prediction</em>, but not in <em>interpretability</em> of the model <span class="citation">(Bzdok, <a href="bibliography.html#ref-Bzdok2017-li" role="doc-biblioref">2017</a>; Haufe et al., <a href="bibliography.html#ref-Haufe2014-el" role="doc-biblioref">2014</a>; Hebart &amp; Baker, <a href="bibliography.html#ref-Hebart2017-jn" role="doc-biblioref">2017</a>)</span>. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is <em>only</em> using information in the neuroimaging data that is related to the variable that is being decoded <span class="citation">(Ritchie et al., <a href="bibliography.html#ref-Ritchie2017-gl" role="doc-biblioref">2017</a>)</span>. In this scenario, source ambiguity (i.e., the presence of <em>multiple</em> sources of information) <em>is</em> problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is <em>confounding</em> the decoding analysis of gender. Here, we define a confound as <em>a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.</em></p>
<p>To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable “schizophrenia-or-not” is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender <span class="citation">(i.e., men are more often diagnosed with schizophrenia than women; McGrath et al., <a href="bibliography.html#ref-McGrath2008-oj" role="doc-biblioref">2008</a>)</span> and substance abuse <span class="citation">(Dixon, <a href="bibliography.html#ref-Dixon1999-kl" role="doc-biblioref">1999</a>)</span>, which are also known to affect gray matter <span class="citation">(Bangalore et al., <a href="bibliography.html#ref-Bangalore2008-kc" role="doc-biblioref">2008</a>; Gur et al., <a href="bibliography.html#ref-Gur1999-qj" role="doc-biblioref">1999</a>; Van Haren et al., <a href="bibliography.html#ref-Van_Haren2013-iv" role="doc-biblioref">2013</a>)</span>. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate <em>prediction</em> of schizophrenia, then this ambiguity due to confounding sources of information is problematic.</p>
<p>Importantly, as our definition suggests, what <em>is</em> or <em>is not</em> regarded as a confound is relative — it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable “schizophrenia-or-no”” becomes the confounding variable. In other words, one researcher’s signal is another researcher’s confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the “true signal” (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the “confounded signal” (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure <a href="confounds-decoding.html#fig:fig-confounds-decoding-1">3.1</a>). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.</p>
<div class="figure"><span id="fig:fig-confounds-decoding-1"></span>
<img src="_bookdown_files/confounds-decoding-files/figures/figure_1.png" alt="Visualization of how variance in brain data (X) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (X), the confound (C), and the target (y). Overlapping circles indicate a non-zero (squared) correlation between the two variables."  />
<p class="caption">
Figure 3.1: Visualization of how variance in brain data (<em>X</em>) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (<em>X</em>), the confound (<em>C</em>), and the target (<em>y</em>). Overlapping circles indicate a non-zero (squared) correlation between the two variables.
</p>
</div>

<p>In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.</p>
</div>
<div id="confounds-decoding-introduction-methods-confound-control" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Methods for confound control</h3>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The terms “gender” and “sex” are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.<a href="confounds-decoding.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration “squeezes” relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa <span class="citation">(Douaud et al., <a href="bibliography.html#ref-Douaud2007-sw" role="doc-biblioref">2007</a>)</span>. This effect of global brain size similarly affects functional MRI analyses <span class="citation">(Brodtmann et al., <a href="bibliography.html#ref-brodtmann2009regional" role="doc-biblioref">2009</a>)</span>.<a href="confounds-decoding.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant’s gender.<a href="confounds-decoding.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shared-states.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="AOMIC.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
