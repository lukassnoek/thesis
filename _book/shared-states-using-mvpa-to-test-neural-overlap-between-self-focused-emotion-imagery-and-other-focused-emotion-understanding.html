<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain</title>
  <meta name="description" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="how-to-control-for-confounds-in-decoding-analyses-of-neuroimaging-data.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#learning-from-the-brain"><i class="fa fa-check"></i><b>1.1</b> Learning from the brain</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states-using-mvpa-to-test-neural-overlap-between-self-focused-emotion-imagery-and-other-focused-emotion-understanding.html"><a href="shared-states-using-mvpa-to-test-neural-overlap-between-self-focused-emotion-imagery-and-other-focused-emotion-understanding.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states-using-mvpa-to-test-neural-overlap-between-self-focused-emotion-imagery-and-other-focused-emotion-understanding.html"><a href="shared-states-using-mvpa-to-test-neural-overlap-between-self-focused-emotion-imagery-and-other-focused-emotion-understanding.html#Sharedstates-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="how-to-control-for-confounds-in-decoding-analyses-of-neuroimaging-data.html"><a href="how-to-control-for-confounds-in-decoding-analyses-of-neuroimaging-data.html"><i class="fa fa-check"></i><b>3</b> How to control for confounds in decoding analyses of neuroimaging data</a></li>
<li class="chapter" data-level="4" data-path="the-amsterdam-open-mri-collection-a-set-of-multimodal-mri-datasets-for-individual-difference-analyses.html"><a href="the-amsterdam-open-mri-collection-a-set-of-multimodal-mri-datasets-for-individual-difference-analyses.html"><i class="fa fa-check"></i><b>4</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a></li>
<li class="chapter" data-level="5" data-path="choosing-to-view-morbid-information-involves-reward-circuitry.html"><a href="choosing-to-view-morbid-information-involves-reward-circuitry.html"><i class="fa fa-check"></i><b>5</b> Choosing to view morbid information involves reward circuitry</a></li>
<li class="chapter" data-level="6" data-path="using-predictive-modeling-to-quantify-the-importance-and-limitations-of-action-units-in-emotion-perception.html"><a href="using-predictive-modeling-to-quantify-the-importance-and-limitations-of-action-units-in-emotion-perception.html"><i class="fa fa-check"></i><b>6</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a></li>
<li class="chapter" data-level="7" data-path="comparing-models-of-dynamic-facial-expression-perception.html"><a href="comparing-models-of-dynamic-facial-expression-perception.html"><i class="fa fa-check"></i><b>7</b> Comparing models of dynamic facial expression perception</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning from the brain</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shared-states-using-mvpa-to-test-neural-overlap-between-self-focused-emotion-imagery-and-other-focused-emotion-understanding" class="section level1">
<h1><span class="header-section-number">2</span> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Oosterwijk, S.*, Snoek, L.*, Rotteveel, M., Barrett, L. F., &amp; Scholte, H. S. (2017). Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding. <em>Social cognitive and affective neuroscience, 12</em>(7), 1025-1035.</p>
<p>* Shared first authorship

</p>
<p><strong>Abstract</strong></p>
<p>The present study tested whether the neural patterns that support imagining “performing an action”, “feeling a bodily sensation” or “being in a situation” are directly involved in understanding <em>other people’s</em> actions, bodily sensations and situations. Subjects imagined the content of short sentences describing emotional actions, interoceptive sensations and situations (self-focused task), and processed scenes and focused on <em>how</em> the target person was expressing an emotion, <em>what</em> this person was feeling, and <em>why</em> this person was feeling an emotion (other-focused task). Using a linear support vector machine classifier on brain-wide multi-voxel patterns, we accurately decoded each individual class in the self-focused task. When generalizing the classifier from the self-focused task to the other-focused task, we also accurately decoded whether subjects focused on the emotional actions, interoceptive sensations and situations of <em>others</em>. These results show that the neural patterns that underlie self-imagined experience are involved in understanding the experience of other people. This supports the theoretical assumption that the basic components of emotion experience and understanding share resources in the brain.</p>
<div id="Sharedstates-introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>To navigate the social world successfully it is crucial to understand other people. But how do people generate meaningful representations of other people’s actions, sensations, thoughts and emotions? The dominant view assumes that representations of other people’s experiences are supported by the same neural systems as those that are involved in generating experience in the self <span class="citation">(e.g., Gallese et al., <a href="bibliography.html#ref-gallese2004unifying" role="doc-biblioref">2004</a>; see for an overview Singer, <a href="bibliography.html#ref-singer2012past" role="doc-biblioref">2012</a>)</span>. We tested this principle of self-other neural overlap directly, using multi-voxel pattern analysis (MVPA), across three different aspects of experience that are central to emotions: actions, sensations from the body and situational knowledge.</p>
<p>In recent years, evidence has accumulated that suggests a similarity between the neural patterns representing the self and others. For example, a great variety of studies have shown that observing actions and sensations in other people engages similar neural circuits as acting and feeling in the self <span class="citation">(see for an overview Bastiaansen et al., <a href="bibliography.html#ref-bastiaansen2009evidence" role="doc-biblioref">2009</a>)</span>. Moreover, an extensive research program on pain has demonstrated an overlap between the experience of physical pain and the observation of pain in other people, utilizing both neuroimaging techniques <span class="citation">(e.g., Lamm et al., <a href="bibliography.html#ref-lamm2011meta" role="doc-biblioref">2011</a>)</span> and analgesic interventions <span class="citation">(e.g., Rütgen et al., <a href="bibliography.html#ref-rutgen2015placebo" role="doc-biblioref">2015</a>; Mischkowski et al., <a href="bibliography.html#ref-mischkowski2016painkiller" role="doc-biblioref">2016</a>)</span>. This process of “vicarious experience” or “simulation” is viewed as an important component of empathy <span class="citation">(Carr et al., <a href="bibliography.html#ref-carr2003neural" role="doc-biblioref">2003</a>; Decety, <a href="bibliography.html#ref-decety2011dissecting" role="doc-biblioref">2011</a>; Keysers &amp; Gazzola, <a href="bibliography.html#ref-keysers2014dissociating" role="doc-biblioref">2014</a>)</span>. In addition, it is argued that mentalizing (e.g. understanding the mental states of other people) involves the same brain networks as those involved in self-generated thoughts <span class="citation">(Uddin et al., <a href="bibliography.html#ref-uddin2007self" role="doc-biblioref">2007</a>; Waytz &amp; Mitchell, <a href="bibliography.html#ref-waytz2011two" role="doc-biblioref">2011</a>)</span>. Specifying this idea further, a constructionist view on emotion proposes that both emotion experience and interpersonal emotion understanding are produced by the same large-scale distributed brain networks that support the processing of sensorimotor, interoceptive and situationally relevant information <span class="citation">(Barrett &amp; Satpute, <a href="bibliography.html#ref-barrett2013large" role="doc-biblioref">2013</a>; Oosterwijk &amp; Barrett, <a href="bibliography.html#ref-oosterwijk2014embodiment" role="doc-biblioref">2014</a>)</span>. An implication of these views is that the representation of self- and other-focused emotional actions, interoceptive sensations and situations overlap in the brain.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="how-to-control-for-confounds-in-decoding-analyses-of-neuroimaging-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
