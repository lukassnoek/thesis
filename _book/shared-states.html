<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain</title>
  <meta name="description" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding | Learning from the brain" />
  
  
  

<meta name="author" content="Lukas Snoek" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="shared-states-results.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD thesis of Lukas Snoek</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-brain-is-not-a-dictionary"><i class="fa fa-check"></i><b>1.1</b> The brain is not a dictionary</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-brain-probably-does-not-care-about-your-hypothesis"><i class="fa fa-check"></i><b>1.2</b> The brain (probably) does not care about your hypothesis</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretability-and-prediction-are-a-trade-off-for-now"><i class="fa fa-check"></i><b>1.3</b> Interpretability and prediction are a trade-off (for now)</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#exploration-should-be-embraced-more"><i class="fa fa-check"></i><b>1.4</b> Exploration should be embraced more</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#proper-generalization-is-hard"><i class="fa fa-check"></i><b>1.5</b> Proper generalization is hard</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#psychology-is-complex-so-it-needs-complex-models"><i class="fa fa-check"></i><b>1.6</b> Psychology is complex, so it needs complex models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="shared-states.html"><a href="shared-states.html"><i class="fa fa-check"></i><b>2</b> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</a><ul>
<li class="chapter" data-level="2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-subjects"><i class="fa fa-check"></i><b>2.2.1</b> Subjects</a></li>
<li class="chapter" data-level="2.2.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-experimental-design"><i class="fa fa-check"></i><b>2.2.2</b> Experimental design</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-procedure"><i class="fa fa-check"></i><b>2.3</b> Procedure</a></li>
<li class="chapter" data-level="2.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-image-acquisition"><i class="fa fa-check"></i><b>2.4</b> Image acquisition</a></li>
<li class="chapter" data-level="2.5" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-model-optimization-procedure"><i class="fa fa-check"></i><b>2.5</b> Model optimization procedure</a></li>
<li class="chapter" data-level="2.6" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-preprocessing"><i class="fa fa-check"></i><b>2.6</b> Preprocessing and single-trial modeling</a></li>
<li class="chapter" data-level="2.7" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa"><i class="fa fa-check"></i><b>2.7</b> Multi-voxel pattern analysis</a><ul>
<li class="chapter" data-level="2.7.1" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa-pipeline"><i class="fa fa-check"></i><b>2.7.1</b> MVPA pipeline</a></li>
<li class="chapter" data-level="2.7.2" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa-cv-and-bagging"><i class="fa fa-check"></i><b>2.7.2</b> Cross-validation scheme and bagging procedure</a></li>
<li class="chapter" data-level="2.7.3" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa-statistical-evaluation"><i class="fa fa-check"></i><b>2.7.3</b> Statistical evaluation</a></li>
<li class="chapter" data-level="2.7.4" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-mvpa-spatial-representation"><i class="fa fa-check"></i><b>2.7.4</b> Spatial representation</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-additional-analyses"><i class="fa fa-check"></i><b>2.8</b> Additional analyses</a></li>
<li class="chapter" data-level="2.9" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-univariate-analysis"><i class="fa fa-check"></i><b>2.9</b> Univariate analysis</a></li>
<li class="chapter" data-level="2.10" data-path="shared-states.html"><a href="shared-states.html#shared-states-methods-code-availability"><i class="fa fa-check"></i><b>2.10</b> Code availability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="shared-states-results.html"><a href="shared-states-results.html"><i class="fa fa-check"></i><b>3</b> Results</a></li>
<li class="chapter" data-level="4" data-path="confounds-decoding.html"><a href="confounds-decoding.html"><i class="fa fa-check"></i><b>4</b> How to control for confounds in decoding analyses of neuroimaging data</a></li>
<li class="chapter" data-level="5" data-path="AOMIC.html"><a href="AOMIC.html"><i class="fa fa-check"></i><b>5</b> The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</a></li>
<li class="chapter" data-level="6" data-path="morbid-curiosity.html"><a href="morbid-curiosity.html"><i class="fa fa-check"></i><b>6</b> Choosing to view morbid information involves reward circuitry</a></li>
<li class="chapter" data-level="7" data-path="au-limitations.html"><a href="au-limitations.html"><i class="fa fa-check"></i><b>7</b> Using predictive modeling to quantify the importance and limitations of action units in emotion perception</a></li>
<li class="chapter" data-level="8" data-path="facial-expression-models.html"><a href="facial-expression-models.html"><i class="fa fa-check"></i><b>8</b> Comparing models of dynamic facial expression perception</a></li>
<li class="chapter" data-level="9" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html"><i class="fa fa-check"></i><b>9</b> Summary and general discussion</a><ul>
<li class="chapter" data-level="9.1" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#explore"><i class="fa fa-check"></i><b>9.1</b> Explore!</a></li>
<li class="chapter" data-level="9.2" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#think-big"><i class="fa fa-check"></i><b>9.2</b> Think <em>big</em></a></li>
<li class="chapter" data-level="9.3" data-path="summary-and-general-discussion.html"><a href="summary-and-general-discussion.html#rethink-psychology-education"><i class="fa fa-check"></i><b>9.3</b> Rethink psychology education</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html"><i class="fa fa-check"></i><b>A</b> Supplement to Chapter 2</a><ul>
<li class="chapter" data-level="A.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#stimuli-used-for-sf-task"><i class="fa fa-check"></i><b>A.1</b> Stimuli used for SF-task</a></li>
<li class="chapter" data-level="A.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#instructions"><i class="fa fa-check"></i><b>A.2</b> Instructions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-other-focused-emotion-understanding-task"><i class="fa fa-check"></i><b>A.2.1</b> Full instruction for the other-focused emotion understanding task</a></li>
<li class="chapter" data-level="A.2.2" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#full-instruction-for-the-self-focused-emotion-imagery-task"><i class="fa fa-check"></i><b>A.2.2</b> Full instruction for the self-focused emotion imagery task</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#behavioral-results"><i class="fa fa-check"></i><b>A.3</b> Behavioral results</a></li>
<li class="chapter" data-level="A.4" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#optimization-results"><i class="fa fa-check"></i><b>A.4</b> Optimization results</a></li>
<li class="chapter" data-level="A.5" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#bagging-procedure"><i class="fa fa-check"></i><b>A.5</b> Bagging procedure</a></li>
<li class="chapter" data-level="A.6" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#precision-vs.-recall"><i class="fa fa-check"></i><b>A.6</b> Precision vs. recall</a></li>
<li class="chapter" data-level="A.7" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#self-vs.-other-classification"><i class="fa fa-check"></i><b>A.7</b> Self vs. other classification</a></li>
<li class="chapter" data-level="A.8" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#condition-average-results"><i class="fa fa-check"></i><b>A.8</b> Condition-average results</a></li>
<li class="chapter" data-level="A.9" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#individual-subject-scores"><i class="fa fa-check"></i><b>A.9</b> Individual subject scores</a></li>
<li class="chapter" data-level="A.10" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#brain-region-importance"><i class="fa fa-check"></i><b>A.10</b> Brain region importance</a></li>
<li class="chapter" data-level="A.11" data-path="shared-states-supplement.html"><a href="shared-states-supplement.html#general-note-about-tables-with-voxel-coordinates"><i class="fa fa-check"></i><b>A.11</b> General note about tables with voxel-coordinates</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="confounds-decoding-supplement.html"><a href="confounds-decoding-supplement.html"><i class="fa fa-check"></i><b>B</b> Supplement to Chapter 4</a></li>
<li class="chapter" data-level="C" data-path="morbid-curiosity-supplement.html"><a href="morbid-curiosity-supplement.html"><i class="fa fa-check"></i><b>C</b> Supplement to Chapter 5</a></li>
<li class="chapter" data-level="D" data-path="au-limitations-supplement.html"><a href="au-limitations-supplement.html"><i class="fa fa-check"></i><b>D</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="E" data-path="facial-expression-models-supplement.html"><a href="facial-expression-models-supplement.html"><i class="fa fa-check"></i><b>E</b> Supplement to Chapter 6</a></li>
<li class="chapter" data-level="F" data-path="resources-supplement.html"><a href="resources-supplement.html"><i class="fa fa-check"></i><b>F</b> Data, code and materials</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="contributions-to-the-chapters.html"><a href="contributions-to-the-chapters.html"><i class="fa fa-check"></i>Contributions to the chapters</a></li>
<li class="chapter" data-level="" data-path="list-of-other-publications.html"><a href="list-of-other-publications.html"><i class="fa fa-check"></i>List of other publications</a></li>
<li class="chapter" data-level="" data-path="nederlandse-samenvatting-summary-in-dutch.html"><a href="nederlandse-samenvatting-summary-in-dutch.html"><i class="fa fa-check"></i>Nederlandse samenvatting (Summary in Dutch)</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning from the brain</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shared-states" class="section level1">
<h1><span class="header-section-number">2</span> Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding</h1>


<hr />

<p>
<em>This chapter has been published as</em>: Oosterwijk, S.*, Snoek, L.*, Rotteveel, M., Barrett, L. F., &amp; Scholte, H. S. (2017). Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding. <em>Social cognitive and affective neuroscience, 12</em>(7), 1025-1035.</p>
<p>* Shared first authorship

</p>
<p><strong>Abstract</strong></p>
<p>The present study tested whether the neural patterns that support imagining “performing an action”, “feeling a bodily sensation” or “being in a situation” are directly involved in understanding <em>other people’s</em> actions, bodily sensations and situations. Subjects imagined the content of short sentences describing emotional actions, interoceptive sensations and situations (self-focused task), and processed scenes and focused on <em>how</em> the target person was expressing an emotion, <em>what</em> this person was feeling, and <em>why</em> this person was feeling an emotion (other-focused task). Using a linear support vector machine classifier on brain-wide multi-voxel patterns, we accurately decoded each individual class in the self-focused task. When generalizing the classifier from the self-focused task to the other-focused task, we also accurately decoded whether subjects focused on the emotional actions, interoceptive sensations and situations of <em>others</em>. These results show that the neural patterns that underlie self-imagined experience are involved in understanding the experience of other people. This supports the theoretical assumption that the basic components of emotion experience and understanding share resources in the brain.</p>
<div id="shared-states-introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>To navigate the social world successfully it is crucial to understand other people. But how do people generate meaningful representations of other people’s actions, sensations, thoughts and emotions? The dominant view assumes that representations of other people’s experiences are supported by the same neural systems as those that are involved in generating experience in the self <span class="citation">(e.g., Gallese et al., <a href="bibliography.html#ref-gallese2004unifying" role="doc-biblioref">2004</a>; see for an overview Singer, <a href="bibliography.html#ref-singer2012past" role="doc-biblioref">2012</a>)</span>. We tested this principle of self-other neural overlap directly, using multi-voxel pattern analysis (MVPA), across three different aspects of experience that are central to emotions: actions, sensations from the body and situational knowledge.</p>
<p>In recent years, evidence has accumulated that suggests a similarity between the neural patterns representing the self and others. For example, a great variety of studies have shown that observing actions and sensations in other people engages similar neural circuits as acting and feeling in the self <span class="citation">(see for an overview Bastiaansen et al., <a href="bibliography.html#ref-bastiaansen2009evidence" role="doc-biblioref">2009</a>)</span>. Moreover, an extensive research program on pain has demonstrated an overlap between the experience of physical pain and the observation of pain in other people, utilizing both neuroimaging techniques <span class="citation">(e.g., Lamm et al., <a href="bibliography.html#ref-lamm2011meta" role="doc-biblioref">2011</a>)</span> and analgesic interventions <span class="citation">(e.g., Rütgen et al., <a href="bibliography.html#ref-rutgen2015placebo" role="doc-biblioref">2015</a>; Mischkowski et al., <a href="bibliography.html#ref-mischkowski2016painkiller" role="doc-biblioref">2016</a>)</span>. This process of “vicarious experience” or “simulation” is viewed as an important component of empathy <span class="citation">(Carr et al., <a href="bibliography.html#ref-carr2003neural" role="doc-biblioref">2003</a>; Decety, <a href="bibliography.html#ref-decety2011dissecting" role="doc-biblioref">2011</a>; Keysers &amp; Gazzola, <a href="bibliography.html#ref-keysers2014dissociating" role="doc-biblioref">2014</a>)</span>. In addition, it is argued that mentalizing (e.g. understanding the mental states of other people) involves the same brain networks as those involved in self-generated thoughts <span class="citation">(Uddin et al., <a href="bibliography.html#ref-uddin2007self" role="doc-biblioref">2007</a>; Waytz &amp; Mitchell, <a href="bibliography.html#ref-waytz2011two" role="doc-biblioref">2011</a>)</span>. Specifying this idea further, a constructionist view on emotion proposes that both emotion experience and interpersonal emotion understanding are produced by the same large-scale distributed brain networks that support the processing of sensorimotor, interoceptive and situationally relevant information <span class="citation">(Barrett &amp; Satpute, <a href="bibliography.html#ref-barrett2013large" role="doc-biblioref">2013</a>; Oosterwijk &amp; Barrett, <a href="bibliography.html#ref-oosterwijk2014embodiment" role="doc-biblioref">2014</a>)</span>. An implication of these views is that the representation of self- and other-focused emotional actions, interoceptive sensations and situations overlap in the brain.</p>
<p>Although there is experimental and theoretical support for the idea of self-other neural overlap, the present study is the first to directly test this process using MVPA across three different aspects of experience (i.e. actions, interoceptive sensations and situational knowledge). Our experimental design consisted of two different tasks aimed at generating self- and other-focused representations with a relatively large weight given to either action information, interoceptive information or situational information.</p>
<p>In the <em>self-focused</em> emotion imagery task (SF-task) subjects imagined performing or experiencing actions (e.g., <em>pushing someone away</em>), interoceptive sensations (e.g., <em>increased heart rate</em>) and situations (e.g., <em>alone in a park at night</em>) associated with emotion. Previous research has demonstrated that processing linguistic descriptions of (emotional) actions and feeling states can result in neural patterns of activation associated with, respectively, the representation and generation of actions and internal states <span class="citation">(Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2015concepts" role="doc-biblioref">2015</a>; Pulvermüller &amp; Fadiga, <a href="bibliography.html#ref-pulvermuller2010active" role="doc-biblioref">2010</a>)</span>. Furthermore, imagery-based inductions of emotion have been successfully used in the MRI scanner before <span class="citation">(Oosterwijk et al., <a href="bibliography.html#ref-oosterwijk2012states" role="doc-biblioref">2012</a>; Wilson-Mendenhall et al., <a href="bibliography.html#ref-wilson2011grounding" role="doc-biblioref">2011</a>)</span>, and are seen as robust inducers of emotional experience <span class="citation">(Lench et al., <a href="bibliography.html#ref-lench2011discrete" role="doc-biblioref">2011</a>)</span>. In the <em>other-focused</em> emotion understanding task (OF-task), subjects viewed images of people in emotional situations and focused on actions (i.e., <em>How</em> does this person express his/her emotions?), interoceptive sensations (i.e., <em>What</em> does this person feel in his/her body) or the situation (i.e., <em>Why</em> does this person feel an emotion?). This task is based on previous research studying the neural basis of emotion oriented mentalizing <span class="citation">(Spunt &amp; Lieberman, <a href="bibliography.html#ref-spunt2012integrative" role="doc-biblioref">2012</a>)</span>.</p>
<p>With MVPA, we examined to what extent the SF- and OF-task evoked similar neural patterns. MVPA allows researchers to assess whether the neural pattern associated with one set of experimental conditions can be used to distinguish between another set of experimental conditions. This relatively novel technique has been successfully applied to the field of social neuroscience in general <span class="citation">(e.g., Gilbert et al., <a href="bibliography.html#ref-gilbert2012evaluative" role="doc-biblioref">2012</a>; Brosch et al., <a href="bibliography.html#ref-brosch2013implicit" role="doc-biblioref">2013</a>; Parkinson et al., <a href="bibliography.html#ref-parkinson2014common" role="doc-biblioref">2014</a>)</span>, and the field of self-other neural overlap in particular. For example, several MVPA studies recently assessed whether experiencing pain and observing pain in others involved similar neural patterns <span class="citation">(Corradi-Dell’Acqua et al., <a href="bibliography.html#ref-corradi2016cross" role="doc-biblioref">2016</a>; Krishnan et al., <a href="bibliography.html#ref-krishnan2016somatic" role="doc-biblioref">2016</a>)</span>. Although there is an ongoing discussion about the specifics of shared representation in pain based on these MVPA results <span class="citation">(see for an overview Zaki et al., <a href="bibliography.html#ref-zaki2016anatomy" role="doc-biblioref">2016</a>)</span>, many authors emphasize the importance of this technique in the scientific study of self-other neural overlap <span class="citation">(e.g., Corradi-Dell’Acqua et al., <a href="bibliography.html#ref-corradi2016cross" role="doc-biblioref">2016</a>; Krishnan et al., <a href="bibliography.html#ref-krishnan2016somatic" role="doc-biblioref">2016</a>)</span>.</p>
<p>MVPA is an analysis technique that decodes latent categories from fMRI data in terms of multi-voxel patterns of activity <span class="citation">(Norman et al., <a href="bibliography.html#ref-norman2006beyond" role="doc-biblioref">2006</a>)</span>. This technique is particularly suited for our research question for several reasons. First of all, although univariate techniques can demonstrate that tasks activate the same brain regions, only MVPA can statistically test for shared representation <span class="citation">(Lamm &amp; Majdandžić, <a href="bibliography.html#ref-lamm2015role" role="doc-biblioref">2015</a>)</span>. We will evaluate whether multivariate brain patterns that distinguish between mental events in the SF-task can be used to distinguish, above chance level, between mental events in the OF-task. Second, MVPA analyses are particularly useful in research that is aimed at examining distributed representations <span class="citation">(Singer, <a href="bibliography.html#ref-singer2012past" role="doc-biblioref">2012</a>)</span>. Based on our constructionist framework, we indeed hypothesize that the neural patterns that will represent self- and other focused mental events are distributed across large-scale brain networks. To capture these distributed patterns, we used MVPA in combination with data-driven univariate feature selection on whole-brain voxel patterns, instead of limiting our analysis to specific regions-of-interest <span class="citation">(Haynes, <a href="bibliography.html#ref-haynes2015primer" role="doc-biblioref">2015</a>)</span>. And third, in contrast to univariate analyses that aggregate data across subjects, MVPA can be performed within-subjects and is thus able to incorporate individual variation in the representational content of multivariate brain patterns. In that aspect within-subject MVPA is sensitive to individual differences in how people imagine actions, sensations and situations, and how they understand others. In short, for our purpose to explicitly test the assumption that self and other focused processes share neural resources, MVPA is the designated method.</p>
<p>We tested the following two hypotheses. First, we tested whether we could classify <em>self-imagined</em> actions, interoceptive sensations and situations above chance level. Second, we tested whether the multivariate pattern underlying this classification could also be used to classify the how, what and why condition in the <em>other-focused</em> task.</p>
</div>
<div id="shared-states-methods" class="section level2">
<h2><span class="header-section-number">2.2</span> Methods</h2>
<div id="shared-states-methods-subjects" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Subjects</h3>
<p>In total, we tested 22 Dutch undergraduate students from the University of Amsterdam (14 females; M<sub>age</sub> = 21.48, s.d.<sub>age</sub> = 1.75). Of those 22 subjects, 13 subjects were tested twice in 2 sessions about 1 week apart. Half of those sessions were used for the model optimization procedure. The other half of the sessions, combined with an additional nine subjects (who were tested only once), constituted the model validation set (see Model optimization procedure section). In total, two subjects were excluded from the model validation dataset: one subject was excluded because there was not enough time to complete the experimental protocol and another subject was excluded due to excessive movement (&gt;3 mm within data acquisition runs).</p>
<p>All subjects signed informed consent prior to the experiment. The experiment was approved by the University of Amsterdam’s ethical review board. Subjects received 22.50 euro per session. Standard exclusion criteria regarding MRI safety were applied and people who were on psychopharmacological medication were excluded a priori.</p>
</div>
<div id="shared-states-methods-experimental-design" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Experimental design</h3>
<div id="shared-states-methods-experimental-design-sf-task" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> Self-focused emotion imagery task</h4>
<p>The self-focused emotion imagery task (SF-task) was created to preferentially elicit <em>self-focused</em> processing of action, interoceptive or situational information associated with emotion. Subjects processed short linguistic cues that described actions (e.g., <em>pushing someone away</em>; <em>making a fist</em>), interoceptive sensations (e.g., <em>being out of breath</em>; <em>an increased heart rate</em>), or situations (e.g., <em>alone in a park at night</em>; <em>being falsely accused</em>) and were instructed to imagine performing or experiencing the content. The complete instruction is presented in the Supplementary Materials; all stimuli used in the SF-task are presented in Supplementary Table <a href="shared-states-supplement.html#tab:tab-shared-states-S1">A.1</a>. Linguistic cues were selected from a pilot study performed on an independent sample of subjects (<em>n</em> = 24). Details about this pilot study are available on request. The descriptions generated in this pilot study were used as qualitative input to create short sentences that described actions, sensations or situations that were associated with negative emotions, without including discrete emotion terms. The cues did not differ in number of words, nor in number of characters (<em>F</em> &lt; 1).</p>
<p>The SF-task was performed in two runs subsequent to the other-focused task using the software package Presentation (Version 16.4, <a href="www.neurobs.com">www.neurobs.com</a>). Each run presented 60 sentences on a black background (20 per condition) in a fully randomized event-related fashion, with a different randomization for each subject. Note that implementing a separate randomization for each subject prevents inflated false positive pattern correlations between trials of the same condition, which may occur in single-trial designs with short inter-stimulus intervals (Mumford et al., 2014). A fixed inter-trial–interval of 2 seconds separating trials; 12 null-trials (i.e. a black screen for 8 seconds) were mixed with the experimental trials at random positions during each run (see Figure <a href="shared-states.html#fig:fig-shared-states-1">2.1</a>).</p>
<div class="figure"><span id="fig:fig-shared-states-1"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_1.png" alt="Overview of the self-focused and other-focused task."  />
<p class="caption">
Figure 2.1: Overview of the self-focused and other-focused task.
</p>
</div>
</div>
<div id="shared-states-methods-experimental-design-of-task" class="section level4">
<h4><span class="header-section-number">2.2.2.2</span> Other-focused emotion understanding task</h4>
<p>The other-focused emotion understanding task (OF-task) was created to preferentially elicit <em>other-focused</em> processing of action, interoceptive or situational information associated with emotion. Subjects viewed images of people in negative situations (e.g. a woman screaming at a man, a man held at gunpoint). A red rectangle highlighted the face of the person that the subjects should focus on to avoid ambiguity in images depicting more than one person. Image blocks were preceded by a cue indicating the strategy subjects should use in perceiving the emotional state of the people in the images <span class="citation">(Spunt &amp; Lieberman, <a href="bibliography.html#ref-spunt2012integrative" role="doc-biblioref">2012</a>)</span>. The cue <em>How</em> instructed the subjects to identify actions that were informative about the person’s emotional state (i.e., <em>How</em> does this person express his/her emotions?). The cue <em>What</em> instructed subjects to identify interoceptive sensations that the person could experience (i.e., <em>What</em> does this person feel in his/her body). The cue <em>Why</em> instructed subjects to identify reasons or explanations for the person’s emotional state (i.e., <em>Why</em> does this person feel an emotion?). The complete instruction is presented in the <a href="shared-states-supplement.html#shared-states-supplement">Supplementary Materials</a>.</p>
<p>Stimuli for the OF-task were selected from the International Affective Picture System database <span class="citation">(IAPS; Lang, <a href="bibliography.html#ref-lang2005international" role="doc-biblioref">2005</a>; Lang et al., <a href="bibliography.html#ref-lang1997international" role="doc-biblioref">1997</a>)</span>, the image set developed by the Kveraga lab <span class="citation">(<a href="http://www.kveragalab.org/stimuli.html" role="doc-biblioref">http://www.kveragalab.org/stimuli.html</a>; Kveraga et al., <a href="bibliography.html#ref-kveraga2015if" role="doc-biblioref">2015</a>)</span> and the internet (Google images). We selected images based on a pilot study, performed on an independent sample of subjects (<em>n</em> = 22). Details about this pilot study are available on request.</p>
<p>The OF-task was presented using the software package Presentation. The task presented thirty images on a black background in blocked fashion, with each block starting with a what, why or how cue (see Figure <a href="shared-states.html#fig:fig-shared-states-1">2.1</a>). Each image was shown three times, once for each cue type. Images were presented in blocks of six, each lasting 6 seconds, followed by a fixed inter trial interval of 2 seconds. Null-trials were inserted at random positions within the blocks. Both the order of the blocks and the specific stimuli within and across blocks were fully randomized, with a different randomization for each subject.</p>
</div>
</div>
</div>
<div id="shared-states-methods-procedure" class="section level2">
<h2><span class="header-section-number">2.3</span> Procedure</h2>
<p>Each experimental session lasted about 2 hours. Subjects who underwent two sessions had them on different days within a time span of 1 week. On arrival, subjects gave informed consent and received thorough task instructions, including practice trials (see the <a href="shared-states-supplement.html#shared-states-supplement">Supplementary Materials</a> for a translation of the task instructions). The actual time in the scanner was 55 minutes, and included a rough 3D scout image, shimming sequence, 3-min structural T1-weighted scan, one functional run for the OF-task and two functional runs for the SF-task. We deliberately chose to present the SF-task after the OF-task to exclude the possibility that the SF-task affected the OF-task, thereby influencing the success of the decoding procedure.</p>
<p>After each scanning session, subjects rated their success rate for the SF-task and OF-task (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S1">A.1</a>). In the second session, subjects filled out three personality questionnaires that will not be further discussed in this paper and were debriefed about the purpose of the study.</p>
</div>
<div id="shared-states-methods-image-acquisition" class="section level2">
<h2><span class="header-section-number">2.4</span> Image acquisition</h2>
<p>Subjects were tested using a Philips Achieva 3T MRI scanner and a 32-channel SENSE headcoil. A survey scan was made for spatial planning of the subsequent scans. Following the survey scan, a 3-min structural T1-weighted scan was acquired using 3D fast field echo (TR: 82 ms, TE: 38 ms, flip angle: 8°, FOV: 240 × 188 mm, 220 slices acquired using single-shot ascending slice order and a voxel size of 1.0 × 1.0 × 1.0 mm). After the T1-weighted scan, functional T2*-weighted sequences were acquired using single shot gradient echo, echo planar imaging (TR = 2000 ms, TE = 27.63 ms, flip angle: 76.1°, FOV: 240 × 240 mm, in-plane resolution 64 × 64, 37 slices (with ascending acquisition), slice thickness 3 mm, slice gap 0.3 mm, voxel size 3 × 3 × 3 mm), covering the entire brain. For the SF-task, 301 volumes were acquired; for the OF-task 523 volumes were acquired.</p>
</div>
<div id="shared-states-methods-model-optimization-procedure" class="section level2">
<h2><span class="header-section-number">2.5</span> Model optimization procedure</h2>
<p>As MVPA is a fairly novel technique, no consistent, optimal MVPA pipeline has been established <span class="citation">(Etzel et al., <a href="bibliography.html#ref-etzel2011impact" role="doc-biblioref">2011</a>)</span>. Therefore, we adopted a validation strategy in the present study that is advised in the pattern classification field <span class="citation">(Kay et al., <a href="bibliography.html#ref-kay2008identifying" role="doc-biblioref">2008</a>; Kriegeskorte et al., <a href="bibliography.html#ref-kriegeskorte2009circular" role="doc-biblioref">2009</a>)</span>. This strategy entailed that we separated our data into an optimization dataset to find the most optimal parameters for preprocessing and analysis, and a validation dataset to independently verify classification success with those optimal parameters. We generated an optimization and validation dataset by running the SF-task and OF-task twice, in two identical experimental sessions for a set of thirteen subjects. The sessions were equally split between the optimization and validation set (see Figure 2A); first and second sessions were counterbalanced between the two sets. Based on a request received during the review process, we added nine new subjects to the validation dataset. Ultimately, the optimization-set held 13 sessions and the validation-set, after exclusion of 2 subjects (see Subjects section), held 20 sessions.</p>
<div class="figure"><span id="fig:fig-shared-states-2"></span>
<img src="_bookdown_files/shared-states-files/figures/figure_1.png" alt="Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, F(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, F(2, 17) = 17.74, p &lt; 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (M = 74.00, SE = 2.10) were significantly less successful (p &lt; 0.001) than both action-trials (M = 85.50, SE = 1.85) and situation trials (M = 90.00, SE = 1.92)."  />
<p class="caption">
Figure 2.2: Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, <em>F</em>(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, <em>F</em>(2, 17) = 17.74, <em>p</em> &lt; 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (<em>M</em> = 74.00, <em>SE</em> = 2.10) were significantly less successful (<em>p</em> &lt; 0.001) than both action-trials (<em>M</em> = 85.50, <em>SE</em> = 1.85) and situation trials (<em>M</em> = 90.00, <em>SE</em> = 1.92).
</p>
</div>

<p>In the optimization-set, we explored how different preprocessing options and the so-called ‘hyperparameters’ in the MVPA pipeline affected the performance of the (multivariate) analyses (visualized in Figure <a href="shared-states.html#fig:fig-shared-states-2">2.2</a>B; see <a href="shared-states.html#shared-states-methods-mvpa-pipeline">MVPA pipeline</a> subsection for more details). Thus, we performed the self- and cross-analyses <em>on the data of the optimization set</em> multiple times with different preprocessing options (i.e., smoothing kernel, low-pass filter and ICA-based denoising strategies) and MVPA hyperparameter values (i.e., univariate feature selection <em>threshold</em> and train/test size ratio during cross-validation). We determined the optimal parameters on the basis of classification performance, which was operationalized as the mean precision value after a repeated random subsampling procedure with 1000 iterations. A list with the results from the optimization procedure can be found in Supplementary Table <a href="shared-states-supplement.html#tab:tab-shared-states-S2">A.2</a> and Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S2">A.2</a>. The optimal parameters were then used for preprocessing and the self- and cross-analysis within the validation-set, in which the findings from the optimization-set were replicated. All findings discussed in the @ref{shared-states-results} section follow from the validation-set (see Supplementary Figure <a href="shared-states-supplement.html#fig:fig-shared-states-S3">A.3</a> for an overview of the findings from the optimization-set).</p>
</div>
<div id="shared-states-methods-preprocessing" class="section level2">
<h2><span class="header-section-number">2.6</span> Preprocessing and single-trial modeling</h2>
</div>
<div id="shared-states-methods-mvpa" class="section level2">
<h2><span class="header-section-number">2.7</span> Multi-voxel pattern analysis</h2>
<div id="shared-states-methods-mvpa-pipeline" class="section level3">
<h3><span class="header-section-number">2.7.1</span> MVPA pipeline</h3>
</div>
<div id="shared-states-methods-mvpa-cv-and-bagging" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Cross-validation scheme and bagging procedure</h3>
</div>
<div id="shared-states-methods-mvpa-statistical-evaluation" class="section level3">
<h3><span class="header-section-number">2.7.3</span> Statistical evaluation</h3>
</div>
<div id="shared-states-methods-mvpa-spatial-representation" class="section level3">
<h3><span class="header-section-number">2.7.4</span> Spatial representation</h3>
</div>
</div>
<div id="shared-states-methods-additional-analyses" class="section level2">
<h2><span class="header-section-number">2.8</span> Additional analyses</h2>
</div>
<div id="shared-states-methods-univariate-analysis" class="section level2">
<h2><span class="header-section-number">2.9</span> Univariate analysis</h2>
</div>
<div id="shared-states-methods-code-availability" class="section level2">
<h2><span class="header-section-number">2.10</span> Code availability</h2>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shared-states-results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
