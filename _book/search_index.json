[
["index.html", "Learning from the brain Preface", " Learning from the brain Best practices for the use of neuroimaging data in psychology research Lukas Snoek maandag 21 oktober 2021 Preface This is the html output of the University of Amsterdam PhD thesis template from the amsterdown package. You can view access the pdf version(s) of the output from the toolbar above. This section will only show in the html version. Here you can add some general info about your thesis that’s typically included in the pdf. For example: who your supervisors were where the project was conducted the license the thesis is published under funding info an image of the cover of your thesis etc. For these and other ideas, see what this section looks like in my PhD thesis (and also check out the source file here). "],
["introduction.html", "1 Introduction 1.1 The brain is not a dictionary 1.2 The brain (probably) does not care about your hypothesis 1.3 Interpretability and prediction are a trade-off (for now) 1.4 Exploration should be embraced more 1.5 Proper generalization is hard 1.6 Psychology is complex, so it needs complex models", " 1 Introduction The first chapter of the thesis, which introduces your PhD project. The filler-text below was created with the postmodernism generator. Something about the coming about of this thesis. More of a “lessons learned” rather than a coherent research topic. 1.1 The brain is not a dictionary Something about looking at populations of neurons/voxels/areas rather than simple one-to-one relationships. Shared states. 1.2 The brain (probably) does not care about your hypothesis Facial expression models. 1.3 Interpretability and prediction are a trade-off (for now) A plea for prediction but a cautionary tale for interpreting predictive models (confounds) 1.4 Exploration should be embraced more Something about the “context of discovery” (cf. TCM), preregistration, and confirmation vs. exploration (Morbid curiosity.) 1.5 Proper generalization is hard Within and between subject variance is not noise, but unexplained variance (AU limitations). 1.6 Psychology is complex, so it needs complex models Which need to be fit on complex, large datasets. (AOMIC) "],
["shared-states.html", "2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding 2.1 Introduction 2.2 Methods 2.3 Model optimization procedure 2.4 Results 2.5 Discussion 2.6 Acknowledgements", " 2 Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding This chapter has been published as: Oosterwijk, S.*, Snoek, L.*, Rotteveel, M., Barrett, L. F., &amp; Scholte, H. S. (2017). Shared states: using MVPA to test neural overlap between self-focused emotion imagery and other-focused emotion understanding. Social cognitive and affective neuroscience, 12(7), 1025-1035. * Shared first authorship Abstract The present study tested whether the neural patterns that support imagining “performing an action”, “feeling a bodily sensation” or “being in a situation” are directly involved in understanding other people’s actions, bodily sensations and situations. Subjects imagined the content of short sentences describing emotional actions, interoceptive sensations and situations (self-focused task), and processed scenes and focused on how the target person was expressing an emotion, what this person was feeling, and why this person was feeling an emotion (other-focused task). Using a linear support vector machine classifier on brain-wide multi-voxel patterns, we accurately decoded each individual class in the self-focused task. When generalizing the classifier from the self-focused task to the other-focused task, we also accurately decoded whether subjects focused on the emotional actions, interoceptive sensations and situations of others. These results show that the neural patterns that underlie self-imagined experience are involved in understanding the experience of other people. This supports the theoretical assumption that the basic components of emotion experience and understanding share resources in the brain. 2.1 Introduction To navigate the social world successfully it is crucial to understand other people. But how do people generate meaningful representations of other people’s actions, sensations, thoughts and emotions? The dominant view assumes that representations of other people’s experiences are supported by the same neural systems as those that are involved in generating experience in the self (e.g., Gallese et al., 2004; see for an overview Singer, 2012). We tested this principle of self-other neural overlap directly, using multi-voxel pattern analysis (MVPA), across three different aspects of experience that are central to emotions: actions, sensations from the body and situational knowledge. In recent years, evidence has accumulated that suggests a similarity between the neural patterns representing the self and others. For example, a great variety of studies have shown that observing actions and sensations in other people engages similar neural circuits as acting and feeling in the self (see for an overview Bastiaansen et al., 2009). Moreover, an extensive research program on pain has demonstrated an overlap between the experience of physical pain and the observation of pain in other people, utilizing both neuroimaging techniques (e.g., Lamm et al., 2011) and analgesic interventions (e.g., Rütgen et al., 2015; Mischkowski et al., 2016). This process of “vicarious experience” or “simulation” is viewed as an important component of empathy (Carr et al., 2003; Decety, 2011; Keysers &amp; Gazzola, 2014). In addition, it is argued that mentalizing (e.g. understanding the mental states of other people) involves the same brain networks as those involved in self-generated thoughts (Uddin et al., 2007; Waytz &amp; Mitchell, 2011). Specifying this idea further, a constructionist view on emotion proposes that both emotion experience and interpersonal emotion understanding are produced by the same large-scale distributed brain networks that support the processing of sensorimotor, interoceptive and situationally relevant information (Barrett &amp; Satpute, 2013; Oosterwijk &amp; Barrett, 2014). An implication of these views is that the representation of self- and other-focused emotional actions, interoceptive sensations and situations overlap in the brain. Although there is experimental and theoretical support for the idea of self-other neural overlap, the present study is the first to directly test this process using MVPA across three different aspects of experience (i.e. actions, interoceptive sensations and situational knowledge). Our experimental design consisted of two different tasks aimed at generating self- and other-focused representations with a relatively large weight given to either action information, interoceptive information or situational information. In the self-focused emotion imagery task (SF-task) subjects imagined performing or experiencing actions (e.g., pushing someone away), interoceptive sensations (e.g., increased heart rate) and situations (e.g., alone in a park at night) associated with emotion. Previous research has demonstrated that processing linguistic descriptions of (emotional) actions and feeling states can result in neural patterns of activation associated with, respectively, the representation and generation of actions and internal states (Oosterwijk et al., 2015; Pulvermüller &amp; Fadiga, 2010). Furthermore, imagery-based inductions of emotion have been successfully used in the MRI scanner before (Oosterwijk et al., 2012; Wilson-Mendenhall et al., 2011), and are seen as robust inducers of emotional experience (Lench et al., 2011). In the other-focused emotion understanding task (OF-task), subjects viewed images of people in emotional situations and focused on actions (i.e., How does this person express his/her emotions?), interoceptive sensations (i.e., What does this person feel in his/her body) or the situation (i.e., Why does this person feel an emotion?). This task is based on previous research studying the neural basis of emotion oriented mentalizing (Spunt &amp; Lieberman, 2012). With MVPA, we examined to what extent the SF- and OF-task evoked similar neural patterns. MVPA allows researchers to assess whether the neural pattern associated with one set of experimental conditions can be used to distinguish between another set of experimental conditions. This relatively novel technique has been successfully applied to the field of social neuroscience in general (e.g., Gilbert et al., 2012; Brosch et al., 2013; Parkinson et al., 2014), and the field of self-other neural overlap in particular. For example, several MVPA studies recently assessed whether experiencing pain and observing pain in others involved similar neural patterns (Corradi-Dell’Acqua et al., 2016; Krishnan et al., 2016). Although there is an ongoing discussion about the specifics of shared representation in pain based on these MVPA results (see for an overview Zaki et al., 2016), many authors emphasize the importance of this technique in the scientific study of self-other neural overlap (e.g., Corradi-Dell’Acqua et al., 2016; Krishnan et al., 2016). MVPA is an analysis technique that decodes latent categories from fMRI data in terms of multi-voxel patterns of activity (Norman et al., 2006a). This technique is particularly suited for our research question for several reasons. First of all, although univariate techniques can demonstrate that tasks activate the same brain regions, only MVPA can statistically test for shared representation (Lamm &amp; Majdandžić, 2015). We will evaluate whether multivariate brain patterns that distinguish between mental events in the SF-task can be used to distinguish, above chance level, between mental events in the OF-task. Second, MVPA analyses are particularly useful in research that is aimed at examining distributed representations (Singer, 2012). Based on our constructionist framework, we indeed hypothesize that the neural patterns that will represent self- and other focused mental events are distributed across large-scale brain networks. To capture these distributed patterns, we used MVPA in combination with data-driven univariate feature selection on whole-brain voxel patterns, instead of limiting our analysis to specific regions-of-interest (Haynes, 2015). And third, in contrast to univariate analyses that aggregate data across subjects, MVPA can be performed within-subjects and is thus able to incorporate individual variation in the representational content of multivariate brain patterns. In that aspect within-subject MVPA is sensitive to individual differences in how people imagine actions, sensations and situations, and how they understand others. In short, for our purpose to explicitly test the assumption that self and other focused processes share neural resources, MVPA is the designated method. We tested the following two hypotheses. First, we tested whether we could classify self-imagined actions, interoceptive sensations and situations above chance level. Second, we tested whether the multivariate pattern underlying this classification could also be used to classify the how, what and why condition in the other-focused task. 2.2 Methods 2.2.1 Subjects In total, we tested 22 Dutch undergraduate students from the University of Amsterdam (14 females; Mage = 21.48, s.d.age = 1.75). Of those 22 subjects, 13 subjects were tested twice in 2 sessions about 1 week apart. Half of those sessions were used for the model optimization procedure. The other half of the sessions, combined with an additional nine subjects (who were tested only once), constituted the model validation set (see Model optimization procedure section). In total, two subjects were excluded from the model validation dataset: one subject was excluded because there was not enough time to complete the experimental protocol and another subject was excluded due to excessive movement (&gt;3 mm within data acquisition runs). All subjects signed informed consent prior to the experiment. The experiment was approved by the University of Amsterdam’s ethical review board. Subjects received 22.50 euro per session. Standard exclusion criteria regarding MRI safety were applied and people who were on psychopharmacological medication were excluded a priori. 2.2.2 Experimental design 2.2.2.1 Self-focused emotion imagery task The self-focused emotion imagery task (SF-task) was created to preferentially elicit self-focused processing of action, interoceptive or situational information associated with emotion. Subjects processed short linguistic cues that described actions (e.g., pushing someone away; making a fist), interoceptive sensations (e.g., being out of breath; an increased heart rate), or situations (e.g., alone in a park at night; being falsely accused) and were instructed to imagine performing or experiencing the content. The complete instruction is presented in the Supplementary Materials; all stimuli used in the SF-task are presented in Supplementary Table A.1. Linguistic cues were selected from a pilot study performed on an independent sample of subjects (n = 24). Details about this pilot study are available on request. The descriptions generated in this pilot study were used as qualitative input to create short sentences that described actions, sensations or situations that were associated with negative emotions, without including discrete emotion terms. The cues did not differ in number of words, nor in number of characters (F &lt; 1). The SF-task was performed in two runs subsequent to the other-focused task using the software package Presentation (Version 16.4, www.neurobs.com). Each run presented 60 sentences on a black background (20 per condition) in a fully randomized event-related fashion, with a different randomization for each subject. Note that implementing a separate randomization for each subject prevents inflated false positive pattern correlations between trials of the same condition, which may occur in single-trial designs with short inter-stimulus intervals (Mumford et al., 2014). A fixed inter-trial–interval of 2 seconds separating trials; 12 null-trials (i.e. a black screen for 8 seconds) were mixed with the experimental trials at random positions during each run (see Figure 2.1). Figure 2.1: Overview of the self-focused and other-focused task. 2.2.2.2 Other-focused emotion understanding task The other-focused emotion understanding task (OF-task) was created to preferentially elicit other-focused processing of action, interoceptive or situational information associated with emotion. Subjects viewed images of people in negative situations (e.g. a woman screaming at a man, a man held at gunpoint). A red rectangle highlighted the face of the person that the subjects should focus on to avoid ambiguity in images depicting more than one person. Image blocks were preceded by a cue indicating the strategy subjects should use in perceiving the emotional state of the people in the images (Spunt &amp; Lieberman, 2012). The cue How instructed the subjects to identify actions that were informative about the person’s emotional state (i.e., How does this person express his/her emotions?). The cue What instructed subjects to identify interoceptive sensations that the person could experience (i.e., What does this person feel in his/her body). The cue Why instructed subjects to identify reasons or explanations for the person’s emotional state (i.e., Why does this person feel an emotion?). The complete instruction is presented in the Supplementary Materials. Stimuli for the OF-task were selected from the International Affective Picture System database (IAPS; Lang, 2005; Lang et al., 1997), the image set developed by the Kveraga lab (http://www.kveragalab.org/stimuli.html; Kveraga et al., 2015) and the internet (Google images). We selected images based on a pilot study, performed on an independent sample of subjects (n = 22). Details about this pilot study are available on request. The OF-task was presented using the software package Presentation. The task presented thirty images on a black background in blocked fashion, with each block starting with a what, why or how cue (see Figure 2.1). Each image was shown three times, once for each cue type. Images were presented in blocks of six, each lasting 6 seconds, followed by a fixed inter trial interval of 2 seconds. Null-trials were inserted at random positions within the blocks. Both the order of the blocks and the specific stimuli within and across blocks were fully randomized, with a different randomization for each subject. 2.2.3 Procedure Each experimental session lasted about 2 hours. Subjects who underwent two sessions had them on different days within a time span of 1 week. On arrival, subjects gave informed consent and received thorough task instructions, including practice trials (see the Supplementary Materials for a translation of the task instructions). The actual time in the scanner was 55 minutes, and included a rough 3D scout image, shimming sequence, 3-min structural T1-weighted scan, one functional run for the OF-task and two functional runs for the SF-task. We deliberately chose to present the SF-task after the OF-task to exclude the possibility that the SF-task affected the OF-task, thereby influencing the success of the decoding procedure. After each scanning session, subjects rated their success rate for the SF-task and OF-task (see Supplementary Figure A.1). In the second session, subjects filled out three personality questionnaires that will not be further discussed in this paper and were debriefed about the purpose of the study. 2.2.4 Image acquisition Subjects were tested using a Philips Achieva 3T MRI scanner and a 32-channel SENSE headcoil. A survey scan was made for spatial planning of the subsequent scans. Following the survey scan, a 3-min structural T1-weighted scan was acquired using 3D fast field echo (TR: 82 ms, TE: 38 ms, flip angle: 8°, FOV: 240 × 188 mm, 220 slices acquired using single-shot ascending slice order and a voxel size of 1.0 × 1.0 × 1.0 mm). After the T1-weighted scan, functional T2*-weighted sequences were acquired using single shot gradient echo, echo planar imaging (TR = 2000 ms, TE = 27.63 ms, flip angle: 76.1°, FOV: 240 × 240 mm, in-plane resolution 64 × 64, 37 slices (with ascending acquisition), slice thickness 3 mm, slice gap 0.3 mm, voxel size 3 × 3 × 3 mm), covering the entire brain. For the SF-task, 301 volumes were acquired; for the OF-task 523 volumes were acquired. 2.3 Model optimization procedure As MVPA is a fairly novel technique, no consistent, optimal MVPA pipeline has been established (Etzel et al., 2011). Therefore, we adopted a validation strategy in the present study that is advised in the pattern classification field (Kay et al., 2008; Kriegeskorte et al., 2009). This strategy entailed that we separated our data into an optimization dataset to find the most optimal parameters for preprocessing and analysis, and a validation dataset to independently verify classification success with those optimal parameters. We generated an optimization and validation dataset by running the SF-task and OF-task twice, in two identical experimental sessions for a set of thirteen subjects. The sessions were equally split between the optimization and validation set (see Figure 2A); first and second sessions were counterbalanced between the two sets. Based on a request received during the review process, we added nine new subjects to the validation dataset. Ultimately, the optimization-set held 13 sessions and the validation-set, after exclusion of 2 subjects (see Subjects section), held 20 sessions. Figure 2.2: Schematic overview of the cross-validation procedures. A) The partitioning of the dataset into an optimization-set (used for tuning of preprocessing and MVPA hyperparameters) and a validation-set (used to get a fully cross-validated, unbiased estimate of classification performance). The preprocessing and MVPA hyperparameters yielded from the optimization procedure were subsequently applied to the preprocessing and MVPA pipeline of the validation-set. B) The within-subject MVPA pipeline of the self- and cross-analysis implemented in a repeated random subsampling scheme with 100,000 iterations. In each iteration, 90% of the self-data trials (i.e. train-set) were used for estimating the scaling parameters, performing feature selection and fitting the SVM. These steps of the pipeline (i.e. scaling, feature selection, SVM fitting) were subsequently applied to the independent test-set of both the self-data trials and the other-data trials. In the optimization-set, we explored how different preprocessing options and the so-called ‘hyperparameters’ in the MVPA pipeline affected the performance of the (multivariate) analyses (visualized in Figure 2.2B; see MVPA pipeline subsection for more details). Thus, we performed the self- and cross-analyses on the data of the optimization set multiple times with different preprocessing options (i.e., smoothing kernel, low-pass filter and ICA-based denoising strategies) and MVPA hyperparameter values (i.e., univariate feature selection threshold and train/test size ratio during cross-validation). We determined the optimal parameters on the basis of classification performance, which was operationalized as the mean precision value after a repeated random subsampling procedure with 1000 iterations. A list with the results from the optimization procedure can be found in Supplementary Table A.2 and Supplementary Figure A.2. The optimal parameters were then used for preprocessing and the self- and cross-analysis within the validation-set, in which the findings from the optimization-set were replicated. All findings discussed in the 2.4 section follow from the validation-set (see Supplementary Figure A.3 for an overview of the findings from the optimization-set). 2.3.1 Preprocessing and single-trial modeling Functional and structural data were preprocessed and analyzed using FSL 5.0 (Jenkinson et al., 2012) and MATLAB (2012b; www.mathworks.com/products/matlab), using an in-house developed preprocessing pipeline and the parameters established in the optimization procedure. Functional data were corrected for motion (using FSL MCFLIRT) and slice timing and was spatially smoothed (5 mm isotropic kernel). After preprocessing, individual time series were modeled using a double-gamma hemodynamic response function in a single-trial GLM design using FSL’s FEAT. Resulting beta values were converted to t-values (Misaki et al., 2010), constituting a whole-brain pattern of t-values per trial. Subsequently, the data were indexed by a gray-matter mask (excluding most white-matter, CSF and brainstem voxels). Thus, the data points for the MVPA consist of whole-brain (gray matter) t-value patterns per trial. For the optimization analyses, the data were transformed to standard space (MNI152, 2 mm) using FSL’s FNIRT. To reduce computation time for the validation data, and in particular its corresponding permutation analysis, analyses on the validation dataset were performed on data in native (functional) space. 2.3.2 Multi-voxel pattern analysis 2.3.2.1 MVPA pipeline Within the optimization and validation dataset, we implemented an iterated cross-validation scheme that separated the data into a train-set and a test-set (this procedure is described in more detail in the next section). Before fitting the classifier on the train-set in each iteration of the cross-validation scheme, standardization and voxel selection were estimated and applied to the train-set. Standardization ensured that each feature (i.e., voxel) had zero mean and unit variance across trials. After standardization, voxel selection was performed in each iteration on the train-set by extracting the voxels with the highest average pairwise Euclidian distance across classes, which will be subsequently referred to as a voxel’s differentiation score. More specifically, differentiation scores were calculated by subtracting the mean value across trials per class from each other (i.e., action—interoception, action—situation, interoception—situation), normalizing these values across voxels (yielding “z-scores”), and taking their absolute value. The three resulting values per voxel were averaged and the most differentiating voxels (z-score threshold: 2.3, as determined by the optimization procedure; see Model optimization procedure section) were extracted and used as features when fitting the classifier. Importantly, the standardization parameters (voxel mean and variance) and voxel indices (i.e. which voxels had differentiation scores above threshold) were estimated from the train-set only and subsequently applied to the test-set to ensure independence between the train- and test-set (see Figure 2B). After standardization and voxel selection in each iteration, a support vector classifier (SVC) was fit on the train-set and cross-validated on the test-set, generating a class probability for each trial in the test-set. Our classifier of choice was the SVC implementation from the scikit-learn svm module (Pedregosa et al., 2011) with a linear kernel, fixed regularization parameter (C) of 1.0, one-vs-one multiclass strategy, estimation of class probability output (instead of discrete class prediction) and otherwise default parameters. 2.3.2.2 Cross-validation scheme and bagging procedure Cross-validation of the classification analysis was implemented using a repeated random subsampling cross-validation scheme (also known as Monte Carlo cross-validation), meaning that, for each iteration of the analysis, the classification pipeline (i.e., standardization, voxel selection and SVM fitting) was applied on a random subset of data points (i.e., the train-set) and cross-validated on the remaining data (i.e., the test-set). Each trial belonged to one out of three classes: action, interoception or situation. Following the results from the parameter optimization process, we selected four trials per class for testing, amounting to 12 test-trials per iteration. Per iteration, the classifier was fit on the train-set from the SF-data. Subsequently, this classifier was cross-validated on 12 test SF-trials (test-set “self-analysis”) and 12 test OF-trials (test-set “cross-analysis”; see Figure 2B). This process was subsequently iterated 100 000 times to generate a set of class distributions for each trial. After all iterations, the final predicted class of each trial was determined by its highest summed class probability across iterations (also known as “soft voting”; see Supplementary Figure A.4). This strategy of a random sub-sampling cross-validation scheme in combination with majority (soft) voting is more commonly known as “bagging” (Breiman, 1996). An important advantage of bagging is that it reduces model overfitting by averaging over an ensemble of models, which is especially useful for multi-voxel pattern analyses because fMRI data is known to display high variance (Varoquaux, 2018). After generating a final prediction for all trials using the soft voting method, we constructed confusion matrices for both the self- and cross-analysis. In each raw confusion matrix with prediction counts per class, cells were normalized by dividing prediction counts by the sum over rows (i.e., the total amount of predictions per class), yielding precision-scores (also known as positive predictive value). In other words, this metric represents the ratio of true positives to the sum of true positives and false positives (see Supplementary Figure A.5 for a description of the results expressed as recall estimates, or the ratio of true positives to the total number of samples in that class). This classification pipeline generated subject-specific confusion matrices that were subsequently averaged to generate the final classification scores. 2.3.2.3 Statistical evaluation To evaluate the statistical significance of the observed average precision-scores in the confusion matrices, we permuted the original self- and cross-analysis 1300 times per subject with randomly shuffled class labels, yielding 1300 confusion matrices (with precision-scores). We then averaged the confusion matrices across subjects, yielding 1300 permuted confusion matrices reflecting the null-distribution of each cell of the matrix (which is centered around chance level classification, i.e., 33%). For each cell in the diagonal of the observed confusion matrix, p-values were calculated as the proportion of instances of values in the permuted matrix which were higher than the values in the observed matrix (Nichols &amp; Holmes, 2002). To correct for multiple comparisons, p-values were tested against a Bonferroni-corrected threshold. The distribution of precision-scores and the relationship between precision-scores in the self- and cross-analysis is reported in Supplementary Figure A.6. 2.3.2.4 Spatial representation To visualize the classifier feature weights, we plotted the absolute feature weights averaged over iterations, subjects and pairwise classifiers (action vs interoception, action vs situation, interoception vs situation) that underlie our multiclass classification analysis. We chose to visualize the spatial representation of our model by plotting the average absolute feature weights, because the absolute value of feature weights in linear SVMs can be interpreted as how important the weights are in constructing the model’s decision hyperplane (Ethofer et al., 2009; Guyon et al., 2002; Stelzer et al., 2014). To correct for a positive bias in plotting absolute weights, we ran the main classification analysis again with permuted labels to extract the average absolute feature weights that one would expect by chance. Subsequently, a voxel-wise independent t-test was performed for all feature weights across subjects, using the average permuted feature weights as the null-hypothesis, yielding an interpretable t-value map (see the supplementary code notebook on our Github repository for computational details). 2.3.3 Additional analyses In addition to the self-analysis and the self-to-other cross-analysis presented in the main text, we also performed a within-subjects other-to-self cross-analysis (see for a similar approach Corradi-Dell’Acqua et al., 2016) and a between-subjects self-analysis and self-to-other cross-analysis. These analyses forward largely similar results as the analyses presented in the main text. Due to space constraints, we present these additional analyses in the Supplementary Materials. Supplementary Figure A.7 represents confusion matrices with precision and recall estimates for the other-to-self cross-analysis. Supplementary Figure A.8 presents the results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. 2.3.4 Univariate analysis To be complete, we also report a set of univariate analyses performed on the SF-task and the OF-task data. The univariate analyses were performed on the validation dataset, and were subject to the same preprocessing steps as the MVPA analysis, except that we did not model each trial, but each condition as a separate regressor. The group-level analysis was performed with FSL’s FLAME1 option. To examine differences in neural activity between conditions, we calculated contrasts between the three classes in the SF-task (self-action vs self-interoception; self-action vs self-situation and self-interoception vs self-situation) and the three classes in the OF-task (other-action vs other-interoception; other-action vs other-situation and other-interoception vs other-situation). We report clusters that were corrected using cluster-correction with a voxel-wise threshold of 0.005 (z = 2.7) and a cluster-wise p-value threshold of 0.05. 2.3.5 Code availability The MVPA-analysis and subsequent (statistical) analyses were implemented using custom Python scripts, which depend heavily on the skbold package, a set of tools for machine learning analyses of fMRI data developed in-house (see https://github.com/lukassnoek/skbold). The original scripts were documented and are hosted at the following Github repository: https://github.com/lukassnoek/SharedStates. 2.4 Results 2.4.1 Multi-voxel pattern analysis The analyses of the SF-task demonstrated that voxel patterns reflecting imagined self-focused actions, interoceptive sensations and situations associated with emotion could be decoded accurately for each individual class (all p &lt; 0.001, see Figure 2.3). Furthermore, when we generalized the classifier based on the SF-task to the data from the OF-task (i.e. cross-analysis), we found that neural representations of emotional actions, interoceptive sensations and situations of others could also be reliably decoded above chance (all p &lt; 0.001; see Figure 2.3). Supplementary Table A.3 presents mean precision-scores across classes for each subject separately. As predicted, our findings demonstrate that self-imagined actions, interoceptive sensations and situations are associated with distinct neural patterns. Furthermore, and as predicted, our findings demonstrate that the patterns associated with self-imagined actions, sensations and situations can be used to decode other-focused actions, interoceptive sensations and situations (see Supplementary Figure A.7 for the complementary other-to-self cross-analysis). Figure 2.3: Confusion matrices for the self- (left diagram) and cross-analysis (right diagram). Values indicate precision-scores, representing the proportion of true positives given all predictions for a certain class. Note that action and interoception columns in the cross-analysis confusion matrix do not add up to 1, which is caused by the fact that, for some subjects, no trials were predicted as action or interoception, rendering the calculation of precision ill-defined (i.e., division by zero). In this case, precision scores were set to zero. To visualize which neural regions were involved in the successful decoding of the three classes in the OF-task and SF-task, we display in Figure 2.4 the averaged absolute values of the SVM feature weights. Note that Figure 2.4 only displays one feature map, as both the self and cross-analysis depend on the same model. Regions displaying high and consistent feature weights across subjects were frontal pole (including parts of the dorsomedial prefrontal cortex and ventromedial prefrontal cortex), orbitofrontal cortex (OFC), inferior frontal gyrus (IFG), superior frontal gyrus (SFG), middle frontal gyrus (MFG), insular cortex, precentral gyrus, postcentral gyrus, posterior cingulate cortex/precuneus, superior parietal lobule (SPL), supramarginal gyrus (SMG), angular gyrus (AG), middle temporal gyrus (MTG), temporal pole (TP), lateral occipital cortex (lOC) and occipital pole (see Supplementary Table A.4 for an overview of all involved regions). Figure 2.4: Uncorrected t-value map of average feature weights across subjects; t-values were calculated by dividing the average absolute feature weights, which was corrected for positive bias by subtracting the mean permuted absolute weight across all iterations, by the standard error across subjects. Only voxels belonging to clusters of 20 or more voxels are shown. 2.4.2 Univariate analyses Figure 2.5 displays the pattern of neural activity revealed by univariate contrasts between the three different classes in the SF-task and the OF-task. For the sake of brevity, we summarize the most relevant univariate results here. Please see the Supplementary Materials and the study’s Github repository for an overview of all clusters. Figure 2.5: Univariate contrasts for the self-focused and other-focused task. In the SF-task, action was associated with increased involvement of the MFG, SFG, AG, SMG, lOC and middle temporal gyrus (temporo-occipital) when compared with interoception, and increased involvement of the IFG, MFG, SFG, anterior cingulate cortex (ACC), supplementary motor area (SMA), precentral gyrus, postcentral gyrus, insular cortex, SMG, SPL, lOC and middle temporal gyrus (temporo-occipital) when compared with situation. Interoception was associated with increased involvement of the insular cortex, precentral gyrus, postcentral gyrus and central operculum when compared with action, and increased involvement of the insular cortex, central operculum, parietal operculum, IFG, frontal pole, ACC, SMA, precentral gyrus, postcentral gyrus, SMG, SPL and putamen when compared with situation. The situation vs action contrast and the situation vs interoception contrast forwarded clusters in similar regions, including the temporal pole, superior/middle temporal gyrus, IFG, SFG, frontal pole, medial prefrontal cortex (mPFC), OFC, precuneus, posterior cingulate cortex (PCC), lOC, fusiform gyrus, hippocampus and lingual gyrus. In the OF-task, action was associated with increased involvement of the IFG, MFG, SFG, precentral gyrus, postcentral gyrus, SMG, SPL, middle/inferior temporal gyrus (temporo-occipital), lOC and fusiform gyrus, when compared with interoception, and increased involvement of the IFG, MFG, SFG, frontal pole, precentral gyrus, postcentral gyrus, SMG, SPL, middle/inferior temporal gyrus (temporo-occipital) and lOC, when compared with situation. Interoception was associated with increased involvement of the left frontal pole when compared with action, and increased involvement of the SMG, SPL, precentral gyrus, postcentral gyrus, PCC, IFG and frontal pole, when compared with situation. The situation vs action contrast and the situation vs interoception contrast forwarded clusters in similar regions, including the temporal pole, superior/middle temporal gyrus, frontal pole, mPFC, PCC, precuneus, AG, lOC, occipital pole, fusiform gyrus and lingual gyrus. 2.5 Discussion In this study, we investigated the neural overlap between self-focused emotion imagery and other-focused emotion understanding using a decoding approach. The results confirmed our hypothesis that other-focused representations of emotion-related actions, bodily sensations and situations can be decoded from neural patterns associated with accessing similar sources of information in a self-focused task. This cross-classification was successful even though the tasks employed different stimulus materials and instructions. Thus, the observed neural overlap between the underlying processes in the SF-task and OF-task cannot be attributed to similarities in stimulus dimensions or task instructions. Rather, we conclude from our findings that emotion experience and emotion understanding have basic psychological processes in common. Although we could successfully classify the interoception class in the SF-task (across both datasets), and in the OF-task in the validation dataset, we were not able to successfully classify the interoception class in the OF-task in the optimization dataset. Furthermore, although precision and recall metrics demonstrated similar results for the action and situation cross-classification in the validation dataset, these metrics demonstrated different results for the classification of the interoception class (see Supplementary Figure A.5). This difference was partly driven by the fact that trials were very infrequently classified as interoception in the cross-classification analysis. The finding that subjects reported lower success rates for the what trials in which they were asked to identify interoceptive sensations in other people than for the how (action) and why (situation) trials may point to a possible explanation for the inconsistent findings regarding interoception. Although speculative, it may be relatively easy to recognize (and represent) interoceptive sensations when they are described in words (as in the SF-task), but relatively hard to deduce these sensations when only diffuse cues about someone’s internal state are available (e.g. posture, frowning facial expression, as in the OF-task). An exploration of the spatial characteristics of the distributed neural pattern associated with successful decoding of the SF-task and OF-task revealed regions that are commonly active during self- and other-focused processing. First, we found that successful classification was associated with voxels in the precentral gyrus, IFG, SMA and SPL. These same regions were also revealed by the univariate analyses, in particular for the action and interoception classes. These regions are part of the so-called “mirror” network, which is argued to support both action planning and action understanding (Bastiaansen et al., 2009; Gallese et al., 2004; Spunt &amp; Lieberman, 2012; Van Overwalle &amp; Baetens, 2009). Furthermore, we found that successful classification was associated with voxels in the lateral occipital cortex and fusiform gyrus, which have been linked in the literature to the processing of both concrete and abstract action (Wurm &amp; Lingnau, 2015) and the (visual) processing of emotional scenes, faces and bodies (Gelder et al., 2010; Sabatinelli et al., 2011). The univariate analyses demonstrated activity in the lOC and the fusiform gyrus in particular for the situation class, both when subjects viewed images of other people in emotional situations, and when subjects imagined being in an emotional situation themselves. Second, we found that successful classification was associated with voxels in regions associated with somatosensory processing (postcentral gyrus) and the representation of interoceptive sensations (insular cortex, see Craig &amp; Craig, 2009; Medford &amp; Critchley, 2010). Univariate analyses of the SF-task also demonstrated involvement of these regions for both the action and interoception classes. This pattern of activation is consistent with embodied cognition views that propose that thinking about or imagining bodily states is grounded in simulations of somatosensory and interoceptive sensations (Barsalou, 2009). In contrast to previous work on interoceptive simulation when observing pain or disgust in other people (cf. Bastiaansen et al., 2009; Lamm et al., 2011), the univariate analyses of the OF-task did not demonstrate insular cortex activation for the interoception class. And third, we found that successful classification was associated with voxels in the middle temporal gyrus (including the temporal pole), PCC/precuneus, dmPFC and vmPFC. These regions are part of the so-called “mentalizing” network (or “default” network). This same network was also revealed by the univariate analyses, in particular for the situation class. Meta-analyses have demonstrated that the mentalizing network is commonly active during tasks involving emotion experience and perception (Lindquist et al., 2012), mentalizing/theory of mind (Spreng et al., 2009; Van Overwalle &amp; Baetens, 2009), judgments about the self and others (Denny et al., 2012) and semantic/conceptual processing in general (Binder et al., 2009). Moreover, this network contributes to the representation of emotion knowledge (Peelen et al., 2010) and is involved in both empathy (Keysers &amp; Gazzola, 2014; Zaki &amp; Ochsner, 2012) and self-generated thought (Andrews-Hanna et al., 2014). We propose that this network supports the implementation of situated knowledge and personal experience that is necessary to generate rich mental models of emotional situations, both when experienced individually, and when understood in someone else (cf. Barrett &amp; Satpute, 2013; Oosterwijk &amp; Barrett, 2014). The most important contribution of our study is that it provides direct evidence for the idea of shared neural resources between self-and other focused processes. It is important, however, to specify what we think this “sharedness” entails. In research on pain, there is an ongoing discussion about whether experiencing pain and observing pain in others are distinct processes (Krishnan et al., 2016), or whether experiencing and observing pain involve a shared domain-specific representation (e.g., a discrete pain-specific brain state; Corradi-Dell’Acqua et al., 2016) and/or the sharing of domain-general processes (e.g. general negative affect; Zaki et al., 2016). Connecting to this discussion, we think that it is unlikely that our decoding success reflects the sharing of discrete experiential states between the SF-task and OF-task. After all, unlike in studies on pain, the stimuli in our tasks referred to a large variety of different actions, sensations and situations. Instead, decoding success in our study is most likely due to shared brain state configurations, reflecting the similar engagement of domain-general processes evoked by self- and other-focused instances of action (or interoceptive sensation or situation). This interpretation is consistent with views that suggests that global processes are shared between pain experience and pain observation (Lamm et al., 2011; Zaki et al., 2016) or between self- and other-focused tasks in general (e.g., Legrand &amp; Ruby, 2009). Moreover, this interpretation is consistent with the suggestion that neural re-use is a general principle of brain functioning (e.g., Anderson, 2016). In our constructionist view, we posit that emotion imagery and understanding share basic psychological processes (cf. Oosterwijk &amp; Barrett, 2014). More specifically, both emotion imagery and understanding are “conceptual acts” in which the brain generates predictions based on concept knowledge (including sensorimotor and interoceptive predictions) that are meaningful within a particular situational context (Barrett, 2012; Barrett &amp; Simmons, 2015). Based on accumulating evidence, we propose that these predictions are implemented in domain-general brain networks (cf. Oosterwijk et al., 2012; Barrett &amp; Satpute, 2013). The relative contribution of these networks depends on the demands of the situational context. Specifically, in contexts where people are focused on actions and expressions (their own or someone else’s) a network that supports the representation of sensorimotor states (i.e., the mirror system) may contribute relatively heavily; in contexts where people are focused on bodily states (their own or someone else’s) a network that supports the representation of interoceptive states (i.e., the salience network) may contribute relatively heavily; and in contexts where people are focused on interpreting a situation (their own or someone else’s) a network that supports a general inferential meaning-making function (i.e., the mentalizing network) may contribute relatively heavily (see also Oosterwijk et al., 2015). We believe that it is likely that our ability to successfully distinguish between classes in the self-task relies on the relatively different patterns of activity across these networks for actions, interoceptive sensations and situations. Regarding our ability to successfully generalize from the self- to the other-focused task, we believe that this relies on the relatively similar pattern of activity across these networks when people generate self-focused or other-focused instances of action (or interoceptive sensation or situation). Our explicit manipulation of the weight of action, interoceptive and situational information in the SF-task and the OF-task tests the possibility of shared representation in a novel way. Although this procedure may seem artificial, social neuroscience studies support the notion that there is contextual variety in the contribution of action, interoceptive, and situation information when understanding other people (Oosterwijk et al., 2015; Van Overwalle &amp; Baetens, 2009). Moreover, this weighting may mimic the variability with which these sources of information contribute to different instances of subjective emotional experience in reality (Barrett, 2012). In future directions, it may be relevant to apply the current paradigm to the study of individuals in which access to these sources of information is disturbed (e.g., individuals with different types of psychopathology) or facilitated (e.g., individuals with high interoceptive sensitivity). In short, the present study demonstrates that the neural patterns that support imagining “performing an action”, “feeling a bodily sensation” or “being in a situation” are directly involved in understanding other people’s actions, sensations and situations. This supports our prediction that self- and other-focused emotion processes share resources in the brain. 2.6 Acknowledgements The authors would like to thank David Amodio for his helpful comments on a previous draft of this manuscript. "],
["confounds-decoding.html", "3 How to control for confounds in decoding analyses of neuroimaging data 3.1 Introduction 3.2 Methods 3.3 Results 3.4 Discussion 3.5 Conclusions", " 3 How to control for confounds in decoding analyses of neuroimaging data This chapter has been published as: Snoek, L.*, Miletić, S.*, &amp; Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. NeuroImage, 184, 741-760. * Shared first authorship Abstract Over the past decade, multivariate “decoding analyses” have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound “brain size”. We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one’s decoding analysis. 3.1 Introduction In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data (Haxby, 2012; Norman et al., 2006b). The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is “decoding”, in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns (Haxby et al., 2001) and working memory representations from EEG data (LaRocque et al., 2013), as well between-subject factors such as Alzheimer’s disease (vs. healthy controls) from structural MRI data (Cuingnet et al., 2011) and major depressive disorder (vs. healthy controls) from resting-state functional connectivity (Craddock et al., 2009). One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do (Jimura &amp; Poldrack, 2012). A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses (Hebart &amp; Baker, 2017). In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical (Allefeld et al., 2016; Davis et al., 2014; Gilron et al., 2017; Haufe et al., 2014) and more conceptual (Naselaris &amp; Kay, 2015; Weichwald et al., 2015) in nature. For the purposes of the current study, we focus on the specific criticism put forward by Naselaris &amp; Kay (2015) , who argue that decoding analyses are inherently ambiguous in terms of what information they use (see Popov et al., 2018 for a similar argument in the context of encoding analyses). This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information (see also Carlson &amp; Wardle, 2015; Ritchie et al., 2017; Weichwald et al., 2015). The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of “source ambiguity”. To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender1 (male/female) from structural MRI with the aim of contributing to the understanding of gender differences — an endeavor that generated considerable interest and controversy (Chekroud et al., 2016; Del Giudice et al., 2016; Glezerman, 2016; Joel &amp; Fausto-Sterling, 2016; Rosenblatt, 2016). By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant’s gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways (Good et al., 2001; O’Brien et al., 2011). First, there is a global difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight (18.7%; Gur et al., 1999; Lüders et al., 2002)2. Second, brains of men and women are known to differ locally: some specific brain areas are on average larger in women than in men (e.g., in superior and middle temporal cortex; Good et al., 2001) and vice versa (e.g., in frontomedial cortex; Goldstein et al., 2001). One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male bodies are on average larger than female bodies (Gur et al., 1999; Sepehrband et al., 2018). As such, our hypothetical researcher is likely primarily interested in the local sources of variation in the neuroanatomy of male and female brains. Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for “uninteresting” sources of information. 3.1.1 Partitioning effects into true signal and confounded signal Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis (Hebart &amp; Baker, 2017). In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate prediction, but not in interpretability of the model (Bzdok, 2017; Haufe et al., 2014; Hebart &amp; Baker, 2017). In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem3. In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is only using information in the neuroimaging data that is related to the variable that is being decoded (Ritchie et al., 2017). In this scenario, source ambiguity (i.e., the presence of multiple sources of information) is problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is confounding the decoding analysis of gender. Here, we define a confound as a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data. To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable “schizophrenia-or-not” is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender (i.e., men are more often diagnosed with schizophrenia than women; McGrath et al., 2008) and substance abuse (Dixon, 1999), which are also known to affect gray matter (Bangalore et al., 2008; Gur et al., 1999; Van Haren et al., 2013). As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate prediction of schizophrenia, then this ambiguity due to confounding sources of information is problematic. Importantly, as our definition suggests, what is or is not regarded as a confound is relative — it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable “schizophrenia-or-no”” becomes the confounding variable. In other words, one researcher’s signal is another researcher’s confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the “true signal” (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the “confounded signal” (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure 3.1). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses. Figure 3.1: Visualization of how variance in brain data (X) can partitioned into “True signal” and “Confounded signal”, depending on the correlation structure between the brain data (X), the confound (C), and the target (y). Overlapping circles indicate a non-zero (squared) correlation between the two variables. In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds. 3.1.2 Methods for confound control In decoding analyses, one aims to predict a certain target variable from patterns of neuroimaging data. Various methods discussed in this section are supplemented with a mathematical formalization; for consistency and readability, we define the notation we will use in Table 3.1. Table 3.1: Notation. Symbol Dims. Description \\(N\\) Number of samples (usually subjects or trials) \\(K\\) Number of neuroimaging features (e.g., voxels or sensors) \\(P\\) Number of confound variables (e.g., age, reaction time, or brain size) \\(X_{ij}\\) \\(N \\times K\\) The neuroimaging patterns (often called the “data” in the current article), where the subescript \\(i \\in {1 \\dots N}\\) refers to the individual samples (rows), and the subscript \\(j \\in {1 \\dots K}\\) to individual features (columns) \\(y\\) \\(N \\times 1\\) The target variable (i.e., what is to be decoded) \\(C\\) \\(N \\times P\\) The confound variable(s) \\(\\hat{\\beta}\\) \\(K + 1\\) The parameters estimated in a general linear model (GLM) \\(w\\) \\(K + 1\\) The parameters estimated in a decoding model \\(r_{Cy}\\) Sample Pearson correlation coefficient between \\(C\\) and \\(y\\) \\(r_{y(X.C)}\\) Sample semipartial Pearson correlation coefficient between \\(X\\) and \\(y\\), controlled for \\(C\\) \\(p(r_{Cy})\\) \\(p\\)-value of sample Pearson correlation between \\(C\\) and \\(y\\) Note: Format based on Diedrichsen and Kriegeskorte (2017). For the correlations (\\(r\\)), we assume that \\(P = 1\\) and thus that the correlations in the table reduce to a scalar. 3.1.2.1 A priori counterbalancing Ideally, one would prevent confounding variables from influencing the results as much as possible before the acquisition of the neuroimaging data.5 One common way do this (in both traditional “activation-based” and decoding analyses) is to make sure that potential confounding variables are counterbalanced in the experimental design (Görgen et al., 2017). In experimental research, this would entail randomly assigning subjects to design cells (e.g., treatment groups) such that there is no structural correlation between characteristics of the subjects and design cells. In observational designs (e.g., in the gender/brain size example described earlier), it means that the sample is chosen such that there is no correlation between the confound (brain size) and observed target variable (gender). That is, given that men on average have larger brains than women, this would entail including only men with relatively small brains and women with relatively large brains.6 The distinction between experimental and observational studies is important because the former allow the researcher to randomly draw samples from the population, while the latter require the researcher to choose a sample that is not representative of the population, which limits the conclusions that can be drawn about the population (we will revisit this issue in the Discussion section). 3.1.2.2 Include confounds in the data 3.1.2.3 Control for confounds during pattern estimation 3.1.2.4 Post hoc counterbalancing of confounds 3.1.2.5 Confound regression 3.1.3 Current study 3.2 Methods 3.2.1 Data 3.2.1.1 VBM acquisition &amp; analysis 3.2.1.2 TBSS acquisition &amp; analysis 3.2.1.3 Brain size estimation 3.2.1.4 Data and code availability 3.2.2 Decoding pipeline 3.2.3 Evaluated methods for confound control 3.2.3.1 Post hoc counterbalancing 3.2.3.2 Confound regression 3.2.3.3 Control for confounds during pattern estimation 3.2.4 Analyses of simulated data 3.2.4.1 Efficacy analyses 3.2.4.2 Analysis of positive bias after post hoc counterbalancing 3.2.4.3 Analysis of negative bias after WDCR 3.3 Results 3.3.1 Influence of brain size 3.3.2 Baseline model: no confound control 3.3.3 Post hoc counterbalancing 3.3.3.1 Empirical results 3.3.3.2 Efficacy analysis 3.3.3.3 Analysis of positive bias after post hoc counterbalancing 3.3.4 Whole-dataset confound regression (WDCR) 3.3.4.1 Empirical results 3.3.4.2 Efficacy analysis 3.3.4.3 Analysis of negative bias after WDCR 3.3.5 Cross-validated confound regression (CVCR) 3.3.5.1 Empirical results 3.3.5.2 Efficacy analysis 3.3.6 Summary methods for confound control 3.4 Discussion 3.4.1 Relevance and consequences for previous and future research 3.4.1.1 A priori and post hoc counterbalancing 3.4.1.2 Confound regression 3.4.1.3 Relevance to other analysis methods 3.4.1.4 Importance for gender decoding studies 3.4.2 Choosing a confound model: linear vs. nonlinear models 3.4.3 Practical recommendations 3.5 Conclusions The terms “gender” and “sex” are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.↩︎ Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration “squeezes” relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa (Douaud et al., 2007). This effect of global brain size similarly affects functional MRI analyses (Brodtmann et al., 2009).↩︎ However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant’s gender.↩︎ "],
["AOMIC.html", "4 The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses", " 4 The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses "],
["morbid-curiosity.html", "5 Choosing to view morbid information involves reward circuitry", " 5 Choosing to view morbid information involves reward circuitry "],
["au-limitations.html", "6 Using predictive modeling to quantify the importance and limitations of action units in emotion perception", " 6 Using predictive modeling to quantify the importance and limitations of action units in emotion perception "],
["facial-expression-models.html", "7 Comparing models of dynamic facial expression perception", " 7 Comparing models of dynamic facial expression perception "],
["summary-and-general-discussion.html", "8 Summary and general discussion 8.1 Explore! 8.2 Think big 8.3 Rethink psychology education", " 8 Summary and general discussion My view on going forward. 8.1 Explore! Theories are like toothbrushes, no one likes to use someone else’s. 8.2 Think big Big, complex datasets to train big, complex models. 8.3 Rethink psychology education Embrace and teach interdisciplinary. "],
["shared-states-supplement.html", "A Supplement to Chapter 2 A.1 Stimuli used for SF-task A.2 Instructions A.3 Behavioral results A.4 Optimization results A.5 Bagging procedure A.6 Precision vs. recall A.7 Self vs. other classification A.8 Condition-average results A.9 Individual subject scores A.10 Brain region importance A.11 General note about tables with voxel-coordinates", " A Supplement to Chapter 2 A.1 Stimuli used for SF-task Table A.1: Stimuli used for SF-task Class Dutch English translation Action Hard wegrennen Running away fast Iemand wegduwen Pushing someone away Iemand stevig vastpakken Holding someone tightly Je hoofd schudden Shaking your head Heftige armgebaren maken Making big arm gestures Ergens voor terugdeinzen Recoiling from something Je ogen dichtknijpen Closing your eyes tightly Je ogen wijd open sperren Opening your eyes widely Je wenkbrauwen fronsen Frowning with your eyebrows Je schouders ophalen Raising your shoulders Op de vloer stampen Stamping on the floor In elkaar duiken Cowering Je schouders laten hangen Slumping your shoulders Je vuisten ballen Tighten your fists Je borst vooruit duwen Push your chest forward Je tanden op elkaar zetten Clench your teeth Je hand voor je mond slaan Put your hand in front of your mouth Onrustig bewegen Moving restlessly Heen en weer lopen Walking back and forth Je hoofd afkeren Turning your head away Interoception Een brok in je keel A lump in your throat Buiten adem zijn Being out of breath Een versnelde hartslag A fast beating heart Je hart klopt in de keel You heart is beating in your throat Een benauwd gevoel An oppressed feeling Een misselijk gevoel Being nauseous Druk op je borst A pressure on your chest Strak aangespannen spieren Tense muscles Een droge keel A dry throat Koude rillingen hebben Cold shivers Bloed stroomt naar je hoofd Blood is going to your head Een verdoofd gevoel A numb feeling Je hebt tintelende ledenmaten Tingling limbs Een verlaagde hartslag A slow heartbeat Je hebt zware ledematen Heavy limbs Een versnelde ademhaling Fast breathing Je hebt hoofdpijn Headache Je hebt buikpijn Stomachache Zweet staat in je handen Sweaty palms Je maag keert zich om Your stomach churns Situation Vals beschuldigd worden Being falsely accused Dierbare overlijdt A loved one dies Vlees is bedorven Meat that has gone off Je wordt bijna aangereden You are almost hit by a car Iemand naast je braakt Someone next to you vomits Huis staat in brand House is on fire Zonder reden ontslagen worden Being fired for no reason Een ongemakkelijke stilte An uncomfortable silence Alleen in donker park Alone in a dark park Inbraak in je huis A house burglary Een gewond dier zien Seeing a wounded animal Tentamen verknallen Messing up your exam Je partner bedriegt je You partner cheats on you Dierbare is vermist A loved one is missing Belangrijke sollicitatie vergeten Forgot a job interview Onvoorbereid presentatie geven Giving a presentation unprepared Je baas beledigt je Your boss offends you Goede vriend negeert je A good friend neglects you Slecht nieuws bij arts Bad news at the doctor Bommelding in metro A bomb alarm in the metro Note: The stimulus materials presented in Table S1 were selected from a pilot study. In this pilot study we asked an independent sample of twenty-four subjects to describe how they would express an emotion in their behavior, body posture or facial expression (action information), what specific sensations they would feel inside their body when they would experience an emotion (interoceptive information), and for what reason or in what situation they would experience an emotion (situational information). These three questions were asked in random order for twenty-eight different negative emotional states, including anger, fear, disgust, sadness, contempt, worry, disappointment, regret and shame. The descriptions generated by these subjects were used as qualitative input in order to create our stimulus set of twenty short sentences that described emotional actions, sensations or situations. With this procedure, we ensured that our stimulus set held sentences that were validated and ecologically appropriate for our sample. A.2 Instructions A.2.1 Full instruction for the other-focused emotion understanding task Translated from Dutch; task presented first. In this study we are interested in how the brain responds when people understand the emotions of others in different ways. In the scanner you will see images that display emotional situations, sometimes with multiple people. In every image one person will be marked with a red square. While viewing the image we ask you to focus on the emotion of that person in three different ways. With some images we ask you to focus on HOW this person expresses his or her emotion. Here we ask you to identify expressions in the face or body that are informative about the emotional state that the person is experiencing. With other images we ask you to focus on WHAT this person may feel in his or her body. Here we ask you to identify sensations, such as a change in heart rate, breathing or other internal feeling, that the person might feel in this situation. With other images we ask you to focus on WHY this person experiences an emotion. Here we ask you to identify a specific reason or cause that explains why the person feels what he or she feels. Every image will be presented for six seconds. During this period we ask you to silently focus on HOW this person expresses emotion, WHAT this person feels in his/her body, and WHY this person feels an emotion. Before you will enter the scanner we will practice. I will show you three images and will ask you to perform each of the three instructions out loud. It is important to note that there are no correct or incorrect answers, it is about how you interpret the image. For the success of the study it is very important that you apply the HOW, WHAT or WHY instruction for each image. Please do not skip any images and try to apply each instruction with the same motivation. It is also important to treat every image separately, although it is possible that you have similar interpretations for different images. The three instructions are combined with the images in blocks. In every block you will see five images with the same instruction. Each block will start with a cue that tells you what to focus on in that block. Each image is combined with all three instructions, so you will see the same image multiple times. In between images you will sometimes see a black screen for a longer period of time. Do you have any questions? A.2.2 Full instruction for the self-focused emotion imagery task Translated from Dutch; task presented second. In this study we are interested in how the brain responds when people imagine different aspects of emotion. In the scanner you will see sentences that describe aspects of emotional experience. We ask you to try to imagine the content of each sentence as rich and detailed as possible. Some sentences describe actions and expressions. We ask you to imagine that you are performing this action or expression. Other sentences describe sensations or feelings that you can have inside your body. We ask you to imagine that you are experiencing this sensation or feeling. Other sentences describe emotional situations. We ask you to imagine that you are experiencing this specific situation. We ask you to always imagine that YOU have the experience. Thus, it is about imagining an action or expression of your body, a sensation inside your body, or a situation that you are part of. I will give some examples now. For each sentence you have six seconds to imagine the content. All sentences will be presented twice. In between sentences you will sometimes see a black screen for a longer period of time. For this experiment to succeed it is important that you imagine each sentence with the same motivation, even if you have seen the sentence before. Please do not skip sentences. Do you have any questions? A.3 Behavioral results Figure A.1: Mean percentage of trials successfully executed for the SF-task (left panel) and OF-task (right panel). Error bars indicate 95% confidence intervals. A one-way ANOVA of the success-rates of the SF-task (left-panel) indicated no significant overall differences, F(2, 17) = 1.03, p = 0.38. In the OF-task (right panel) however, a one-way ANOVA indicated that success-rates differed significantly between classes, F(2, 17) = 17.74, p &lt; 0.001. Follow-up pairwise comparisons (Bonferroni corrected, two tailed) revealed that interoception-trials (M = 74.00, SE = 2.10) were significantly less successful (p &lt; 0.001) than both action-trials (M = 85.50, SE = 1.85) and situation trials (M = 90.00, SE = 1.92). A.4 Optimization results Figure A.2: Results of the parameter-optimization procedure. Reported scores reflect the classification scores averaged over subjects and classes (i.e. the diagonal of the confusion matrix). All optimization analyses were iterated 5000 times. A) Classification results for different smoothing kernels (0, 2, 5, and 10 mm) and z-value threshold for differentiation scores during feature selection (see MVPA pipeline section in the main text for a description of the particular feature selection method we employed). Numbers reflect the average number of voxels selected across iterations. B) Classification results of using a low-pass filter (2 seconds) or not. C) Classification results for different numbers of test-trials per class (1 to 5). D) Classification results when preprocessing the data with Independent Component Analysis (ICA) or not. Table A.2: Parameters assessed in the optimization set Parameter Options Final choice Smoothing kernel 0 mm, 2 mm, 5 mm, 10 mm 5 mm Feature selection threshold 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3 2.3 Number of test-trials 1, 2, 3, 4, 5 4 Low-pass filter 2 seconds vs. none None ICA denoising ICA vs. no ICA No ICA Note: The first set of parameters we evaluated in the optimization-set were different smoothing factors and feature selection thresholds (see MVPA pipeline section in the main text). On average, across the self- and cross-analysis, a 5 mm smoothing kernel yielded the best results in combination with a feature selection threshold of 2.25, which we rounded up to 2.3 as this number represents a normalized (z-transformed) score, which corresponds to the top 1% scores within a normal distribution. Next, the difference between using a low-pass (of 2 seconds, i.e. 1 TR) versus none was assessed, establishing no low-pass filter as the optimal choice. Next, different numbers of test-trials (1 to 5) per class per iteration were assessed. Four trials yielded the best results. Lastly, the effect of “cleaning” the data with an independent component analysis was examined (FSL: MELODIC and FIX; Salimi-Khorshidi et al., 2014). Not performing ICA yielded the best results. These parameters – 5 mm smoothing kernel, 2.3 feature selection thresholded, no low-pass filter, and four test-trials per iteration – were subsequently used in the analysis of the validation set. Figure A.3: Confusion matrices displaying precision-values yielded by the classification analysis of the optimization dataset with the final set of parameters. Because no permutation statistics were calculated for the optimization set, significance was calculated using a one-sample independent t-test against chance-level classification (i.e. 0.333) for each cell in the diagonal of the confusion matrices. Here, all t-statistics use a degrees of freedom of 12 (i.e. 13 subjects - 1) and are evaluated against a significance level of 0.05, Bonferroni-corrected. For the diagonal of the self-analysis confusion matrix, all values were significantly above chance-level, all p &lt; 0.0001. For the diagonal of the cross-analysis confusion matrix, both the action (43% correct) and situation (44% correct) classes scored significantly above chance, p = 0.014 and p = 0.0007 respectively. Interoception was classified at chance level, p = 0.99, which stands in contrast with the results in the validation-set. A.5 Bagging procedure Figure A.4: Schematic overview of the bagging procedure. Class probabilities across different bagging iterations are summed and the class with the maximum probability determines each trial’s final predicted class, which are subsequently summarized in a confusion matrix on which final recall/precision scores are calculated. A.6 Precision vs. recall Figure A.5: A comparison between precision and recall confusion matrices of the self- and cross-analysis of the validation dataset. Precision refers to the amount of true positive predictions of a given class relative to all predictions for that class. Recall refers to the amount of true positive predictions of a given class relative to the total number of samples in that class. In the self-analysis, all classes were decoded significantly above chance for both precision and recall (all p &lt; 0.001). In the cross-analysis, all classes were decoded significantly above chance for precision (all p &lt; 0.001); for recall both action and situation were decoded significantly above chance (p = 0.0013 and p &lt; 0.001, respectively), while interoception was decoded below chance. All p-values were calculated using a permutation test with 1300 permutations (as described in the Methods section in the main text). When comparing precision and recall scores for both analyses, precision and recall showed very little differences in the self-analysis, while the cross-analysis shows a clear difference between metrics, especially for interoception and situation. For the interoception class, the relatively high precision score (44%) compared to its recall score (14%) suggests that trials are very infrequently classified as interoception, but when they are, it is (relatively) often correct. For the situation class, the relatively high recall score (72 %) compared to its precision score (44%) suggests that situation is strongly over-classified, which is especially clear in the lower-right confusion matrix, which indicates that 59% of the interoception-trials are misclassified as situation-trials. A.7 Self vs. other classification Figure A.6: Relation between self- and cross-analysis scores across subjects and their respective distributions. Note that the scores here represent the average of the class-specific precision scores. A) There is no significant correlation between precision-scores on the self-analysis and the corresponding scores on the cross-analysis, r = -0.04, p = .86, implying that classification scores in the self-analysis is not predictive of scores in the cross-analysis. B) The distribution of precision-scores in the self-analysis, appearing to be normally distributed. C) The distribution of precision-scores in the cross-analysis, on the other hand, appears to be bimodal, with one group of subjects having scores around chance level (0.333) while another group of subjects clearly scores above chance level (see individual scores and follow-up analyses in (ref:fig-shared-states-S4). Figure A.7: Confusion matrices with precision (left matrix) and recall (right matrix) estimates of the other-to-self decoding analysis. The MVPA-pipeline used was exactly the same as for the (self-to-other) cross-analysis in the main text. P-values corresponding to the classification scores were calculated using a permutation analysis with 1000 permutations of the other-to-self analysis with randomly shuffled class-labels. Similar to the self-to-other analysis, the precision-scores for all classes in the other-to-self analysis were significant, p(action) &lt; 0.001, p(interoception) = 0.008, p(situation) &lt; 0.001. For recall, classification scores for action and interoception were significant (both p &lt; 0.001), but not significant for situation (p = 0.062). The discrepancy between the self-to-other and other-to-self decoding analyses can be explained by two factors. First, the other-to-self classifier was trained on fewer samples (i.e. 90 trials) than the self-to-other classifier (which was trained on 120 trials), which may cause a substantial drop in power. Second, the preprocessing pipeline and MVPA hyperparameters were optimized based on the self-analysis and self-to-other cross-analysis. Given the vast differences between the nature of the self- and other-data, these optimal preprocessing and MVPA hyperparameters for the original analyses may not cross-validate well to the other-to-self decoding analysis. A.8 Condition-average results Figure A.8: Results of MVPA analyses using condition-average voxel patterns across subjects instead of single-trial patterns within subjects. Here, patterns are estimated in a GLM in which each condition, as opposed to each trial, is modeled with a single regressor, from which whole-brain t-value patterns were extracted. In this condition-average multi-voxel pattern analysis, condition-average patterns across subjects were used as samples. The condition-average patterns were extracted from the univariate first-level contrasts. In total, this yielded 120 samples for the self-data (3 conditions x 2 runs x 20 participants) and 60 samples for the other-data (3 conditions * 20 participants). For these analyses, the same hyperparameters were used as the original analyses reported in the main text, except with regard to the cross-validation and bagging procedure. Here, we used (stratified) 10-fold cross-validation without bagging. The upper panels show precision scores (per class) for the self- and (self-to-other) cross-analysis; the lower panels show results from the same analyses but expressed in recall-estimates (error bars indicate 95% confidence intervals). Apart from interoception in the cross-analysis (both precision and recall), all scores were significant (p &lt; 0.001) in a permutation test with 1000 permutations. These results largely replicate our findings as reported in the main text. This suggests that the neural patterns involved in self-focused emotional imagery and other-focused emotion understanding are relatively consistent in terms of spatial distribution across subjects. We explain this consistency by assuming that our tasks engage domain-general psychological processes that are present in all individuals. A.9 Individual subject scores Table A.3: Mean general classification scores per subject for the self- and cross-analysis on the validation-set only. Subject nr. Self-analysis precision Cross-analysis precision Session Part of optimization-set? 1 0.758 0.445 2 y 2 0.487 0.336 2 y 3 0.629 0.316 1 y 4 0.524 0.577 2 y 5 0.457 0.492 1 y 6 0.741 0.296 2 y 7 0.600 0.542 1 y 8 0.431 0.240 2 y 9 0.629 0.497 1 y 10 0.734 0.268 2 y 11 0.683 0.386 1 y 12 0.415 0.525 2 y 13 0.623 0.604 1 y 14 0.810 0.610 1 n 15 0.538 0.578 1 n 16 0.486 0.455 1 n 17 0.549 0.415 1 n 18 0.488 0.494 1 n 19 0.590 0.289 1 n 20 0.600 0.502 1 n Note: Supplementary Table 3 suggests individual variability in the extent to which neural resources are shared between self- and other-focused processes. In the SF-task all subjects demonstrated a mean classification score well above .33 (i.e., score associated with chance). When generalizing the SF-classifier to the OF-task, however, the classification scores appear to be bimodally distributed (see Supplementary Figure 5C). As can be seen in Table 3, some subjects demonstrated a relatively high mean classification score (i.e., &gt; .45), whereas other subjects demonstrated a classification score at chance level or lower. Note that there is no significant difference between the OF classification scores for subjects who participated in the experiment for the first or second time (“Session” column in table; t(18) = 1.73, p = 0.10), nor for subjects who were or were not part of the optimization-set (“Part of optimization-set?” column in table; t(18) = -.95, p = 0.35), suggesting that inclusion in the optimization-set or session ordering is not a confound in the analyses. Regarding individual variability in self-other neural overlap, it is important to note that in the field of embodied cognition, there is increasing attention for the idea that simulation is both individually and contextually dynamic (Oosterwijk &amp; Barrett, 2014; Winkielman, Niedenthal, Wielgosz &amp; Kavanagh, 2015; see also Barrett, 2009). To better distinguish between meaningful individual variation and variation due to other factors (e.g., random noise), future research should test a priori formulated hypotheses about how and when individual variation is expected to occur. A.10 Brain region importance Table A.4: Most important voxels in terms of their average weight across iterations and subjects. Brain region k Max Mean Std Frontal pole 1827 5.05 2.35 0.52 Occipital pole 1714 5.15 2.45 0.56 Supramarginal gyrus anterior 1573 7.48 2.84 0.91 Lateral occipital cortex superior 1060 4.52 2.18 0.39 Lateral occipital cortex inferior 923 4.73 2.36 0.49 Angular gyrus 856 4.52 2.24 0.40 Supramarginal gyrus posterior 806 4.49 2.29 0.45 Middle temporal gyrus temporo-occipital 798 4.00 2.33 0.48 Temporal pole 711 4.38 2.37 0.54 Precentral gyrus 568 3.54 2.14 0.31 Superior temporal gyrus posterior 549 3.64 2.27 0.41 Superior frontal gyrus 510 3.83 2.18 0.38 Postcentral gyrus 489 4.61 2.43 0.60 Inferior frontal gyrus parstriangularis 488 4.22 2.35 0.50 Inferior frontal gyrus parsopercularis 441 3.54 2.14 0.31 Middle temporal gyrus posterior 417 5.68 2.34 0.52 Occipital fusiform 400 4.28 2.14 0.37 Middle temporal gyrus anterior 398 5.68 2.58 0.76 Middle frontal gyrus 300 3.01 2.06 0.25 Precuneus 282 3.34 2.14 0.31 Note: Brain regions were extracted from the Harvard-Oxford (bilateral) Cortical atlas. A minimum threshold for the probabilistic masks of 20 was chosen to minimize overlap between adjacent masks while maximizing coverage of the entire brain. The column k represents the absolute number of above-threshold voxels in the masks. The columns Max, Mean, and Std represent the maximum, mean, and standard deviation from the t-values included in the masks. Note that the t-values, corresponding to the mean weight across subjects normalized by the standard error of the weights across subjects (after correcting for a positive bias when taking the absolute of the weights), were thresholded at a minimum of 1.75, referring to a p-value of 0.05 of a one-sided t-test against zero with 19 degrees of freedom (i.e. n – 1). Note that this t-value map was not corrected for multiple comparisons, and is intended to visualize which regions in the brain were generally involved in our sample of subjects. The X, Y, and Z columns represent the MNI152 (2mm) coordinates of the peak (i.e. max) t-value for each listed brain region. A.11 General note about tables with voxel-coordinates In order to keep the Supplementary materials concise and orderly, we chose not to include the actual tables with the peak coordinates of all significant clusters of the univariate analyses of the self- and other-task data (as these would amount to 25 pages). These tables, however, can be downloaded (as simple tab-separated-value files) from the study’s Github respository. The voxel tables are listed under: SharedStates/RESULTS/Voxel_tables/*.tsv (see green box in image below), and can be downloaded by cloning the remote Github repository locally or downloading the ZIP-file from the website (see red box in image below). "],
["confounds-decoding-supplement.html", "B Supplement to Chapter 4", " B Supplement to Chapter 4 "],
["morbid-curiosity-supplement.html", "C Supplement to Chapter 5", " C Supplement to Chapter 5 "],
["au-limitations-supplement.html", "D Supplement to Chapter 6", " D Supplement to Chapter 6 "],
["facial-expression-models-supplement.html", "E Supplement to Chapter 6", " E Supplement to Chapter 6 "],
["resources-supplement.html", "F Data, code and materials", " F Data, code and materials "],
["bibliography.html", "Bibliography", " Bibliography Allefeld, C., Görgen, K., &amp; Haynes, J.-D. (2016). Valid population inference for information-based imaging: From the second-level t-test to prevalence inference. Neuroimage, 141, 378–392. Anderson, M. L. (2016). Précis of after phrenology: Neural reuse and the interactive brain. Behavioral and Brain Sciences, 39. Andrews-Hanna, J. R., Smallwood, J., &amp; Spreng, R. N. (2014). The default network and self-generated thought: Component processes, dynamic control, and clinical relevance. Annals of the New York Academy of Sciences, 1316(1), 29. Bangalore, S. S., Prasad, K. M. R., Montrose, D. M., Goradia, D. D., Diwadkar, V. A., &amp; Keshavan, M. S. (2008). Cannabis use and brain structural alterations in first episode schizophrenia—a region of interest, voxel based morphometric study. Schizophr. Res., 99(1), 1–6. Barrett, L. F. (2012). Emotions are real. Emotion, 12(3), 413. Barrett, L. F., &amp; Satpute, A. B. (2013). Large-scale brain networks in affective and social neuroscience: Towards an integrative functional architecture of the brain. Current Opinion in Neurobiology, 23(3), 361–372. Barrett, L. F., &amp; Simmons, W. K. (2015). Interoceptive predictions in the brain. Nature Reviews Neuroscience, 16(7), 419–429. Barsalou, L. W. (2009). Simulation, situated conceptualization, and prediction. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1521), 1281–1289. Bastiaansen, J. A., Thioux, M., &amp; Keysers, C. (2009). Evidence for mirror systems in emotions. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1528), 2391–2404. Binder, J. R., Desai, R. H., Graves, W. W., &amp; Conant, L. L. (2009). Where is the semantic system? A critical review and meta-analysis of 120 functional neuroimaging studies. Cerebral Cortex, 19(12), 2767–2796. Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123–140. Brodtmann, A., Puce, A., Darby, D., &amp; Donnan, G. (2009). Regional fMRI brain activation does correlate with global brain volume. Brain Research, 1259, 17–25. Brosch, T., Bar-David, E., &amp; Phelps, E. A. (2013). Implicit race bias decreases the similarity of neural representations of black and white faces. Psychological Science, 24(2), 160–166. Bzdok, D. (2017). Classical statistics and statistical learning in imaging neuroscience. Front. Neurosci., 11, 543. Carlson, T. A., &amp; Wardle, S. G. (2015). Sensible decoding. Neuroimage, 110, 217–218. Carr, L., Iacoboni, M., Dubeau, M.-C., Mazziotta, J. C., &amp; Lenzi, G. L. (2003). Neural mechanisms of empathy in humans: A relay from neural systems for imitation to limbic areas. Proceedings of the National Academy of Sciences, 100(9), 5497–5502. Chekroud, A. M., Ward, E. J., Rosenberg, M. D., &amp; Holmes, A. J. (2016). Patterns in the human brain mosaic discriminate males from females. Proc. Natl. Acad. Sci. U. S. A., 113(14), E1968. Corradi-Dell’Acqua, C., Tusche, A., Vuilleumier, P., &amp; Singer, T. (2016). Cross-modal representations of first-hand and vicarious pain, disgust and fairness in insular and cingulate cortex. Nature Communications, 7(1), 1–12. Craddock, R. C., Holtzheimer, P. E., 3rd, Hu, X. P., &amp; Mayberg, H. S. (2009). Disease state prediction from resting state functional connectivity. Magn. Reson. Med., 62(6), 1619–1628. Craig, A. D., &amp; Craig, A. (2009). How do you feel–now? The anterior insula and human awareness. Nature Reviews Neuroscience, 10(1). Cuingnet, R., Gerardin, E., Tessieras, J., Auzias, G., Lehéricy, S., Habert, M.-O., Chupin, M., Benali, H., Colliot, O., &amp; Alzheimer’s Disease Neuroimaging Initiative. (2011). Automatic classification of patients with alzheimer’s disease from structural MRI: A comparison of ten methods using the ADNI database. Neuroimage, 56(2), 766–781. Davis, T., LaRocque, K. F., Mumford, J. A., Norman, K. A., Wagner, A. D., &amp; Poldrack, R. A. (2014). What do differences between multi-voxel and univariate analysis mean? How subject-, voxel-, and trial-level variance impact fMRI analysis. Neuroimage, 97, 271–283. Decety, J. (2011). Dissecting the neural mechanisms mediating empathy. Emotion Review, 3(1), 92–108. Del Giudice, M., Lippa, R. A., Puts, D. A., Bailey, D. H., Bailey, J. M., &amp; Schmitt, D. P. (2016). Joel et al.’s method systematically fails to detect large, consistent sex differences. Proc. Natl. Acad. Sci. U. S. A., 113(14), E1965. Denny, B. T., Kober, H., Wager, T. D., &amp; Ochsner, K. N. (2012). A meta-analysis of functional neuroimaging studies of self-and other judgments reveals a spatial gradient for mentalizing in medial prefrontal cortex. Journal of Cognitive Neuroscience, 24(8), 1742–1752. Diedrichsen, J., &amp; Kriegeskorte, N. (2017). Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis. PLoS Comput. Biol., 13(4), e1005508. Dixon, L. (1999). Dual diagnosis of substance abuse in schizophrenia: Prevalence and impact on outcomes. Schizophr. Res., 35 Suppl, S93–100. Douaud, G., Smith, S., Jenkinson, M., Behrens, T., Johansen-Berg, H., Vickers, J., James, S., Voets, N., Watkins, K., Matthews, P. M., &amp; James, A. (2007). Anatomically related grey and white matter abnormalities in adolescent-onset schizophrenia. Brain, 130(Pt 9), 2375–2386. Ethofer, T., Van De Ville, D., Scherer, K., &amp; Vuilleumier, P. (2009). Decoding of emotional information in voice-sensitive cortices. Current Biology, 19(12), 1028–1033. Etzel, J. A., Valchev, N., &amp; Keysers, C. (2011). The impact of certain methodological choices on multivariate analysis of fMRI data with support vector machines. Neuroimage, 54(2), 1159–1167. Gallese, V., Keysers, C., &amp; Rizzolatti, G. (2004). A unifying view of the basis of social cognition. Trends in Cognitive Sciences, 8(9), 396–403. Gelder, B. de, Van den Stock, J., Meeren, H. K., Sinke, C. B., Kret, M. E., &amp; Tamietto, M. (2010). Standing up for the body. Recent progress in uncovering the networks involved in the perception of bodies and bodily expressions. Neuroscience &amp; Biobehavioral Reviews, 34(4), 513–527. Gilbert, S. J., Swencionis, J. K., &amp; Amodio, D. M. (2012). Evaluative vs. Trait representation in intergroup social judgments: Distinct roles of anterior temporal lobe and prefrontal cortex. Neuropsychologia, 50(14), 3600–3611. Gilron, R., Rosenblatt, J., Koyejo, O., Poldrack, R. A., &amp; Mukamel, R. (2017). What’s in a pattern? Examining the type of signal multivariate analysis uncovers at the group level. Neuroimage, 146, 113–120. Glezerman, M. (2016). Yes, there is a female and a male brain: Morphology versus functionality. Proceedings of the National Academy of Sciences, 113(14), E1971–E1971. Goldstein, J. M., Seidman, L. J., Horton, N. J., Makris, N., Kennedy, D. N., Caviness, V. S., Jr, Faraone, S. V., &amp; Tsuang, M. T. (2001). Normal sexual dimorphism of the adult human brain assessed by in vivo magnetic resonance imaging. Cereb. Cortex, 11(6), 490–497. Good, C. D., Johnsrude, I., Ashburner, J., Henson, R. N., Friston, K. J., &amp; Frackowiak, R. S. (2001). Cerebral asymmetry and the effects of sex and handedness on brain structure: A voxel-based morphometric analysis of 465 normal adult human brains. Neuroimage, 14(3), 685–700. Gur, R. C., Turetsky, B. I., Matsui, M., Yan, M., Bilker, W., Hughett, P., &amp; Gur, R. E. (1999). Sex differences in brain gray and white matter in healthy young adults: Correlations with cognitive performance. J. Neurosci., 19(10), 4065–4072. Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46(1), 389–422. Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J.-D., Blankertz, B., &amp; Bießmann, F. (2014). On the interpretation of weight vectors of linear models in multivariate neuroimaging. Neuroimage, 87, 96–110. Haxby, J. V. (2012). Multivariate pattern analysis of fMRI: The early beginnings. Neuroimage, 62(2), 852–855. Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., &amp; Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539), 2425–2430. Haynes, J.-D. (2015). A primer on pattern-based approaches to fMRI: Principles, pitfalls, and perspectives. Neuron, 87(2), 257–270. Hebart, M. N., &amp; Baker, C. I. (2017). Deconstructing multivariate decoding for the study of brain function. Neuroimage. Jenkinson, M., Beckmann, C. F., Behrens, T. E., Woolrich, M. W., &amp; Smith, S. M. (2012). Fsl. Neuroimage, 62(2), 782–790. Jimura, K., &amp; Poldrack, R. A. (2012). Analyses of regional-average activation and multivoxel pattern information tell complementary stories. Neuropsychologia, 50(4), 544–552. Joel, D., &amp; Fausto-Sterling, A. (2016). Beyond sex differences: New approaches for thinking about variation in brain structure and function. Philos. Trans. R. Soc. Lond. B Biol. Sci., 371(1688), 20150451. Kay, K. N., Naselaris, T., Prenger, R. J., &amp; Gallant, J. L. (2008). Identifying natural images from human brain activity. Nature, 452(7185), 352–355. Keysers, C., &amp; Gazzola, V. (2014). Dissociating the ability and propensity for empathy. Trends in Cognitive Sciences, 18(4), 163–166. Kriegeskorte, N., Simmons, W. K., Bellgowan, P. S., &amp; Baker, C. I. (2009). Circular analysis in systems neuroscience: The dangers of double dipping. Nature Neuroscience, 12(5), 535. Krishnan, A., Woo, C.-W., Chang, L. J., Ruzic, L., Gu, X., López-Solà, M., Jackson, P. L., Pujol, J., Fan, J., &amp; Wager, T. D. (2016). Somatic and vicarious pain are represented by dissociable multivariate brain patterns. Elife, 5, e15166. Kveraga, K., Boshyan, J., Adams Jr, R. B., Mote, J., Betz, N., Ward, N., Hadjikhani, N., Bar, M., &amp; Barrett, L. F. (2015). If it bleeds, it leads: Separating threat from mere negativity. Social Cognitive and Affective Neuroscience, 10(1), 28–35. Lamm, C., Decety, J., &amp; Singer, T. (2011). Meta-analytic evidence for common and distinct neural networks associated with directly experienced pain and empathy for pain. Neuroimage, 54(3), 2492–2502. Lamm, C., &amp; Majdandžić, J. (2015). The role of shared neural activations, mirror neurons, and morality in empathy–a critical comment. Neuroscience Research, 90, 15–24. Lang, P. J. (2005). International affective picture system (iaps): Affective ratings of pictures and instruction manual. Technical Report. Lang, P. J., Bradley, M. M., Cuthbert, B. N., &amp; others. (1997). International affective picture system (iaps): Technical manual and affective ratings. NIMH Center for the Study of Emotion and Attention, 1, 39–58. LaRocque, J. J., Lewis-Peacock, J. A., Drysdale, A. T., Oberauer, K., &amp; Postle, B. R. (2013). Decoding attended information in short-term memory: An EEG study. J. Cogn. Neurosci., 25(1), 127–142. Legrand, D., &amp; Ruby, P. (2009). What is self-specific? Theoretical investigation and critical review of neuroimaging results. Psychological Review, 116(1), 252. Lench, H. C., Flores, S. A., &amp; Bench, S. W. (2011). Discrete emotions predict changes in cognition, judgment, experience, behavior, and physiology: A meta-analysis of experimental emotion elicitations. Psychological Bulletin, 137(5), 834. Lindquist, K. A., Wager, T. D., Kober, H., Bliss-Moreau, E., &amp; Barrett, L. F. (2012). The brain basis of emotion: A meta-analytic review. The Behavioral and Brain Sciences, 35(3), 121. Lüders, E., Steinmetz, H., &amp; Jäncke, L. (2002). Brain size and grey matter volume in the healthy human brain. Neuroreport, 13(17), 2371–2374. McGrath, J., Saha, S., Chant, D., &amp; Welham, J. (2008). Schizophrenia: A concise overview of incidence, prevalence, and mortality. Epidemiol. Rev., 30, 67–76. Medford, N., &amp; Critchley, H. D. (2010). Conjoint activity of anterior insular and anterior cingulate cortex: Awareness and response. Brain Structure and Function, 214(5-6), 535–549. Misaki, M., Kim, Y., Bandettini, P. A., &amp; Kriegeskorte, N. (2010). Comparison of multivariate classifiers and response normalizations for pattern-information fMRI. Neuroimage, 53(1), 103–118. Mischkowski, D., Crocker, J., &amp; Way, B. M. (2016). From painkiller to empathy killer: Acetaminophen (paracetamol) reduces empathy for pain. Social Cognitive and Affective Neuroscience, 11(9), 1345–1353. Mumford, J. A., Davis, T., &amp; Poldrack, R. A. (2014). The impact of study design on pattern estimation for single-trial multivariate pattern analysis. Neuroimage, 103, 130–138. Naselaris, T., &amp; Kay, K. N. (2015). Resolving ambiguities of MVPA using explicit models of representation. Trends Cogn. Sci., 19(10), 551–554. Nichols, T. E., &amp; Holmes, A. P. (2002). Nonparametric permutation tests for functional neuroimaging: A primer with examples. Human Brain Mapping, 15(1), 1–25. Norman, K. A., Polyn, S. M., Detre, G. J., &amp; Haxby, J. V. (2006a). Beyond mind-reading: Multi-voxel pattern analysis of fMRI data. Trends in Cognitive Sciences, 10(9), 424–430. Norman, K. A., Polyn, S. M., Detre, G. J., &amp; Haxby, J. V. (2006b). Beyond mind-reading: Multi-voxel pattern analysis of fMRI data. Trends Cogn. Sci., 10(9), 424–430. O’Brien, L. M., Ziegler, D. A., Deutsch, C. K., Frazier, J. A., Herbert, M. R., &amp; Locascio, J. J. (2011). Statistical adjustments for brain size in volumetric neuroimaging studies: Some practical implications in methods. Psychiatry Res., 193(2), 113–122. Oosterwijk, S., &amp; Barrett, L. F. (2014). Embodiment in the construction of emotion experience and emotion understanding. Routledge Handbook of Embodied Cognition. New York: Routledge, 250–260. Oosterwijk, S., Lindquist, K. A., Anderson, E., Dautoff, R., Moriguchi, Y., &amp; Barrett, L. F. (2012). States of mind: Emotions, body feelings, and thoughts share distributed neural networks. NeuroImage, 62(3), 2110–2128. Oosterwijk, S., Mackey, S., Wilson-Mendenhall, C., Winkielman, P., &amp; Paulus, M. P. (2015). Concepts in context: Processing mental state concepts with internal or external focus involves different neural systems. Social Neuroscience, 10(3), 294–307. Parkinson, C., Liu, S., &amp; Wheatley, T. (2014). A common cortical metric for spatial, temporal, and social distance. Journal of Neuroscience, 34(5), 1979–1987. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., &amp; others. (2011). Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12, 2825–2830. Peelen, M. V., Atkinson, A. P., &amp; Vuilleumier, P. (2010). Supramodal representations of perceived emotions in the human brain. Journal of Neuroscience, 30(30), 10127–10134. Popov, V., Ostarek, M., &amp; Tenison, C. (2018). Practices and pitfalls in inferring neural representations. NeuroImage, 174, 340–351. Pulvermüller, F., &amp; Fadiga, L. (2010). Active perception: Sensorimotor circuits as a cortical basis for language. Nature Reviews Neuroscience, 11(5), 351–360. Ritchie, J. B., Kaplan, D. M., &amp; Klein, C. (2017). Decoding the brain: Neural representation and the limits of multivariate pattern analysis in cognitive neuroscience. Br. J. Philos. Sci. Rosenblatt, J. D. (2016). Multivariate revisit to “sex beyond the genitalia”. Proc. Natl. Acad. Sci. U. S. A., 113(14), E1966–7. Rütgen, M., Seidel, E.-M., Silani, G., Riečansky, I., Hummer, A., Windischberger, C., Petrovic, P., &amp; Lamm, C. (2015). Placebo analgesia and its opioidergic regulation suggest that empathy for pain is grounded in self pain. Proceedings of the National Academy of Sciences, 112(41), E5638–E5646. Sabatinelli, D., Fortune, E. E., Li, Q., Siddiqui, A., Krafft, C., Oliver, W. T., Beck, S., &amp; Jeffries, J. (2011). Emotional perception: Meta-analyses of face and natural scene processing. Neuroimage, 54(3), 2524–2533. Sepehrband, F., Lynch, K. M., Cabeen, R. P., Gonzalez-Zacarias, C., Zhao, L., D’Arcy, M., Kesselman, C., Herting, M. M., Dinov, I. D., Toga, A. W., &amp; Clark, K. A. (2018). Neuroanatomical morphometric characterization of sex differences in youth using statistical learning. Neuroimage, 172, 217–227. Singer, T. (2012). The past, present and future of social neuroscience: A european perspective. Neuroimage, 61(2), 437–449. Spreng, R. N., Mar, R. A., &amp; Kim, A. S. (2009). The common neural basis of autobiographical memory, prospection, navigation, theory of mind, and the default mode: A quantitative meta-analysis. Journal of Cognitive Neuroscience, 21(3), 489–510. Spunt, R. P., &amp; Lieberman, M. D. (2012). An integrative model of the neural systems supporting the comprehension of observed emotional behavior. Neuroimage, 59(3), 3050–3059. Stelzer, J., Buschmann, T., Lohmann, G., Margulies, D. S., Trampel, R., &amp; Turner, R. (2014). Prioritizing spatial accuracy in high-resolution fMRI data using multivariate feature weight mapping. Frontiers in Neuroscience, 8, 66. Uddin, L. Q., Iacoboni, M., Lange, C., &amp; Keenan, J. P. (2007). The self and social cognition: The role of cortical midline structures and mirror neurons. Trends in Cognitive Sciences, 11(4), 153–157. Van Haren, N. E., Cahn, W., Hulshoff Pol, H. E., &amp; Kahn, R. S. (2013). Confounders of excessive brain volume loss in schizophrenia. Neurosci. Biobehav. Rev., 37(10 Pt 1), 2418–2423. Van Overwalle, F., &amp; Baetens, K. (2009). Understanding others’ actions and goals by mirror and mentalizing systems: A meta-analysis. Neuroimage, 48(3), 564–584. Varoquaux, G. (2018). Cross-validation failure: Small sample sizes lead to large error bars. Neuroimage, 180, 68–77. Waytz, A., &amp; Mitchell, J. P. (2011). Two mechanisms for simulating other minds: Dissociations between mirroring and self-projection. Current Directions in Psychological Science, 20(3), 197–200. Weichwald, S., Meyer, T., Özdenizci, O., Schölkopf, B., Ball, T., &amp; Grosse-Wentrup, M. (2015). Causal interpretation rules for encoding and decoding models in neuroimaging. Neuroimage, 110, 48–59. Wilson-Mendenhall, C. D., Barrett, L. F., Simmons, W. K., &amp; Barsalou, L. W. (2011). Grounding emotion in situated conceptualization. Neuropsychologia, 49(5), 1105–1127. Wurm, M. F., &amp; Lingnau, A. (2015). Decoding actions at different levels of abstraction. Journal of Neuroscience, 35(20), 7727–7735. Zaki, J., &amp; Ochsner, K. N. (2012). The neuroscience of empathy: Progress, pitfalls and promise. Nature Neuroscience, 15(5), 675–680. Zaki, J., Wager, T. D., Singer, T., Keysers, C., &amp; Gazzola, V. (2016). The anatomy of suffering: Understanding the relationship between nociceptive and empathic pain. Trends in Cognitive Sciences, 20(4), 249–259. "],
["contributions-to-the-chapters.html", "Contributions to the chapters", " Contributions to the chapters "],
["list-of-other-publications.html", "List of other publications", " List of other publications van der Maas, H.L.J., Snoek, L., &amp; Stevenson, C. (2021). How much intelligence is there in artificial intelligence? A 2020 update. Hoogeveen, S., Snoek, L., &amp; Van Elk, M. (2020). Religious belief and cognitive conflict sensitivity: A preregistered fMRI study. Cortex, 129, 247–265. van Elk, M., &amp; Snoek, L. (2020). The relationship between individual differences in gray matter volume and religiosity and mystical experiences: A preregistered voxel‐based morphometry study. European Journal of Neuroscience, 51(3), 850–865. Van Mourik, T., Snoek, L., Knapen, T., &amp; Norris, D. G. (2018). Porcupine: a visual pipeline tool for neuroimaging analysis. PLoS computational biology, 14(5), e1006064. "],
["nederlandse-samenvatting-summary-in-dutch.html", "Nederlandse samenvatting (Summary in Dutch)", " Nederlandse samenvatting (Summary in Dutch) Replace this with the Dutch title of your thesis The summary goes here. "],
["acknowledgments.html", "Acknowledgments", " Acknowledgments This section is optional, but theses typically include acknowledgments (dankwoord in Dutch) at the end. You may want to mix languages to thank people in their native tongue (though most Dutch speakers write it entirely in Dutch). But the standard language of the thesis template is English. You can switch temporarily by wrapping the text in language tags like so: [Your Dutch text here]{lang=nl}. This is important for things like hyphenation to work properly. "]
]
