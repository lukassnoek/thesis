---
nocite: | 
  @Diedrichsen2017-ab
---
```{r setup-confounds-decoding, include=FALSE}
knitr::opts_chunk$set(results = 'hide', echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(kableExtra)
```

# How to control for confounds in decoding analyses of neuroimaging data {#confounds-decoding}
\chaptermark{Confounds in decoding analyses}

\vspace*{\fill}

---

\small
\noindent
_This chapter has been published as_: Snoek, L.\*, Miletić, S.\*, & Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. _NeuroImage_, 184, 741-760.

\* Shared first authorship

\newpage
\normalsize

__Abstract__

Over the past decade, multivariate "decoding analyses" have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound "brain size". We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one's decoding analysis.

## Introduction {#confounds-decoding-introduction}

In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data [@Haxby2012-sd; @Norman2006-bt]. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is "decoding", in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns [@Haxby2001-os] and working memory representations from EEG data [@LaRocque2013-sh], as well between-subject factors such as Alzheimer's disease (vs. healthy controls) from structural MRI data [@Cuingnet2011-hv] and major depressive disorder (vs. healthy controls) from resting-state functional connectivity [@Craddock2009-kz]. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do [@Jimura2012-lv]. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses [@Hebart2017-jn].

In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical [@Allefeld2016-xp; @Davis2014-lw; @Gilron2017-tl; @Haufe2014-el] and more conceptual [@Naselaris2015-jn; @Weichwald2015-aj] in nature. For the purposes of the current study, we focus on the specific criticism put forward by @Naselaris2015-jn , who argue that decoding analyses are inherently ambiguous in terms of what information they use [see @popov2018practices for a similar argument in the context of encoding analyses]. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information [see also @Carlson2015-bz; @Ritchie2017-gl; @Weichwald2015-aj]. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of "source ambiguity".

To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender.^[The terms "gender" and "sex" are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.] (male/female) from structural MRI with the aim of contributing to the understanding of gender differences --- an endeavor that generated considerable interest and controversy [@Chekroud2016-tc; @Del_Giudice2016-ns; @Glezerman2016-xl; @Joel2016-uo; @Rosenblatt2016-oy]. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant's gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways [@OBrien2011-lj; @Good2001-ak]. First, there is a *global* difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight [18.7%; @Gur1999-qj; @Luders2002-ms].^[Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration "squeezes" relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa [@Douaud2007-sw]. This effect of global brain size similarly affects functional MRI analyses [@brodtmann2009regional].] Second, brains of men and women are known to differ *locally*: some specific brain areas are on average larger in women than in men [e.g., in superior and middle temporal cortex; @Good2001-ak] and vice versa [e.g., in frontomedial cortex; @Goldstein2001-dy]. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male *bodies* are on average larger than female bodies [@Gur1999-qj; @Sepehrband2018-dy]. As such, our hypothetical researcher is likely primarily interested in the *local* sources of variation in the neuroanatomy of male and female brains.

Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for "uninteresting" sources of information.

### Partitioning effects into *true* signal and *confounded* signal {#confounds-decoding-introduction-true-vs-confounded}

Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis [@Hebart2017-jn]. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate *prediction*, but not in *interpretability* of the model [@Bzdok2017-li; @Haufe2014-el; @Hebart2017-jn]. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem.^[However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant's gender.] In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is *only* using information in the neuroimaging data that is related to the variable that is being decoded [@Ritchie2017-gl]. In this scenario, source ambiguity (i.e., the presence of *multiple* sources of information) *is* problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is *confounding* the decoding analysis of gender. Here, we define a confound as *a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.*

To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable "schizophrenia-or-not" is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender [i.e., men are more often diagnosed with schizophrenia than women; @McGrath2008-oj] and substance abuse [@Dixon1999-kl], which are also known to affect gray matter [@Bangalore2008-kc; @Gur1999-qj; @Van_Haren2013-iv]. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate *prediction* of schizophrenia, then this ambiguity due to confounding sources of information is problematic.

Importantly, as our definition suggests, what *is* or *is not* regarded as a confound is relative --- it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable "schizophrenia-or-no"” becomes the confounding variable. In other words, one researcher's signal is another researcher's confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the "true signal" (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the "confounded signal" (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure \@ref(fig:fig-confounds-decoding-1)). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.

```{r fig-confounds-decoding-1, fig.cap='(ref:caption-fig-confounds-decoding-1)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_1.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-1) Visualization of how variance in brain data ($X$) can partitioned into "True signal" and "Confounded signal", depending on the correlation structure between the brain data ($X$), the confound ($C$), and the target ($y$). Overlapping circles indicate a non-zero (squared) correlation between the two variables.

In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.

### Methods for confound control {#confounds-decoding-introduction-methods}

In decoding analyses, one aims to predict a certain target variable from patterns of neuroimaging data. Various methods discussed in this section are supplemented with a mathematical formalization; for consistency and readability, we define the notation we will use in Table \@ref(tab:tab-confounds-decoding-1).

```{r tab-confounds-decoding-1, results='asis'}
data = read_csv('_bookdown_files/confounds-decoding-files/table_1_data.csv')
options(knitr.kable.NA = '')
kbl(data, booktabs = T, longtable = T, escape = F, caption = 'Notation.') %>%
    kable_styling(full_width = T, font_size = 10) %>%
    column_spec(column = c(1, 2), width = "3em") %>%
    footnote(footnote_as_chunk = T, title_format = "italic", threeparttable = T, escape = F,
             general = 'Format based on Diedrichsen and Kriegeskorte (2017). For the correlations ($r$), we assume that $P = 1$ and thus that the correlations in the table reduce to a scalar.')
```

#### A priori counterbalancing {#confounds-decoding-introduction-methods-apriori-counterbalancing}

Ideally, one would prevent confounding variables from influencing the results as much as possible before the acquisition of the neuroimaging data.^[In the context of behavioral data, a priori counterbalancing is often called "matching" or a employing a "case-control design" @[Cook2002-hb].] One common way do this (in both traditional "activation-based" and decoding analyses) is to make sure that potential confounding variables are *counterbalanced* in the experimental design [@Gorgen2017-sy]. In experimental research, this would entail randomly assigning subjects to design cells (e.g., treatment groups) such that there is no structural correlation between characteristics of the subjects and design cells. In observational designs (e.g., in the gender/brain size example described earlier), it means that the sample is chosen such that there is no correlation between the confound (brain size) and *observed* target variable (gender). That is, given that men on average have larger brains than women, this would entail including only men with relatively small brains and women with relatively large brains.^[Note that the counterbalancing process is the same for both traditional univariate (activation-based) studies and decoding studies, but the direction of analysis is reversed in univariate (e.g., gender → brain) and decoding studies (e.g., brain → gender). As such, in univariate studies the confound (e.g., brain size) is counterbalanced with respect to the predictor(s) (e.g., gender) while in decoding studies the confound (e.g., brain size) is counterbalanced with respect to the target (e.g., gender).] The distinction between experimental and observational studies is important because the former allow the researcher to randomly draw samples from the population, while the latter require the researcher to choose a sample that is not representative of the population, which limits the conclusions that can be drawn about the population (we will revisit this issue in the [Discussion](#confounds-decoding-discussion) section).

Formally, in decoding analyses, a design is counterbalanced when the confound $C$ and the target $y$ are statistically independent. In practice, this often means that the sample is chosen so that there is no significant correlation coefficient between $C$ and $y$ (although this does not necessarily imply that $C$ and $y$ are actually independent). To illustrate the process of counterbalancing, let's consider another hypothetical experiment: suppose one wants to set up an fMRI experiment in which the goal is to decode abstract object category (e.g., faces vs. houses) from the corresponding fMRI patterns [cf. @Haxby2001-os], while controlling for the potential confounding influence of low-level or mid-level stimulus features, such as luminance, spatial frequency, or texture [@Long2017-fb]. Proper counterbalancing would entail making sure that the images used for this particular experiments have similar values for these low-level and mid-level features across object categories [see for details @Gorgen2017-sy]. Thus, in this example, low-level and mid-level stimulus features should be counterbalanced with respect to object category, such that above chance decoding of object category cannot be attributed to differences in low-level or mid-level stimulus features (i.e., the confounds).

A priori counterbalancing of potential confounds is, however, not always feasible. For one, the exact measurement of a potentially confounding variable may be impossible until data acquisition. For example, the brain size of a participant is only known after data collection. Similarly, @Todd2013-sd found that their decoding analysis of rule representations was confounded by response times of to the to-be-decoded trials. Another example of a "data-driven" confound is participant motion during data acquisition [important in, for example, decoding analyses applied to data from clinical populations such as ADHD; @Yu-Feng2007-sg]. In addition, a priori counterbalancing of confounds may be challenging because of the limited size of populations of interest. Especially in clinical research settings, researchers may not have the luxury of selecting a counterbalanced sample due to the small number of patient subjects available for testing. Lastly, researchers may simply discover confounds after data acquisition.

Given that a priori counterbalancing is not possible or undesirable in many situations, it is paramount to explore the possibilities of controlling for confounding variables after data acquisition for the sake of model interpretability, which we discuss next.

#### Include confounds in the data {#confounds-decoding-introduction-methods-include-in-data}

One perhaps intuitive method to control for confounds in decoding analyses is to include the confound(s) in the data [i.e., the neuroimaging data, $X$; see, e.g., @Sepehrband2018-dy] used by decoding model. That is, when applying a decoding analysis to neuroimaging data, the confound is added to the data as if it were another voxel (or sensor, in electrophysiology). This intuition may stem from the analogous situation in univariate (activation-based) analyses of neuroimaging data, in which confounding variables are controlled for by including them in the design matrix together with the stimulus/task regressors. For example, in univariate analyses of functional MRI, movement of the participant is often controlled for by including motion estimates in the design matrix of first-level analyses [@Johnstone2006-tn]; in EEG, some control for activity due to eye-movements by including activity measured by concurrent electro-oculography as covariates in the design-matrix [@Parra2005-um]. Usually, the general linear model is then used to estimate each predictor's influence on the neuroimaging data. Importantly, the parameter estimates ($\hat{\beta}$) are often interpreted as reflecting the unique contribution^[However, parameter estimates only reflect unique variance when ordinary, weighted, or generalized least squares is used to find the model parameters. Other (regularized) linear models, such as ridge regression or LASSO, are not guaranteed to yield parameters that explain unique proportions of variance.] of each predictor variable, independent from the influence of the confound.

Contrary to general linear models as employed in univariate (activation-based) analyses, including confound variables in the data as predictors for *decoding* models is arguably problematic. If a confound is included in the data in the context of decoding models, the parameter estimates of the features (often called "feature weights", $w$, in decoding models) may be corrected for the influence of the confound, but the *model performance* [usually measured as explained variance, $R^2$, or classification accuracy; @Hebart2017-jn] is not. That is, rather than providing an estimate of decoding performance "controlled for" a confound, one obtains a measure of performance when explicitly *including* the confound as an interesting source of variance that the decoder is allowed to use. This is problematic because research using decoding analyses generally does not focus on parameter estimates but on statistics of model performance. Model performance statistics (e.g., $R^2$, classification accuracy) alone cannot disentangle the contribution of different sources of information as they only represent a single summary statistic of model fit [@Ritchie2017-gl]. One might, then, argue that additionally inspecting feature weights of decoding models may help in disambiguating different sources of information [@Sepehrband2018-dy]. However, it has been shown that feature weights cannot be reliably mapped to specific sources of information, i.e., as being task-related or confound-related [e.g., features with large weights may be completely uncorrelated with the target variable; @Haufe2014-el; @Hebart2017-jn]. As such, it does not make sense to include confounds in the set of predictors when the goal is to disambiguate the different sources of information in decoding analyses.

Recently, another approach similar to including confounds in the data has been proposed, which is based on the idea of a dose-response curve [@alizadeh2017decoding]. In this method, instead of adding the confound(s) to the model directly, the relative contribution of true and confounded signal is systematically controlled. The authors show that this approach is able to directly quantify the unique contribution of each source of information, thus effectively controlling for confounded signal. However, while sophisticated in its approach, this method only seems to work for categorical confounds, as it is difficult (if not impossible) to systematically vary the proportion of confound-related information when dealing with continuous confounds or when dealing with more than one confound.

#### Control for confounds during pattern estimation {#confounds-decoding-introduction-methods-pattern-estimation}

Another method that was used in some decoding studies on functional MRI data aims to control for confounds in the initial procedure of estimating activity patterns of the to-be-decoded events, by leveraging the ability of the GLM to yield parameter estimates reflecting unique variance [@Woolgar2014-jb]. In this method, an initial "first-level" (univariate) analysis models the fMRI time series ($s$) as a function of both predictors-of-interest ($X$) and the confounds ($C$), often using the GLM^[Note that $X$ and $C$, here, refer to (usually HRF-convolved) predictors of the time series signal ($s$) for a single voxel. In the rest of the article, $X$ and $C$ refer to features that are defined across samples (not time).]:

\begin{equation}
s = X\beta_{x} + C\beta_{c} + \epsilon
\end{equation}

Then, only the estimated parameters ($\hat{\beta}$, or normalized parameters, such as *t*-values or *z*-values) corresponding to the predictors-of-interest ($\hat{\beta}_{x}$) are used as activity estimates (i.e., the  used for predicting the target $y$) in the subsequent decoding analyses. This method thus takes advantage of the shared variance partitioning in the pattern estimation step to control for potential confounding variables. However, while elegant in principle, this method is not applicable in between-subject decoding studies [e.g., clinical decoding studies; @Van_Waarde2014-sh; @Cuingnet2011-hv], in which confounding variables are defined across subjects, or in electrophysiology studies, in which activity patterns do not have to be^[Note that, technically, one could use the "Control for confounds during pattern estimation" method in electrophysiology as well, by first fitting a univariate model explaining the neuroimaging data ($X_{j}$ for $j = 1 \dots K$) as a function of both the target ($y$) and the confound ($C$) and subsequently only using the parameter estimates of the target-predictor ($\hat{\beta}_{x}$) as patterns in the subsequent decoding analysis.] estimated in a first-level model, thus limiting the applicability of this method.

#### Post hoc counterbalancing of confounds {#confounds-decoding-introduction-methods-posthoc-counterbalancing}

When a priori counterbalancing is not possible, some have argued that post hoc counterbalancing might control for the influence of confounds [@Rao2017-bw, p. 24, 38]. In this method, given that there is some sample correlation between the target and confound ($r_{Cy} \neq 0$) in the entire dataset, one takes a subset of samples in which there is no empirical relation between the confound and the target (e.g., when $r_{Cy} \approx 0$). In other words, post hoc counterbalancing is a way to *decorrelate* the confound and the target by subsampling the data. Then, subsequent decoding analysis on the subsampled data can only capitalize on true signal, as there is no confounded signal anymore (see Figure \@ref(fig:fig-confounds-decoding-2)). While intuitive in principle, we are not aware of whether this method has been evaluated before and whether it yields unbiased performance estimates.

```{r fig-confounds-decoding-2, fig.cap='(ref:caption-fig-confounds-decoding-2)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_2.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-2) A schematic visualization how the main two confound control methods evaluated in this article deal with the "confounded signal", making sure decoding models only capitalize on the "true signal".

#### Confound regression {#confounds-decoding-introduction-methods-confound-regression}

### Current study {#confounds-decoding-introduction-current-study}

## Methods {#confounds-decoding-methods}

### Data {#confounds-decoding-methods-data}

#### VBM acquisition & analysis {#confounds-decoding-methods-data-vbm}

#### TBSS acquisition & analysis {#confounds-decoding-methods-data-tbss}

#### Brain size estimation {#confounds-decoding-methods-data-brainsize}

#### Data and code availability {#confounds-decoding-methods-data-data-and-code}

### Decoding pipeline {#confounds-decoding-methods-pipeline}

### Evaluated methods for confound control {#confounds-decoding-methods-evaluated-methods}

#### Post hoc counterbalancing {#confounds-decoding-methods-evaluated-methods-counterbalancing}

#### Confound regression

#### Control for confounds during pattern estimation

### Analyses of simulated data

#### Efficacy analyses

#### Analysis of positive bias after post hoc counterbalancing

#### Analysis of negative bias after WDCR

## Results

### Influence of brain size

### Baseline model: no confound control

### Post hoc counterbalancing

#### Empirical results

#### Efficacy analysis

#### Analysis of positive bias after post hoc counterbalancing

### Whole-dataset confound regression (WDCR)

#### Empirical results

#### Efficacy analysis

#### Analysis of negative bias after WDCR

### Cross-validated confound regression (CVCR)

#### Empirical results

#### Efficacy analysis

### Summary methods for confound control

## Discussion {#confounds-decoding-discussion}

### Relevance and consequences for previous and future research

#### A priori and post hoc counterbalancing

#### Confound regression

#### Relevance to other analysis methods

#### Importance for gender decoding studies

### Choosing a confound model: linear vs. nonlinear models

### Practical recommendations

## Conclusions











