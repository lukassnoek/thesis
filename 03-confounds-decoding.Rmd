# How to control for confounds in decoding analyses of neuroimaging data {#confounds-decoding}
\chaptermark{Confounds in decoding analyses}

\vspace*{\fill}

---

\small
\noindent
_This chapter has been published as_: Snoek, L.\*, Miletić, S.\*, & Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. _NeuroImage_, 184, 741-760.

\* Shared first authorship

\newpage
\normalsize

__Abstract__

Over the past decade, multivariate "decoding analyses" have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound "brain size". We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one's decoding analysis.

## Introduction {#confounds-decoding-introduction}

In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data [@Haxby2012-sd; @Norman2006-bt]. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is "decoding", in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns [@Haxby2001-os] and working memory representations from EEG data [@LaRocque2013-sh], as well between-subject factors such as Alzheimer's disease (vs. healthy controls) from structural MRI data [@Cuingnet2011-hv] and major depressive disorder (vs. healthy controls) from resting-state functional connectivity [@Craddock2009-kz]. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do [@Jimura2012-lv]. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses [@Hebart2017-jn].

In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical [@Allefeld2016-xp; @Davis2014-lw; @Gilron2017-tl; @Haufe2014-el] and more conceptual [@Naselaris2015-jn; @Weichwald2015-aj] in nature. For the purposes of the current study, we focus on the specific criticism put forward by @Naselaris2015-jn , who argue that decoding analyses are inherently ambiguous in terms of what information they use [see @popov2018practices for a similar argument in the context of encoding analyses]. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information [see also @Carlson2015-bz; @Ritchie2017-gl; @Weichwald2015-aj]. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of "source ambiguity".

To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender^[The terms "gender" and "sex" are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.] (male/female) from structural MRI with the aim of contributing to the understanding of gender differences --- an endeavor that generated considerable interest and controversy [@Chekroud2016-tc; @Del_Giudice2016-ns; @Glezerman2016-xl; @Joel2016-uo; @Rosenblatt2016-oy]. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant's gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways [@OBrien2011-lj; @Good2001-ak]. First, there is a *global* difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight [18.7%; @Gur1999-qj; @Luders2002-ms]^[Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration "squeezes" relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa [@Douaud2007-sw]. This effect of global brain size similarly affects functional MRI analyses [@brodtmann2009regional].]. Second, brains of men and women are known to differ *locally*: some specific brain areas are on average larger in women than in men [e.g., in superior and middle temporal cortex; @Good2001-ak] and vice versa [e.g., in frontomedial cortex; @Goldstein2001-dy]. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male *bodies* are on average larger than female bodies [@Gur1999-qj; @Sepehrband2018-dy]. As such, our hypothetical researcher is likely primarily interested in the *local* sources of variation in the neuroanatomy of male and female brains.

Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for "uninteresting" sources of information.

### Partitioning effects into *true* signal and *confounded* signal {#confounds-decoding-introduction-true-vs-confounded}

Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis [@Hebart2017-jn]. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate *prediction*, but not in *interpretability* of the model [@Bzdok2017-li; @Haufe2014-el; @Hebart2017-jn]. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem^[However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant's gender.]. In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is *only* using information in the neuroimaging data that is related to the variable that is being decoded [@Ritchie2017-gl]. In this scenario, source ambiguity (i.e., the presence of *multiple* sources of information) *is* problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is *confounding* the decoding analysis of gender. Here, we define a confound as *a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.*

To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable "schizophrenia-or-not" is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender [i.e., men are more often diagnosed with schizophrenia than women; @McGrath2008-oj] and substance abuse [@Dixon1999-kl], which are also known to affect gray matter [@Bangalore2008-kc; @Gur1999-qj; @Van_Haren2013-iv]. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate *prediction* of schizophrenia, then this ambiguity due to confounding sources of information is problematic.

Importantly, as our definition suggests, what *is* or *is not* regarded as a confound is relative --- it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable "schizophrenia-or-no"” becomes the confounding variable. In other words, one researcher's signal is another researcher's confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the "true signal" (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the "confounded signal" (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure \@ref(fig:fig-confounds-decoding-1)). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.

```{r fig-confounds-decoding-1, fig.cap='(ref:caption-fig-confounds-decoding-1)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_1.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-1) Visualization of how variance in brain data (*X*) can partitioned into "True signal" and "Confounded signal", depending on the correlation structure between the brain data (*X*), the confound (*C*), and the target (*y*). Overlapping circles indicate a non-zero (squared) correlation between the two variables.

In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.

### Methods for confound control {#confounds-decoding-introduction-methods-confound-control}
