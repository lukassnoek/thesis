---
nocite: | 
  @Diedrichsen2017-ab
---
```{r setup-confounds-decoding, include=FALSE}
knitr::opts_chunk$set(results = 'hide', echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(kableExtra)
```

# How to control for confounds in decoding analyses of neuroimaging data {#confounds-decoding}
\chaptermark{Confounds in decoding analyses}

\vspace*{\fill}

---

\small
\noindent
_This chapter has been published as_: Snoek, L.\*, Miletić, S.\*, & Scholte, H.S. (2019). How to control for confounds in decoding analyses of neuroimaging data. _NeuroImage_, 184, 741-760.

\* Shared first authorship

\newpage
\normalsize

__Abstract__

Over the past decade, multivariate "decoding analyses" have become a popular alternative to traditional mass-univariate analyses in neuroimaging research. However, a fundamental limitation of using decoding analyses is that it remains ambiguous which source of information drives decoding performance, which becomes problematic when the to-be-decoded variable is confounded by variables that are not of primary interest. In this study, we use a comprehensive set of simulations as well as analyses of empirical data to evaluate two methods that were previously proposed and used to control for confounding variables in decoding analyses: post hoc counterbalancing and confound regression. In our empirical analyses, we attempt to decode gender from structural MRI data while controlling for the confound "brain size". We show that both methods introduce strong biases in decoding performance: post hoc counterbalancing leads to better performance than expected (i.e., positive bias), which we show in our simulations is due to the subsampling process that tends to remove samples that are hard to classify or would be wrongly classified; confound regression, on the other hand, leads to worse performance than expected (i.e., negative bias), even resulting in significant below chance performance in some realistic scenarios. In our simulations, we show that below chance accuracy can be predicted by the variance of the distribution of correlations between the features and the target. Importantly, we show that this negative bias disappears in both the empirical analyses and simulations when the confound regression procedure is performed in every fold of the cross-validation routine, yielding plausible (above chance) model performance. We conclude that, from the various methods tested, cross-validated confound regression is the only method that appears to appropriately control for confounds which thus can be used to gain more insight into the exact source(s) of information driving one's decoding analysis.

## Introduction {#confounds-decoding-introduction}

In the past decade, multivariate pattern analysis (MVPA) has emerged as a popular alternative to traditional univariate analyses of neuroimaging data [@Haxby2012-sd; @Norman2006-bt]. The defining feature of MVPA is that it considers patterns of brain activation instead of single units of activation (i.e., voxels in MRI, sensors in MEG/EEG). One of the most-often used type of MVPA is "decoding", in which machine learning algorithms are applied to neuroimaging data to predict a particular stimulus, task, or psychometric feature. For example, decoding analyses have been used to successfully predict various experimental conditions within subjects, such as object category from fMRI activity patterns [@Haxby2001-os] and working memory representations from EEG data [@LaRocque2013-sh], as well between-subject factors such as Alzheimer's disease (vs. healthy controls) from structural MRI data [@Cuingnet2011-hv] and major depressive disorder (vs. healthy controls) from resting-state functional connectivity [@Craddock2009-kz]. One reason for the popularity of MVPA, and especially decoding, is that these methods appear to be more sensitive than traditional mass-univariate methods in detecting effects of interest. This increased sensitivity is often attributed to the ability to pick up multidimensional, spatially distributed representations which univariate methods, by definition, cannot do [@Jimura2012-lv]. A second important reason to use decoding analyses is that they allow researchers to make predictions about samples beyond the original dataset, which is more difficult using traditional univariate analyses [@Hebart2017-jn].

In the past years, however, the use of MVPA has been criticized for a number of reasons, both statistical [@Allefeld2016-xp; @Davis2014-lw; @Gilron2017-tl; @Haufe2014-el] and more conceptual [@Naselaris2015-jn; @Weichwald2015-aj] in nature. For the purposes of the current study, we focus on the specific criticism put forward by @Naselaris2015-jn , who argue that decoding analyses are inherently ambiguous in terms of what information they use [see @popov2018practices for a similar argument in the context of encoding analyses]. This type of ambiguity arises when the classes of the to-be-decoded variable systematically vary in more than one source of information [see also @Carlson2015-bz; @Ritchie2017-gl; @Weichwald2015-aj]. The current study aims to investigate how decoding analyses can be made more interpretable by reducing this type of "source ambiguity".

To illustrate the problem of source ambiguity, consider, for example, the scenario in which a researcher wants to decode gender.^[The terms "gender" and "sex" are both used in the relevant research literature. Here, we use the term gender because we refer to self-reported identity in the data described below.] (male/female) from structural MRI with the aim of contributing to the understanding of gender differences --- an endeavor that generated considerable interest and controversy [@Chekroud2016-tc; @Del_Giudice2016-ns; @Glezerman2016-xl; @Joel2016-uo; @Rosenblatt2016-oy]. By performing a decoding analysis on the MRI data, the researcher hopes to capture meaningful patterns of variation in the data of male and female participants that are predictive of the participant's gender. The literature suggests that gender dimorphism in the brain is manifested in two major ways [@OBrien2011-lj; @Good2001-ak]. First, there is a *global* difference between male and female brains: men have on average about 15% larger intracranial volume than women, which falls in the range of mean gender differences in height (8.2%) and weight [18.7%; @Gur1999-qj; @Luders2002-ms].^[Note that information related to global brain size persists when researchers analyze the structural MRI data in a common, normalized brain space, because spatial registration "squeezes" relatively large brains into a smaller template, increasing voxel statistics (e.g., gray matter density in VBM analyses), and vice versa [@Douaud2007-sw]. This effect of global brain size similarly affects functional MRI analyses [@brodtmann2009regional].] Second, brains of men and women are known to differ *locally*: some specific brain areas are on average larger in women than in men [e.g., in superior and middle temporal cortex; @Good2001-ak] and vice versa [e.g., in frontomedial cortex; @Goldstein2001-dy]. One could argue that, given that one is interested in explaining behavioral or mental gender differences, global differences are relatively uninformative, as it reflects the fact than male *bodies* are on average larger than female bodies [@Gur1999-qj; @Sepehrband2018-dy]. As such, our hypothetical researcher is likely primarily interested in the *local* sources of variation in the neuroanatomy of male and female brains.

Now, supposing that the researcher is able to decode gender from the MRI data significantly above chance, it remains unclear on which source of information the decoder is capitalizing: the (arguably meaningful) local difference in brain structure or the (in the context of this question arguably uninteresting) global difference in brain size? In other words, the data contain more than one source of information that may be used to predict gender. In the current study, we aim to evaluate methods that improve the interpretability of decoding analyses by controlling for "uninteresting" sources of information.

### Partitioning effects into *true* signal and *confounded* signal {#confounds-decoding-introduction-true-vs-confounded}

Are multiple sources of information necessarily problematic? And what makes a source of information interesting or uninteresting? The answers to these questions depend on the particular goal of the researcher using the decoding analysis [@Hebart2017-jn]. In principle, multiple sources of information in the data do not pose a problem if a researcher is only interested in accurate *prediction*, but not in *interpretability* of the model [@Bzdok2017-li; @Haufe2014-el; @Hebart2017-jn]. In brain-computer interfaces (BCI), for example, accurate prediction is arguably more important than interpretability, i.e., knowing which sources of information are driving the decoder. Similarly, if the researcher from our gender decoding example is only interested in accurately predicting gender regardless of model interpretability, source ambiguity is not a problem.^[However, if accurate prediction is the only goal in this scenario, we would argue that there are probably easier and less expensive methods than neuroimaging to predict a participant's gender.] In most scientific applications of decoding analyses, however, model interpretability is important, because researchers are often interested in the relative contributions of different sources of information to decoding performance. Specifically, in most decoding analyses, researchers often (implicitly) assume that the decoder is *only* using information in the neuroimaging data that is related to the variable that is being decoded [@Ritchie2017-gl]. In this scenario, source ambiguity (i.e., the presence of *multiple* sources of information) *is* problematic as it violates this (implicit) assumption. Another way to conceptualize the problem of source ambiguity is that, using the aforementioned example, (global) brain size is *confounding* the decoding analysis of gender. Here, we define a confound as *a variable that is not of primary interest, correlates with the to-be-decoded variable (the target), and is encoded in the neuroimaging data.*

To illustrate the issue of confounding variables in the context of decoding clinical disorders, suppose one is interested in building a classifier that is able to predict whether subjects are suffering from schizophrenia or not based on the subjects’ gray matter data. Here, the variable "schizophrenia-or-not" is the variable of interest, which is assumed to be encoded in the neuroimaging data (i.e., the gray matter) and can thus be decoded. However, there are multiple factors known to covary with schizophrenia, such as gender [i.e., men are more often diagnosed with schizophrenia than women; @McGrath2008-oj] and substance abuse [@Dixon1999-kl], which are also known to affect gray matter [@Bangalore2008-kc; @Gur1999-qj; @Van_Haren2013-iv]. As such, the variables gender and substance abuse can be considered confounds according to our definition, because they are both correlated with the target (schizophrenia or not) and are known to be encoded in the neuroimaging data (i.e., the effect of these variables is present in the gray matter data). Now, if one is able to classify schizophrenia with above-chance accuracy from gray matter data, one cannot be sure which source of information within the data is picked up by the decoder: information (uniquely) associated with schizophrenia or (additionally) information associated with gender or substance abuse? If one is interested in more than mere accurate *prediction* of schizophrenia, then this ambiguity due to confounding sources of information is problematic.

Importantly, as our definition suggests, what *is* or *is not* regarded as a confound is relative --- it depends on whether the researchers deems it of (primary) interest or not. In the aforementioned hypothetical schizophrenia decoding study, for example, one may equally well define the severity of substance abuse as the to-be-decoded variable, in which the variable "schizophrenia-or-no"” becomes the confounding variable. In other words, one researcher's signal is another researcher's confound. Regardless, if decoding analyses of neuroimaging data are affected by confounds, the data thus contain two types of information: the "true signal" (i.e., variance in the neuroimaging data related to the target, but unrelated to the confound) and the "confounded signal" (i.e., variance in the neuroimaging data related to the target that is also related to the confound; see Figure \@ref(fig:fig-confounds-decoding-1)). In other words, source ambiguity arises due to the presence of both true signal and confounded signal and, thus, controlling for confounds (by removing the confounded signal) provides a crucial methodological step forward in improving the interpretability of decoding analyses.

```{r fig-confounds-decoding-1, fig.cap='(ref:caption-fig-confounds-decoding-1)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_1.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-1) Visualization of how variance in brain data ($X$) can partitioned into "True signal" and "Confounded signal", depending on the correlation structure between the brain data ($X$), the confound ($C$), and the target ($y$). Overlapping circles indicate a non-zero (squared) correlation between the two variables.

In the decoding literature, various methods have been applied to control for confounds. We next provide an overview of these methods, highlight their advantages and disadvantages, and discuss their rationale and the types of research settings they can be applied in. Subsequently, we focus on two of these methods to test whether these methods succeed in controlling for the influence of confounds.

### Methods for confound control {#confounds-decoding-introduction-methods}

In decoding analyses, one aims to predict a certain target variable from patterns of neuroimaging data. Various methods discussed in this section are supplemented with a mathematical formalization; for consistency and readability, we define the notation we will use in Table \@ref(tab:tab-confounds-decoding-1).

```{r tab-confounds-decoding-1, results='asis'}
data = read_csv('_bookdown_files/confounds-decoding-files/table_1_data.csv')
options(knitr.kable.NA = '')
kbl(data, booktabs = T, longtable = T, escape = F, caption = 'Notation.') %>%
    kable_styling(full_width = T, font_size = 10) %>%
    column_spec(column = c(1, 2), width = "3em") %>%
    footnote(footnote_as_chunk = T, title_format = "italic", threeparttable = T, escape = F,
             general = 'Format based on Diedrichsen and Kriegeskorte (2017). For the correlations ($r$), we assume that $P = 1$ and thus that the correlations in the table reduce to a scalar.')
```

#### A priori counterbalancing {#confounds-decoding-introduction-methods-apriori-counterbalancing}

Ideally, one would prevent confounding variables from influencing the results as much as possible before the acquisition of the neuroimaging data.^[In the context of behavioral data, a priori counterbalancing is often called "matching" or a employing a "case-control design" @[Cook2002-hb].] One common way do this (in both traditional "activation-based" and decoding analyses) is to make sure that potential confounding variables are *counterbalanced* in the experimental design [@Gorgen2017-sy]. In experimental research, this would entail randomly assigning subjects to design cells (e.g., treatment groups) such that there is no structural correlation between characteristics of the subjects and design cells. In observational designs (e.g., in the gender/brain size example described earlier), it means that the sample is chosen such that there is no correlation between the confound (brain size) and *observed* target variable (gender). That is, given that men on average have larger brains than women, this would entail including only men with relatively small brains and women with relatively large brains.^[Note that the counterbalancing process is the same for both traditional univariate (activation-based) studies and decoding studies, but the direction of analysis is reversed in univariate (e.g., gender → brain) and decoding studies (e.g., brain → gender). As such, in univariate studies the confound (e.g., brain size) is counterbalanced with respect to the predictor(s) (e.g., gender) while in decoding studies the confound (e.g., brain size) is counterbalanced with respect to the target (e.g., gender).] The distinction between experimental and observational studies is important because the former allow the researcher to randomly draw samples from the population, while the latter require the researcher to choose a sample that is not representative of the population, which limits the conclusions that can be drawn about the population (we will revisit this issue in the [Discussion](#confounds-decoding-discussion) section).

Formally, in decoding analyses, a design is counterbalanced when the confound $C$ and the target $y$ are statistically independent. In practice, this often means that the sample is chosen so that there is no significant correlation coefficient between $C$ and $y$ (although this does not necessarily imply that $C$ and $y$ are actually independent). To illustrate the process of counterbalancing, let's consider another hypothetical experiment: suppose one wants to set up an fMRI experiment in which the goal is to decode abstract object category (e.g., faces vs. houses) from the corresponding fMRI patterns [cf. @Haxby2001-os], while controlling for the potential confounding influence of low-level or mid-level stimulus features, such as luminance, spatial frequency, or texture [@Long2017-fb]. Proper counterbalancing would entail making sure that the images used for this particular experiments have similar values for these low-level and mid-level features across object categories [see for details @Gorgen2017-sy]. Thus, in this example, low-level and mid-level stimulus features should be counterbalanced with respect to object category, such that above chance decoding of object category cannot be attributed to differences in low-level or mid-level stimulus features (i.e., the confounds).

A priori counterbalancing of potential confounds is, however, not always feasible. For one, the exact measurement of a potentially confounding variable may be impossible until data acquisition. For example, the brain size of a participant is only known after data collection. Similarly, @Todd2013-sd found that their decoding analysis of rule representations was confounded by response times of to the to-be-decoded trials. Another example of a "data-driven" confound is participant motion during data acquisition [important in, for example, decoding analyses applied to data from clinical populations such as ADHD; @Yu-Feng2007-sg]. In addition, a priori counterbalancing of confounds may be challenging because of the limited size of populations of interest. Especially in clinical research settings, researchers may not have the luxury of selecting a counterbalanced sample due to the small number of patient subjects available for testing. Lastly, researchers may simply discover confounds after data acquisition.

Given that a priori counterbalancing is not possible or undesirable in many situations, it is paramount to explore the possibilities of controlling for confounding variables after data acquisition for the sake of model interpretability, which we discuss next.

#### Include confounds in the data {#confounds-decoding-introduction-methods-include-in-data}

One perhaps intuitive method to control for confounds in decoding analyses is to include the confound(s) in the data [i.e., the neuroimaging data, $X$; see, e.g., @Sepehrband2018-dy] used by decoding model. That is, when applying a decoding analysis to neuroimaging data, the confound is added to the data as if it were another voxel (or sensor, in electrophysiology). This intuition may stem from the analogous situation in univariate (activation-based) analyses of neuroimaging data, in which confounding variables are controlled for by including them in the design matrix together with the stimulus/task regressors. For example, in univariate analyses of functional MRI, movement of the participant is often controlled for by including motion estimates in the design matrix of first-level analyses [@Johnstone2006-tn]; in EEG, some control for activity due to eye-movements by including activity measured by concurrent electro-oculography as covariates in the design-matrix [@Parra2005-um]. Usually, the general linear model is then used to estimate each predictor's influence on the neuroimaging data. Importantly, the parameter estimates ($\hat{\beta}$) are often interpreted as reflecting the unique contribution^[However, parameter estimates only reflect unique variance when ordinary, weighted, or generalized least squares is used to find the model parameters. Other (regularized) linear models, such as ridge regression or LASSO, are not guaranteed to yield parameters that explain unique proportions of variance.] of each predictor variable, independent from the influence of the confound.

Contrary to general linear models as employed in univariate (activation-based) analyses, including confound variables in the data as predictors for *decoding* models is arguably problematic. If a confound is included in the data in the context of decoding models, the parameter estimates of the features (often called "feature weights", $w$, in decoding models) may be corrected for the influence of the confound, but the *model performance* [usually measured as explained variance, $R^2$, or classification accuracy; @Hebart2017-jn] is not. That is, rather than providing an estimate of decoding performance "controlled for" a confound, one obtains a measure of performance when explicitly *including* the confound as an interesting source of variance that the decoder is allowed to use. This is problematic because research using decoding analyses generally does not focus on parameter estimates but on statistics of model performance. Model performance statistics (e.g., $R^2$, classification accuracy) alone cannot disentangle the contribution of different sources of information as they only represent a single summary statistic of model fit [@Ritchie2017-gl]. One might, then, argue that additionally inspecting feature weights of decoding models may help in disambiguating different sources of information [@Sepehrband2018-dy]. However, it has been shown that feature weights cannot be reliably mapped to specific sources of information, i.e., as being task-related or confound-related [e.g., features with large weights may be completely uncorrelated with the target variable; @Haufe2014-el; @Hebart2017-jn]. As such, it does not make sense to include confounds in the set of predictors when the goal is to disambiguate the different sources of information in decoding analyses.

Recently, another approach similar to including confounds in the data has been proposed, which is based on the idea of a dose-response curve [@alizadeh2017decoding]. In this method, instead of adding the confound(s) to the model directly, the relative contribution of true and confounded signal is systematically controlled. The authors show that this approach is able to directly quantify the unique contribution of each source of information, thus effectively controlling for confounded signal. However, while sophisticated in its approach, this method only seems to work for categorical confounds, as it is difficult (if not impossible) to systematically vary the proportion of confound-related information when dealing with continuous confounds or when dealing with more than one confound.

#### Control for confounds during pattern estimation {#confounds-decoding-introduction-methods-pattern-estimation}

Another method that was used in some decoding studies on functional MRI data aims to control for confounds in the initial procedure of estimating activity patterns of the to-be-decoded events, by leveraging the ability of the GLM to yield parameter estimates reflecting unique variance [@Woolgar2014-jb]. In this method, an initial "first-level" (univariate) analysis models the fMRI time series ($s$) as a function of both predictors-of-interest ($X$) and the confounds ($C$), often using the GLM^[Note that $X$ and $C$, here, refer to (usually HRF-convolved) predictors of the time series signal ($s$) for a single voxel. In the rest of the article, $X$ and $C$ refer to features that are defined across samples (not time).]:

\begin{equation}
s = X\beta_{x} + C\beta_{c} + \epsilon
\end{equation}

Then, only the estimated parameters ($\hat{\beta}$, or normalized parameters, such as *t*-values or *z*-values) corresponding to the predictors-of-interest ($\hat{\beta}_{x}$) are used as activity estimates (i.e., the  used for predicting the target $y$) in the subsequent decoding analyses. This method thus takes advantage of the shared variance partitioning in the pattern estimation step to control for potential confounding variables. However, while elegant in principle, this method is not applicable in between-subject decoding studies [e.g., clinical decoding studies; @Van_Waarde2014-sh; @Cuingnet2011-hv], in which confounding variables are defined across subjects, or in electrophysiology studies, in which activity patterns do not have to be^[Note that, technically, one could use the "Control for confounds during pattern estimation" method in electrophysiology as well, by first fitting a univariate model explaining the neuroimaging data ($X_{j}$ for $j = 1 \dots K$) as a function of both the target ($y$) and the confound ($C$) and subsequently only using the parameter estimates of the target-predictor ($\hat{\beta}_{x}$) as patterns in the subsequent decoding analysis.] estimated in a first-level model, thus limiting the applicability of this method.

#### Post hoc counterbalancing of confounds {#confounds-decoding-introduction-methods-posthoc-counterbalancing}

When a priori counterbalancing is not possible, some have argued that post hoc counterbalancing might control for the influence of confounds [@Rao2017-bw, p. 24, 38]. In this method, given that there is some sample correlation between the target and confound ($r_{Cy} \neq 0$) in the entire dataset, one takes a subset of samples in which there is no empirical relation between the confound and the target (e.g., when $r_{Cy} \approx 0$). In other words, post hoc counterbalancing is a way to *decorrelate* the confound and the target by subsampling the data. Then, subsequent decoding analysis on the subsampled data can only capitalize on true signal, as there is no confounded signal anymore (see Figure \@ref(fig:fig-confounds-decoding-2)). While intuitive in principle, we are not aware of whether this method has been evaluated before and whether it yields unbiased performance estimates.

```{r fig-confounds-decoding-2, fig.cap='(ref:caption-fig-confounds-decoding-2)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_2.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-2) A schematic visualization how the main two confound control methods evaluated in this article deal with the "confounded signal", making sure decoding models only capitalize on the "true signal".

#### Confound regression {#confounds-decoding-introduction-methods-confound-regression}

The last and perhaps most common method to control for confounds is removing the variance that can be explained by the confound (i.e., the confounded signal) from the neuroimaging data directly [@Abdulkadir2014-bh; @Dukart2011-aq; @Kostro2014-cm; @Rao2017-bw; @Todd2013-sd] --- a process we refer to as *confound regression* [also known as "image correction"; @Rao2017-bw]. In this method, a (usually linear) regression model is fitted on each feature in the neuroimaging data (i.e., a single voxel or sensor) with the confound(s) as predictor(s). Thus, each feature  in the neuroimaging data $X$ is modelled as a linear function of the confounding variable(s), $C$:

\begin{equation}
X_{j} = C\beta + \epsilon
\end{equation}

We can estimate the parameter(s)  for feature  using, for example, ordinary least squares as follows [for an example using a different model, see @Abdulkadir2014-bh]:

\begin{equation}
\hat{\beta}_{j} = (C^{T}C)^{-1}C^{T}X_{j}
\end{equation}

Then, to remove the variance of (or "regress out") the confound from the neuroimaging data, we can subtract the variance in the data associated with confound ($C\hat{\beta}_{j}$) from the original data:

\begin{equation}
X_{j,\mathrm{corr}} = X_{j} - C\hat{\beta}_{j}
\end{equation}

In which $X_{j,\mathrm{corr}}$ represents the neuroimaging feature $X_{j}$ from which all variance of the confound is removed (including the variance shared with $y$, i.e., the confounded signal; see Figure \@ref(fig:fig-confounds-decoding-2)). When subsequently applying a decoding analysis on this corrected data, one can be sure that the decoder is not capitalizing on signal that is correlated with the confound, which thus improves interpretability of the decoding analysis.

Confound regression has been applied in several decoding studies. @Todd2013-sd were, as far as the current authors are aware, the first to use this method to control for a confound (in their case, reaction time) that was shown to correlate with their target variable (rule A vs. rule B). Notably, they both regressed out reaction time from the first-level time series data (similar to the "Control for confounds during pattern estimation" method) *and* regressed out reaction time from the trial-by-trial activity estimates (i.e., confound regression as described in this section). They showed that controlling for reaction time in this way completely eliminated the above chance decoding performance. Similarly, @Kostro2014-cm observe a substantial drop in classification accuracy when controlling for scanner site in the decoding analysis of Huntington's disease, but only when scanner site and disease status were actually correlated. Lastly, @Rao2017-bw found that, in contrast to Kostro et al. and Todd et al., confound regression yielded similar (or slightly lower, but still significant) performance compared to the model without confound control, but it should be noted that this study used a regression model (instead of a classification model) and evaluated confound control in the specific situation when the training set is confounded, but the test set is not.^[Note that we did not discuss studies that implement a different confound regression procedure [e.g., @Abdulkadir2014-bh; @Dukart2011-aq], in which confound regression is only estimated on the samples from a single class of the target variable (e.g., in our gender decoding example, this would mean that confound regression models are only estimated on the data from male, or female, subjects). As this form of confound regression does not disambiguate the sources of information driving the decoder, it is not discussed further in this article.] In sum, while confound regression has been used before, it has yielded variable results, possibly due to slightly different approaches and differences in the correlation between the confounding variable and the target.

### Current study {#confounds-decoding-introduction-current-study}

In summary, multiple methods have been proposed to deal with confounds in decoding analyses. Often, these methods have specific assumptions about the nature or format of the data (such as "A priori counterbalancing" and "Confound control during pattern estimation"), differ in their objective (e.g., *prediction* vs. *interpretation*, such as in "Include confounds in the data"), or have yielded variable results (such as "Confound regression"). Therefore, given that we are specifically interested in interpreting decoding analyses, the current study evaluates the two methods that are applicable in most contexts: post hoc counterbalancing and confound regression (but see [Supplementary Materials](#confounds-decoding-supplement) for a tentative evaluation of this method based on simulated functional MRI data). In addition to these two methods, we propose a third method --- a modified version of confound regression —-- which we show yields plausible, seemingly unbiased, and interpretable results.

To test whether these methods are able to effectively control for confounds and whether they yield plausible results, we apply them to empirical data, as well as to simulated data in which the ground truth with respect to the signal in the data (i.e., the proportion of true signal and confounded signal) is known. For our empirical data, we enact the previously mentioned hypothetical study in which participant gender is decoded from structural MRI data. We use a large dataset ($N = 217$) of structural MRI data and try to predict subjects' gender (male/female) from gray and white matter patterns while controlling for the confound of "brain size" using the aforementioned methods, which we compare to a baseline model in which confounds are not controlled for. Given the previously reported high correlations between brain size and gender [@Barnes2010-pu; @Smith2018-th], we expect that successfully controlling for brain size yields lower decoding performance than using uncorrected data, but not below chance level. Note that higher decoding performance after controlling for confounds is theoretically possible when the correlation between the confound and variance in the data *unrelated* to the target (e.g., noise) is sufficiently high to cause suppressor effects [see Figure 1 in @Haufe2014-el; @Hebart2017-jn]. However, because our confound, brain size, is known to correlate strongly with our target gender [approx. $r = 0.63$; @Smith2018-th], it is improbable that it also correlates highly with variance in brain data that is unrelated to gender. It follows then that classical suppression effects are unlikely and we thus expect lower model performance after controlling for brain size.

However, shown in detail below, both post hoc counterbalancing and confound regression lead to unexpected results in our empirical analyses: counterbalancing fails to reduce model performance while confound regression consistently yields low model performance up to the point of significant below chance accuracy. In subsequent analyses of simulated data, we show that both methods lead to *biased* results: post hoc counterbalancing yields inflated model performance (i.e., positive bias) because subsampling selectively selects a subset of samples in which features correlate more strongly with the target variable, suggesting (indirect) circularity in the analysis [@Kriegeskorte2009-yz]. Furthermore, our simulations show that negative bias (including significant below chance classification) after confound regression on the entire dataset is due to reducing the signal below what is expected by chance [@jamalabadi2016classification], which we show is related to and can be predicted by the standard deviation of the empirical distribution of correlations between the features in the data and the target. We propose a minor but crucial addition to the confound regression procedure, in which we cross-validate the confound regression models (which we call "cross-validated confound regression", CVCR), which solves the below chance accuracy issue and yields plausible model performance in both our empirical and simulated data.

## Methods {#confounds-decoding-methods}

### Data {#confounds-decoding-methods-data}

For the empirical analyses, we used voxel-based morphometry (VBM) data based on T1-weighted scans and tract-based spatial statistics (TBSS) data based on diffusion tensor images from 217 participants (122 women, 95 men), acquired with a Philips Achieva 3T MRI-scanner and a 32-channel head coil at the Spinoza Centre for Neuroimaging (Amsterdam, The Netherlands).

#### VBM acquisition & analysis {#confounds-decoding-methods-data-vbm}

The T1-weighted scans with a voxel size of 1.0 × 1.0 × 1.0 mm were acquired using 3D fast field echo (TR: 8.1 ms, TE: 3.7 ms, flip angle: 8°, FOV: 240 × 188 mm, 220 slices). We used "FSL-VBM" protocol [@Douaud2007-sw] from the FSL software package [version 5.0.9; @Smith2004-sc]; using default and recommended parameters (including non-linear registration to standard space). The resulting VBM-maps were spatially smoothed using a Gaussian kernel (3 mm FWHM). Subsequently, we organized the data in the standard pattern-analysis format of a 2D ($N \times K$) array of shape 217 (subjects) × 412473 (non-zero voxels).

#### TBSS acquisition & analysis {#confounds-decoding-methods-data-tbss}

Diffusion tensor images with a voxel size of 2.0 × 2.0 × 2.0 mm were acquired using a spin-echo echo-planar imaging (SE-EPI) protocol (TR: 7476 ms, TE: 86 ms, flip angle: 90°, FOV: 224 × 224 mm, 60 slices), which acquired a single b = 0 (non-diffusion-weighted) image and 32 (diffusion-weighted) b = 1000 images. All volumes were corrected for eddy-currents and motion (using the FSL command "eddy_correct") and the non-diffusion-weighted image was skullstripped (using FSL-BET with the fractional intensity threshold set to 0.3) to create a mask that was subsequently used in the fractional anisotropy (FA) estimation. The FA-images resulting from the diffusion tensor fitting procedure were subsequently processed by FSL's tract-based spatial statistics (TBSS) pipeline [@Smith2006-sf], using the recommended parameters (i.e., non-linear registration to FSL's 1 mm FA image, construction of mean FA-image and skeletonized mean FA-image based on the data from all subjects, and a threshold of 0.2 for the skeletonized FA-mask). Subsequently, we organized the resulting skeletonized FA-maps into a 2D ($N \times K$) array of shape 217 (subjects) × 128340 (non-zero voxels).

#### Brain size estimation {#confounds-decoding-methods-data-brainsize}

To estimate the values for our confound, global brain size, we calculated for each subject the total number of non-zero voxels in the gray matter and white matter map resulting from the segmentation step in the FSL-VBM pipeline [using FSL's segmentation algorithm "FAST"; @Zhang2001-wa]. The number of non-zero voxels from the gray matter map was used as the confound for the VBM-based analyses and the number of non-zero voxels from the white matter map was used as the confound for the TBSS-based analyses. Note that brain size estimates from total white matter volume and total gray matter volume correlated strongly, $r (216) = 0.93$, $p < 0.001$.

#### Data and code availability {#confounds-decoding-methods-data-data-and-code}

In the Github repository corresponding to this article ([https://github.com/lukassnoek/MVCA](https://github.com/lukassnoek/MVCA)), we included a script (`download_data.py`) to download the data (the 4D VBM and TBSS nifti-images as well as the non-zero 2D samples × features arrays). The repository also contains detailed Jupyter notebooks with the annotated empirical analyses and simulations reported in this article.

### Decoding pipeline {#confounds-decoding-methods-pipeline}

All empirical analyses and simulations used a common decoding pipeline, implemented using functionality from the *scikit-learn* Python package for machine learning [@Abraham2014-ef; @Pedregosa2011-bp]. This pipeline included univariate feature selection (based on a prespecified amount of voxels with highest univariate difference in terms of the ANOVA *F*-statistic), feature-scaling (ensuring zero mean and unit standard deviation for each feature), and a support vector classifier (SVC) with a linear kernel, fixed regularization parameter (*C* = 1), and sample weights set to be inversely proportional to class frequency (to account for class imbalance). In our empirical analyses, we evaluated model performance for different numbers of voxels (as selected by the univariate feature selection). For our empirical analyses, we report model performance as the $F_{1}$ score, which is insensitive to class imbalance (which, in addition to adjusted sample weights, prevents the classifier from learning the relative probabilities of target classes instead of representative information in the features; see also Supplementary Figure \@ref(fig:fig-shared-states-S14) for a replication of part of the results using AUROC, another metric that is insensitive to class imbalance). At chance level classification, the $F_{1}$ score is expected to be 0.5. For our simulations, in which there is no class imbalance, we report model performance using accuracy scores. In figures showing error bars around the average model performance scores, the error bars represent 95% confidence intervals estimated using the "bias-corrected and accelerated" (BCA) bootstrap method using 10,000 bootstrap replications [@efron1987better]. For calculating BCA bootstrap confidence intervals, we used the implementation from the open source "scikits.bootstrap" Python package ([https://github.com/cgevans/scikits-bootstrap](https://github.com/cgevans/scikits-bootstrap)). Statistical significance was calculated using non-parametric permutation tests, as implemented in scikit-learn, with 1000 permutations [@Ojala2010-rc].

### Evaluated methods for confound control {#confounds-decoding-methods-evaluated-methods}

#### Post hoc counterbalancing {#confounds-decoding-methods-evaluated-methods-counterbalancing}

We implemented post hoc counterbalancing in two steps. First, to quantify the strength of relation between the confound and the target in our dataset, we estimated the point-biserial correlation coefficient between the confound, $C$ (brain size), and the target, $y$ (gender) across the entire dataset (including all samples $i = 1 \dots N$). Because of both sampling noise and measurement noise, sample correlation coefficients vary around the population correlation coefficient and are thus improbable to be 0 *exactly*.^[For continuous confounds, it is practically impossible to achieve a correlation with the target of *exactly* zero, which is the reason we subsample until it is smaller than a prespecified threshold. For categorical confounds, however, a correlation between the confound and the target of exactly zero is possible [this amounts to equal proportions of levels of $c$ within each class of $y$; @Gorgen2017-sy], even *necessary*, because it is impossible to find a (*K*-fold) cross-validation partitioning in which each split is counterbalanced w.r.t. the confound if the correlation *in the entire dataset* between the target and the confound is not zero.] Therefore, in the next step, we subsampled the data until the correlation coefficient between  and  becomes non-significant at some significance threshold $\alpha$: $p(r_{Cy}) > \alpha$.

In our analyses, we used an $\alpha$ of 0.1. Note that this is more "strict"^[We refer to a relatively high α as “strict”, here, because we use it here for the purpose of demonstrating no effect.] than the conventionally used threshold ($\alpha = 0.05$), but given that decoding analyses are often more sensitive to signal in the data (whether it is confounded or true signal), we chose to err on the safe side and counterbalance the data using a relatively strict threshold of $\alpha = 0.1$.

Subsampling was done by iteratively removing samples that contribute most to the correlation between the confound and the target until the correlation becomes non-significant. In our empirical data in which brain size is positively correlated with gender (coded as male = 1, female = 0) this amounted to iteratively removing the male subject with the largest brain size and the female subject with the smallest brain size. This procedure optimally balances (1) minimizing the correlation between target and confound and (2) maximizing sample size. As an alternative to this "targeted subsampling", we additionally implemented a procedure which draws random subsamples of a given sample size until it finds a subsample with a non-significant correlation coefficient. If such a subsample cannot be found after 10,000 random draws, sample size is decreased by 1, which is repeated until a subsample is found. This procedure resulted in much smaller subsamples than the targeted subsampling procedure (i.e., a larger power loss) since the optimal subsample is hard to find randomly.^[One could run the "random subsampling" procedure with more than 10,000 draws in order to reduce the aforementioned power loss; but in the extreme, this would result in the same optimal subsample that can be found much faster by targeted subsampling.] In the analyses below, therefore, we used the targeted subsampling procedure. Importantly, even with extreme power loss, random subsampling can cause the same biases as will be described for the targeted subsampling method below (cf. Figure \@ref(fig:fig-confounds-decoding-8) and Figure \@ref(fig:fig-confounds-decoding-10) and Supplementary Figures \@ref(fig:fig-confounds-decoding-S13) and \@ref(fig:fig-confounds-decoding-S14)).

Then, given that the subsampled dataset is counterbalanced with respect to the confound, a random stratified K-fold cross-validation scheme is repeatedly initialized until a scheme is found in which *all* splits are counterbalanced as well [cf. @Gorgen2017-sy]. This particular counterbalanced cross-validation scheme is subsequently used to cross-validate the MVPA pipeline. We implemented this post hoc counterbalancing method as a scikit-learn-style cross-validator class, available from the aforementioned Github repository (in the `counterbalance.py` module).

#### Confound regression

In our empirical analyses and simulations, we tested two different versions of confound regression, which we call "whole-dataset confound regression" (WDCR) and "cross-validated confound regression" (CVCR). In WDCR, we regressed out the confounds from the predictors *from the entire dataset at once*, i.e., before entering the iterative cross-validated MVPA pipeline [the approach taken by @Abdulkadir2014-bh; @Dubois2017-fl; @Kostro2014-cm; @Todd2013-sd]. Note that we can do this for all $K$ voxels at once using the closed-form OLS solution, in which we first estimated the parameters $\hat{\beta}_{C}$:

\begin{equation}
\hat{\beta}_{C} = (C^{T}C)^{-1}C^{T}X
\end{equation}

where $C$ is an  array in which the first column contained an intercept and the second column contained the confound brain size. Accordingly, $\hat{\beta}_{C}$ is an $2 \times K$ array. We then removed the variance associated with the confound from our neuroimaging data as follows:

\begin{equation}
X_{\mathrm{corr}} = X - C\hat{\beta}_{C}
\end{equation}

Now, $X_{\mathrm{corr}}$ is an array with the same shape as the original $X$ array, but without any variance that can be explained by confound, $C$ (i.e., $X$ is residualized with regard to $C$).

In our proposed cross-validated version of confound regression [which was mentioned but not evaluated by @Rao2017-bw, p. 25], "CVCR", we similarly regressed out the confounds from the neuroimaging data, but instead of estimating $\hat{\beta}_{C}$ on the entire dataset, we estimated this within each fold of training data ($X_{\mathrm{train}}$):

\begin{equation}
\hat{\beta}_{C,\mathrm{train}} = (C^{T}_{\mathrm{train}}C_{\mathrm{train}})^{-1}C^{T}_{\mathrm{train}}X_{\mathrm{train}}
\end{equation}

And we subsequently used these parameters ($\hat{\beta}_{C,\mathrm{train}}$) to remove the variance related to the confound from both the train set ($X_{\mathrm{train}}$ and $C_{\mathrm{train}}$):

\begin{equation}
X_{\mathrm{train, corr}} = X_{\mathrm{train}} - C_{\mathrm{train}}\hat{\beta}_{C,\mathrm{train}}
(\#eq:cvcr-train)
\end{equation}

and the test test ($X_{\mathrm{test}}$ and $C_{\mathrm{test}}$):

\begin{equation}
X_{\mathrm{test, corr}} = X_{\mathrm{test}} - C_{\mathrm{test}}\hat{\beta}_{C,\mathrm{test}}
(\#eq:cvcr-test)
\end{equation}

Thus, essentially, CVCR is the cross-validated version of WDCR. One might argue that regressing the confound from the train set only, i.e., implementing only equation \@ref(eq:cvcr-train), not equation \@ref(eq:cvcr-test), is sufficient to control for confounds as it prevents the decoding model from relying on signal related to the confound. We evaluated this method and report the corresponding results in Supplementary Figure \@ref(fig:fig-confounds-decoding-S10).

We implemented these confound regression techniques as a *scikit-learn* compatible transformer class, available in the open-source *skbold* Python package ([https://github.com/lukassnoek/skbold](https://github.com/lukassnoek/skbold)) and in the aforementioned Github repository.

#### Control for confounds during pattern estimation

In addition to post hoc counterbalancing and confound regression, we also evaluated how well the "control for confounds during pattern estimation" method controls for the influence of confounds in decoding analyses of (simulated) fMRI data. The simulation methods and results can be found in the [Supplementary Materials](#confounds-decoding-supplement).

### Analyses of simulated data

In addition to the empirical evaluation of counterbalancing and confound regression in the gender decoding example, we ran three additional analyses on simulated data. First, we investigated the efficacy of the three confound control methods on synthetic data with known quantities of "true signal" and "confounded signal", in order to detect potential biases. Second, we ran additional analyses on simulated data to investigate the positive bias in model performance observed after post hoc counterbalancing. Third, we ran additional analyses on simulated data to investigate the negative bias in model performance observed after WDCR. In the [Supplementary Materials](#confounds-decoding-supplement), we investigate whether the confound regression results generalize to (simulated) functional MRI data (Supplementary Figure \@ref(fig:fig-confounds-decoding-S1) and \@ref(fig:fig-confounds-decoding-S2)).

#### Efficacy analyses

In this simulation, we evaluated the efficacy of the three methods for confound control on synthetic data with a prespecified correlation between the confound and the target, $r_{Cy}$, and varying amounts of "confounded signal" (i.e., the explained variance in $y$ driven by shared variance between $X$ and $y$). These simulations allowed us to have full control over (and knowledge of) the influence of both signal and confound in the data, and thereby help us diagnose biases associated with post hoc counterbalancing and confound regression.

Specifically, in this efficacy analysis, we generated hypothetical data sets holding the correlation coefficient between $C$ and $y$ constant, while varying the amount of true signal and confounded signal. We operationalized true signal as the squared semipartial Pearson correlation between $y$ and each feature in $X$, controlled for $C$. As such, we will refer to this term as $\mathrm{signal}\ R^{2}$:

\begin{equation}
\mathrm{signal}\ R^{2} = r_{y(X.C)}^{2}
\end{equation}

In the simulations reported and shown in the main article, we used $r_{Cy} = 0.65$, which corresponds to the observed correlation between brain size and gender in our dataset. To generate synthetic data with this prespecified structure, we generated (1) a data matrix $X$ of shape $N\times K$, (2) a target variable $y$ of shape $N \times 1$, and (3) a confound variable $C$ of shape $N \times P$. For all simulations, we used the following parameters: $N = 200$, $K = 5$, and $P = 1$  (i.e., a single confound variable). We generated $y$ as a categorical variable with binary values, $y \in \{0, 1\}$, with equal class probabilities (i.e., 50%), given that most decoding studies focus on binary classification. We generated $C$ as a continuous random variable drawn from a standard normal distribution. We generated each feature $X_{j}$ as a linear combination of $y$ and $C$ plus Gaussian noise. Thus, for each predictor $j = 1 \dots K$ in $X_{j}$:

\begin{equation}
X_{j} = \beta_{y}y + \beta_{C}C + \epsilon, \epsilon \sim \mathcal{N}(0, \gamma)
\end{equation}

in which $\beta_{y}$ represented the weight given to $y$, and $\beta_{C}$ represented the weight given to $C$ in the generation of the feature $X_{j}$, and $\mathcal{N}(0, \gamma)$ is the normal distribution with zero mean and standard deviation $\gamma$. The parameters $\beta_{y}$ and $\beta_{C}$ were both initialized with a value of 1. First, if the difference between the total variance explained and the sum of the desired signal $R^2$ and confound $R^2$ values was larger than 0.01, the standard deviation of the normal distribution from which the errors were drawn (i.e., $\gamma$) was adjusted (decreased with 0.01 when the total $R^2$ is too low, increased with 0.01 when the total $R^2$ is too high), after which  was generated again. This process was iterated until the target total $R^2$ value is found. Then, the total variance explained was partitioned into confound $R^2$ and signal $R^2$. If one or both of these values differed from the targeted values by more than 0.01, the generative parameters $\beta_{y}$ and $\beta_{C}$ were adjusted: if signal $R^2$ is too low,  was increased with 0.01, and decreased with 0.01 otherwise. If confound $R^2$ is too low, $\beta_{C}$ was increased with 0.01, and decreased with 0.01 otherwise. After adjusting these parameters, $X_{j}$ was generated again. This process was iterated until the data contain the desired "true signal" and "confounded signal".

We evaluated the different methods for confound control for two values of signal $R^2$ (0.004, representing plausible null data,^[Note that plausible null data do not reflect a signal $R^2$ of 0, because this statistic is biased towards values larger than 0 (because it represents a squared number) when dealing with noisy data, hence our choice of signal $R^2 = 0.004$.] and 0.1, representing a plausible true effect) and a range of confound $R^2$ values (in steps of 0.05: $0.00, 0.05, 0.10, \dots , 0.35$). This simulation was iterated 10 times (with different partitions of the folds) to ensure the results were not influenced by random noise. Importantly, the specific scenario in which confound $R^2$ equals 0, which represents data without any confounded signal ($r_{yX}^2$), served as “reference model performance” to which we can compare the efficacy the confound control methods. This comparison allowed us to detect potential biases.

After the data were generated, a baseline model (no confound control) and the three methods outlined above (post hoc counterbalancing, WDCR, and CVCR) were applied to the simulated data using the standard pipeline described in the [Decoding pipeline](#confounds-decoding-methods-pipeline) section (but without univariate feature selection) and compared to the reference performance.

#### Analysis of positive bias after post hoc counterbalancing

As detailed below, post hoc counterbalancing did not lead to the expected decrease in model performance; instead, there appeared to be a trend towards an *increase* in model performance. To further investigate the cause of this unexpected result, we simulated a multivariate normal dataset with three variables, reflecting our data ($X$), target ($y$), and confound ($C$), with 1000 samples ($N$) and a single feature ($K = 1$). We iterated this data generation process 1000 times and subsequently selected the dataset which yielded the largest (positive) difference between model performance after post hoc counterbalancing versus no confound control. In other words, we used the dataset in which the counterbalancing issue was most apparent. While not necessarily representative of typical (neuroimaging) datasets, this process allowed us to explain and visualize how it is possible that model performance increases after counterbalancing the data.

To generate data from a multivariate normal distribution, we first generated variance-covariance matrices with unit variance for all variables, so that covariances can be interpreted as correlations. The covariances in the matrix were generated as pairwise correlations ($r_{yX}$, $r_{Cy}$, ${r_CX}$), each sampled from a uniform distribution with range $[-0.65, 0.65]$. We generated data using such prespecified correlation structure because the relative increase in model performance after counterbalancing did not appear to occur when generating completely random (normally distributed) data. Moreover, we restricted the range of the uniform distribution from which the pairwise correlations are drawn to $[-0.65, 0.65]$ because a larger range can result in covariance matrices that are not positive-semidefinite. After generating the three variables, we binarized the target variable ($y$) using a mean-split ($y = 0$ if $y < \bar{y}$, $y = 1$ otherwise) to frame the analysis as a classification problem rather than a regression problem.

We then subsampled the selected dataset using our post hoc counterbalancing algorithm and subsequently ran the decoding pipeline (without univariate feature selection) on the subsampled ("retained") data in a 10-fold stratified cross-validation scheme. Notably, we cross-validated our fitted pipeline not only to the left-out *retained* data, but also to the data that did not survive the subsampling procedure (the *rejected* data; see Figure \@ref(fig:fig-confounds-decoding-3)). Across the 10 folds, we kept track of two statistics from the retained and rejected samples: (1) the classification performance, and (2) the signed distance to the decision boundary. Negative distances in binary classification (in simple binary classification with $y \in \{0, 1\}$) reflect a prediction of the sample as $y = 0$, while positive distances reflect a prediction of the sample as $y = 1$. As such, a correctly classified sample of class 0 has a negative distance from the decision boundary, while a correctly classified sample of class 1 has a positive distance from the decision boundary. Here, however, we wanted to count the distance of samples that are on the "incorrect" side of the decision boundary as *negative* distances, while counting the distance of samples that are on the "correct" side of the decision boundary as positive distances. To this end, we used a "re-coded" version of the target variable ($y^{*} = -1$ if $y = 0$, $y^{*} = 1$ otherwise) and multiplied it with the distance. Consequently, negative distances of *correct* samples of condition 0 become positive and positive distances of *incorrect* samples of condition 0 become negative (by multiplying them by $-1$). As such, we calculated the signed distance from the decision boundary ($\delta_{i}$) for any sample $i$ as:

\begin{equation}
\delta_{i} = y^{*}(w^{T}X_{i} + b)
\end{equation}

in which $w$ refers to the feature weights (coefficients) and $b$ refers to the intercept term. Any differences in these two statistics (proportion correctly classified and signed distance to the classification boundary) between the retained and rejected samples may signify biases in model performance estimates (i.e., better cross-validated model performance on the retained data than on the rejected data would confirm positive bias, as it indicates that subsampling tends to reject hard-to-classify samples). We applied this analysis also to the empirical data (separately for the different values of $K$) to show that the effect of counterbalancing, as demonstrated using simulated data, also occurs in the empirical data.

```{r fig-confounds-decoding-3, fig.cap='(ref:caption-fig-confounds-decoding-3)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_3.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-3) Visualization of method to evaluate whether counterbalancing yields unbiased cross-validated model performance estimates.

#### Analysis of negative bias after WDCR

As also detailed below, WDCR can lead to significantly below chance accuracy. To investigate the cause of this below chance performance (and to demonstrate that CVCR does not lead to such results), we performed two follow-up simulations. The first follow-up simulation shows that the occurrence of below chance accuracy depends on the distribution of feature-target correlations [$r_{yX}$; see for a similar argument @jamalabadi2016classification], and the second follow-up simulation shows that WDCR artificially narrows this distribution. This artificial narrowing of the distribution is exacerbated both by an increasing number of features ($K$), as well as higher correlations between the target and confound ($r_{Cy}$).

In the first simulation, we simulated random null data (drawn from a standard normal distribution) with 100 samples ($N$) and 200 features ($K$), as well as a binary target feature ($y \in \{0, 1\}$). We then calculated the cross-validated prediction accuracy using the standard pipeline (without univariate feature selection) described in the [Decoding pipeline](#confounds-decoding-methods-pipeline) section; we iterate this process 500 times. Then, we show that the variance of the cross-validated accuracy is accurately predicted by the standard deviation (i.e., "width") of the distribution of correlations between the features and the target ($r_{yX_{j}}$ with $j = 1 \dots K$), which we will denote by $sd(r_{yX})$. Importantly, we show that below chance accuracy likely occurs when the standard deviation of the feature-target correlation distribution is lower than the standard deviation of the sampling distribution of the Pearson correlation coefficient parameterized with the same number of samples ($N = 200$) and the same effect size (i.e., $\rho = 0$, because we simulated random null data). The sampling distribution of the Pearson correlation coefficient is described by @kendall1973functional. When $\rho = 0$ (as in our simulations), the equation is as follows:

\begin{equation}
f(r; N) = (1 -r^2)^{\frac{N-4}{2}}[\mathcal{B}](\frac{1}{2}, \frac{N-2}{2})^{-1}
\end{equation}

where $\mathcal{B}(a, b)$ represents the Beta-function.

Then, in a second simulation, we similarly simulated null data as in the previous simulation, but now we also generate a continuous confound ($C$) with a varying correlation with the target ($r_{Cy} \in \{0.0, 0.1, 0.2, \dots, 1.0\}$). Before subjecting the data to the decoding pipeline, we regressed out the confound from the data (i.e., WDCR). We did this for different numbers of features ($K \in \{1, 5, 10, 50, 100, 500, 1000\}$). Then, we applied CVCR on the simulated data as well for comparison.

## Results

### Influence of brain size

Before evaluating the different methods for confound control, we determined whether brain size is truly a confound given our proposed definition ("a variable that is not of primary interest, correlates with the target, and is encoded in the neuroimaging data"). We evaluated the relationship between the target and the confound in two ways. First, we calculated the (point-biserial) correlation between gender and brain size, which was significant for both the estimation based on white matter, $r(216) = .645, p < 0.001$, and the estimation based on grey matter, $r(216) = .588, p < 0.001$, corroborating the findings by @Smith2018-th. Second, as recommended by @Gorgen2017-sy, who argue that the potential influence of confounds can be discovered by running a classification analysis using the confound as the (single) feature predicting the target, we ran our decoding pipeline (without univariate feature selection) using brain size as a single feature to predict gender. This analysis yielded a mean classification performance ($F_{1}$ score) of 0.78 (*SD* = .10) when using brain size estimated from white matter and 0.81 (*SD* = .09) when using brain size estimated from gray matter, which are both significant with $p < 0.001$ (see Figure \@ref(fig:fig-confounds-decoding-4)A).

```{r fig-confounds-decoding-4, fig.cap='(ref:caption-fig-confounds-decoding-4)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_4.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-4) A) Model performance when using brain size to predict gender for both brain-size estimated from grey matter (left) and from white matter (right). Points in yellow depict individual $F_{1}$ scores per fold in the 10-fold cross-validation scheme. Whiskers of the box plot are 1.5x the interquartile range. B) Distributions of observed correlations between brain size and voxels ($r_{XC}$), overlayed with the analytic sampling distribution of correlation coefficients when $\rho = 0$ and $N = 217$, for both the VBM data (left) and TBSS data (right). Density estimates are obtained by kernel density estimation with a Gaussian kernel and Scott's rule [@scott1979optimal] for bandwidth selection.

To estimate whether brain size is encoded in the neuroimaging data, we compared the distribution of bivariate correlation coefficients (of each voxel with brain size) with the sampling distribution of correlation coefficients when $\rho = 0$ and $N = 217$ (see section [Analysis of negative bias after WDCR](#confounds-decoding-results-wdcr-bias) for details). Under the null hypothesis that there are no correlations between brain size and voxel intensities, each individual correlation coefficient between a voxel and the confound can be regarded as an independent sample with $N = 217$ (ignoring correlations between voxels for simplicity). Because $K$ is very large for both the VBM and TBSS data, the empirical distribution of correlation coefficients should, under the null hypothesis, approach the analytic distribution of correlation coefficients parametrized by $\rho = 0$ and $N = 217$. Contrarily, the density plots in Fig. \@ref(fig:fig-confounds-decoding-4)B clearly show that the observed correlation coefficients distribution does not follow the sampling distribution (with both an increase in variance and a shift of the mode). This indicates that at least some of the correlation coefficients between voxel intensities and brain size are extremely unlikely under the null hypothesis. Note that this interpretation is contingent on the assumption that the relation between brain size and VBM/TBSS data is linear. In the Supplementary Materials and Results (Supplementary Figures \@ref(fig:fig-confounds-decoding-S7)-\@ref(fig:fig-confounds-decoding-S9)), we provide some evidence for the validity of this assumption.

### Baseline model: no confound control

In our baseline model on the empirical data, for different numbers of voxels, we predicted gender from structural MRI data (VBM and TBSS) without controlling for brain size (see Figure \@ref(fig:fig-confounds-decoding-5)). The results show significant above chance performance of the MVPA pipeline based on both the VBM data and the TBSS data. All performance scores averaged across folds were significant ($p < 0.001$).

```{r fig-confounds-decoding-5, fig.cap='(ref:caption-fig-confounds-decoding-5)', results='show'}
knitr::include_graphics("_bookdown_files/confounds-decoding-files/figures/figure_5.png", auto_pdf = TRUE)
```

(ref:caption-fig-confounds-decoding-5) Baseline scores using the VBM (left) and TBSS (right) data without any confound control. Scores reflect the average $F_{1}$ score across 10 folds; error bars reflect 95% confidence intervals. The dashed black line reflect theoretical chance-level performance and the dashed orange line reflects the average model performance when only brain size is used as a predictor for reference; Asterisks indicates significant performance above chance: *** = $p < 0.001$, ** = $p < 0.01$, * = $p < 0.05$.

These above chance performance estimates replicate previous studies on gender decoding using structural MRI data [@Del_Giudice2016-ns; @Rosenblatt2016-oy; @Sepehrband2018-dy] and will serve as a baseline estimate of model performance to which the confound control methods will be compared.

In the next three subsections, we will report the results from the three discussed methods to control for confounds: post hoc counterbalancing, whole-dataset confound regression (WDCR), and cross-validated confound regression (CVCR).

### Post hoc counterbalancing




#### Empirical results

#### Efficacy analysis

#### Analysis of positive bias after post hoc counterbalancing

### Whole-dataset confound regression (WDCR)

#### Empirical results

#### Efficacy analysis

#### Analysis of negative bias after WDCR {#confounds-decoding-results-wdcr-bias}

### Cross-validated confound regression (CVCR)

#### Empirical results

#### Efficacy analysis

### Summary methods for confound control

## Discussion {#confounds-decoding-discussion}

### Relevance and consequences for previous and future research

#### A priori and post hoc counterbalancing

#### Confound regression

#### Relevance to other analysis methods

#### Importance for gender decoding studies

### Choosing a confound model: linear vs. nonlinear models

### Practical recommendations

## Conclusions











